{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p> Unified Python toolkit for visual document processing </p> <p>   Extract text, detect layouts, and run OCR on documents with a clean, type-safe API. </p>"},{"location":"#quick-install","title":"Quick Install","text":"<pre><code>pip install omnidocs[pytorch]\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n\n# Load a PDF\ndoc = Document.from_pdf(\"document.pdf\")\n\n# Extract text as Markdown\nextractor = QwenTextExtractor(\n    backend=QwenPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\"\n    )\n)\n\nresult = extractor.extract(doc.get_page(0), output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"#features","title":"Features","text":"Feature Description Unified API Single <code>.extract()</code> method for all tasks Type-Safe Pydantic configs with full IDE autocomplete Multi-Backend PyTorch, VLLM, MLX, API - choose what fits Production Ready GPU-accelerated, battle-tested"},{"location":"#what-can-you-do","title":"What Can You Do?","text":"<ul> <li> <p> Text Extraction</p> <p>Convert documents to Markdown or HTML</p> <p> Guide</p> </li> <li> <p> Layout Analysis</p> <p>Detect structure: titles, tables, figures</p> <p> Guide</p> </li> <li> <p> OCR Extraction</p> <p>Get text with bounding box coordinates</p> <p> Guide</p> </li> <li> <p> Batch Processing</p> <p>Process hundreds of documents efficiently</p> <p> Guide</p> </li> </ul>"},{"location":"#choose-your-path","title":"Choose Your Path","text":""},{"location":"#just-get-started-5-min","title":"Just Get Started (5 min)","text":"<ol> <li>Installation - Set up your environment</li> <li>Quick Start - Your first extraction</li> <li>Text Extraction Guide - Learn the basics</li> </ol>"},{"location":"#understand-the-system-30-min","title":"Understand the System (30 min)","text":"<ol> <li>Architecture Overview - How it works</li> <li>Backend System - PyTorch vs VLLM vs MLX</li> <li>Config Pattern - Configuration design</li> </ol>"},{"location":"#deploy-to-production-1-hour","title":"Deploy to Production (1 hour)","text":"<ol> <li>Choosing Backends - Pick the right one</li> <li>Modal Deployment - Cloud GPU setup</li> <li>Batch Processing - Scale efficiently</li> </ol>"},{"location":"#supported-models","title":"Supported Models","text":""},{"location":"#text-extraction","title":"Text Extraction","text":"Model Backends Best For Qwen3-VL PyTorch, VLLM, MLX, API General purpose DotsOCR PyTorch, VLLM Layout-aware extraction"},{"location":"#layout-analysis","title":"Layout Analysis","text":"Model Backends Best For DocLayoutYOLO PyTorch Fast detection Qwen Layout PyTorch, VLLM, MLX, API Custom labels"},{"location":"#ocr","title":"OCR","text":"Model Backends Best For Tesseract CPU Free, offline <p> Full Model Comparison</p>"},{"location":"#installation-options","title":"Installation Options","text":"<pre><code># PyTorch (recommended for most users)\npip install omnidocs[pytorch]\n\n# VLLM (high-throughput production)\npip install omnidocs[vllm]\n\n# Apple Silicon\npip install omnidocs[mlx]\n\n# API-only (no local GPU)\npip install omnidocs[api]\n\n# Everything\npip install omnidocs[all]\n</code></pre>"},{"location":"#faq","title":"FAQ","text":"Text Extraction vs OCR - What's the difference? <p>Text Extraction gives you formatted Markdown/HTML - use when you want document content.</p> <p>OCR gives you text WITH coordinates - use when you need to know WHERE text is located.</p> Which model should I use? <ul> <li>Text Extraction: Start with Qwen, try DotsOCR for layout-aware</li> <li>Layout Detection: DocLayoutYOLO for speed, Qwen for custom labels</li> <li>OCR: Tesseract for free/CPU, PaddleOCR for Asian languages</li> </ul> Which backend should I use? <ul> <li>PyTorch: Development, local GPU (recommended start)</li> <li>VLLM: Production, high throughput</li> <li>MLX: Apple Silicon only</li> <li>API: No GPU needed</li> </ul> What are the hardware requirements? <ul> <li>PyTorch: NVIDIA GPU (CUDA 12+) or CPU</li> <li>VLLM: NVIDIA GPU required</li> <li>MLX: Apple M1/M2/M3+</li> <li>API: No GPU needed</li> </ul>"},{"location":"#performance","title":"Performance","text":"Task Model Speed Memory Text Extraction Qwen3-VL-8B 2-5 sec/page 16GB Layout Detection DocLayoutYOLO 0.5 sec/page 4GB OCR Tesseract 0.2 sec/page 100MB <p>Benchmarks on A10G GPU with 1024x1280 images</p>"},{"location":"#licensing","title":"Licensing","text":"<p>OmniDocs: Apache 2.0</p> <p>Models: Each model has its own license - check the Model Card on Hugging Face before use.</p>"},{"location":"#get-help","title":"Get Help","text":"<ul> <li> Documentation</li> <li> GitHub Issues</li> <li> Contributing</li> </ul> Get Started \u2192"},{"location":"LICENSE/","title":"License","text":"<p>Apache License Version 2.0, January 2004 http://www.apache.org/licenses/</p>"},{"location":"LICENSE/#terms-and-conditions-for-use-reproduction-and-distribution","title":"TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION","text":""},{"location":"LICENSE/#1-definitions","title":"1. Definitions","text":"<p>\"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.</p>"},{"location":"LICENSE/#2-grant-of-copyright-license","title":"2. Grant of Copyright License","text":"<p>Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.</p>"},{"location":"LICENSE/#3-grant-of-patent-license","title":"3. Grant of Patent License","text":"<p>Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.</p>"},{"location":"LICENSE/#4-redistribution","title":"4. Redistribution","text":"<p>You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:</p> <ol> <li>You must give any other recipients of the Work or Derivative Works a copy of this License; and</li> <li>You must cause any modified files to carry prominent notices stating that You changed the files; and</li> <li>You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and</li> <li>If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.</li> </ol> <p>You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.</p>"},{"location":"LICENSE/#5-submission-of-contributions","title":"5. Submission of Contributions","text":"<p>Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.</p>"},{"location":"LICENSE/#6-trademarks","title":"6. Trademarks","text":"<p>This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.</p>"},{"location":"LICENSE/#7-disclaimer-of-warranty","title":"7. Disclaimer of Warranty","text":"<p>Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.</p>"},{"location":"LICENSE/#8-limitation-of-liability","title":"8. Limitation of Liability","text":"<p>In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.</p>"},{"location":"LICENSE/#9-accepting-warranty-or-additional-liability","title":"9. Accepting Warranty or Additional Liability","text":"<p>While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.</p>"},{"location":"LICENSE/#end-of-terms-and-conditions","title":"END OF TERMS AND CONDITIONS","text":"<p>Copyright 2025 OmniDocs Contributors</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"ROADMAP/","title":"OmniDocs Development Roadmap","text":""},{"location":"ROADMAP/#target-model-support","title":"\ud83d\udce6 Target Model Support","text":"<p>Research Date: January 2026 Status: Comprehensive model research completed Models Ordered By: Release date (newest first within each size category)</p>"},{"location":"ROADMAP/#quick-reference-model-capabilities-backend-support","title":"\ud83c\udfaf Quick Reference: Model Capabilities &amp; Backend Support","text":""},{"location":"ROADMAP/#comprehensive-model-comparison-table","title":"Comprehensive Model Comparison Table","text":"Model Size PyTorch VLLM MLX OpenAI API Tasks Release Granite-Docling-258M 258M \u2705 \u26a0\ufe0f \u2705 \u274c T, L, Tab, F Dec 2024 GOT-OCR2.0 700M \u2705 \u274c \u274c \u274c T, O, F, Tab Sep 2024 PaddleOCR-VL 900M \u2705 \u26a0\ufe0f \u274c \u274c T, L, O, Tab, F Oct 2025 LightOnOCR-1B 1B \u2705 \u274c \u274c \u274c T, O Oct 2025 LightOnOCR-2-1B 1B \u2705 \u2705 \u274c \u274c T, O Jan 2025 LightOnOCR-2-1B-bbox 1B \u2705 \u2705 \u274c \u274c T, L, O Jan 2025 MinerU2.5-2509-1.2B 1.2B \u2705 \u2705 \u2705 \u274c T, L, Tab, F, O Sep 2024 dots.ocr 1.7B \u2705 \u2705 \u274c \u274c T, L, Tab, F, O Dec 2024 Qwen3-VL-2B 2B \u2705 \u2705 \u2705 \u2705 T, L, S, O, Tab Oct 2025 DeepSeek-OCR 3B \u2705 \u2705 \u2705 \u2705 T, O, Tab Oct 2024 Qwen2.5-VL-3B 3B \u2705 \u2705 \u2705 \u2705 T, L, S, O 2024 Nanonets-OCR2-3B 3B \u2705 \u2705 \u2705 \u274c T, F, O 2024 Qwen3-VL-4B 4B \u2705 \u2705 \u2705 \u274c T, L, S, O, Tab Oct 2025 Gemma-3-4B-IT 4B \u2705 \u274c \u274c \u2705 T, S, O 2025 olmOCR-2-7B 7B \u2705 \u2705 \u274c \u2705 T, O, Tab, F Oct 2025 Qwen2.5-VL-7B 7B \u2705 \u2705 \u2705 \u2705 T, L, S, O, Tab 2024 Qwen3-VL-8B 8B \u2705 \u2705 \u2705 \u2705 T, L, S, O, Tab, F Oct 2025 Chandra 9B \u2705 \u2705 \u274c \u274c T, L, O, Tab, F 2024 Qwen3-VL-32B 32B \u2705 \u2705 \u2705 \u2705 T, L, S, O, Tab, F Oct 2025 Qwen2.5-VL-32B 32B \u2705 \u2705 \u2705 \u2705 T, L, S, O, Tab 2024 <p>Legend: - Tasks: T=Text Extract, L=Layout, O=OCR, S=Structured, Tab=Table, F=Formula - \u2705 = Fully supported | \u26a0\ufe0f = Limited/Partial support | \u274c = Not supported</p>"},{"location":"ROADMAP/#backend-details","title":"Backend Details","text":""},{"location":"ROADMAP/#pytorch-support","title":"PyTorch Support","text":"<ul> <li>All models support PyTorch via HuggingFace Transformers</li> <li>Primary development backend for all models</li> <li>Requirements: <code>transformers&gt;=4.46</code>, <code>torch&gt;=2.0</code></li> </ul>"},{"location":"ROADMAP/#vllm-support-high-throughput-production","title":"VLLM Support (High-Throughput Production)","text":"<p>Fully Supported (\u2705): - Qwen3-VL Series (vllm&gt;=0.11.0) - Qwen2.5-VL Series - DeepSeek-OCR (official upstream) - dots.ocr (recommended, vllm&gt;=0.9.1) - MinerU2.5 - olmOCR-2 (via olmOCR toolkit) - Chandra - LightOnOCR-2-1B (vllm&gt;=0.11.1) - Nanonets-OCR2-3B</p> <p>Limited Support (\u26a0\ufe0f): - Granite-Docling-258M (untied weights required) - PaddleOCR-VL (possible but not officially confirmed)</p> <p>Not Supported (\u274c): - GOT-OCR2.0 - Gemma-3-4B-IT - LightOnOCR-1B (legacy)</p>"},{"location":"ROADMAP/#mlx-support-apple-silicon-m1m2m3","title":"MLX Support (Apple Silicon M1/M2/M3+)","text":"<p>Fully Supported via mlx-community (\u2705): - Qwen3-VL Series - Collection   - 2B, 4B, 8B, 32B (4-bit, 8-bit variants) - Qwen2.5-VL Series - Collection   - 3B, 7B, 32B, 72B (4-bit, 8-bit variants) - DeepSeek-OCR - 4-bit, 8-bit - Granite-Docling-258M - Official MLX - MinerU2.5 - bf16 - Nanonets-OCR2-3B - 4-bit</p> <p>Usage: <pre><code>pip install mlx-vlm\npython -m mlx_vlm.generate --model mlx-community/Qwen3-VL-8B-Instruct-4bit \\\n  --prompt \"Extract text from this document\" --image doc.png\n</code></pre></p>"},{"location":"ROADMAP/#openai-compatible-api-providers","title":"OpenAI-Compatible API Providers","text":"<p>OpenRouter (openrouter.ai): - \u2705 Qwen3-VL-235B-A22B ($0.45/$3.50 per M tokens) - \u2705 Qwen3-VL-30B-A3B - \u2705 Qwen2.5-VL-3B (SOTA visual understanding) - \u2705 Qwen2.5-VL-32B (structured outputs, math) - \u2705 Qwen2.5-VL-72B (best overall)</p> <p>Novita AI (novita.ai): - \u2705 DeepSeek-OCR (Model Page) - \u2705 Qwen2.5-VL-72B (OCR + scientific reasoning) - \u2705 Qwen3-VL-8B ($0.08/$0.50 per M tokens)</p> <p>Together AI (together.ai): - \u2705 Various vision-language models - \u2705 Lightweight models with multilingual support</p> <p>Replicate (replicate.com): - \u2705 Vision models collection - \u2705 Pay-per-use inference</p> <p>Others: - DeepInfra: olmOCR-2-7B - Parasail: olmOCR-2-7B - Cirrascale: olmOCR-2-7B</p> <p>API Integration Example: <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenAPIConfig\n\n# OpenRouter\nextractor = QwenTextExtractor(\n    backend=QwenAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=\"YOUR_OPENROUTER_KEY\",\n        base_url=\"https://openrouter.ai/api/v1\"\n    )\n)\n\n# Novita AI\nextractor = QwenTextExtractor(\n    backend=QwenAPIConfig(\n        model=\"novita/qwen3-vl-8b-instruct\",\n        api_key=\"YOUR_NOVITA_KEY\",\n        base_url=\"https://api.novita.ai/v3/openai\"\n    )\n)\n</code></pre></p>"},{"location":"ROADMAP/#task-capability-matrix","title":"Task Capability Matrix","text":"Task Description Model Count Top Models Text Extract (T) Document \u2192 Markdown/HTML 18 LightOnOCR-2, Chandra, Qwen3-VL-8B Layout (L) Structure detection with bboxes 8 Qwen3-VL-8B, Chandra, MinerU2.5 OCR (O) Text + bbox coordinates 15 LightOnOCR-2, olmOCR-2, Chandra Structured (S) Schema-based extraction 5 Qwen3-VL (all), Qwen2.5-VL (all), Gemma-3 Table (Tab) Table detection/extraction 12 Qwen3-VL-8B, DeepSeek-OCR, olmOCR-2 Formula (F) Math expression recognition 8 Nanonets-OCR2, Qwen3-VL-8B, GOT-OCR2.0"},{"location":"ROADMAP/#model-overview-by-task-capability","title":"Model Overview by Task Capability","text":""},{"location":"ROADMAP/#task-categories","title":"Task Categories","text":"Task Description Model Count text_extract Document to Markdown/HTML conversion 18 layout Document structure detection with bounding boxes 8 ocr Text extraction with bbox coordinates 6 structured Schema-based data extraction 5 table Table detection and extraction 4 formula Mathematical expression recognition 3"},{"location":"ROADMAP/#core-models-by-size-release-date","title":"\ud83c\udfaf Core Models (By Size &amp; Release Date)","text":""},{"location":"ROADMAP/#ultra-compact-models-1b-parameters","title":"Ultra-Compact Models (&lt;1B Parameters)","text":""},{"location":"ROADMAP/#1-ibm-granite-docling-258m","title":"1. IBM Granite-Docling-258M","text":"<p>Released: December 2024 | Parameters: 258M | License: Apache 2.0</p> <p>HuggingFace: ibm-granite/granite-docling-258M</p> <p>Description: Ultra-compact vision-language model (VLM) for converting documents to machine-readable formats while fully preserving layout, tables, equations, and lists. Built on Idefics3 architecture with siglip2-base-patch16-512 vision encoder and Granite 165M LLM.</p> <p>Key Features: - End-to-end document understanding at 258M parameters - Handles inline/floating math, code, table structure - Rivals systems several times its size - Extremely cost-effective</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 MLX (Apple Silicon) - ibm-granite/granite-docling-258M-mlx - \u2705 WebGPU - Demo Space</p> <p>Integration: <pre><code>pip install docling  # Automatically downloads model\n</code></pre></p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>pillow</code>, <code>docling</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>table</code>, <code>formula</code></p> <p>Links: - Model Card - MLX Version - Demo Space - Official Docs - Collection</p>"},{"location":"ROADMAP/#2-stepfun-ai-got-ocr20","title":"2. stepfun-ai GOT-OCR2.0","text":"<p>Released: September 2024 | Parameters: 700M | License: Apache 2.0</p> <p>HuggingFace: stepfun-ai/GOT-OCR2_0</p> <p>Description: General OCR Theory model for multilingual OCR on plain documents, scene text, formatted documents, tables, charts, mathematical formulas, geometric shapes, molecular formulas, and sheet music.</p> <p>Key Features: - Interactive OCR with region-specific recognition (coordinate or color-based) - Plain text OCR + formatted text OCR (markdown, LaTeX) - Multi-page document processing - Wide range of specialized content types</p> <p>Model Variations: - stepfun-ai/GOT-OCR2_0 - Original with custom code - stepfun-ai/GOT-OCR-2.0-hf - HuggingFace-native transformers integration</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 Custom inference pipeline</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>pillow</code></p> <p>Tasks: <code>text_extract</code>, <code>ocr</code>, <code>formula</code>, <code>table</code></p> <p>Links: - Model Card - HF-Native Version</p>"},{"location":"ROADMAP/#compact-models-1-2b-parameters","title":"Compact Models (1-2B Parameters)","text":""},{"location":"ROADMAP/#3-rednote-hilab-dotsocr","title":"3. rednote-hilab dots.ocr","text":"<p>Released: December 2024 | Parameters: 1.7B | License: MIT</p> <p>HuggingFace: rednote-hilab/dots.ocr</p> <p>Description: Multilingual documents parsing model based on 1.7B LLM with SOTA performance. Provides faster inference than many high-performing models based on larger foundations.</p> <p>Key Features: - Task switching via prompt alteration only - Competitive detection vs traditional models (DocLayout-YOLO) - Built-in VLLM support for high throughput - Released with paper arXiv:2512.02498</p> <p>Model Variations: - rednote-hilab/dots.ocr - Full model - rednote-hilab/dots.ocr.base - Base variant</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM (Recommended for production) - vLLM 0.9.1+</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>vllm&gt;=0.9.1</code> (recommended)</p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>table</code>, <code>formula</code>, <code>ocr</code></p> <p>Links: - Model Card - GitHub - Live Demo - Paper - Collection</p>"},{"location":"ROADMAP/#4-paddlepaddle-paddleocr-vl","title":"4. PaddlePaddle PaddleOCR-VL","text":"<p>Released: October 2025 | Parameters: 900M | License: Apache 2.0</p> <p>HuggingFace: PaddlePaddle/PaddleOCR-VL</p> <p>Description: Ultra-compact multilingual documents parsing VLM with SOTA performance. Integrates NaViT-style dynamic resolution visual encoder with ERNIE-4.5-0.3B language model.</p> <p>Key Features: - Supports 109 languages - Excels in recognizing complex elements (text, tables, formulas, charts) - Minimal resource consumption - Fast inference speeds - SOTA in page-level parsing and element-level recognition</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers - officially integrated) - \u2705 PaddlePaddle framework</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>paddlepaddle</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>ocr</code>, <code>table</code>, <code>formula</code></p> <p>Links: - Model Card - Online Demo - Collection - Transformers Docs - GitHub - PaddleOCR</p>"},{"location":"ROADMAP/#5-lighton-ai-lightonocr-series","title":"5. LightOn AI LightOnOCR Series","text":"<p>Released: January 2025 (v2), October 2025 (v1) | Parameters: 1B | License: Apache 2.0</p> <p>HuggingFace Models: - lightonai/LightOnOCR-2-1B - Recommended for OCR - lightonai/LightOnOCR-2-1B-bbox - Best localization - lightonai/LightOnOCR-2-1B-bbox-soup - Balanced OCR + bbox - lightonai/LightOnOCR-1B-1025 - Legacy v1</p> <p>Description: Compact, end-to-end vision-language model for OCR and document understanding. State-of-the-art accuracy in its weight class while being several times faster than larger VLMs.</p> <p>Key Features: - LightOnOCR-2-1B: SOTA on OlmOCR-Bench (83.2 \u00b1 0.9), outperforms Chandra-9B - Performance: 3.3\u00d7 faster than Chandra, 1.7\u00d7 faster than OlmOCR, 5\u00d7 faster than dots.ocr - Variants: OCR-only, bbox-capable (figure/image localization), and balanced checkpoints - Paper: arXiv:2601.14251</p> <p>Model Comparison: | Model | Use Case | Bbox Support | |-------|----------|--------------| | LightOnOCR-2-1B | Default for PDF\u2192Text/Markdown | \u274c | | LightOnOCR-2-1B-bbox | Best localization of figures/images | \u2705 Best | | LightOnOCR-2-1B-bbox-soup | Balanced OCR + localization | \u2705 Balanced |</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers - upstream support) - \u26a0\ufe0f Requires transformers from source for v2 (not yet in stable release)</p> <p>Quantized Versions: - GGUF format</p> <p>Dependencies: <code>transformers&gt;=4.48</code> (from source for v2), <code>torch</code>, <code>pillow</code></p> <p>Tasks: <code>text_extract</code>, <code>ocr</code>, <code>layout</code> (bbox variants only)</p> <p>Links: - LightOnOCR-2 Blog - LightOnOCR-1 Blog - Demo Space - Paper (arXiv) - Organization</p>"},{"location":"ROADMAP/#6-opendatalab-mineru25","title":"6. opendatalab MinerU2.5","text":"<p>Released: September 2024 | Parameters: 1.2B | License: Apache 2.0</p> <p>HuggingFace: opendatalab/MinerU2.5-2509-1.2B</p> <p>Description: Decoupled vision-language model for efficient high-resolution document parsing with state-of-the-art accuracy and low computational overhead.</p> <p>Key Features: - Two-stage parsing: global layout analysis on downsampled images \u2192 fine-grained content recognition on native-resolution crops - Outperforms Gemini-2.5 Pro, Qwen2.5-VL-72B, GPT-4o, MonkeyOCR, dots.ocr, PP-StructureV3 - Large-scale diverse data engine for pretraining/fine-tuning - New performance records in text, formula, table recognition, and reading order</p> <p>Model Variations: - opendatalab/MinerU2.5-2509-1.2B - Official model - mlx-community/MinerU2.5-2509-1.2B-bf16 - MLX for Apple Silicon - Mungert/MinerU2.5-2509-1.2B-GGUF - GGUF quantized</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM (with OpenAI API specs) - \u2705 MLX (Apple Silicon)</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>vllm</code> (optional)</p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>table</code>, <code>formula</code>, <code>ocr</code></p> <p>Links: - Model Card - Paper (arXiv:2509.22186) - MLX Version - GGUF Version</p>"},{"location":"ROADMAP/#small-models-2-4b-parameters","title":"Small Models (2-4B Parameters)","text":""},{"location":"ROADMAP/#7-qwen3-vl-2b-instruct","title":"7. Qwen3-VL-2B-Instruct","text":"<p>Released: October 2025 | Parameters: 2B | License: Apache 2.0</p> <p>HuggingFace: Qwen/Qwen3-VL-2B-Instruct</p> <p>Description: Multimodal LLM from Alibaba Cloud's Qwen team with comprehensive upgrades: superior text understanding/generation, deeper visual perception/reasoning, extended context, and stronger agent interaction.</p> <p>Key Features: - Dense and MoE architectures that scale from edge to cloud - Instruct and reasoning-enhanced \"Thinking\" editions - Enhanced spatial and video dynamics comprehension - Part of Qwen3-VL multimodal retrieval framework (arXiv:2601.04720, 2026)</p> <p>Model Variations: - Qwen/Qwen3-VL-2B-Instruct - Instruction-tuned - Qwen/Qwen3-VL-2B-Thinking - Reasoning-enhanced - Qwen/Qwen3-VL-2B-Instruct-GGUF - Quantized GGUF</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM - \u2705 MLX (via mlx-community) - \u2705 API (via cloud providers)</p> <p>Dependencies: <code>transformers&gt;=4.46</code>, <code>torch</code>, <code>qwen-vl-utils</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>structured</code>, <code>ocr</code>, <code>table</code></p> <p>Links: - Model Card - GitHub - Collection - GGUF Version</p>"},{"location":"ROADMAP/#8-deepseek-ocr","title":"8. DeepSeek-OCR","text":"<p>Released: October 2024 | Parameters: ~3B | License: MIT</p> <p>HuggingFace: deepseek-ai/DeepSeek-OCR</p> <p>Description: High-accuracy OCR model from DeepSeek-AI for extracting text from complex visual inputs (documents, screenshots, receipts, natural scenes).</p> <p>Key Features: - Built for real-world documents: PDFs, forms, tables, handwritten/noisy text - Outputs clean, structured Markdown - VLLM support upstream - ~2500 tokens/s on A100 with vLLM - Paper: arXiv:2510.18234</p> <p>Model Variations: - deepseek-ai/DeepSeek-OCR - Official BF16 (~6.7 GB) - NexaAI/DeepSeek-OCR-GGUF - Quantized GGUF</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM (officially supported)</p> <p>Requirements: - Python 3.12.9 + CUDA 11.8 - <code>torch==2.6.0</code>, <code>transformers==4.46.3</code>, <code>flash-attn==2.7.3</code> - L4 / A100 GPUs (\u226516 GB VRAM)</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>vllm</code>, <code>flash-attn</code>, <code>einops</code></p> <p>Tasks: <code>text_extract</code>, <code>ocr</code>, <code>table</code></p> <p>Links: - Model Card - GitHub - GGUF Version - Demo Space</p>"},{"location":"ROADMAP/#9-nanonets-ocr2-3b","title":"9. Nanonets-OCR2-3B","text":"<p>Released: 2024 | Parameters: 3B | License: Apache 2.0</p> <p>HuggingFace: nanonets/Nanonets-OCR2-3B</p> <p>Description: State-of-the-art image-to-markdown OCR model that transforms documents into structured markdown with intelligent content recognition and semantic tagging, optimized for LLM downstream processing.</p> <p>Key Features: - LaTeX equation recognition (inline $...$ and display $$...$$) - Intelligent image description with structured tags (logos, charts, graphs) - 125K context window - ~7.53 GB model size</p> <p>Model Variations: - nanonets/Nanonets-OCR2-3B - Full BF16 - Mungert/Nanonets-OCR2-3B-GGUF - GGUF quantized - mlx-community/Nanonets-OCR2-3B-4bit - MLX 4-bit - yasserrmd/Nanonets-OCR2-3B - Ollama format</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 MLX (Apple Silicon) - \u2705 Ollama</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>pillow</code></p> <p>Tasks: <code>text_extract</code>, <code>formula</code>, <code>ocr</code></p> <p>Links: - Model Card - GGUF Version - MLX 4-bit - Ollama</p>"},{"location":"ROADMAP/#10-qwen3-vl-4b-instruct","title":"10. Qwen3-VL-4B-Instruct","text":"<p>Released: October 2025 | Parameters: 4B | License: Apache 2.0</p> <p>HuggingFace: Qwen/Qwen3-VL-4B-Instruct</p> <p>Description: Mid-size Qwen3-VL model with balanced performance and efficiency. Part of comprehensive multimodal model series with text understanding, visual reasoning, and agent capabilities.</p> <p>Model Variations: - Qwen/Qwen3-VL-4B-Instruct - Instruction-tuned - Qwen/Qwen3-VL-4B-Thinking - Reasoning-enhanced</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM - \u2705 MLX (via mlx-community) - \u2705 API (via cloud providers)</p> <p>Dependencies: <code>transformers&gt;=4.46</code>, <code>torch</code>, <code>qwen-vl-utils</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>structured</code>, <code>ocr</code>, <code>table</code></p> <p>Links: - Collection - GitHub</p>"},{"location":"ROADMAP/#11-google-gemma-3-4b-it","title":"11. Google Gemma-3-4B-IT","text":"<p>Released: 2025 | Parameters: 4B | License: Gemma License</p> <p>HuggingFace: google/gemma-3-4b-it</p> <p>Description: Lightweight, state-of-the-art multimodal model from Google built from same research/technology as Gemini. Handles text and image input, generates text output.</p> <p>Key Features: - 128K context window - Multilingual support (140+ languages) - SigLIP image encoder (896\u00d7896 square images) - Gemma-3-4B-IT beats Gemma-2-27B-IT on benchmarks</p> <p>Model Variations: - google/gemma-3-4b-it - Instruction-tuned (vision-capable) - google/gemma-3-4b-pt - Pre-trained base - google/gemma-3-4b-it-qat-q4_0-gguf - Quantized GGUF - bartowski/google_gemma-3-4b-it-GGUF - Community GGUF</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 Google AI SDK - \u2705 API (Google AI Studio)</p> <p>Dependencies: <code>transformers&gt;=4.46</code>, <code>torch</code>, <code>pillow</code></p> <p>Tasks: <code>text_extract</code>, <code>structured</code>, <code>ocr</code></p> <p>Links: - Model Card - Blog Post - Transformers Docs - Google Docs - DeepMind Page</p>"},{"location":"ROADMAP/#medium-models-7-9b-parameters","title":"Medium Models (7-9B Parameters)","text":""},{"location":"ROADMAP/#12-allenai-olmocr-2-7b-1025","title":"12. allenai olmOCR-2-7B-1025","text":"<p>Released: October 2025 | Parameters: 7B | License: Apache 2.0</p> <p>HuggingFace: allenai/olmOCR-2-7B-1025</p> <p>Description: State-of-the-art OCR for English-language digitized print documents. Fine-tuned from Qwen2.5-VL-7B-Instruct using olmOCR-mix-1025 dataset + GRPO RL training.</p> <p>Key Features: - 82.4 points on olmOCR-Bench (SOTA for real-world documents) - Substantial improvements where OCR often fails (math equations, tables, tricky cases) - Boosted via reinforcement learning (GRPO)</p> <p>Model Variations: - allenai/olmOCR-2-7B-1025 - Full BF16 version - allenai/olmOCR-2-7B-1025-FP8 - Recommended FP8 quantized (practical use except fine-tuning) - bartowski/allenai_olmOCR-2-7B-1025-GGUF - GGUF quantized - richardyoung/olmOCR-2-7B-1025-GGUF - Alternative GGUF</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM (recommended via olmOCR toolkit) - \u2705 API (DeepInfra, Parasail, Cirrascale)</p> <p>Best Usage: Via olmOCR toolkit with VLLM for efficient inference at scale (millions of documents).</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>vllm</code>, <code>olmocr</code> (toolkit Tasks: <code>text_extract</code>, <code>ocr</code>, <code>table</code>, <code>formula</code></p> <p>Links: - Model Card - FP8 Version - Blog Post - GGUF (bartowski)</p>"},{"location":"ROADMAP/#13-qwen3-vl-8b-instruct","title":"13. Qwen3-VL-8B-Instruct","text":"<p>Released: October 2025 | Parameters: 8B | License: Apache 2.0</p> <p>HuggingFace: Qwen/Qwen3-VL-8B-Instruct</p> <p>Description: Primary model in Qwen3-VL series with optimal balance of performance and efficiency. Enhanced document parsing over Qwen2.5-VL with improved visual perception, text understanding, and advanced reasoning.</p> <p>Key Features: - Custom layout label support (flexible VLM) - Extended context length - Enhanced spatial and video comprehension - Stronger agent interaction capabilities</p> <p>Model Variations: - Qwen/Qwen3-VL-8B-Instruct - Instruction-tuned - Qwen/Qwen3-VL-8B-Thinking - Reasoning-enhanced</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM - \u2705 MLX (Apple Silicon) - mlx-community/Qwen3-VL-8B-Instruct-4bit - \u2705 API (Novita AI, OpenRouter, etc.)</p> <p>API Providers: - Novita AI: Context 131K tokens, Max output 33K tokens   - Pricing: $0.08/M input tokens, $0.50/M output tokens</p> <p>Dependencies: <code>transformers&gt;=4.46</code>, <code>torch</code>, <code>qwen-vl-utils</code>, <code>vllm</code> (optional)</p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>structured</code>, <code>ocr</code>, <code>table</code>, <code>formula</code></p> <p>Links: - Model Card - Collection - GitHub - MLX 4-bit</p>"},{"location":"ROADMAP/#14-datalab-to-chandra","title":"14. datalab-to Chandra","text":"<p>Released: 2024 | Parameters: 9B | License: Apache 2.0</p> <p>HuggingFace: datalab-to/chandra</p> <p>Description: OCR model handling complex tables, forms, and handwriting with full layout preservation. Uses Qwen3VL for document understanding.</p> <p>Key Features: - 83.1 \u00b1 0.9 overall on OlmOCR benchmark (outperforms DeepSeek OCR, dots.ocr, olmOCR) - Strong grounding capabilities - Supports 40+ languages - Layout-aware output with bbox coordinates for every text block, table, and image - Outputs in HTML, Markdown, and JSON with detailed layout</p> <p>Use Cases: - Handwritten forms - Mathematical notation - Multi-column layouts - Complex tables</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM (production throughput)</p> <p>Installation: <pre><code>pip install chandra-ocr\n</code></pre></p> <p>Model Variations: - datalab-to/chandra - Official model - noctrex/Chandra-OCR-GGUF - GGUF quantized</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>vllm</code> (optional), <code>chandra-ocr</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>ocr</code>, <code>table</code>, <code>formula</code></p> <p>Links: - Model Card - GitHub - Blog Post - DeepWiki Docs - GGUF Version</p>"},{"location":"ROADMAP/#large-models-32b-parameters","title":"Large Models (32B+ Parameters)","text":""},{"location":"ROADMAP/#15-qwen3-vl-32b-instruct","title":"15. Qwen3-VL-32B-Instruct","text":"<p>Released: October 2025 | Parameters: 32B | License: Apache 2.0</p> <p>HuggingFace: Qwen/Qwen3-VL-32B-Instruct</p> <p>Description: Largest Qwen3-VL model with maximum performance for complex document understanding and multimodal reasoning tasks.</p> <p>Key Features: - Superior performance on complex documents - Extended context length - Enhanced reasoning capabilities - Production-grade for demanding applications</p> <p>Model Variations: - Qwen/Qwen3-VL-32B-Instruct - Instruction-tuned - Qwen/Qwen3-VL-32B-Thinking - Reasoning-enhanced</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM (recommended for production) - \u2705 API (cloud providers)</p> <p>GPU Requirements: A100 40GB+ or multi-GPU setup</p> <p>Dependencies: <code>transformers&gt;=4.46</code>, <code>torch</code>, <code>qwen-vl-utils</code>, <code>vllm</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>structured</code>, <code>ocr</code>, <code>table</code>, <code>formula</code></p> <p>Links: - Model Card - Collection - GitHub</p>"},{"location":"ROADMAP/#specialized-models","title":"Specialized Models","text":""},{"location":"ROADMAP/#16-docling-projectdocling-models","title":"16. docling-project/docling-models","text":"<p>Released: 2024 | Parameters: Various | License: Apache 2.0</p> <p>HuggingFace: docling-project/docling-models</p> <p>Description: Collection of models powering the Docling PDF document conversion package. Includes layout detection (RT-DETR) and table structure recognition (TableFormer).</p> <p>Models Included: 1. Layout Model: RT-DETR for detecting document components    - Labels: Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title 2. TableFormer Model: Table structure identification from images</p> <p>Note: Superseded by granite-docling-258M for end-to-end document conversion (receives updates and support).</p> <p>Backends Supported: - \u2705 PyTorch (via Docling library)</p> <p>Integration: <pre><code>pip install docling\n</code></pre></p> <p>Dependencies: <code>docling</code>, <code>transformers</code>, <code>torch</code></p> <p>Tasks: <code>layout</code>, <code>table</code></p> <p>Links: - Model Card - Vision Models Docs - SmolDocling (legacy)</p>"},{"location":"ROADMAP/#optional-models-legacyalternative","title":"\ud83d\udce6 Optional Models (Legacy/Alternative)","text":""},{"location":"ROADMAP/#qwen-25-vl-series-previous-generation","title":"Qwen 2.5-VL Series (Previous Generation)","text":""},{"location":"ROADMAP/#qwen25-vl-3b-instruct","title":"Qwen2.5-VL-3B-Instruct","text":"<p>Released: 2024 | Parameters: 3B | License: Apache 2.0</p> <p>HuggingFace: Qwen/Qwen2.5-VL-3B-Instruct</p> <p>Description: Previous generation Qwen VLM with strong visual understanding, agentic capabilities, video understanding (1+ hour), and structured outputs.</p> <p>Key Features: - Analyzes texts, charts, icons, graphics, layouts - Visual agent capabilities (computer use, phone use) - Video comprehension with temporal segment pinpointing - ViT architecture with SwiGLU and RMSNorm - Dynamic resolution + dynamic FPS sampling</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM - \u2705 MLX - \u2705 API</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>qwen-vl-utils</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>structured</code>, <code>ocr</code></p> <p>Links: - Model Card - Collection</p>"},{"location":"ROADMAP/#qwen25-vl-7b-instruct","title":"Qwen2.5-VL-7B-Instruct","text":"<p>Released: 2024 | Parameters: 7B | License: Apache 2.0</p> <p>HuggingFace: Qwen/Qwen2.5-VL-7B-Instruct</p> <p>Description: Mid-size Qwen2.5-VL model with same capabilities as 3B variant but enhanced performance.</p> <p>Model Variations: - Qwen/Qwen2.5-VL-7B-Instruct - Official - unsloth/Qwen2.5-VL-7B-Instruct-GGUF - GGUF quantized - nvidia/Qwen2.5-VL-7B-Instruct-NVFP4 - NVIDIA FP4 optimized</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM - \u2705 MLX - \u2705 API</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>qwen-vl-utils</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>structured</code>, <code>ocr</code>, <code>table</code></p> <p>Links: - Model Card - Collection - GGUF Version</p>"},{"location":"ROADMAP/#model-comparison-summary","title":"\ud83d\udcca Model Comparison Summary","text":""},{"location":"ROADMAP/#by-release-date-2024-2026","title":"By Release Date (2024-2026)","text":"Model Release Params Benchmark Score LightOnOCR-2-1B Jan 2025 1B 83.2 (OlmOCR) dots.ocr Dec 2024 1.7B 79.1 (OlmOCR) Granite-Docling-258M Dec 2024 258M N/A Chandra 2024 9B 83.1 (OlmOCR) Qwen3-VL Series Oct 2025 2-32B SOTA PaddleOCR-VL Oct 2025 900M SOTA olmOCR-2-7B Oct 2025 7B 82.4 (OlmOCR) DeepSeek-OCR Oct 2024 3B 75.4 (OlmOCR) GOT-OCR2.0 Sep 2024 700M N/A MinerU2.5 Sep 2024 1.2B SOTA"},{"location":"ROADMAP/#by-performance-olmocr-bench","title":"By Performance (OlmOCR-Bench)","text":"Rank Model Score Params 1 LightOnOCR-2-1B 83.2 \u00b1 0.9 1B 2 Chandra 83.1 \u00b1 0.9 9B 3 olmOCR-2-7B 82.4 7B 4 dots.ocr 79.1 1.7B 5 olmOCR (v1) 78.5 7B 6 DeepSeek-OCR 75.4 \u00b1 1.0 3B"},{"location":"ROADMAP/#by-speed-relative-performance","title":"By Speed (Relative Performance)","text":"Model Speed Multiplier Params LightOnOCR-2-1B Fastest baseline 1B PaddleOCR-VL 1.73\u00d7 slower 900M DeepSeek-OCR (vLLM) 1.73\u00d7 slower 3B olmOCR-2 1.7\u00d7 slower 7B Chandra 3.3\u00d7 slower 9B dots.ocr 5\u00d7 slower 1.7B"},{"location":"ROADMAP/#backend-support-matrix","title":"\ud83d\udd27 Backend Support Matrix","text":"Model PyTorch VLLM MLX API GGUF Granite-Docling-258M \u2705 \u274c \u2705 \u274c \u274c dots.ocr \u2705 \u2705 \u274c \u274c \u274c GOT-OCR2.0 \u2705 \u274c \u274c \u274c \u274c PaddleOCR-VL \u2705 \u274c \u274c \u274c \u274c MinerU2.5 \u2705 \u2705 \u2705 \u274c \u2705 LightOnOCR-2-1B \u2705 \u274c \u274c \u274c \u2705 Qwen3-VL (all) \u2705 \u2705 \u2705 \u2705 \u2705 DeepSeek-OCR \u2705 \u2705 \u274c \u274c \u2705 Nanonets-OCR2-3B \u2705 \u274c \u2705 \u274c \u2705 Gemma-3-4B-IT \u2705 \u274c \u274c \u2705 \u2705 olmOCR-2-7B \u2705 \u2705 \u274c \u2705 \u2705 Chandra \u2705 \u2705 \u274c \u274c \u2705 Qwen2.5-VL (all) \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"ROADMAP/#recommended-model-selection-guide","title":"\ud83d\udcda Recommended Model Selection Guide","text":""},{"location":"ROADMAP/#by-use-case","title":"By Use Case","text":"Use Case Recommended Model Why Edge/Mobile Deployment Granite-Docling-258M Ultra-compact (258M), MLX support Fast OCR (CPU) LightOnOCR-2-1B Fastest in class, SOTA accuracy Multilingual Documents PaddleOCR-VL 109 languages, minimal resources High-Throughput Serving dots.ocr + VLLM Built for VLLM, fast inference Best Accuracy (English) LightOnOCR-2-1B or Chandra SOTA on OlmOCR-Bench Custom Layout Detection Qwen3-VL-8B Flexible VLM with prompt-based labels Production Balanced Qwen3-VL-8B or olmOCR-2-7B Performance + reliability Complex Documents Chandra or Qwen3-VL-32B Handles tables, forms, handwriting Apple Silicon (M1/M2/M3) Granite-Docling-258M (MLX) Native MLX optimization Cost-Effective API Qwen3-VL-8B (Novita) $0.08/M tokens input"},{"location":"ROADMAP/#quick-start-examples","title":"\ud83d\ude80 Quick Start Examples","text":""},{"location":"ROADMAP/#ultra-compact-258m-granite-docling","title":"Ultra-Compact (258M) - Granite-Docling","text":"<pre><code>from omnidocs.tasks.text_extraction import GraniteDoclingOCR, GraniteDoclingConfig\n\nextractor = GraniteDoclingOCR(\n    config=GraniteDoclingConfig(device=\"cuda\")\n)\nresult = extractor.extract(image, output_format=\"markdown\")\n</code></pre>"},{"location":"ROADMAP/#fastest-ocr-1b-lightonocr-2","title":"Fastest OCR (1B) - LightOnOCR-2","text":"<pre><code>from omnidocs.tasks.text_extraction import LightOnOCRExtractor, LightOnOCRConfig\n\nextractor = LightOnOCRExtractor(\n    config=LightOnOCRConfig(\n        model=\"lightonai/LightOnOCR-2-1B\",\n        device=\"cuda\"\n    )\n)\nresult = extractor.extract(image, output_format=\"markdown\")\n</code></pre>"},{"location":"ROADMAP/#high-throughput-17b-dotsocr-vllm","title":"High-Throughput (1.7B) - dots.ocr + VLLM","text":"<pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nextractor = DotsOCRTextExtractor(\n    backend=DotsOCRVLLMConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.9\n    )\n)\nresult = extractor.extract(image, output_format=\"markdown\")\n</code></pre>"},{"location":"ROADMAP/#best-accuracy-7-9b-olmocr-2-or-chandra","title":"Best Accuracy (7-9B) - olmOCR-2 or Chandra","text":"<pre><code>from omnidocs.tasks.text_extraction import OlmOCRExtractor, ChandraTextExtractor\nfrom omnidocs.tasks.text_extraction.olm import OlmOCRVLLMConfig\nfrom omnidocs.tasks.text_extraction.chandra import ChandraPyTorchConfig\n\n# Option 1: olmOCR-2-7B with VLLM\nextractor = OlmOCRExtractor(\n    backend=OlmOCRVLLMConfig(\n        model=\"allenai/olmOCR-2-7B-1025-FP8\",\n        tensor_parallel_size=1\n    )\n)\n\n# Option 2: Chandra-9B\nextractor = ChandraTextExtractor(\n    backend=ChandraPyTorchConfig(\n        model=\"datalab-to/chandra\",\n        device=\"cuda\"\n    )\n)\n</code></pre>"},{"location":"ROADMAP/#flexible-custom-layouts-8b-qwen3-vl","title":"Flexible Custom Layouts (8B) - Qwen3-VL","text":"<pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenPyTorchConfig\n\nlayout = QwenLayoutDetector(\n    backend=QwenPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\"\n    )\n)\n\nresult = layout.extract(\n    image,\n    custom_labels=[\"code_block\", \"sidebar\", \"diagram\"]\n)\n</code></pre>"},{"location":"ROADMAP/#current-focus-layout-analysis-models","title":"\ud83c\udfaf Current Focus: Layout Analysis Models","text":""},{"location":"ROADMAP/#phase-1-multi-backend-vlm-integration","title":"Phase 1: Multi-Backend VLM Integration","text":""},{"location":"ROADMAP/#1-qwen3-vl-8b-instruct-integration","title":"1. Qwen3-VL-8B-Instruct Integration","text":"<p>Status: \ud83d\udfe1 In Progress</p> <p>Integrate Qwen3-VL-8B-Instruct for flexible layout detection with custom label support across all backends.</p> <p>Key Features: - Enhanced document parsing over Qwen2.5-VL - Improved visual perception and text understanding - Advanced reasoning capabilities - Custom layout label support</p>"},{"location":"ROADMAP/#implementation-checklist","title":"Implementation Checklist:","text":"<ul> <li>[ ] HuggingFace/PyTorch Backend (<code>QwenLayoutDetector</code> + <code>QwenPyTorchConfig</code>)</li> </ul> <p>Model: <code>Qwen/Qwen3-VL-8B-Instruct</code></p> <p>Config Class: <code>omnidocs/tasks/layout_analysis/qwen/pytorch.py</code> <pre><code>class QwenPyTorchConfig(BaseModel):\n    model: str = \"Qwen/Qwen3-VL-8B-Instruct\"\n    device: str = \"cuda\"\n    torch_dtype: Literal[\"auto\", \"float16\", \"bfloat16\"] = \"auto\"\n    attn_implementation: Optional[str] = None  # \"flash_attention_2\" if available\n    cache_dir: Optional[str] = None\n</code></pre></p> <p>Dependencies:   - <code>torch</code>, <code>transformers</code>   - <code>qwen-vl-utils</code> (model-specific utility)</p> <p>Reference Implementation: See <code>scripts/layout/modal_qwen3_vl_layout.py</code> in the repository</p> <p>Testing:   - Validate on synthetic document images   - Compare detection accuracy with ground truth   - Test custom label support</p> <ul> <li>[ ] VLLM Backend (<code>QwenVLLMConfig</code>)</li> </ul> <p>Model: <code>Qwen/Qwen3-VL-8B-Instruct</code></p> <p>Config Class: <code>omnidocs/tasks/layout_analysis/qwen/vllm.py</code> <pre><code>class QwenVLLMConfig(BaseModel):\n    model: str = \"Qwen/Qwen3-VL-8B-Instruct\"\n    tensor_parallel_size: int = 1\n    gpu_memory_utilization: float = 0.9\n    max_model_len: Optional[int] = None\n    trust_remote_code: bool = True\n</code></pre></p> <p>Dependencies:   - <code>vllm&gt;=0.4.0</code>   - <code>torch&gt;=2.0</code></p> <p>Use Case: High-throughput batch processing (10+ documents/second)</p> <p>Modal Config:   - GPU: <code>A10G:1</code> (minimum), <code>A100:1</code> (recommended for production)   - Image: VLLM GPU Image with flash-attn</p> <p>Testing:   - Benchmark throughput vs PyTorch   - Validate output consistency   - Test batch processing</p> <ul> <li>[ ] MLX Backend (<code>QwenMLXConfig</code>)</li> </ul> <p>Model: <code>mlx-community/Qwen3-VL-8B-Instruct-4bit</code></p> <p>Config Class: <code>omnidocs/tasks/layout_analysis/qwen/mlx.py</code> <pre><code>class QwenMLXConfig(BaseModel):\n    model: str = \"mlx-community/Qwen3-VL-8B-Instruct-4bit\"\n    quantization: Literal[\"4bit\", \"8bit\"] = \"4bit\"\n    max_tokens: int = 4096\n</code></pre></p> <p>Dependencies:   - <code>mlx&gt;=0.10</code>   - <code>mlx-lm&gt;=0.10</code></p> <p>Platform: Apple Silicon only (M1/M2/M3+)</p> <p>Use Case: Local development and testing on macOS</p> <p>Note: \u26a0\ufe0f DO NOT deploy MLX to Modal - local development only</p> <ul> <li>[ ] API Backend (<code>QwenAPIConfig</code>)</li> </ul> <p>Model: <code>qwen3-vl-8b-instruct</code></p> <p>Config Class: <code>omnidocs/tasks/layout_analysis/qwen/api.py</code> <pre><code>class QwenAPIConfig(BaseModel):\n    model: str = \"novita/qwen3-vl-8b-instruct\"\n    api_key: str\n    base_url: Optional[str] = None\n    max_tokens: int = 4096\n    temperature: float = 0.1\n</code></pre></p> <p>Provider: Novita AI   - Context Length: 131K tokens   - Max Output: 33K tokens   - Pricing:     - Input: $0.08/M tokens     - Output: $0.50/M tokens</p> <p>Dependencies:   - <code>litellm&gt;=1.30</code>   - <code>openai&gt;=1.0</code></p> <p>Use Case:   - Serverless deployments   - No GPU infrastructure required   - Cost-effective for low-volume processing</p> <ul> <li>[ ] Main Extractor Class (<code>omnidocs/tasks/layout_analysis/qwen.py</code>)</li> </ul> <p>Implement unified <code>QwenLayoutDetector</code> class:   <pre><code>from typing import Union, List, Optional\nfrom PIL import Image\nfrom .base import BaseLayoutExtractor\nfrom .models import LayoutOutput\nfrom .qwen import (\n    QwenPyTorchConfig,\n    QwenVLLMConfig,\n    QwenMLXConfig,\n    QwenAPIConfig,\n)\n\nQwenBackendConfig = Union[\n    QwenPyTorchConfig,\n    QwenVLLMConfig,\n    QwenMLXConfig,\n    QwenAPIConfig,\n]\n\nclass QwenLayoutDetector(BaseLayoutExtractor):\n    \"\"\"Flexible VLM-based layout detector with custom label support.\"\"\"\n\n    def __init__(self, backend: QwenBackendConfig):\n        self.backend_config = backend\n        self._backend = self._create_backend()\n\n    def extract(\n        self,\n        image: Image.Image,\n        custom_labels: Optional[List[str]] = None,\n    ) -&gt; LayoutOutput:\n        \"\"\"\n        Detect layout elements with optional custom labels.\n\n        Args:\n            image: PIL Image\n            custom_labels: Optional custom layout categories\n                Default: [\"title\", \"paragraph\", \"table\", \"figure\",\n                         \"caption\", \"formula\", \"list\"]\n\n        Returns:\n            LayoutOutput with detected bounding boxes\n        \"\"\"\n        # Implementation...\n</code></pre></p> <ul> <li>[ ] Integration Tests</li> </ul> <p>Test suite covering:   - All backend configurations   - Custom label functionality   - Cross-backend output consistency   - Edge cases (empty images, single elements, complex layouts)</p> <ul> <li> <p>[ ] Documentation</p> </li> <li> <p>API reference with examples for each backend</p> </li> <li>Performance comparison table (PyTorch vs VLLM vs MLX vs API)</li> <li>Migration guide from Qwen2.5-VL</li> <li> <p>Custom label usage examples</p> </li> <li> <p>[ ] Modal Deployment Script</p> </li> </ul> <p>Create production-ready deployment:   - <code>scripts/layout_omnidocs/modal_qwen_layout_vllm_online.py</code>   - Web endpoint for layout detection API   - Batch processing support   - Monitoring and logging</p>"},{"location":"ROADMAP/#phase-2-additional-layout-models","title":"Phase 2: Additional Layout Models","text":""},{"location":"ROADMAP/#2-rt-detr-layout-detector","title":"2. RT-DETR Layout Detector","text":"<ul> <li>[ ] Single-Backend Implementation (PyTorch only)</li> <li>Model: <code>RT-DETR</code> (Facebook AI)</li> <li>Fixed label support (COCO-based)</li> <li>Real-time detection optimization</li> </ul>"},{"location":"ROADMAP/#3-surya-layout-detector","title":"3. Surya Layout Detector","text":"<ul> <li>[ ] Single-Backend Implementation (PyTorch only)</li> <li>Model: <code>vikp/surya_layout</code></li> <li>Multi-language document support</li> <li>Optimized for speed</li> </ul>"},{"location":"ROADMAP/#4-florence-2-layout-detector","title":"4. Florence-2 Layout Detector","text":"<ul> <li>[ ] Multi-Backend Implementation</li> <li>HuggingFace/PyTorch backend</li> <li>API backend (Microsoft Azure)</li> <li>Object detection + dense captioning</li> </ul>"},{"location":"ROADMAP/#future-phases","title":"\ud83d\udd2e Future Phases","text":"<p>Additional task categories will be added after layout analysis is complete:</p> <ul> <li>OCR Extraction: Surya-OCR, PaddleOCR, Qwen-OCR</li> <li>Text Extraction: VLM-based Markdown/HTML extraction</li> <li>Table Extraction: Table Transformer, Surya-Table</li> <li>Math Expression Extraction: UniMERNet, Surya-Math</li> <li>Advanced Features: Reading order, image captioning, chart understanding</li> <li>Package &amp; Distribution: PyPI publishing, comprehensive documentation</li> </ul>"},{"location":"ROADMAP/#success-metrics-layout-analysis","title":"\ud83c\udfaf Success Metrics (Layout Analysis)","text":""},{"location":"ROADMAP/#performance-targets","title":"Performance Targets","text":"Metric Target Current Layout Detection Accuracy (mAP) &gt;90% TBD Inference Speed (PyTorch) &lt;2s per page TBD Inference Speed (VLLM) &lt;0.5s per page TBD Custom Label Support 100% functional TBD"},{"location":"ROADMAP/#quality-targets","title":"Quality Targets","text":"<ul> <li>[ ] Type hints coverage: 100%</li> <li>[ ] Docstring coverage: 100%</li> <li>[ ] Test coverage: &gt;80%</li> <li>[ ] All backends tested on production data</li> <li>[ ] Cross-backend output consistency validated</li> </ul>"},{"location":"ROADMAP/#infrastructure","title":"\ud83d\udd27 Infrastructure","text":""},{"location":"ROADMAP/#modal-deployment-standards","title":"Modal Deployment Standards","text":"<p>Consistency Requirements (as per CLAUDE.md):</p> <ul> <li>Volume Name: <code>omnidocs</code></li> <li>Secret Name: <code>adithya-hf-wandb</code></li> <li>CUDA Version: <code>12.4.0-devel-ubuntu22.04</code></li> <li>Python Version: <code>3.11</code> (3.12 for Qwen3-VL)</li> <li>Cache Directory: <code>/data/.cache</code> (HuggingFace)</li> <li>Model Cache: <code>/data/omnidocs_models</code></li> <li>Dependency Management: <code>.uv_pip_install()</code> (NO version pinning)</li> </ul>"},{"location":"ROADMAP/#gpu-configurations","title":"GPU Configurations","text":"GPU Use Case Cost (est.) <code>A10G:1</code> Development &amp; Testing $0.60/hr <code>A100:1</code> Production Inference $3.00/hr <code>A100:2</code> High-Throughput VLLM $6.00/hr"},{"location":"ROADMAP/#references","title":"\ud83d\udcda References","text":""},{"location":"ROADMAP/#design-documents","title":"Design Documents","text":"<ul> <li>Backend Architecture - Core design principles (see <code>IMPLEMENTATION_PLAN/BACKEND_ARCHITECTURE.md</code>)</li> <li>Developer Experience (DevEx) - API design and patterns (see <code>IMPLEMENTATION_PLAN/DEVEX.md</code>)</li> <li>Claude Development Guide - Implementation standards (see <code>CLAUDE.md</code> in repo root)</li> </ul>"},{"location":"ROADMAP/#external-resources","title":"External Resources","text":"<ul> <li>Qwen3-VL Model Card</li> <li>Qwen3-VL MLX (4bit)</li> <li>Modal Documentation</li> <li>UV Package Manager</li> </ul>"},{"location":"ROADMAP/#notes","title":"\ud83d\udcdd Notes","text":""},{"location":"ROADMAP/#implementation-order-rationale","title":"Implementation Order Rationale","text":"<ol> <li>Qwen3-VL Priority: Multi-backend support demonstrates v2.0 architecture</li> <li>RT-DETR: Fast fixed-label detection for production use</li> <li>Surya: Multi-language support and speed optimization</li> <li>Florence-2: Microsoft's advanced VLM capabilities</li> </ol>"},{"location":"ROADMAP/#breaking-changes-from-v10","title":"Breaking Changes from v1.0","text":"<ul> <li>String-based factory pattern removed (use class imports)</li> <li>Document class is now stateless (doesn't store results)</li> <li>Config classes are model-specific (not generic)</li> <li>Backend selection via config type (not string parameter)</li> </ul> <p>Last Updated: January 21, 2026 Maintainer: Adithya S Kolavi Version: 2.0.0-dev</p>"},{"location":"concepts/","title":"OmniDocs Concepts","text":"<p>Deep-dive documentation explaining the \"why\" behind OmniDocs architecture.</p> <p>This section explains the fundamental concepts and design decisions that power OmniDocs. Rather than \"how to use,\" these documents explain \"how it works\" and \"why it's designed this way.\"</p>"},{"location":"concepts/#documentation-files","title":"Documentation Files","text":""},{"location":"concepts/#1-architecture-overview-2100-words","title":"1. Architecture Overview - ~2100 words","text":"<p>What you'll learn: - Six core design principles that shape every decision - Component architecture (Document, Extractors, Configs, Backends) - Data flow from user code to inference result - Why stateless documents matter - Complete Qwen text extraction walkthrough</p> <p>Best for: Understanding the big picture, how components fit together, design rationale</p> <p>Key Concepts: - Unified API via <code>.extract()</code> method - Type-safe Pydantic configurations - Multi-backend support with config-driven selection - Stateless Document class - Separation of init (model setup) vs extract (task parameters) - Backend discoverability through imports</p>"},{"location":"concepts/#2-document-model-2400-words","title":"2. Document Model - ~2400 words","text":"<p>What you'll learn: - Why Document class is stateless (clean separation of concerns) - How lazy page loading works (memory efficiency) - All Document methods and their use cases - DocumentMetadata structure - Memory management and caching strategies - Common processing patterns</p> <p>Best for: Understanding document loading, page access, memory efficiency</p> <p>Key Concepts: - LazyPage wrapper for efficient rendering - Multiple source formats (PDF, URL, bytes, images) - Page caching and explicit memory control - Iterator interface for large documents - Page-level and full-text caching - When to use Document vs direct image input</p>"},{"location":"concepts/#3-backend-system-2000-words","title":"3. Backend System - ~2000 words","text":"<p>What you'll learn: - Four backend options: PyTorch, VLLM, MLX, API - How backend selection works at runtime - Config classes drive backend choice via <code>isinstance</code> checks - Lazy imports to avoid unnecessary dependencies - Adding new backends (extension points) - Trade-offs: speed, complexity, cost, privacy</p> <p>Best for: Understanding multi-backend support, adding new backends, choosing the right backend</p> <p>Key Concepts: - PyTorch for development, VLLM for production, MLX for Apple Silicon, API for managed - Type name detection in <code>_load_model()</code> - Lazy imports prevent dependency bloat - Each backend has separate config file - Backend discoverability through importable configs - Decision matrix for backend selection</p>"},{"location":"concepts/#4-config-pattern-2000-words","title":"4. Config Pattern - ~2000 words","text":"<p>What you'll learn: - Single-backend vs multi-backend models (how to tell the difference) - Pydantic config class structure and validation - Clear separation: init (config) vs extract (task parameters) - What parameters go where and why - Real examples from codebase (DocLayoutYOLO, QwenTextExtractor) - Extending configs with custom validation</p> <p>Best for: Understanding config design, building new extractors, parameter organization</p> <p>Key Concepts: - Single-backend: <code>{Model}Config</code> with <code>config=</code> parameter - Multi-backend: <code>{Model}{Backend}Config</code> with <code>backend=</code> parameter - Pydantic validation with Field() and validators - Init parameters: model, device, quantization, hardware - Extract parameters: format, custom prompts, task options - Decision tree for where parameters belong</p>"},{"location":"concepts/#how-to-use-these-docs","title":"How to Use These Docs","text":""},{"location":"concepts/#im-new-to-omnidocs","title":"I'm new to OmniDocs","text":"<ol> <li>Start with Architecture Overview to understand the system design</li> <li>Read Document Model to learn how to load and access documents</li> <li>Skim Config Pattern to understand parameter organization</li> </ol>"},{"location":"concepts/#i-want-to-add-a-new-model","title":"I want to add a new model","text":"<ol> <li>Read Architecture Overview for design principles</li> <li>Check Config Pattern for how to structure configs</li> <li>Refer to Backend System if supporting multiple backends</li> </ol>"},{"location":"concepts/#im-deploying-to-production","title":"I'm deploying to production","text":"<ol> <li>Review Backend System for trade-offs</li> <li>Check the comparison table to choose VLLM or API</li> <li>See configuration options in Config Pattern</li> </ol>"},{"location":"concepts/#im-optimizing-memory-usage","title":"I'm optimizing memory usage","text":"<ol> <li>Read Document Model for lazy loading and caching</li> <li>Check memory management strategies for large documents</li> <li>Use <code>iter_pages()</code> and <code>clear_cache()</code> patterns shown in examples</li> </ol>"},{"location":"concepts/#quick-reference","title":"Quick Reference","text":""},{"location":"concepts/#single-backend-model","title":"Single-Backend Model","text":"<pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\nconfig = DocLayoutYOLOConfig(device=\"cuda\")\nextractor = DocLayoutYOLO(config=config)\nresult = extractor.extract(image)\n</code></pre> <p>Pattern: <code>{Model}Config</code> \u2192 <code>config=</code> parameter</p>"},{"location":"concepts/#multi-backend-model","title":"Multi-Backend Model","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nconfig = QwenTextPyTorchConfig(device=\"cuda\")\nextractor = QwenTextExtractor(backend=config)\nresult = extractor.extract(image, output_format=\"markdown\")\n</code></pre> <p>Pattern: <code>{Model}{Backend}Config</code> \u2192 <code>backend=</code> parameter</p>"},{"location":"concepts/#config-validation","title":"Config Validation","text":"<pre><code>from pydantic import BaseModel, Field, ConfigDict\n\nclass MyConfig(BaseModel):\n    param1: str = Field(..., description=\"Required parameter\")\n    param2: int = Field(default=10, ge=1, le=100)\n    param3: Literal[\"a\", \"b\"] = Field(default=\"a\")\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n# Validated at creation time\nconfig = MyConfig(param1=\"value\")\n</code></pre>"},{"location":"concepts/#document-loading","title":"Document Loading","text":"<pre><code>from omnidocs import Document\n\n# Multiple source formats\ndoc = Document.from_pdf(\"file.pdf\")\ndoc = Document.from_url(\"https://example.com/doc.pdf\")\ndoc = Document.from_bytes(pdf_bytes)\ndoc = Document.from_image(\"page.png\")\ndoc = Document.from_images([\"p1.png\", \"p2.png\"])\n\n# Memory efficient processing\nfor page in doc.iter_pages():\n    result = extractor.extract(page)\n    doc.clear_cache()\n</code></pre>"},{"location":"concepts/#backend-selection","title":"Backend Selection","text":"<pre><code># PyTorch: Development\nQwenTextPyTorchConfig(device=\"cuda\", torch_dtype=\"bfloat16\")\n\n# VLLM: Production high-throughput\nQwenTextVLLMConfig(tensor_parallel_size=4, gpu_memory_utilization=0.9)\n\n# MLX: Apple Silicon\nQwenTextMLXConfig(quantization=\"4bit\")\n\n# API: Managed services\nQwenTextAPIConfig(api_key=\"sk-...\", base_url=\"https://...\")\n</code></pre>"},{"location":"concepts/#key-design-decisions","title":"Key Design Decisions","text":"Decision Benefit Unified <code>.extract()</code> API Easy to swap models, consistent interface Pydantic configs Type validation, IDE autocomplete, documentation Multi-backend support Choose infrastructure (GPU, Apple Silicon, API) Stateless Document Clear separation: source data vs. analysis Config-driven backend Backend selection explicit and discoverable Lazy page loading Memory efficient for large documents Separation of init vs extract Model setup vs. task parameters clear"},{"location":"concepts/#understanding-the-trade-offs","title":"Understanding the Trade-Offs","text":""},{"location":"concepts/#memory-vs-speed","title":"Memory vs Speed","text":"<ul> <li>Render all pages at once (fast access) vs lazy render (low memory)</li> <li>Solution: Lazy render with caching (best of both)</li> </ul>"},{"location":"concepts/#setup-complexity-vs-throughput","title":"Setup Complexity vs Throughput","text":"<ul> <li>PyTorch (simple setup, single GPU) vs VLLM (complex setup, high throughput)</li> <li>Solution: Choose based on deployment scenario</li> </ul>"},{"location":"concepts/#privacy-vs-convenience","title":"Privacy vs Convenience","text":"<ul> <li>Local inference (private data, controlled) vs API (managed infrastructure, simple)</li> <li>Solution: Support all options, user chooses</li> </ul>"},{"location":"concepts/#flexibility-vs-discoverability","title":"Flexibility vs Discoverability","text":"<ul> <li>Magic strings (flexible, unclear) vs config types (clear, discoverable)</li> <li>Solution: Config types make available options obvious</li> </ul>"},{"location":"concepts/#common-patterns","title":"Common Patterns","text":""},{"location":"concepts/#pattern-single-page-processing","title":"Pattern: Single Page Processing","text":"<pre><code>doc = Document.from_pdf(\"paper.pdf\")\npage = doc.get_page(0)\nresult = extractor.extract(page)\n</code></pre>"},{"location":"concepts/#pattern-multi-page-with-memory-control","title":"Pattern: Multi-Page with Memory Control","text":"<pre><code>doc = Document.from_pdf(\"large_book.pdf\")\nfor i, page in enumerate(doc.iter_pages()):\n    result = extractor.extract(page)\n    if i &gt; 0:\n        doc.clear_cache(i - 1)  # Free previous page\n</code></pre>"},{"location":"concepts/#pattern-conditional-backend-selection","title":"Pattern: Conditional Backend Selection","text":"<pre><code>import os\n\nbackend = os.environ.get(\"BACKEND\", \"pytorch\")\nif backend == \"vllm\":\n    config = QwenTextVLLMConfig(...)\nelse:\n    config = QwenTextPyTorchConfig(...)\n\nextractor = QwenTextExtractor(backend=config)\n</code></pre>"},{"location":"concepts/#pattern-batch-processing-with-different-outputs","title":"Pattern: Batch Processing with Different Outputs","text":"<pre><code>extractor = QwenTextExtractor(backend=QwenTextVLLMConfig(...))\n\nresults = []\nfor page in doc.iter_pages():\n    # Same extractor, different outputs\n    markdown = extractor.extract(page, output_format=\"markdown\")\n    html = extractor.extract(page, output_format=\"html\")\n    results.append({\"markdown\": markdown, \"html\": html})\n</code></pre>"},{"location":"concepts/#document-statistics","title":"Document Statistics","text":"<ul> <li>Total: ~8,600 words across 4 documents</li> <li>Architecture Overview: ~2,100 words (system design, principles, flow)</li> <li>Document Model: ~2,400 words (lazy loading, memory, methods)</li> <li>Backend System: ~2,000 words (four backends, selection, trade-offs)</li> <li>Config Pattern: ~2,000 words (single vs multi-backend, validation)</li> </ul>"},{"location":"concepts/#relationship-to-other-documentation","title":"Relationship to Other Documentation","text":"<ul> <li>CLAUDE.md: Development workflow and implementation guide (how to add features)</li> <li>docs/concepts/: Architecture and design (why it's built this way)</li> <li>docs/guides/: Practical usage (how to use OmniDocs)</li> <li>docs/api/: API reference (what each class does)</li> </ul>"},{"location":"concepts/#next-steps","title":"Next Steps","text":"<ul> <li>For Usage: See <code>docs/guides/</code> for practical examples</li> <li>For API Reference: See <code>docs/api/</code> for method signatures</li> <li>For Development: See <code>CLAUDE.md</code> for contribution workflow</li> <li>For Theory: Continue reading concept documents</li> </ul> <p>Last Updated: February 2026 Maintained By: Adithya S Kolavi Status: Complete v1.0</p>"},{"location":"concepts/architecture-overview/","title":"OmniDocs Architecture Overview","text":"<p>Core Philosophy: Unified interface, type-safe configurations, multi-backend support, and stateless documents.</p>"},{"location":"concepts/architecture-overview/#table-of-contents","title":"Table of Contents","text":"<ol> <li>System Design Principles</li> <li>Component Architecture</li> <li>Data Flow</li> <li>Core Design Decisions</li> <li>How It Works (Nothing Magic)</li> <li>Example: Qwen Text Extraction Flow</li> </ol>"},{"location":"concepts/architecture-overview/#system-design-principles","title":"System Design Principles","text":"<p>OmniDocs is built on six core principles that shape every design decision:</p>"},{"location":"concepts/architecture-overview/#1-unified-api","title":"1. Unified API","text":"<p>Problem Solved: Different document processing tasks (layout analysis, OCR, text extraction) come from different libraries, each with different APIs.</p> <p>Solution: All extractors implement the same <code>.extract()</code> method.</p> <pre><code># All these follow the same pattern\nlayout_result = layout_extractor.extract(image)\nocr_result = ocr_extractor.extract(image)\ntext_result = text_extractor.extract(image, output_format=\"markdown\")\n</code></pre> <p>This consistency makes it trivial to swap models or add new tasks - the user code doesn't change.</p>"},{"location":"concepts/architecture-overview/#2-type-safe-configurations","title":"2. Type-Safe Configurations","text":"<p>Problem Solved: Magic strings and untyped dictionaries make it impossible to catch configuration errors early.</p> <p>Solution: Pydantic-based config classes with IDE autocomplete and validation.</p> <pre><code># Config is validated at creation time\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    torch_dtype=\"bfloat16\",  # Wrong! IDE catches this\n    device=\"cuda\",\n)\n# Error: 'torch_dtype' should be \"float16\", \"bfloat16\", \"float32\", or \"auto\"\n</code></pre>"},{"location":"concepts/architecture-overview/#3-multi-backend-support","title":"3. Multi-Backend Support","text":"<p>Problem Solved: Not all users have the same hardware. A researcher might use PyTorch locally, while a startup deploys on VLLM, and another team uses MLX on Apple Silicon.</p> <p>Solution: The same model can run on multiple backends. Backend selection is explicit via config classes.</p> <pre><code># Switch backends by changing the config type - same extractor\nextractor = QwenTextExtractor(backend=QwenTextPyTorchConfig(...))\nextractor = QwenTextExtractor(backend=QwenTextVLLMConfig(...))\nextractor = QwenTextExtractor(backend=QwenTextMLXConfig(...))\n</code></pre>"},{"location":"concepts/architecture-overview/#4-stateless-document","title":"4. Stateless Document","text":"<p>Problem Solved: Models that store extraction results create confusion about what's source data vs. analysis output, and make it hard to run multiple tasks on the same document.</p> <p>Solution: The <code>Document</code> class only holds source data (PDF bytes, pages). Users manage task results.</p> <pre><code># Document doesn't know about extraction results\ndoc = Document.from_pdf(\"paper.pdf\")\npage = doc.get_page(0)  # Returns PIL Image\n\n# Different extractors process the same page independently\nlayout = layout_extractor.extract(page)\nocr = ocr_extractor.extract(page)\ntext = text_extractor.extract(page)\n\n# Users combine results as needed\ncombined = {\n    \"layout\": layout,\n    \"ocr\": ocr,\n    \"text\": text,\n}\n</code></pre>"},{"location":"concepts/architecture-overview/#5-separation-of-init-vs-extract","title":"5. Separation of Init vs Extract","text":"<p>Problem Solved: It's unclear what configuration belongs to model setup vs. task parameters.</p> <p>Solution: Clear separation in method signatures.</p> <pre><code># Init (via config) = Model setup, happens once\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",  # Model choice\n        device=\"cuda\",                      # Hardware\n        torch_dtype=\"bfloat16\",             # Quantization\n    )\n)\n\n# Extract (method call) = Task parameters, changes per call\nresult = extractor.extract(\n    image,\n    output_format=\"markdown\",   # Task parameter\n    include_layout=True,        # Task-specific option\n)\n</code></pre>"},{"location":"concepts/architecture-overview/#6-backend-discoverability","title":"6. Backend Discoverability","text":"<p>Problem Solved: Users don't know which backends a model supports.</p> <p>Solution: If you can import a config class, that backend is supported.</p> <pre><code># These imports work = backends are supported\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenTextPyTorchConfig,  \u2713 PyTorch backend\n    QwenTextVLLMConfig,     \u2713 VLLM backend\n    QwenTextMLXConfig,      \u2713 MLX backend\n    QwenTextAPIConfig,      \u2713 API backend\n)\n\n# This import fails = backend not implemented yet\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextONNXConfig  # ImportError\n</code></pre>"},{"location":"concepts/architecture-overview/#component-architecture","title":"Component Architecture","text":"<p>OmniDocs has four main components:</p>"},{"location":"concepts/architecture-overview/#1-document-class","title":"1. Document Class","text":"<p>The entry point for loading and accessing document data.</p> <pre><code>Document (stateless)\n\u251c\u2500\u2500 from_pdf(path)           \u2192 Load PDF file\n\u251c\u2500\u2500 from_url(url)            \u2192 Download and load PDF\n\u251c\u2500\u2500 from_bytes(data)         \u2192 Load from memory\n\u251c\u2500\u2500 from_image(path)         \u2192 Load single image\n\u2514\u2500\u2500 from_images(paths)       \u2192 Load multi-page images\n\nDocument properties:\n\u251c\u2500\u2500 page_count: int          \u2192 Number of pages\n\u251c\u2500\u2500 metadata: Metadata       \u2192 Source info, DPI, etc.\n\u251c\u2500\u2500 get_page(i): Image       \u2192 Get single page (0-indexed)\n\u251c\u2500\u2500 iter_pages(): Iterator   \u2192 Iterate pages (memory efficient)\n\u2514\u2500\u2500 text: str                \u2192 Full document text (cached)\n</code></pre> <p>Key Design: Lazy page rendering. Pages are NOT rendered until accessed. This enables: - Fast document loading (no rendering upfront) - Memory efficiency (only rendered pages stay in RAM) - Page-level caching (rendered pages cached automatically)</p>"},{"location":"concepts/architecture-overview/#2-extractors","title":"2. Extractors","text":"<p>Task-specific processors. All inherit from a base class and implement <code>.extract()</code>.</p> <pre><code>BaseTextExtractor (abstract)\n\u251c\u2500\u2500 _load_model() \u2192 Load model into memory\n\u2514\u2500\u2500 extract(image, ...) \u2192 Process image\n\nImplementations:\n\u251c\u2500\u2500 QwenTextExtractor (multi-backend)\n\u251c\u2500\u2500 DotsOCRTextExtractor (multi-backend)\n\u2514\u2500\u2500 Other models...\n</code></pre> <p>Each extractor handles: - Model loading and device placement - Input preprocessing - Inference - Output formatting</p>"},{"location":"concepts/architecture-overview/#3-config-classes","title":"3. Config Classes","text":"<p>Pydantic models that specify how to initialize extractors.</p> <p>Single-Backend Model: <pre><code>DocLayoutYOLOConfig\n\u2514\u2500\u2500 Parameters for PyTorch inference only\n\nQwenTextPyTorchConfig\n\u2514\u2500\u2500 Parameters for PyTorch inference of Qwen\n</code></pre></p> <p>Multi-Backend Model: <pre><code>QwenTextPyTorchConfig    \u2192 PyTorch backend\nQwenTextVLLMConfig       \u2192 VLLM backend\nQwenTextMLXConfig        \u2192 MLX backend\nQwenTextAPIConfig        \u2192 API backend\n</code></pre></p> <p>Configs drive: - Which backend to use (via <code>isinstance</code> checks in <code>_load_model()</code>) - Model hyperparameters - Device placement - Quantization settings</p>"},{"location":"concepts/architecture-overview/#4-backend-inference","title":"4. Backend Inference","text":"<p>Low-level inference implementations. Hidden from users (internal).</p> <pre><code>Backend Inference Hierarchy:\n\nFor each model + backend combination:\n\u251c\u2500\u2500 QwenTextExtractor receives QwenTextPyTorchConfig\n\u2502   \u2514\u2500\u2500 _load_model() detects it's PyTorch\n\u2502       \u2514\u2500\u2500 Initialize PyTorch model, processor\n\u2502       \u2514\u2500\u2500 Store for use in extract()\n\u2502\n\u251c\u2500\u2500 QwenTextExtractor receives QwenTextVLLMConfig\n\u2502   \u2514\u2500\u2500 _load_model() detects it's VLLM\n\u2502       \u2514\u2500\u2500 Initialize VLLM server, engine\n\u2502       \u2514\u2500\u2500 Store for use in extract()\n\u2502\n\u2514\u2500\u2500 etc for MLX, API...\n</code></pre> <p>Users never interact with backend code directly - it's all encapsulated in the extractor.</p>"},{"location":"concepts/architecture-overview/#data-flow","title":"Data Flow","text":""},{"location":"concepts/architecture-overview/#single-page-processing","title":"Single-Page Processing","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  User Code                                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502\n                   \u25bc\n        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n        \u2502   Document         \u2502\n        \u2502 .from_pdf(\"x.pdf\") \u2502\n        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                   \u2502\n                   \u25bc\n     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n     \u2502 document.get_page(0)         \u2502\n     \u2502 Returns: PIL.Image (RGB)     \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                \u2502\n                \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Extractor                 \u2502\n    \u2502  (e.g., QwenTextExtractor) \u2502\n    \u2502  .extract(image)           \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Backend _load_model()      \u2502\n    \u2502  (detects config type)      \u2502\n    \u2502  (loads model once)         \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Image Preprocessing        \u2502\n    \u2502  PIL \u2192 numpy or model input \u2502\n    \u2502  Normalization, resizing    \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Model Inference            \u2502\n    \u2502  (GPU/CPU/API)              \u2502\n    \u2502  Returns: model output      \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Post-processing            \u2502\n    \u2502  Parse model output         \u2502\n    \u2502  Format as structured data  \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2502\n               \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Return TextOutput         \u2502\n    \u2502  (Pydantic model)          \u2502\n    \u2502  with content, metadata    \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concepts/architecture-overview/#multi-page-processing-memory-efficient","title":"Multi-Page Processing (Memory Efficient)","text":"<pre><code>for page in doc.iter_pages():\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502  Load page (lazy)    \u2502 \u2190 Only rendered when accessed\n    \u2502  Render to image     \u2502\n    \u2502  Send to extractor   \u2502\n    \u2502  Get result          \u2502\n    \u2502  Store result        \u2502 \u2190 User manages results\n    \u2502  Clear page cache    \u2502 \u2190 Memory freed\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"concepts/architecture-overview/#core-design-decisions","title":"Core Design Decisions","text":""},{"location":"concepts/architecture-overview/#why-pydantic-for-configs","title":"Why Pydantic for Configs?","text":"<p>Benefits: 1. Validation at creation time: Errors caught immediately 2. IDE autocomplete: All parameters visible in editor 3. Documentation: Field descriptions in docstrings and type hints 4. Serialization: Configs can be saved/loaded as JSON 5. Immutability: <code>frozen=True</code> prevents accidental changes</p> <pre><code># IDE shows all valid parameters with descriptions\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",  # \u2190 IDE autocomplete\n    torch_dtype=\"bfloat16\",              # \u2190 Type validation\n    device=\"cuda\",\n)\n\n# Typo caught immediately (Pydantic validation)\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    torch_dtype=\"float64\",               # ERROR: Invalid literal\n)\n</code></pre>"},{"location":"concepts/architecture-overview/#why-stateless-document","title":"Why Stateless Document?","text":"<p>Problem with Stateful Document: <pre><code># BAD: Document stores results\ndoc = Document.from_pdf(\"paper.pdf\")\ndoc.extract_layout()  # Stores layout internally\ndoc.extract_text()    # Stores text internally\n\n# Now what is doc.get_page(0)?\n# Is it the page, or the extracted data?\n# Can I access original pixels?\n</code></pre></p> <p>Solution: Stateless Document: <pre><code># GOOD: Document is just source data\ndoc = Document.from_pdf(\"paper.pdf\")\npage = doc.get_page(0)  # \u2190 Always returns PIL Image\n\n# Extractors are independent\nlayout = layout_extractor.extract(page)\ntext = text_extractor.extract(page)\n\n# User combines results as needed\nresult = {\"layout\": layout, \"text\": text}\n</code></pre></p> <p>Benefits: - Clear separation: Document = source, Extractors = analysis - Reuse: Run multiple tasks on same pages - Composition: Easy to build pipelines - Caching: User controls what to cache</p>"},{"location":"concepts/architecture-overview/#why-backend-selection-via-config-type","title":"Why Backend Selection via Config Type?","text":"<p>Alternative (Magic String): <pre><code># BAD: Which backends are available?\nextractor = QwenTextExtractor(backend=\"pytorch\")\n# No IDE support, typos not caught, unclear what's available\n</code></pre></p> <p>Solution (Config Type): <pre><code># GOOD: IDE shows available configs\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(...)  # IDE autocomplete shows all options\n)\nextractor = QwenTextExtractor(\n    backend=QwenTextVLLMConfig(...)     # Obvious which are available\n)\n</code></pre></p> <p>How It Works: <pre><code>def _load_model(self) -&gt; None:\n    \"\"\"Load appropriate backend based on config type.\"\"\"\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"QwenTextPyTorchConfig\":\n        # Load PyTorch model\n    elif config_type == \"QwenTextVLLMConfig\":\n        # Load VLLM engine\n    elif config_type == \"QwenTextMLXConfig\":\n        # Load MLX model\n    # etc\n</code></pre></p>"},{"location":"concepts/architecture-overview/#how-it-works-nothing-magic","title":"How It Works (Nothing Magic)","text":""},{"location":"concepts/architecture-overview/#load-model-first-time-only","title":"Load Model (First Time Only)","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cuda\",\n    torch_dtype=\"bfloat16\",\n)\n\nextractor = QwenTextExtractor(backend=config)\n# This calls __init__ \u2192 _load_model()\n</code></pre> <p>What happens in <code>_load_model()</code>:</p> <ol> <li> <p>Detect config type <pre><code>config_type = type(self.backend_config).__name__\n# \"QwenTextPyTorchConfig\"\n</code></pre></p> </li> <li> <p>Load model code <pre><code>from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\nmodel = Qwen2VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen3-VL-8B-Instruct\",\n    torch_dtype=torch.bfloat16,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\nprocessor = AutoProcessor.from_pretrained(...)\n</code></pre></p> </li> <li> <p>Store for later use <pre><code>self._model = model\nself._processor = processor\n</code></pre></p> </li> </ol>"},{"location":"concepts/architecture-overview/#extract-every-time","title":"Extract (Every Time)","text":"<pre><code>doc = Document.from_pdf(\"paper.pdf\")\npage = doc.get_page(0)  # PIL Image\n\nresult = extractor.extract(page, output_format=\"markdown\")\n</code></pre> <p>What happens in <code>extract()</code>:</p> <ol> <li> <p>Prepare image <pre><code>if isinstance(page, Image.Image):\n    image = page.convert(\"RGB\")  # Ensure RGB\n</code></pre></p> </li> <li> <p>Process with vision encoder <pre><code>inputs = self._processor(\n    text=\"Extract text in markdown format\",\n    images=image,\n    return_tensors=\"pt\",\n)\n</code></pre></p> </li> <li> <p>Run model inference <pre><code>with torch.no_grad():\n    outputs = self._model.generate(**inputs)\n</code></pre></p> </li> <li> <p>Decode and clean <pre><code>raw_text = self._processor.decode(outputs[0])\ncleaned = _clean_markdown_output(raw_text)\n</code></pre></p> </li> <li> <p>Return structured output <pre><code>return TextOutput(\n    content=cleaned,\n    format=\"markdown\",\n    metadata={\"model\": \"Qwen3-VL-8B\"},\n)\n</code></pre></p> </li> </ol>"},{"location":"concepts/architecture-overview/#example-qwen-text-extraction-flow","title":"Example: Qwen Text Extraction Flow","text":"<p>Let's trace a complete example from start to finish.</p>"},{"location":"concepts/architecture-overview/#setup-phase","title":"Setup Phase","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# 1. Create config (validated immediately)\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cuda\",\n    torch_dtype=\"bfloat16\",\n    max_new_tokens=8192,\n)\n# \u2713 Pydantic validates all fields\n# \u2713 No typos possible (IDE catches them)\n\n# 2. Create extractor (loads model)\nextractor = QwenTextExtractor(backend=config)\n# \u2192 __init__ called\n# \u2192 _load_model() called\n# \u2192 Detects \"QwenTextPyTorchConfig\"\n# \u2192 Imports transformers, loads Qwen3-VL model\n# \u2192 Loads processor for vision encoding\n# \u2192 Stores both in self._model, self._processor\n</code></pre>"},{"location":"concepts/architecture-overview/#load-document-phase","title":"Load Document Phase","text":"<pre><code># 3. Load document (lazy, no page rendering yet)\ndoc = Document.from_pdf(\"research_paper.pdf\")\n# \u2192 Reads PDF file bytes\n# \u2192 Creates pypdfium2 document object\n# \u2192 Creates metadata (page count, file size, etc.)\n# \u2192 Creates LazyPage wrappers (not rendered)\n# \u2192 Returns Document\n\nprint(doc.page_count)  # 12 pages\nprint(doc.metadata.file_name)  # \"research_paper.pdf\"\n</code></pre>"},{"location":"concepts/architecture-overview/#process-pages-phase","title":"Process Pages Phase","text":"<pre><code># 4. Process first page\npage = doc.get_page(0)\n# \u2192 Accesses LazyPage[0]\n# \u2192 Renders page to PIL Image (150 DPI default)\n# \u2192 Caches image in LazyPage\n# \u2192 Returns PIL Image\n\n# 5. Extract text from page\nresult = extractor.extract(\n    page,\n    output_format=\"markdown\",\n)\n# \u2192 Validates image input\n# \u2192 Processes with processor\n# \u2192 Runs model inference on GPU\n# \u2192 Cleans markdown output\n# \u2192 Returns TextOutput(content=..., format=\"markdown\")\n\nprint(result.content[:200])  # \"# Research Paper Title\\n\\n## Abstract\\n...\"\n</code></pre>"},{"location":"concepts/architecture-overview/#scale-to-multiple-pages-phase","title":"Scale to Multiple Pages Phase","text":"<pre><code># 6. Process all pages efficiently\nresults = []\nfor i, page in enumerate(doc.iter_pages()):\n    # Each iteration:\n    # - Loads next page (lazy)\n    # - Renders to image (cached)\n    # - Extracts text via Qwen\n    # - Clears cache to free memory\n    result = extractor.extract(page, output_format=\"markdown\")\n    results.append(result)\n\n    if i &gt; 0:\n        doc.clear_cache(i - 1)  # Free previous page from memory\n\n# Results list: [TextOutput, TextOutput, ...]\nfull_content = \"\\n\\n\".join([r.content for r in results])\n</code></pre>"},{"location":"concepts/architecture-overview/#key-points-in-this-flow","title":"Key Points in This Flow","text":"Step Component Purpose Config creation Pydantic Validate parameters upfront Model loading Extractor._load_model() One-time initialization Document loading Document.from_pdf() Lazy, fast, no rendering Page access doc.get_page() Renders on demand, caches Extraction extractor.extract() Runs model on page Memory management doc.clear_cache() Frees rendered pages"},{"location":"concepts/architecture-overview/#why-this-design-matters","title":"Why This Design Matters","text":"<pre><code># Without lazy loading\ndoc = Document.from_pdf(\"500_page_book.pdf\")\n# Would need to render all 500 pages in memory = SLOW, HIGH MEMORY\n\n# With lazy loading + caching\nfor page in doc.iter_pages():\n    result = extractor.extract(page)\n    doc.clear_cache()  # Free each page\n# Only 1 page in memory at a time = FAST, LOW MEMORY\n</code></pre>"},{"location":"concepts/architecture-overview/#summary","title":"Summary","text":"<p>OmniDocs architecture is built on six principles:</p> <ol> <li>Unified API - All extractors implement <code>.extract()</code></li> <li>Type-Safe Configs - Pydantic validates at creation time</li> <li>Multi-Backend Support - Same model, different hardware</li> <li>Stateless Document - Source data separate from analysis</li> <li>Separation of Init vs Extract - Clear parameter ownership</li> <li>Backend Discoverability - Imports reveal what's available</li> </ol> <p>The result is a system that: - Catches errors early (type validation) - Scales efficiently (lazy loading, memory management) - Remains flexible (swap models or backends easily) - Stays understandable (clear data flow, no magic)</p>"},{"location":"concepts/architecture-overview/#next-steps","title":"Next Steps","text":"<ul> <li>See Document Model for details on loading and accessing documents</li> <li>See Backend System for understanding multi-backend support</li> <li>See Config Pattern for designing extractors</li> </ul>"},{"location":"concepts/backend-system/","title":"Backend System","text":"<p>Core Principle: The same model can run on different backends. Backend selection is explicit via config classes and detected at runtime.</p>"},{"location":"concepts/backend-system/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Four Backends</li> <li>How Backend Selection Works</li> <li>Config Drives Backend</li> <li>Lazy Imports</li> <li>Adding New Backends</li> <li>Backend Trade-Offs</li> <li>Real Code Examples</li> </ol>"},{"location":"concepts/backend-system/#four-backends","title":"Four Backends","text":"<p>OmniDocs supports four inference backends, each optimized for different scenarios:</p>"},{"location":"concepts/backend-system/#1-pytorch-backend","title":"1. PyTorch Backend","text":"<p>Use When: Local GPU inference, development, small-to-medium workloads</p> <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n        device_map=\"auto\",\n    )\n)\n</code></pre> <p>Dependencies: <code>torch</code>, <code>transformers</code>, <code>accelerate</code></p> <p>Best For: - Development and prototyping - Single GPU inference - When you control the environment - Interactive notebooks</p> <p>Limitations: - Single GPU only (use tensor parallelism for multi-GPU) - Must manage memory explicitly - No built-in batching optimization</p>"},{"location":"concepts/backend-system/#2-vllm-backend","title":"2. VLLM Backend","text":"<p>Use When: Production serving, high throughput, multi-GPU</p> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\n\nextractor = QwenTextExtractor(\n    backend=QwenTextVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=4,  # 4 GPUs\n        gpu_memory_utilization=0.9,\n        max_model_len=8192,\n    )\n)\n</code></pre> <p>Dependencies: <code>vllm</code>, <code>torch</code></p> <p>Best For: - Production inference servers - High throughput requirements - Multiple requests in parallel - Multi-GPU deployments</p> <p>Limitations: - Requires CUDA (GPU only) - More complex setup - Higher memory overhead</p>"},{"location":"concepts/backend-system/#3-mlx-backend","title":"3. MLX Backend","text":"<p>Use When: Apple Silicon (M1, M2, M3+), local inference without GPU cost</p> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextMLXConfig\n\nextractor = QwenTextExtractor(\n    backend=QwenTextMLXConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct-MLX\",\n        quantization=\"4bit\",\n    )\n)\n</code></pre> <p>Dependencies: <code>mlx</code>, <code>mlx-lm</code></p> <p>Best For: - Development on Apple Silicon Macs - Local inference without cloud costs - Battery efficiency (can use GPU without NVIDIA) - Privacy-focused applications</p> <p>Limitations: - Apple Silicon only (M1, M2, M3+) - Fewer models available in MLX format - Potentially slower than GPU for large models</p>"},{"location":"concepts/backend-system/#4-api-backend","title":"4. API Backend","text":"<p>Use When: Hosted models, no local GPU needed, managed inference</p> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextAPIConfig\n\nextractor = QwenTextExtractor(\n    backend=QwenTextAPIConfig(\n        model=\"qwen-vision-max\",\n        api_key=\"sk-...\",\n        base_url=\"https://api.openrouter.co/openai/v1\",\n        rate_limit=10,  # requests per second\n    )\n)\n</code></pre> <p>Dependencies: <code>litellm</code>, <code>requests</code></p> <p>Best For: - Minimal setup (no local GPU) - Managed infrastructure - Cost-per-request billing - Using cloud-hosted models</p> <p>Limitations: - Network latency - API rate limits - Dependency on external service - Cost per request</p>"},{"location":"concepts/backend-system/#how-backend-selection-works","title":"How Backend Selection Works","text":""},{"location":"concepts/backend-system/#the-detection-mechanism","title":"The Detection Mechanism","text":"<p>When you initialize an extractor with a config, the <code>_load_model()</code> method detects which backend to use by checking the config class name:</p> <pre><code>class QwenTextExtractor(BaseTextExtractor):\n    def __init__(self, backend: QwenTextBackendConfig):\n        self.backend_config = backend\n        self._load_model()\n\n    def _load_model(self) -&gt; None:\n        \"\"\"Load appropriate backend based on config type.\"\"\"\n        config_type = type(self.backend_config).__name__\n\n        if config_type == \"QwenTextPyTorchConfig\":\n            self._load_pytorch()\n        elif config_type == \"QwenTextVLLMConfig\":\n            self._load_vllm()\n        elif config_type == \"QwenTextMLXConfig\":\n            self._load_mlx()\n        elif config_type == \"QwenTextAPIConfig\":\n            self._load_api()\n        else:\n            raise ValueError(f\"Unknown backend: {config_type}\")\n</code></pre>"},{"location":"concepts/backend-system/#why-type-checking","title":"Why Type Checking?","text":"<p>Using <code>isinstance()</code> or type name checking has important advantages:</p> <pre><code># Using isinstance() - Pythonic\nif isinstance(self.backend_config, QwenTextPyTorchConfig):\n    self._load_pytorch()\n\n# Using type name - More robust to import issues\nconfig_type = type(self.backend_config).__name__\nif config_type == \"QwenTextPyTorchConfig\":\n    self._load_pytorch()\n</code></pre> <p>Type name approach benefits: - Works even if you only import the config type-checked (via <code>TYPE_CHECKING</code>) - No circular imports - Cleaner code structure</p>"},{"location":"concepts/backend-system/#example-full-backend-detection","title":"Example: Full Backend Detection","text":"<pre><code>from typing import TYPE_CHECKING, Union\n\nif TYPE_CHECKING:\n    from .pytorch import QwenTextPyTorchConfig\n    from .vllm import QwenTextVLLMConfig\n\nclass QwenTextExtractor(BaseTextExtractor):\n    def __init__(self, backend: Union[\"QwenTextPyTorchConfig\", \"QwenTextVLLMConfig\"]):\n        self.backend_config = backend\n        self._load_model()\n\n    def _load_model(self) -&gt; None:\n        config_type = type(self.backend_config).__name__\n\n        if config_type == \"QwenTextPyTorchConfig\":\n            # Import only if actually using this backend\n            from transformers import AutoModel\n            # ... actual loading code\n\n        elif config_type == \"QwenTextVLLMConfig\":\n            # Import only if actually using this backend\n            from vllm import LLM\n            # ... actual loading code\n</code></pre>"},{"location":"concepts/backend-system/#config-drives-backend","title":"Config Drives Backend","text":""},{"location":"concepts/backend-system/#single-vs-multi-backend-models","title":"Single vs Multi-Backend Models","text":"<p>The presence of config classes determines backend support:</p>"},{"location":"concepts/backend-system/#single-backend-model-doclayoutyolo","title":"Single-Backend Model: DocLayoutYOLO","text":"<pre><code># Only one config class = only one backend\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLOConfig\n\n# Can only be initialized one way\nextractor = DocLayoutYOLO(config=DocLayoutYOLOConfig(...))\n\n# Other config types don't exist (ImportError)\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLOVLLMConfig  # ImportError!\n</code></pre> <p>File structure: <pre><code>omnidocs/tasks/layout_extraction/\n\u251c\u2500\u2500 doc_layout_yolo.py\n\u2502   \u2514\u2500\u2500 DocLayoutYOLOConfig  \u2190 Single config\n\u2502   \u2514\u2500\u2500 DocLayoutYOLO        \u2190 Single backend (PyTorch)\n</code></pre></p>"},{"location":"concepts/backend-system/#multi-backend-model-qwentextextractor","title":"Multi-Backend Model: QwenTextExtractor","text":"<pre><code># Four config classes = four backends\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenTextPyTorchConfig,    # \u2190 PyTorch backend\n    QwenTextVLLMConfig,       # \u2190 VLLM backend\n    QwenTextMLXConfig,        # \u2190 MLX backend\n    QwenTextAPIConfig,        # \u2190 API backend\n)\n\n# Can be initialized multiple ways\nextractor1 = QwenTextExtractor(backend=QwenTextPyTorchConfig(...))\nextractor2 = QwenTextExtractor(backend=QwenTextVLLMConfig(...))\nextractor3 = QwenTextExtractor(backend=QwenTextMLXConfig(...))\nextractor4 = QwenTextExtractor(backend=QwenTextAPIConfig(...))\n</code></pre> <p>File structure: <pre><code>omnidocs/tasks/text_extraction/\n\u251c\u2500\u2500 qwen/\n\u2502   \u251c\u2500\u2500 pytorch.py\n\u2502   \u2502   \u2514\u2500\u2500 QwenTextPyTorchConfig\n\u2502   \u251c\u2500\u2500 vllm.py\n\u2502   \u2502   \u2514\u2500\u2500 QwenTextVLLMConfig\n\u2502   \u251c\u2500\u2500 mlx.py\n\u2502   \u2502   \u2514\u2500\u2500 QwenTextMLXConfig\n\u2502   \u251c\u2500\u2500 api.py\n\u2502   \u2502   \u2514\u2500\u2500 QwenTextAPIConfig\n\u2502   \u2514\u2500\u2500 extractor.py\n\u2502       \u2514\u2500\u2500 QwenTextExtractor \u2190 Uses all configs\n</code></pre></p>"},{"location":"concepts/backend-system/#how-config-determines-behavior","title":"How Config Determines Behavior","text":"<p>Each config contains parameters specific to its backend:</p> <pre><code># PyTorch config has torch-specific parameters\nclass QwenTextPyTorchConfig(BaseModel):\n    device: str = \"cuda\"\n    torch_dtype: Literal[\"float16\", \"bfloat16\", \"float32\"]\n    device_map: Optional[str] = \"auto\"\n    use_flash_attention: bool = False\n\n# VLLM config has VLLM-specific parameters\nclass QwenTextVLLMConfig(BaseModel):\n    tensor_parallel_size: int = 1\n    gpu_memory_utilization: float = 0.9\n    enforce_eager: bool = False\n\n# MLX config has MLX-specific parameters\nclass QwenTextMLXConfig(BaseModel):\n    quantization: Literal[\"4bit\", \"8bit\", None] = None\n    max_tokens: int = 2048\n\n# API config has API-specific parameters\nclass QwenTextAPIConfig(BaseModel):\n    api_key: str\n    base_url: Optional[str] = None\n    rate_limit: int = 10\n    timeout: int = 60\n</code></pre> <p>When the extractor loads the model, it uses these backend-specific parameters:</p> <pre><code>def _load_pytorch(self):\n    from transformers import AutoModel\n    config = self.backend_config  # QwenTextPyTorchConfig\n\n    model = AutoModel.from_pretrained(\n        config.model,\n        device_map=config.device_map,  # From config\n        torch_dtype=config.torch_dtype,  # From config\n    )\n\ndef _load_vllm(self):\n    from vllm import LLM\n    config = self.backend_config  # QwenTextVLLMConfig\n\n    model = LLM(\n        model=config.model,\n        tensor_parallel_size=config.tensor_parallel_size,  # From config\n        gpu_memory_utilization=config.gpu_memory_utilization,  # From config\n    )\n</code></pre>"},{"location":"concepts/backend-system/#lazy-imports","title":"Lazy Imports","text":"<p>OmniDocs uses lazy imports to avoid requiring all dependencies upfront.</p>"},{"location":"concepts/backend-system/#the-problem-with-eager-imports","title":"The Problem with Eager Imports","text":"<pre><code># \u274c BAD: Requires all dependencies at startup\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextExtractor\n# Tries to import:\n# - transformers (for PyTorch)\n# - vllm (for VLLM)\n# - mlx (for MLX)\n# - litellm (for API)\n# Even if user only wants PyTorch!\n# ImportError if any dependency is missing!\n</code></pre>"},{"location":"concepts/backend-system/#solution-lazy-imports","title":"Solution: Lazy Imports","text":"<pre><code># \u2705 GOOD: Import extractor without dependencies\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\n\n# Dependencies imported only when loading that backend\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(...)  # Only torch imported now\n)\n</code></pre>"},{"location":"concepts/backend-system/#how-its-implemented","title":"How It's Implemented","text":"<pre><code>class QwenTextExtractor(BaseTextExtractor):\n    def _load_model(self) -&gt; None:\n        config_type = type(self.backend_config).__name__\n\n        if config_type == \"QwenTextPyTorchConfig\":\n            try:\n                from transformers import AutoModel\n                # Import only happens here\n            except ImportError:\n                raise ImportError(\n                    \"PyTorch backend requires transformers. \"\n                    \"Install with: pip install omnidocs[pytorch]\"\n                )\n\n        elif config_type == \"QwenTextVLLMConfig\":\n            try:\n                from vllm import LLM\n                # Import only happens here\n            except ImportError:\n                raise ImportError(\n                    \"VLLM backend requires vllm. \"\n                    \"Install with: pip install omnidocs[vllm]\"\n                )\n        # ... other backends\n</code></pre>"},{"location":"concepts/backend-system/#benefits","title":"Benefits","text":"<pre><code># User only has PyTorch installed\npip install omnidocs[pytorch]\n\n# Can still import extractor (no error)\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\n\n# Can use PyTorch backend (works)\nextractor = QwenTextExtractor(backend=QwenTextPyTorchConfig(...))\n\n# Can't use VLLM (helpful error message)\ntry:\n    extractor = QwenTextExtractor(backend=QwenTextVLLMConfig(...))\nexcept ImportError as e:\n    print(e)  # \"VLLM backend requires vllm. Install with: pip install omnidocs[vllm]\"\n</code></pre>"},{"location":"concepts/backend-system/#adding-new-backends","title":"Adding New Backends","text":"<p>To add a new backend to an existing model, follow this pattern:</p>"},{"location":"concepts/backend-system/#step-1-create-config-class","title":"Step 1: Create Config Class","text":"<pre><code># omnidocs/tasks/text_extraction/qwen/mynew_backend.py\n\nfrom pydantic import BaseModel, Field, ConfigDict\n\nclass QwenTextMyNewBackendConfig(BaseModel):\n    \"\"\"Configuration for MyNewBackend text extraction.\"\"\"\n\n    model: str = Field(\n        default=\"Qwen/Qwen3-VL-8B-Instruct\",\n        description=\"Model identifier\"\n    )\n    param1: str = Field(default=\"value\", description=\"Backend-specific param\")\n    param2: int = Field(default=10, ge=1, description=\"Another param\")\n\n    model_config = ConfigDict(extra=\"forbid\")\n</code></pre>"},{"location":"concepts/backend-system/#step-2-update-extractor","title":"Step 2: Update Extractor","text":"<pre><code># omnidocs/tasks/text_extraction/qwen/extractor.py\n\nfrom typing import Union, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from .mynew_backend import QwenTextMyNewBackendConfig\n\nQwenTextBackendConfig = Union[\n    \"QwenTextPyTorchConfig\",\n    \"QwenTextVLLMConfig\",\n    \"QwenTextMyNewBackendConfig\",  # Add here\n]\n\nclass QwenTextExtractor(BaseTextExtractor):\n    def _load_model(self) -&gt; None:\n        config_type = type(self.backend_config).__name__\n\n        if config_type == \"QwenTextPyTorchConfig\":\n            self._load_pytorch()\n        elif config_type == \"QwenTextVLLMConfig\":\n            self._load_vllm()\n        elif config_type == \"QwenTextMyNewBackendConfig\":  # Add handler\n            self._load_mynew_backend()\n        else:\n            raise ValueError(f\"Unknown backend: {config_type}\")\n\n    def _load_mynew_backend(self):\n        \"\"\"Load MyNewBackend.\"\"\"\n        try:\n            import mynew_backend_lib\n        except ImportError:\n            raise ImportError(\n                \"MyNewBackend requires mynew_backend_lib. \"\n                \"Install with: pip install mynew_backend_lib\"\n            )\n\n        config = self.backend_config\n        # Load model with mynew_backend_lib...\n</code></pre>"},{"location":"concepts/backend-system/#step-3-export-config","title":"Step 3: Export Config","text":"<pre><code># omnidocs/tasks/text_extraction/qwen/__init__.py\n\nfrom .pytorch import QwenTextPyTorchConfig\nfrom .vllm import QwenTextVLLMConfig\nfrom .mynew_backend import QwenTextMyNewBackendConfig  # Add export\nfrom .extractor import QwenTextExtractor\n\n__all__ = [\n    \"QwenTextPyTorchConfig\",\n    \"QwenTextVLLMConfig\",\n    \"QwenTextMyNewBackendConfig\",  # Add to public API\n    \"QwenTextExtractor\",\n]\n</code></pre>"},{"location":"concepts/backend-system/#step-4-update-dependencies","title":"Step 4: Update Dependencies","text":"<pre><code># Add backend dependency to optional group\ncd Omnidocs/\nuv add --group mynew_backend mynew_backend_lib\n</code></pre>"},{"location":"concepts/backend-system/#backend-trade-offs","title":"Backend Trade-Offs","text":"Aspect PyTorch VLLM MLX API Setup Complexity Low High Low Very Low Latency 500-2000ms 200-500ms 1000-3000ms 1000-5000ms Throughput 1 req/s 10-100 req/s 1-5 req/s 0.5-5 req/s Memory (8B model) 16GB VRAM 8GB VRAM 6GB RAM - Cost Infra + GPU Infra + GPU None Pay/request Privacy Local Local Local Cloud Hardware Any NVIDIA GPU Multi-GPU Apple Silicon None (API) Scaling Single GPU Multi-GPU Single device Unlimited"},{"location":"concepts/backend-system/#decision-matrix","title":"Decision Matrix","text":"<p>Choose PyTorch if: - Developing locally - Single GPU available - Want simplicity - Cost not a concern</p> <p>Choose VLLM if: - Production serving - High throughput needed - Multiple GPUs available - Can handle complexity</p> <p>Choose MLX if: - Developing on Apple Silicon - Want local inference - Battery efficiency matters - Minimal setup</p> <p>Choose API if: - No GPU available - Want managed infrastructure - Pay-per-request pricing OK - Minimal setup needed</p>"},{"location":"concepts/backend-system/#real-code-examples","title":"Real Code Examples","text":""},{"location":"concepts/backend-system/#example-1-multi-backend-same-code","title":"Example 1: Multi-Backend Same Code","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenTextPyTorchConfig,\n    QwenTextVLLMConfig,\n)\nfrom omnidocs import Document\n\ndoc = Document.from_pdf(\"paper.pdf\")\npage = doc.get_page(0)\n\n# Backend 1: PyTorch\nprint(\"=== PyTorch Backend ===\")\nextractor1 = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(device=\"cuda\")\n)\nresult1 = extractor1.extract(page, output_format=\"markdown\")\nprint(result1.content[:200])\n\n# Backend 2: VLLM (same extractor, different backend)\nprint(\"\\n=== VLLM Backend ===\")\nextractor2 = QwenTextExtractor(\n    backend=QwenTextVLLMConfig(tensor_parallel_size=2)\n)\nresult2 = extractor2.extract(page, output_format=\"markdown\")\nprint(result2.content[:200])\n\n# Results should be identical (or very similar)\n# Code is identical except for config!\n</code></pre>"},{"location":"concepts/backend-system/#example-2-detecting-available-backends","title":"Example 2: Detecting Available Backends","text":"<pre><code>from omnidocs.tasks.text_extraction.qwen import (\n    QwenTextPyTorchConfig,\n    QwenTextVLLMConfig,\n    QwenTextMLXConfig,\n    QwenTextAPIConfig,\n)\n\nbackends = [\n    (\"PyTorch\", QwenTextPyTorchConfig),\n    (\"VLLM\", QwenTextVLLMConfig),\n    (\"MLX\", QwenTextMLXConfig),\n    (\"API\", QwenTextAPIConfig),\n]\n\nprint(\"Available backends:\")\nfor name, config_class in backends:\n    print(f\"  - {name}: {config_class.__doc__.split(chr(10))[0]}\")\n\n# Output:\n# Available backends:\n#   - PyTorch: PyTorch/HuggingFace backend configuration for Qwen text extraction.\n#   - VLLM: VLLM backend configuration for Qwen text extraction.\n#   - MLX: MLX backend configuration for Qwen text extraction.\n#   - API: API backend configuration for Qwen text extraction.\n</code></pre>"},{"location":"concepts/backend-system/#example-3-conditional-backend-selection","title":"Example 3: Conditional Backend Selection","text":"<pre><code>import os\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenTextPyTorchConfig,\n    QwenTextVLLMConfig,\n    QwenTextMLXConfig,\n    QwenTextAPIConfig,\n)\n\ndef get_extractor():\n    \"\"\"Select best available backend.\"\"\"\n    backend_choice = os.environ.get(\"OMNIDOCS_BACKEND\", \"auto\")\n\n    if backend_choice == \"pytorch\":\n        config = QwenTextPyTorchConfig(device=\"cuda\")\n    elif backend_choice == \"vllm\":\n        config = QwenTextVLLMConfig(tensor_parallel_size=2)\n    elif backend_choice == \"mlx\":\n        config = QwenTextMLXConfig()\n    elif backend_choice == \"api\":\n        config = QwenTextAPIConfig(api_key=os.environ[\"OPENROUTER_API_KEY\"])\n    else:  # auto\n        # Try PyTorch first, fall back to others\n        try:\n            config = QwenTextPyTorchConfig(device=\"cuda\")\n        except ImportError:\n            try:\n                config = QwenTextVLLMConfig()\n            except ImportError:\n                config = QwenTextAPIConfig(\n                    api_key=os.environ.get(\"OPENROUTER_API_KEY\")\n                )\n\n    return QwenTextExtractor(backend=config)\n\nextractor = get_extractor()\n</code></pre>"},{"location":"concepts/backend-system/#example-4-single-backend-model","title":"Example 4: Single-Backend Model","text":"<pre><code>from omnidocs.tasks.layout_extraction import (\n    DocLayoutYOLO,\n    DocLayoutYOLOConfig,\n)\n\n# This model only has one backend (PyTorch)\n# Config type is obvious\nconfig = DocLayoutYOLOConfig(device=\"cuda\", confidence=0.3)\nextractor = DocLayoutYOLO(config=config)\n\n# These don't exist (it's single-backend)\n# from omnidocs.tasks.layout_extraction import (\n#     DocLayoutYOLOVLLMConfig,  # \u274c ImportError\n#     DocLayoutYOLOAPIConfig,   # \u274c ImportError\n# )\n</code></pre>"},{"location":"concepts/backend-system/#summary","title":"Summary","text":"Concept Key Points 4 Backends PyTorch, VLLM, MLX, API - different use cases Config-Driven Config class type determines which backend loads Detection <code>_load_model()</code> checks <code>type(backend_config).__name__</code> Lazy Imports Dependencies only imported when backend used Backend Discoverability If config exists, backend is available Trade-Offs Speed vs complexity, cost vs control, privacy vs convenience"},{"location":"concepts/backend-system/#next-steps","title":"Next Steps","text":"<ul> <li>See Config Pattern for how configs are structured</li> <li>See Architecture Overview for system design</li> <li>Read backend-specific documentation for detailed setup instructions</li> </ul>"},{"location":"concepts/config-pattern/","title":"Config Pattern","text":"<p>Core Principle: Pydantic configs drive both backend selection AND model initialization. Separation of concerns: configs for setup, method parameters for task options.</p>"},{"location":"concepts/config-pattern/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Single vs Multi-Backend</li> <li>How to Tell Which Is Which</li> <li>Pydantic Config Structure</li> <li>Init vs Extract Separation</li> <li>What Goes Where</li> <li>Real Examples</li> <li>Extending Configs</li> </ol>"},{"location":"concepts/config-pattern/#single-vs-multi-backend","title":"Single vs Multi-Backend","text":"<p>OmniDocs models fall into two categories based on backend support:</p>"},{"location":"concepts/config-pattern/#single-backend-models","title":"Single-Backend Models","text":"<p>A model that works with ONLY ONE backend has a single config class.</p> <pre><code># Example: DocLayout-YOLO (PyTorch only)\nfrom omnidocs.tasks.layout_extraction import (\n    DocLayoutYOLO,\n    DocLayoutYOLOConfig,  # \u2190 Single config\n)\n\n# Parameter name is always 'config='\nextractor = DocLayoutYOLO(config=DocLayoutYOLOConfig(...))\n</code></pre> <p>Characteristics: - One config class named <code>{Model}Config</code> - Parameter name: <code>config=</code> - Model optimized for specific backend - Can't switch backends</p> <p>When This Happens: - Model only supports one framework (e.g., YOLO models are PyTorch) - Backend has unique requirements - Model implementation already optimized for one backend</p>"},{"location":"concepts/config-pattern/#multi-backend-models","title":"Multi-Backend Models","text":"<p>A model that works with MULTIPLE backends has multiple config classes.</p> <pre><code># Example: Qwen (PyTorch, VLLM, MLX, API)\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenTextPyTorchConfig,    # \u2190 Config 1\n    QwenTextVLLMConfig,       # \u2190 Config 2\n    QwenTextMLXConfig,        # \u2190 Config 3\n    QwenTextAPIConfig,        # \u2190 Config 4\n)\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\n\n# Parameter name is always 'backend='\nextractor = QwenTextExtractor(backend=QwenTextPyTorchConfig(...))\nextractor = QwenTextExtractor(backend=QwenTextVLLMConfig(...))\nextractor = QwenTextExtractor(backend=QwenTextMLXConfig(...))\nextractor = QwenTextExtractor(backend=QwenTextAPIConfig(...))\n</code></pre> <p>Characteristics: - Multiple config classes: <code>{Model}{Backend}Config</code> - Parameter name: <code>backend=</code> - One extractor works with all configs - Can switch backends by changing config type - Each backend has optimized implementation</p> <p>When This Happens: - Model available on multiple frameworks (Qwen on PyTorch, VLLM, MLX) - Want to support different hardware (GPU, Apple Silicon, API) - Need flexibility for different deployment scenarios</p>"},{"location":"concepts/config-pattern/#how-to-tell-which-is-which","title":"How to Tell Which Is Which","text":""},{"location":"concepts/config-pattern/#method-1-look-at-imports","title":"Method 1: Look at Imports","text":"<pre><code># Single-backend: One config class\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLOConfig\n\n# Multi-backend: Multiple config classes\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenTextPyTorchConfig,\n    QwenTextVLLMConfig,\n    QwenTextMLXConfig,\n    QwenTextAPIConfig,\n)\n</code></pre>"},{"location":"concepts/config-pattern/#method-2-check-parameter-name","title":"Method 2: Check Parameter Name","text":"<pre><code># If parameter is 'config=' \u2192 Single-backend\nextractor = DocLayoutYOLO(config=DocLayoutYOLOConfig(...))\n\n# If parameter is 'backend=' \u2192 Multi-backend\nextractor = QwenTextExtractor(backend=QwenTextPyTorchConfig(...))\n</code></pre>"},{"location":"concepts/config-pattern/#method-3-try-to-import-other-backends","title":"Method 3: Try to Import Other Backends","text":"<pre><code># Single-backend: ImportError for other backends\ntry:\n    from omnidocs.tasks.layout_extraction import DocLayoutYOLOVLLMConfig\nexcept ImportError:\n    print(\"Single-backend model (PyTorch only)\")\n\n# Multi-backend: All imports succeed\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenTextPyTorchConfig,\n    QwenTextVLLMConfig,\n    QwenTextMLXConfig,\n    QwenTextAPIConfig,\n)\nprint(\"Multi-backend model (4 backends available)\")\n</code></pre>"},{"location":"concepts/config-pattern/#method-4-check-constructor-signature","title":"Method 4: Check Constructor Signature","text":"<pre><code># Single-backend: config parameter\nimport inspect\nsig = inspect.signature(DocLayoutYOLO.__init__)\nprint(sig)  # (__self, config: DocLayoutYOLOConfig)\n\n# Multi-backend: backend parameter\nsig = inspect.signature(QwenTextExtractor.__init__)\nprint(sig)  # (__self, backend: QwenTextBackendConfig)\n</code></pre>"},{"location":"concepts/config-pattern/#pydantic-config-structure","title":"Pydantic Config Structure","text":"<p>All configs are Pydantic BaseModel classes. This provides: - Type validation - IDE autocomplete - Documentation - Default values - Custom validation</p>"},{"location":"concepts/config-pattern/#basic-structure","title":"Basic Structure","text":"<pre><code>from pydantic import BaseModel, ConfigDict, Field\nfrom typing import Literal, Optional\n\nclass MyConfig(BaseModel):\n    \"\"\"\n    Clear docstring explaining what this config does.\n\n    This config is for [purpose/backend].\n    \"\"\"\n\n    # Required parameter (no default)\n    param1: str = Field(\n        ...,  # ... means required\n        description=\"What this parameter does\"\n    )\n\n    # Optional parameter with default\n    param2: str = Field(\n        default=\"default_value\",\n        description=\"Optional parameter\"\n    )\n\n    # Numeric with bounds\n    param3: int = Field(\n        default=10,\n        ge=1,         # greater than or equal\n        le=100,       # less than or equal\n        description=\"Must be between 1 and 100\"\n    )\n\n    # Enumerated values\n    param4: Literal[\"option1\", \"option2\"] = Field(\n        default=\"option1\",\n        description=\"Choose one option\"\n    )\n\n    # Nullable/Optional\n    param5: Optional[str] = Field(\n        default=None,\n        description=\"Can be None or a string\"\n    )\n\n    # Pydantic model config\n    model_config = ConfigDict(\n        extra=\"forbid\",  # Raise error on unknown fields\n    )\n</code></pre>"},{"location":"concepts/config-pattern/#real-example-qwentextpytorchconfig","title":"Real Example: QwenTextPyTorchConfig","text":"<pre><code>from pydantic import BaseModel, ConfigDict, Field\nfrom typing import Literal, Optional\n\nclass QwenTextPyTorchConfig(BaseModel):\n    \"\"\"\n    PyTorch/HuggingFace backend configuration for Qwen text extraction.\n\n    This backend uses the transformers library with PyTorch for local GPU inference.\n    Requires: torch, transformers, accelerate, qwen-vl-utils\n\n    Example:\n        ```python\n        config = QwenTextPyTorchConfig(\n            model=\"Qwen/Qwen3-VL-8B-Instruct\",\n            device=\"cuda\",\n            torch_dtype=\"bfloat16\",\n        )\n        ```\n    \"\"\"\n\n    model: str = Field(\n        default=\"Qwen/Qwen3-VL-8B-Instruct\",\n        description=\"HuggingFace model ID (e.g., Qwen/Qwen3-VL-2B-Instruct, \"\n        \"Qwen/Qwen3-VL-8B-Instruct, Qwen/Qwen3-VL-32B-Instruct)\",\n    )\n\n    device: str = Field(\n        default=\"cuda\",\n        description=\"Device to run inference on. Options: 'cuda', 'mps', 'cpu'. \"\n        \"Auto-detects best available if specified device is unavailable.\",\n    )\n\n    torch_dtype: Literal[\"float16\", \"bfloat16\", \"float32\", \"auto\"] = Field(\n        default=\"auto\",\n        description=\"Torch dtype for model weights. 'auto' lets the model decide.\",\n    )\n\n    device_map: Optional[str] = Field(\n        default=\"auto\",\n        description=\"Device map strategy for model parallelism. \"\n        \"Options: 'auto', 'balanced', 'sequential', or None.\",\n    )\n\n    max_new_tokens: int = Field(\n        default=8192,\n        ge=256,\n        le=32768,\n        description=\"Maximum number of tokens to generate.\",\n    )\n\n    temperature: float = Field(\n        default=0.1,\n        ge=0.0,\n        le=2.0,\n        description=\"Sampling temperature. Lower values are more deterministic.\",\n    )\n\n    model_config = ConfigDict(extra=\"forbid\")\n</code></pre>"},{"location":"concepts/config-pattern/#field-validation-options","title":"Field Validation Options","text":"<pre><code>from pydantic import BaseModel, Field, field_validator\n\nclass ConfigExample(BaseModel):\n    # String constraints\n    device: str = Field(default=\"cuda\", min_length=1, max_length=10)\n\n    # Numeric constraints\n    confidence: float = Field(default=0.5, ge=0.0, le=1.0)\n    batch_size: int = Field(default=32, ge=1, le=1024)\n\n    # Pattern matching\n    api_key: str = Field(default=\"\", pattern=r\"^sk-[a-zA-Z0-9]+$\")\n\n    # Enumerated\n    format: Literal[\"json\", \"xml\", \"csv\"] = Field(default=\"json\")\n\n    # Custom validation\n    @field_validator(\"device\")\n    @classmethod\n    def validate_device(cls, v):\n        if v not in [\"cuda\", \"cpu\", \"mps\"]:\n            raise ValueError(f\"Invalid device: {v}\")\n        return v\n\n    # Ensure exclusive options\n    @field_validator(\"model_size\")\n    @classmethod\n    def validate_model_size(cls, v, info):\n        if info.data.get(\"use_api\") and v &gt; 8:\n            raise ValueError(\"API backend doesn't support models &gt; 8B\")\n        return v\n</code></pre>"},{"location":"concepts/config-pattern/#init-vs-extract-separation","title":"Init vs Extract Separation","text":""},{"location":"concepts/config-pattern/#clear-separation-of-concerns","title":"Clear Separation of Concerns","text":"<p>OmniDocs strictly separates initialization parameters from task parameters.</p>"},{"location":"concepts/config-pattern/#initialization-goes-in-config","title":"Initialization (goes in config)","text":"<p>Config parameters are determined ONCE when you create the extractor.</p> <pre><code>config = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",  # \u2190 Model choice (init)\n    device=\"cuda\",                      # \u2190 Hardware (init)\n    torch_dtype=\"bfloat16\",             # \u2190 Quantization (init)\n)\n\nextractor = QwenTextExtractor(backend=config)\n# Model loaded ONCE here\n</code></pre> <p>Characteristics: - Set once at extractor creation - Don't change during processing - Affect model loading - Backend-specific</p>"},{"location":"concepts/config-pattern/#task-parameters-goes-in-extract","title":"Task Parameters (goes in extract())","text":"<p>Extract parameters change per-call based on task requirements.</p> <pre><code># Same extractor, different task parameters\nresult1 = extractor.extract(\n    image1,\n    output_format=\"markdown\",   # \u2190 Task parameter\n    include_layout=True,        # \u2190 Task parameter\n)\n\nresult2 = extractor.extract(\n    image2,\n    output_format=\"html\",       # \u2190 Different task parameter\n    include_layout=False,       # \u2190 Different task parameter\n)\n</code></pre> <p>Characteristics: - Set per-extraction call - Can vary between calls - Affect task output - Backend-agnostic</p>"},{"location":"concepts/config-pattern/#why-this-matters","title":"Why This Matters","text":"<pre><code># \u274c BAD: Task parameters in config\nconfig = ExtractorConfig(\n    model=\"qwen\",\n    output_format=\"markdown\",  # \u2190 Should be in extract()!\n)\nextractor = MyExtractor(config=config)\nresult = extractor.extract(image)  # Always markdown!\n\n# Can't change output format without recreating extractor\n\n# \u2705 GOOD: Task parameters in extract()\nconfig = QwenTextPyTorchConfig(model=\"qwen\")\nextractor = QwenTextExtractor(backend=config)\n\nresult1 = extractor.extract(image, output_format=\"markdown\")\nresult2 = extractor.extract(image, output_format=\"html\")\n# Same extractor, different outputs!\n</code></pre>"},{"location":"concepts/config-pattern/#what-goes-where","title":"What Goes Where","text":""},{"location":"concepts/config-pattern/#in-config","title":"In Config \u2705","text":"<pre><code>class QwenTextPyTorchConfig(BaseModel):\n    # Model choice\n    model: str  # Which model to load?\n\n    # Hardware configuration\n    device: str  # GPU or CPU?\n    torch_dtype: Literal[...]  # What precision?\n    device_map: Optional[str]  # How to parallelize?\n\n    # Loading options\n    trust_remote_code: bool  # Custom model code?\n    use_flash_attention: bool  # Acceleration method?\n\n    # Resource limits\n    max_new_tokens: int  # Max generation length\n    temperature: float  # Sampling randomness\n</code></pre> <p>Rule: If it determines HOW THE MODEL LOADS, it goes in config.</p>"},{"location":"concepts/config-pattern/#in-extract","title":"In extract() \u2705","text":"<pre><code>def extract(\n    self,\n    image: Image.Image,\n    output_format: Literal[\"markdown\", \"html\"],  # Output format\n    include_layout: bool = False,  # Include layout info?\n    custom_prompt: Optional[str] = None,  # Custom instruction?\n) -&gt; TextOutput:\n    \"\"\"...\"\"\"\n    pass\n</code></pre> <p>Rule: If it affects WHAT THE MODEL OUTPUTS, it goes in extract().</p>"},{"location":"concepts/config-pattern/#decision-tree","title":"Decision Tree","text":"<pre><code>Question: Where does this parameter go?\n\n\u251c\u2500 Does it determine which/how the model loads?\n\u2502  \u2514\u2500 YES \u2192 Goes in Config (init time)\n\u2502\n\u251c\u2500 Does it change between calls for same image?\n\u2502  \u2514\u2500 YES \u2192 Goes in extract() (call time)\n\u2502\n\u251c\u2500 Is it backend-specific?\n\u2502  \u2514\u2500 YES \u2192 Goes in Config (different for each backend)\n\u2502\n\u2514\u2500 Is it task-specific but not backend-specific?\n   \u2514\u2500 YES \u2192 Goes in extract()\n</code></pre>"},{"location":"concepts/config-pattern/#examples","title":"Examples","text":"<pre><code># device: \"cuda\", \"cpu\", \"mps\"\n# \u2705 Config (affects model loading, backend-specific)\n\n# output_format: \"markdown\", \"html\"\n# \u2705 extract() (task output, not model loading)\n\n# model: \"Qwen/Qwen3-VL-8B\"\n# \u2705 Config (determines which model to load)\n\n# include_layout: True/False\n# \u2705 extract() (task option, same model, different output)\n\n# max_new_tokens: 8192\n# \u2705 Config (generation limit, affects model inference)\n\n# custom_prompt: str\n# \u2705 extract() (task-specific instruction)\n\n# quantization: \"4bit\", \"8bit\"\n# \u2705 Config (affects model loading/memory)\n\n# batch_size: 32\n# \u2705 Config (affects GPU memory, set once)\n</code></pre>"},{"location":"concepts/config-pattern/#real-examples","title":"Real Examples","text":""},{"location":"concepts/config-pattern/#example-1-single-backend-model-doclayoutyolo","title":"Example 1: Single-Backend Model (DocLayoutYOLO)","text":"<pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\n# Configuration (one time)\nconfig = DocLayoutYOLOConfig(\n    device=\"cuda\",          # Where to run (config)\n    img_size=1024,          # Model input size (config)\n    confidence=0.3,         # Detection threshold (config)\n)\n\nextractor = DocLayoutYOLO(config=config)\n# Model loads with these settings\n\n# Usage (can vary per call)\nresult1 = extractor.extract(image1)\nresult2 = extractor.extract(image2)\n# Same config, same model, different inputs\n</code></pre> <p>Analysis: - Single config class: <code>DocLayoutYOLOConfig</code> - Parameter name: <code>config=</code> - Config contains: device, input size, threshold - extract() only takes image (no other params)</p>"},{"location":"concepts/config-pattern/#example-2-multi-backend-model-qwentextextractor","title":"Example 2: Multi-Backend Model (QwenTextExtractor)","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenTextPyTorchConfig,\n    QwenTextVLLMConfig,\n)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# SCENARIO 1: PyTorch Backend\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nconfig_pytorch = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",  # Model to load\n    device=\"cuda\",                      # Hardware\n    torch_dtype=\"bfloat16\",             # Precision\n)\n\nextractor_pytorch = QwenTextExtractor(backend=config_pytorch)\n\n# Same extract() call works the same way\nresult = extractor_pytorch.extract(\n    image,\n    output_format=\"markdown\",  # Task parameter\n)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# SCENARIO 2: VLLM Backend\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nconfig_vllm = QwenTextVLLMConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",  # Same model\n    tensor_parallel_size=2,             # GPU parallelism (VLLM-specific)\n    gpu_memory_utilization=0.9,         # Memory limit (VLLM-specific)\n)\n\nextractor_vllm = QwenTextExtractor(backend=config_vllm)\n\n# Same extract() call works the same way\nresult = extractor_vllm.extract(\n    image,\n    output_format=\"markdown\",  # Task parameter\n)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# KEY INSIGHT\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# - Different configs: PyTorchConfig vs VLLMConfig\n# - Different backend initialization\n# - SAME extract() interface!\n# - Backend differences hidden from user\n</code></pre>"},{"location":"concepts/config-pattern/#example-3-custom-validation","title":"Example 3: Custom Validation","text":"<pre><code>from pydantic import BaseModel, Field, field_validator\n\nclass CustomConfig(BaseModel):\n    \"\"\"Config with custom validation.\"\"\"\n\n    model_size: Literal[\"small\", \"medium\", \"large\"] = Field(\n        default=\"medium\",\n        description=\"Model size category\"\n    )\n\n    max_batch_size: int = Field(default=32, ge=1, le=128)\n\n    # Custom validation: batch size depends on model size\n    @field_validator(\"max_batch_size\")\n    @classmethod\n    def validate_batch_size(cls, v, info):\n        model_size = info.data.get(\"model_size\")\n\n        if model_size == \"small\" and v &gt; 128:\n            raise ValueError(\"Small model max batch size is 128\")\n        elif model_size == \"large\" and v &lt; 8:\n            raise ValueError(\"Large model min batch size is 8\")\n\n        return v\n\n# Valid\nconfig = CustomConfig(model_size=\"small\", max_batch_size=64)\n\n# Invalid\nconfig = CustomConfig(model_size=\"large\", max_batch_size=4)\n# ValidationError: Large model min batch size is 8\n</code></pre>"},{"location":"concepts/config-pattern/#example-4-config-with-enum","title":"Example 4: Config with Enum","text":"<pre><code>from enum import Enum\nfrom pydantic import BaseModel, Field\n\nclass QuantizationType(str, Enum):\n    \"\"\"Quantization options.\"\"\"\n    FLOAT32 = \"float32\"\n    FLOAT16 = \"float16\"\n    BFLOAT16 = \"bfloat16\"\n    INT8 = \"int8\"\n    INT4 = \"int4\"\n\nclass MLXConfig(BaseModel):\n    \"\"\"MLX backend config.\"\"\"\n\n    quantization: QuantizationType = Field(\n        default=QuantizationType.INT4,\n        description=\"Quantization type for MLX\"\n    )\n\n# Usage\nconfig = MLXConfig(quantization=QuantizationType.INT4)\n# Or with string (auto-converted)\nconfig = MLXConfig(quantization=\"int4\")\n</code></pre>"},{"location":"concepts/config-pattern/#extending-configs","title":"Extending Configs","text":""},{"location":"concepts/config-pattern/#adding-new-parameters","title":"Adding New Parameters","text":"<pre><code># Original config\nclass MyConfig(BaseModel):\n    model: str\n    device: str\n\n# Extended config\nclass MyConfigV2(BaseModel):\n    model: str\n    device: str\n    # New parameter\n    memory_limit: Optional[int] = Field(\n        default=None,\n        description=\"GPU memory limit in MB\"\n    )\n</code></pre>"},{"location":"concepts/config-pattern/#config-inheritance","title":"Config Inheritance","text":"<pre><code>from pydantic import BaseModel, Field\n\nclass BaseInferenceConfig(BaseModel):\n    \"\"\"Common inference parameters.\"\"\"\n    device: str = Field(default=\"cuda\")\n    max_tokens: int = Field(default=2048)\n\n    model_config = ConfigDict(extra=\"forbid\")\n\nclass SpecificModelConfig(BaseInferenceConfig):\n    \"\"\"Model-specific parameters.\"\"\"\n    model: str = Field(...)  # Required\n    # Inherits: device, max_tokens\n\nconfig = SpecificModelConfig(\n    model=\"my-model\",\n    device=\"cpu\",  # From base\n    max_tokens=4096,  # From base\n)\n</code></pre>"},{"location":"concepts/config-pattern/#frozen-configs","title":"Frozen Configs","text":"<pre><code>from pydantic import ConfigDict\n\nclass ImmutableConfig(BaseModel):\n    \"\"\"Config that can't be changed after creation.\"\"\"\n\n    model: str\n    device: str\n\n    model_config = ConfigDict(\n        frozen=True,  # \u2190 Prevents modification\n        extra=\"forbid\",\n    )\n\nconfig = ImmutableConfig(model=\"qwen\", device=\"cuda\")\nconfig.device = \"cpu\"  # Error: Config is frozen\n</code></pre>"},{"location":"concepts/config-pattern/#summary","title":"Summary","text":"Aspect Single-Backend Multi-Backend Config Classes One (<code>{Model}Config</code>) Multiple (<code>{Model}{Backend}Config</code>) Parameter Name <code>config=</code> <code>backend=</code> Backend Selection Fixed Type of config passed Use Case Model designed for one backend Model supports multiple backends Example <code>DocLayoutYOLOConfig</code> <code>QwenTextPyTorchConfig</code>, <code>QwenTextVLLMConfig</code>, ... <p>Key Rules: 1. Config is for init - Model setup, happens once 2. extract() is for tasks - Task parameters, change per-call 3. Pydantic validates - Errors caught at creation time 4. Types matter - IDE shows all options, typos caught</p>"},{"location":"concepts/config-pattern/#next-steps","title":"Next Steps","text":"<ul> <li>See Backend System for how backends work</li> <li>See Architecture Overview for system design</li> <li>Read model-specific documentation for exact parameters</li> </ul>"},{"location":"concepts/document-model/","title":"Document Model","text":"<p>Core Principle: The Document class represents source data only. It does NOT store task results, does NOT perform analysis, and does NOT modify the original content.</p>"},{"location":"concepts/document-model/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Design Philosophy</li> <li>Lazy Page Loading</li> <li>Document Methods</li> <li>DocumentMetadata</li> <li>Memory Management</li> <li>Common Patterns</li> <li>When to Use Document</li> </ol>"},{"location":"concepts/document-model/#design-philosophy","title":"Design Philosophy","text":""},{"location":"concepts/document-model/#why-stateless","title":"Why Stateless?","text":"<p>The Document class is intentionally stateless - it doesn't store analysis results. This is a deliberate design choice with important consequences.</p>"},{"location":"concepts/document-model/#problem-with-stateful-design","title":"Problem with Stateful Design","text":"<pre><code># \u274c BAD: Document stores extraction results\n\nclass BadDocument:\n    def __init__(self, pdf_path):\n        self.pdf_path = pdf_path\n        self.pages = []  # \u2190 Stores rendered pages\n        self.layout_results = []  # \u2190 Stores layout analysis\n        self.ocr_results = []  # \u2190 Stores OCR results\n        self.text_results = []  # \u2190 Stores extracted text\n\n    def extract_layout(self):\n        # Stores result internally\n        self.layout_results.append(...)\n\n    def extract_text(self):\n        # Stores result internally\n        self.text_results.append(...)\n\n    # Now what is doc.get_page(0)?\n    # Is it the image, or metadata about extractions?\n    # Which extractor's results take priority?\n    # How do I re-run an extractor with different params?\n</code></pre> <p>Problems: - Confusion: What is source data vs. analysis? - Inflexibility: Can't re-extract with different parameters - Memory: Multiple copies of results in document - Coupling: Document depends on all extractors</p>"},{"location":"concepts/document-model/#solution-stateless-design","title":"Solution: Stateless Design","text":"<pre><code># \u2705 GOOD: Document is just source data\n\nclass Document:\n    def __init__(self, pdf_path):\n        self.pdf_path = pdf_path\n        self._lazy_pages = []  # \u2190 Only lazy wrappers, NOT rendered\n\n    def get_page(self, idx):\n        # Returns PIL Image - original pixel data\n        return self._lazy_pages[idx].image\n\n    @property\n    def text(self):\n        # Returns extracted text from PDF metadata\n        # NOT analysis results\n        return self._extracted_text_from_pdf\n\n# Extractors are independent\nlayout_result = layout_extractor.extract(doc.get_page(0))\nocr_result = ocr_extractor.extract(doc.get_page(0))\ntext_result = text_extractor.extract(doc.get_page(0))\n\n# User composes results\ncombined = {\n    \"layout\": layout_result,\n    \"ocr\": ocr_result,\n    \"text\": text_result,\n}\n</code></pre> <p>Benefits: - Clarity: Document = source, Extractors = analysis - Flexibility: Run any extractor, any number of times - Reusability: Same document object, different analyses - Composability: Easy to build pipelines - Caching: User controls what to keep in memory</p>"},{"location":"concepts/document-model/#lazy-page-loading","title":"Lazy Page Loading","text":"<p>The Document class uses lazy evaluation for pages: they're not rendered until accessed.</p>"},{"location":"concepts/document-model/#how-it-works","title":"How It Works","text":"<pre><code>doc = Document.from_pdf(\"paper.pdf\")  # Fast - reads file, creates metadata\n# No page rendering yet!\n\npage = doc.get_page(0)  # Slow - renders page 0\npage = doc.get_page(1)  # Renders page 1\npage = doc.get_page(0)  # Fast - returns cached page 0\n</code></pre>"},{"location":"concepts/document-model/#the-lazypage-class","title":"The LazyPage Class","text":"<p>Behind the scenes, each page is wrapped in a <code>LazyPage</code> object:</p> <pre><code>class LazyPage:\n    def __init__(self, pdf_doc, page_index, dpi=150):\n        self._pdf_doc = pdf_doc\n        self._page_index = page_index\n        self._dpi = dpi\n        self._cached_image = None  # \u2190 Not rendered yet\n        self._cached_text = None\n\n    @property\n    def image(self) -&gt; Image.Image:\n        \"\"\"Render page to PIL Image (cached after first access).\"\"\"\n        if self._cached_image is None:\n            # Render on first access\n            scale = self._dpi / 72\n            page = self._pdf_doc[self._page_index]\n            bitmap = page.render(scale=scale)\n            self._cached_image = bitmap.to_pil().convert(\"RGB\")\n        return self._cached_image  # \u2190 Return cached copy\n\n    @property\n    def text(self) -&gt; str:\n        \"\"\"Extract text (cached).\"\"\"\n        if self._cached_text is None:\n            # Extract on first access\n            page = self._pdf_doc[self._page_index]\n            textpage = page.get_textpage()\n            self._cached_text = textpage.get_text_range()\n        return self._cached_text\n\n    def clear_cache(self):\n        \"\"\"Free memory.\"\"\"\n        self._cached_image = None\n</code></pre>"},{"location":"concepts/document-model/#memory-efficiency","title":"Memory Efficiency","text":"<pre><code># Loading 1000-page document\ndoc = Document.from_pdf(\"big_book.pdf\")\n# Memory used: Just metadata + LazyPage wrapper objects (~1MB)\n\n# Processing pages one at a time\nfor page in doc.iter_pages():\n    result = extractor.extract(page)\n    # Memory used: 1 rendered page at a time (~5-10MB)\n    # Each page only stays in memory while processing\n\n    doc.clear_cache()  # Explicitly free page from memory\n    # Page rendered again if accessed later (but usually not needed)\n</code></pre>"},{"location":"concepts/document-model/#with-vs-without-lazy-loading","title":"With vs Without Lazy Loading","text":"Operation Without Lazy Loading With Lazy Loading Load 100-page PDF 5 seconds 0.1 seconds Access page 0 Instant 0.5 seconds Access page 50 Instant 0.5 seconds Memory for 100 pages 500 MB 1 MB Process all pages sequentially 500 MB RAM 10 MB RAM"},{"location":"concepts/document-model/#document-methods","title":"Document Methods","text":""},{"location":"concepts/document-model/#loading-documents","title":"Loading Documents","text":""},{"location":"concepts/document-model/#from_pdf","title":"from_pdf()","text":"<p>Load a PDF file from disk.</p> <pre><code>from omnidocs import Document\n\n# Basic usage\ndoc = Document.from_pdf(\"paper.pdf\")\n\n# With options\ndoc = Document.from_pdf(\n    \"paper.pdf\",\n    page_range=(0, 4),  # Only pages 0-4 (0-indexed, inclusive)\n    dpi=300,  # Higher resolution (default: 150)\n)\n\n# Properties\nprint(doc.page_count)  # 5 (with page_range specified)\nprint(doc.metadata.file_name)  # \"paper.pdf\"\nprint(doc.metadata.file_size)  # 2048000 bytes\n</code></pre> <p>When to use: Reading PDF files from disk</p> <p>Raises: - <code>DocumentLoadError</code> - File not found - <code>UnsupportedFormatError</code> - Not a PDF file - <code>PageRangeError</code> - Invalid page range</p>"},{"location":"concepts/document-model/#from_url","title":"from_url()","text":"<p>Download and load a PDF from a URL.</p> <pre><code># Basic usage\ndoc = Document.from_url(\"https://example.com/paper.pdf\")\n\n# With options\ndoc = Document.from_url(\n    \"https://arxiv.org/pdf/2105.00001.pdf\",\n    timeout=60,  # Download timeout in seconds\n    page_range=(0, 9),  # Only first 10 pages\n)\n\nprint(doc.metadata.source_path)  # Full URL\nprint(doc.metadata.file_name)  # \"paper.pdf\"\n</code></pre> <p>When to use: Loading PDFs from the internet</p> <p>Raises: - <code>URLDownloadError</code> - Download failed (network error, 404, etc.) - <code>PageRangeError</code> - Invalid page range</p>"},{"location":"concepts/document-model/#from_bytes","title":"from_bytes()","text":"<p>Load PDF from bytes in memory.</p> <pre><code># From reading a file\nwith open(\"paper.pdf\", \"rb\") as f:\n    pdf_bytes = f.read()\ndoc = Document.from_bytes(pdf_bytes, filename=\"paper.pdf\")\n\n# From downloaded content\nresponse = requests.get(\"https://example.com/doc.pdf\")\ndoc = Document.from_bytes(response.content, filename=\"doc.pdf\")\n\n# With page range\ndoc = Document.from_bytes(\n    pdf_bytes,\n    filename=\"paper.pdf\",\n    page_range=(0, 19),  # First 20 pages\n)\n</code></pre> <p>When to use: PDF already in memory, or from API responses</p> <p>Raises: - <code>PageRangeError</code> - Invalid page range</p>"},{"location":"concepts/document-model/#from_image","title":"from_image()","text":"<p>Load a single image as a single-page document.</p> <pre><code>doc = Document.from_image(\"page.png\")\n\nprint(doc.page_count)  # 1\nprint(doc.metadata.format)  # \"png\"\n\npage = doc.get_page(0)  # PIL Image of the page\n</code></pre> <p>When to use: Processing single images, screenshots, or scans</p> <p>Raises: - <code>DocumentLoadError</code> - File not found</p>"},{"location":"concepts/document-model/#from_images","title":"from_images()","text":"<p>Load multiple images as a multi-page document.</p> <pre><code>doc = Document.from_images([\n    \"page1.png\",\n    \"page2.png\",\n    \"page3.jpg\",\n])\n\nprint(doc.page_count)  # 3\nprint(doc.metadata.format)  # \"images\"\n\nfor page in doc.iter_pages():\n    result = extractor.extract(page)\n</code></pre> <p>When to use: Processing scanned documents or image sequences</p> <p>Raises: - <code>DocumentLoadError</code> - Any file not found</p>"},{"location":"concepts/document-model/#accessing-document-data","title":"Accessing Document Data","text":""},{"location":"concepts/document-model/#get_pageidx","title":"get_page(idx)","text":"<p>Get a single page as a PIL Image.</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\n\n# 0-indexed\npage = doc.get_page(0)  # First page (PIL Image, RGB)\npage = doc.get_page(doc.page_count - 1)  # Last page\n\n# Memory efficient for large documents\nfor i in range(doc.page_count):\n    page = doc.get_page(i)\n    result = extractor.extract(page)\n    doc.clear_cache(i)  # Free memory\n</code></pre> <p>Returns: PIL Image in RGB mode</p> <p>Raises: <code>PageRangeError</code> if index out of range</p>"},{"location":"concepts/document-model/#get_page_textpage_num","title":"get_page_text(page_num)","text":"<p>Get text extracted from a PDF page.</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\n\n# NOTE: 1-indexed (like PDF viewers)\ntext = doc.get_page_text(1)  # First page (1-indexed)\ntext = doc.get_page_text(2)  # Second page\n\nprint(len(text))  # String of all text on page\n</code></pre> <p>Note: 1-indexed (PDF convention), not 0-indexed like <code>get_page()</code></p> <p>Returns: String of text extracted from PDF</p> <p>Raises: <code>PageRangeError</code> if index out of range</p>"},{"location":"concepts/document-model/#get_page_sizeidx","title":"get_page_size(idx)","text":"<p>Get page dimensions without rendering.</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\n\nwidth, height = doc.get_page_size(0)  # Fast, no rendering\nprint(f\"Page 0: {width}x{height} pixels\")\n\n# Useful for pre-processing\nif width &gt; 2000 or height &gt; 2000:\n    print(\"High resolution page\")\n</code></pre> <p>Returns: Tuple of (width, height) in pixels</p> <p>Raises: <code>PageRangeError</code> if index out of range</p> <p>Advantages: No rendering needed, very fast</p>"},{"location":"concepts/document-model/#iter_pages","title":"iter_pages()","text":"<p>Iterate over pages one at a time (memory efficient).</p> <pre><code>doc = Document.from_pdf(\"long_document.pdf\")\n\nfor page in doc.iter_pages():\n    # Each iteration:\n    # - Loads next page\n    # - Renders to image\n    # - Provides PIL Image\n    result = extractor.extract(page)\n\n    # Clear memory after use\n    doc.clear_cache()\n\n# Equivalent to:\nfor i in range(doc.page_count):\n    page = doc.get_page(i)\n    result = extractor.extract(page)\n</code></pre> <p>Yields: PIL Images (one at a time)</p> <p>Memory: Only 1 page rendered at a time</p>"},{"location":"concepts/document-model/#document-properties","title":"Document Properties","text":""},{"location":"concepts/document-model/#page_count","title":"page_count","text":"<p>Number of pages in the document.</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nprint(doc.page_count)  # 12\n\ndoc = Document.from_pdf(\"paper.pdf\", page_range=(0, 4))\nprint(doc.page_count)  # 5 (only loaded pages)\n</code></pre>"},{"location":"concepts/document-model/#metadata","title":"metadata","text":"<p>DocumentMetadata object with source information.</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\n\nmeta = doc.metadata\nprint(meta.source_type)  # \"file\"\nprint(meta.source_path)  # \"/absolute/path/to/paper.pdf\"\nprint(meta.file_name)  # \"paper.pdf\"\nprint(meta.file_size)  # Bytes\nprint(meta.page_count)  # Number of pages\nprint(meta.format)  # \"pdf\"\nprint(meta.image_dpi)  # 150\nprint(meta.loaded_at)  # ISO timestamp\n</code></pre>"},{"location":"concepts/document-model/#text","title":"text","text":"<p>Full document text (lazy, cached).</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\n\n# First access: extracts from PDF\nfull_text = doc.text\n# Uses pypdfium2 first (fast), falls back to pdfplumber\n\n# Subsequent accesses: return cached value\ntext_again = doc.text  # Instant\n</code></pre> <p>Performance Note: Caches after first access. Large documents may take a few seconds first time.</p>"},{"location":"concepts/document-model/#pages","title":"pages","text":"<p>List of all page images.</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\n\n# WARNING: Renders ALL pages into memory\nall_pages = doc.pages  # [PIL Image, PIL Image, ...]\n\n# Don't do this for large documents!\n# Better: use iter_pages() or get_page() individually\n</code></pre> <p>Warning: Renders all pages at once. Use <code>iter_pages()</code> or <code>get_page()</code> for better memory efficiency.</p>"},{"location":"concepts/document-model/#documentmetadata","title":"DocumentMetadata","text":"<p>Metadata about the document source.</p> <pre><code>class DocumentMetadata(BaseModel):\n    # Source information\n    source_type: str  # \"file\", \"url\", \"bytes\", \"image\"\n    source_path: Optional[str]  # Path or URL\n    file_name: Optional[str]  # Just the filename\n    file_size: Optional[int]  # Bytes\n\n    # PDF metadata\n    pdf_metadata: Optional[Dict[str, Any]]  # From PDF metadata\n\n    # Document properties\n    page_count: int  # Number of pages\n    format: str  # \"pdf\", \"png\", \"jpg\", \"images\"\n\n    # Image rendering\n    image_dpi: int  # DPI for page rendering (50-600)\n    image_format: str  # Color format (usually \"RGB\")\n\n    # Text extraction\n    text_extraction_engine: Optional[str]  # \"pypdfium2\", \"pdfplumber\"\n\n    # Timestamps\n    loaded_at: str  # ISO 8601 timestamp\n</code></pre>"},{"location":"concepts/document-model/#example-usage","title":"Example Usage","text":"<pre><code>doc = Document.from_pdf(\"paper.pdf\")\n\nmeta = doc.metadata\nprint(f\"Loaded: {meta.file_name}\")\nprint(f\"Pages: {meta.page_count}\")\nprint(f\"Size: {meta.file_size} bytes\")\nprint(f\"DPI: {meta.image_dpi}\")\nprint(f\"Source: {meta.source_type}\")\n\n# PDF-specific metadata\nif meta.pdf_metadata:\n    print(f\"Title: {meta.pdf_metadata.get('Title')}\")\n    print(f\"Author: {meta.pdf_metadata.get('Author')}\")\n\n# Convert to dict\nmeta_dict = meta.model_dump()\n</code></pre>"},{"location":"concepts/document-model/#memory-management","title":"Memory Management","text":""},{"location":"concepts/document-model/#caching-strategy","title":"Caching Strategy","text":"<p>The Document class uses two levels of caching:</p> <ol> <li>Page Cache: Individual pages cached in LazyPage</li> <li>Full Text Cache: Entire document text cached</li> </ol> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\n\n# Page caching (automatic)\npage = doc.get_page(0)  # Rendered and cached\npage = doc.get_page(0)  # Returns cached copy (fast)\n\n# Text caching (automatic)\ntext = doc.text  # Extracted and cached\ntext = doc.text  # Returns cached copy (instant)\n</code></pre>"},{"location":"concepts/document-model/#explicit-cache-control","title":"Explicit Cache Control","text":""},{"location":"concepts/document-model/#clear_cache","title":"clear_cache()","text":"<p>Free cached pages from memory.</p> <pre><code>doc = Document.from_pdf(\"big_book.pdf\")\n\n# Process with memory control\nfor i in range(doc.page_count):\n    page = doc.get_page(i)\n    result = extractor.extract(page)\n\n    # Free previous page from memory\n    if i &gt; 0:\n        doc.clear_cache(i - 1)\n\n# Or clear all at once\ndoc.clear_cache()\n\n# Check memory usage\nimport psutil\nprocess = psutil.Process()\nprint(process.memory_info().rss)  # Resident memory\n</code></pre>"},{"location":"concepts/document-model/#when-processing-large-documents","title":"When Processing Large Documents","text":"<pre><code># \u274c BAD: Renders all pages at once\ndoc = Document.from_pdf(\"huge_book.pdf\")  # 1000 pages\nall_pages = doc.pages  # Tries to render all 1000 pages at once\n# Memory error!\n\n# \u2705 GOOD: Process one page at a time\ndoc = Document.from_pdf(\"huge_book.pdf\")\nfor page in doc.iter_pages():\n    result = extractor.extract(page)\n    # Only 1 page in memory at a time\n    # Doc doesn't cache unless you keep page variables\n</code></pre>"},{"location":"concepts/document-model/#save_images","title":"save_images()","text":"<p>Save all pages as individual image files.</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\n\n# Save all pages\npaths = doc.save_images(\n    output_dir=\"output/\",\n    prefix=\"page\",  # Files: page_001.png, page_002.png, ...\n    format=\"PNG\",  # PNG or JPEG\n)\n\nprint(f\"Saved {len(paths)} images\")\n</code></pre>"},{"location":"concepts/document-model/#common-patterns","title":"Common Patterns","text":""},{"location":"concepts/document-model/#pattern-1-process-one-page","title":"Pattern 1: Process One Page","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\ndoc = Document.from_pdf(\"paper.pdf\")\nlayout = DocLayoutYOLO(config=DocLayoutYOLOConfig())\n\npage = doc.get_page(0)\nresult = layout.extract(page)\n\nprint(f\"Found {len(result.bboxes)} layout elements\")\n</code></pre>"},{"location":"concepts/document-model/#pattern-2-process-all-pages-memory-efficient","title":"Pattern 2: Process All Pages (Memory Efficient)","text":"<pre><code>doc = Document.from_pdf(\"paper.pdf\")\nextractor = DocLayoutYOLO(config=DocLayoutYOLOConfig())\n\nresults = []\nfor i, page in enumerate(doc.iter_pages()):\n    result = extractor.extract(page)\n    results.append({\n        \"page\": i,\n        \"elements\": len(result.bboxes),\n    })\n\n    # Free memory after processing\n    if i &gt; 0:\n        doc.clear_cache(i - 1)\n\nprint(f\"Processed {len(results)} pages\")\n</code></pre>"},{"location":"concepts/document-model/#pattern-3-extract-text-from-entire-document","title":"Pattern 3: Extract Text from Entire Document","text":"<pre><code>doc = Document.from_pdf(\"paper.pdf\")\n\n# Option 1: Use PDF text extraction (fast, no model)\npdf_text = doc.text\nprint(len(pdf_text), \"characters extracted from PDF\")\n\n# Option 2: Use VLM extraction (slower, higher quality)\nextractor = QwenTextExtractor(backend=QwenTextPyTorchConfig())\n\nfull_content = \"\"\nfor page in doc.iter_pages():\n    result = extractor.extract(page, output_format=\"markdown\")\n    full_content += result.content + \"\\n\\n\"\n\nprint(full_content[:500])\n</code></pre>"},{"location":"concepts/document-model/#pattern-4-process-with-page-range","title":"Pattern 4: Process with Page Range","text":"<pre><code># Only process pages 0-9\ndoc = Document.from_pdf(\n    \"large_document.pdf\",\n    page_range=(0, 9),  # First 10 pages only\n)\n\n# Process as usual\nfor page in doc.iter_pages():\n    result = extractor.extract(page)\n</code></pre>"},{"location":"concepts/document-model/#pattern-5-load-from-different-sources","title":"Pattern 5: Load from Different Sources","text":"<pre><code># From file\ndoc = Document.from_pdf(\"local_file.pdf\")\n\n# From URL\ndoc = Document.from_url(\"https://example.com/doc.pdf\")\n\n# From downloaded bytes\nimport requests\nresponse = requests.get(\"https://example.com/file.pdf\")\ndoc = Document.from_bytes(response.content, filename=\"doc.pdf\")\n\n# From images\ndoc = Document.from_images([\"page1.png\", \"page2.png\"])\n\n# Same API for all sources!\nfor page in doc.iter_pages():\n    result = extractor.extract(page)\n</code></pre>"},{"location":"concepts/document-model/#pattern-6-use-as-context-manager","title":"Pattern 6: Use as Context Manager","text":"<pre><code># Automatically close and free resources\nwith Document.from_pdf(\"paper.pdf\") as doc:\n    for page in doc.iter_pages():\n        result = extractor.extract(page)\n# Resources freed automatically\n</code></pre>"},{"location":"concepts/document-model/#when-to-use-document","title":"When to Use Document","text":""},{"location":"concepts/document-model/#use-document-when","title":"Use Document When","text":"<ul> <li>Loading PDFs or images - Natural entry point</li> <li>Need multiple pages - Built-in pagination</li> <li>Memory is constrained - Lazy loading efficiency</li> <li>Want metadata - Source info, page count, etc.</li> <li>Processing in pipelines - Integrates with extractors</li> </ul> <pre><code>doc = Document.from_pdf(\"paper.pdf\")  # \u2713 Use Document\nfor page in doc.iter_pages():\n    result = extractor.extract(page)\n</code></pre>"},{"location":"concepts/document-model/#use-direct-image-when","title":"Use Direct Image When","text":"<ul> <li>Already have PIL Image - Skip Document</li> <li>Single image analysis - Direct extractor call</li> <li>Image from different source - Camera, API, etc.</li> </ul> <pre><code>from PIL import Image\nimage = Image.open(\"screenshot.png\")  # \u2713 Direct image\nresult = layout.extract(image)  # No Document needed\n</code></pre>"},{"location":"concepts/document-model/#pattern-conditional-use","title":"Pattern: Conditional Use","text":"<pre><code>def process_document(source: str):\n    # If source is a path or URL, use Document\n    if source.endswith(\".pdf\"):\n        doc = Document.from_pdf(source)\n        for page in doc.iter_pages():\n            result = extractor.extract(page)\n\n    # If source is a URL\n    elif source.startswith(\"http\"):\n        doc = Document.from_url(source)\n        # ... same pattern\n\n    # If source is an image\n    elif source.endswith((\".png\", \".jpg\")):\n        image = Image.open(source)\n        result = extractor.extract(image)\n</code></pre>"},{"location":"concepts/document-model/#summary","title":"Summary","text":"<p>The Document class provides:</p> Feature Benefit Lazy page loading Fast document loading, efficient memory Automatic caching Repeated access is instant Multiple source formats Unified API for PDF, URL, bytes, images Explicit cache control Fine-grained memory management Metadata Source information and document properties Iterator interface Processing large documents efficiently Context manager Automatic resource cleanup <p>Key Principle: Document is stateless. It loads and provides source data, nothing more. Extractors handle analysis independently.</p>"},{"location":"concepts/document-model/#next-steps","title":"Next Steps","text":"<ul> <li>See Architecture Overview for how Document fits in the system</li> <li>See Config Pattern for how to configure extractors</li> <li>See Backend System for understanding inference backends</li> </ul>"},{"location":"contributing/","title":"Contributing to OmniDocs","text":"<p>Thank you for your interest in contributing to OmniDocs!</p>"},{"location":"contributing/#quick-links","title":"Quick Links","text":"<ul> <li>Development Workflow - 6-phase implementation process</li> <li>Adding Models - How to add new model support</li> <li>Style Guide - Code standards and conventions</li> </ul>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#1-clone-the-repository","title":"1. Clone the repository","text":"<pre><code>git clone https://github.com/adithya-s-k/OmniDocs.git\ncd OmniDocs/Omnidocs\n</code></pre>"},{"location":"contributing/#2-install-dependencies-with-uv","title":"2. Install dependencies with uv","text":"<pre><code># Install uv if you don't have it\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Sync dependencies\nuv sync\n</code></pre>"},{"location":"contributing/#3-run-tests","title":"3. Run tests","text":"<pre><code>uv run pytest tests/ -v\n</code></pre>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>Omnidocs/\n\u251c\u2500\u2500 omnidocs/          # Main package\n\u2502   \u251c\u2500\u2500 document.py    # Document loading\n\u2502   \u251c\u2500\u2500 tasks/         # Task extractors\n\u2502   \u251c\u2500\u2500 inference/     # Backend implementations\n\u2502   \u2514\u2500\u2500 utils/         # Utilities\n\u251c\u2500\u2500 tests/             # Test suite\n\u2502   \u251c\u2500\u2500 fixtures/      # Test data (PDFs, images)\n\u2502   \u2514\u2500\u2500 tasks/         # Task tests\n\u2514\u2500\u2500 docs/              # Documentation\n</code></pre>"},{"location":"contributing/#design-documents","title":"Design Documents","text":"<p>Read Before Implementing</p> <p>Before implementing new features, read the architecture docs:</p> <ul> <li>Architecture Overview - System design</li> <li>Backend System - Multi-backend support</li> <li>Config Pattern - Configuration design</li> </ul>"},{"location":"contributing/#building-documentation","title":"Building Documentation","text":"<pre><code># Install docs dependencies\nuv sync --group docs\n\n# Serve docs with live reload\nuv run mkdocs serve\n\n# Open http://127.0.0.1:8000\n</code></pre>"},{"location":"contributing/#need-help","title":"Need Help?","text":"<ul> <li>Open an issue</li> <li>Check the Roadmap</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the Apache 2.0 License.</p>"},{"location":"contributing/adding-models/","title":"Adding New Models to OmniDocs","text":"<p>This guide provides step-by-step instructions for integrating a new model into OmniDocs.</p>"},{"location":"contributing/adding-models/#before-you-start","title":"Before You Start","text":"<ol> <li>Read Workflow - Understand the 6-phase process</li> <li>Read IMPLEMENTATION_PLAN/BACKEND_ARCHITECTURE.md</li> <li>Verify the model you want to add doesn't already exist</li> </ol>"},{"location":"contributing/adding-models/#step-1-create-experiment-script","title":"Step 1: Create Experiment Script","text":"<p>Create a standalone test script in <code>scripts/</code> to verify the model works.</p>"},{"location":"contributing/adding-models/#for-gpu-models-pytorchvllm","title":"For GPU Models (PyTorch/VLLM)","text":"<p>Create <code>scripts/text_extract/modal_mymodel_pytorch.py</code>:</p> <pre><code>import modal\nfrom pathlib import Path\n\napp = modal.App(\"test-mymodel\")\n\n# Standard CUDA configuration\ncuda_version = \"12.4.0\"\nflavor = \"devel\"\noperating_sys = \"ubuntu22.04\"\ntag = f\"{cuda_version}-{flavor}-{operating_sys}\"\n\n# Base image (cached across scripts)\nBASE_IMAGE = (\n    modal.Image.from_registry(f\"nvidia/cuda:{tag}\", add_python=\"3.12\")\n    .apt_install(\"libgl1-mesa-glx\", \"libglib2.0-0\")\n    .uv_pip_install(\n        \"torch==2.5.1\",\n        \"torchvision\",\n        \"torchaudio\",\n        \"transformers\",\n        \"pillow\",\n        \"numpy\",\n        \"pydantic\",\n        \"huggingface_hub\",\n        \"accelerate\",\n    )\n    .env({\n        \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",\n        \"HF_HOME\": \"/data/.cache\",\n    })\n)\n\n# Model-specific dependencies\nIMAGE = BASE_IMAGE.uv_pip_install(\"mymodel-package\")\n\nvolume = modal.Volume.from_name(\"omnidocs\", create_if_missing=True)\nsecret = modal.Secret.from_name(\"adithya-hf-wandb\")\n\n@app.function(\n    image=IMAGE,\n    gpu=\"A10G:1\",\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=600,\n)\ndef test_model():\n    \"\"\"Test model loading and inference.\"\"\"\n    import torch\n    from transformers import AutoModelForCausalLM, AutoProcessor\n    from PIL import Image\n    import requests\n    from io import BytesIO\n\n    MODEL_NAME = \"vendor/mymodel\"\n\n    # Load model\n    print(\"Loading model...\")\n    processor = AutoProcessor.from_pretrained(\n        MODEL_NAME,\n        trust_remote_code=True,\n    )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        trust_remote_code=True,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n    ).eval()\n\n    # Load test image\n    print(\"Loading test image...\")\n    response = requests.get(\"https://example.com/test.png\")\n    image = Image.open(BytesIO(response.content))\n\n    # Run inference\n    print(\"Running inference...\")\n    inputs = processor(text=\"Extract text\", images=image, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_new_tokens=4096)\n\n    result = processor.decode(outputs[0], skip_special_tokens=True)\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"RESULT:\")\n    print(\"=\"*60)\n    print(result)\n\n    return {\"success\": True, \"length\": len(result)}\n\n@app.local_entrypoint()\ndef main():\n    result = test_model.remote()\n    print(f\"\\nTest completed: {result['success']}\")\n</code></pre>"},{"location":"contributing/adding-models/#for-api-models-local","title":"For API Models (Local)","text":"<p>Create <code>scripts/text_extract/litellm_mymodel_text.py</code>:</p> <pre><code>import os\nfrom PIL import Image\nfrom openai import OpenAI\nimport base64\nfrom io import BytesIO\n\ndef encode_image(image: Image.Image) -&gt; str:\n    \"\"\"Encode PIL Image to base64.\"\"\"\n    buffered = BytesIO()\n    image.save(buffered, format=\"PNG\")\n    img_bytes = buffered.getvalue()\n    return f\"data:image/png;base64,{base64.b64encode(img_bytes).decode()}\"\n\n# Initialize client\nclient = OpenAI(\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n)\n\n# Test image\nimage = Image.open(\"test.png\")\n\n# Run inference\nresponse = client.chat.completions.create(\n    model=\"vendor/mymodel\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image_url\", \"image_url\": {\"url\": encode_image(image)}},\n                {\"type\": \"text\", \"text\": \"Extract text in markdown format\"}\n            ]\n        }\n    ],\n    max_tokens=4096,\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"contributing/adding-models/#run-and-validate","title":"Run and Validate","text":"<pre><code>cd scripts/text_extract/\n\n# GPU models\nmodal run modal_mymodel_pytorch.py\n\n# API models (local)\npython litellm_mymodel_text.py\n</code></pre> <p>Checklist: - [ ] Model loads - [ ] Inference runs - [ ] Output is reasonable - [ ] Error handling works</p>"},{"location":"contributing/adding-models/#step-2-decide-single-or-multi-backend","title":"Step 2: Decide: Single or Multi-Backend?","text":""},{"location":"contributing/adding-models/#single-backend-model","title":"Single-Backend Model","text":"<p>When: Model only works with one inference backend (e.g., YOLO-based models)</p> <p>File Structure: <pre><code>omnidocs/tasks/text_extraction/\n\u251c\u2500\u2500 mymodel.py           # Config + Extractor in same file\n</code></pre></p> <p>Example: <pre><code>from pydantic import BaseModel, Field\n\nclass MyModelConfig(BaseModel):\n    \"\"\"Config for MyModel (PyTorch only).\"\"\"\n    device: str = Field(default=\"cuda\")\n    model_name: str = Field(default=\"vendor/mymodel\")\n    class Config:\n        extra = \"forbid\"\n\nclass MyModelTextExtractor:\n    def __init__(self, config: MyModelConfig):\n        self.config = config\n        # Load model\n\n    def extract(self, image, output_format=\"markdown\"):\n        # Extraction logic\n</code></pre></p>"},{"location":"contributing/adding-models/#multi-backend-model","title":"Multi-Backend Model","text":"<p>When: Model can use multiple backends (PyTorch, VLLM, MLX, API)</p> <p>File Structure: <pre><code>omnidocs/tasks/text_extraction/\n\u251c\u2500\u2500 mymodel.py           # Main extractor class\n\u2514\u2500\u2500 mymodel/             # Backend configurations\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 pytorch.py       # MyModelPyTorchConfig\n    \u251c\u2500\u2500 vllm.py          # MyModelVLLMConfig\n    \u251c\u2500\u2500 mlx.py           # MyModelMLXConfig\n    \u2514\u2500\u2500 api.py           # MyModelAPIConfig\n</code></pre></p> <p>Example: <pre><code>from typing import Union\n\nQwenBackendConfig = Union[\n    MyModelPyTorchConfig,\n    MyModelVLLMConfig,\n    MyModelMLXConfig,\n    MyModelAPIConfig,\n]\n\nclass MyModelTextExtractor:\n    def __init__(self, backend: MyModelBackendConfig):\n        self.backend_config = backend\n        self._backend = self._create_backend()\n\n    def _create_backend(self):\n        if isinstance(self.backend_config, MyModelPyTorchConfig):\n            # Create PyTorch backend\n        elif isinstance(self.backend_config, MyModelVLLMConfig):\n            # Create VLLM backend\n        # ...\n</code></pre></p>"},{"location":"contributing/adding-models/#step-3-create-config-classes","title":"Step 3: Create Config Classes","text":""},{"location":"contributing/adding-models/#single-backend-config","title":"Single-Backend Config","text":"<pre><code># omnidocs/tasks/text_extraction/mymodel.py\n\nfrom pydantic import BaseModel, Field\nfrom typing import Literal, Optional\n\nclass MyModelConfig(BaseModel):\n    \"\"\"Configuration for MyModel (PyTorch only).\"\"\"\n\n    # Required\n    model: str = Field(\n        default=\"vendor/mymodel-8b\",\n        description=\"Model identifier\"\n    )\n\n    # Optional with defaults\n    device: str = Field(\n        default=\"cuda\",\n        description=\"Device to run on (cuda/cpu)\"\n    )\n\n    torch_dtype: Literal[\"float16\", \"bfloat16\", \"float32\"] = Field(\n        default=\"bfloat16\",\n        description=\"Torch data type\"\n    )\n\n    # Numeric with validation\n    max_new_tokens: int = Field(\n        default=4096,\n        ge=1,\n        le=32768,\n        description=\"Maximum tokens to generate\"\n    )\n\n    class Config:\n        extra = \"forbid\"  # CRITICAL: Catch typos\n</code></pre>"},{"location":"contributing/adding-models/#multi-backend-configs","title":"Multi-Backend Configs","text":"<pre><code># omnidocs/tasks/text_extraction/mymodel/pytorch.py\n\nclass MyModelPyTorchConfig(BaseModel):\n    \"\"\"PyTorch backend for MyModel.\"\"\"\n\n    model: str = Field(..., description=\"Model ID\")\n    device: str = Field(default=\"cuda\")\n    torch_dtype: Literal[\"float16\", \"bfloat16\", \"float32\"] = Field(default=\"bfloat16\")\n    device_map: Optional[str] = Field(default=\"auto\")\n\n    class Config:\n        extra = \"forbid\"\n\n# omnidocs/tasks/text_extraction/mymodel/vllm.py\n\nclass MyModelVLLMConfig(BaseModel):\n    \"\"\"VLLM backend for MyModel.\"\"\"\n\n    model: str = Field(..., description=\"Model ID\")\n    tensor_parallel_size: int = Field(default=1, ge=1)\n    gpu_memory_utilization: float = Field(default=0.9, ge=0.1, le=1.0)\n    max_model_len: int = Field(default=32768)\n\n    class Config:\n        extra = \"forbid\"\n\n# omnidocs/tasks/text_extraction/mymodel/api.py\n\nclass MyModelAPIConfig(BaseModel):\n    \"\"\"API backend for MyModel.\"\"\"\n\n    model: str = Field(..., description=\"API model name\")\n    api_key: str = Field(..., description=\"API key\")\n    base_url: Optional[str] = Field(None, description=\"API base URL\")\n\n    class Config:\n        extra = \"forbid\"\n</code></pre>"},{"location":"contributing/adding-models/#step-4-create-extractor-class","title":"Step 4: Create Extractor Class","text":""},{"location":"contributing/adding-models/#single-backend-extractor","title":"Single-Backend Extractor","text":"<pre><code># omnidocs/tasks/text_extraction/mymodel.py\n\nfrom .base import BaseTextExtractor\nfrom .models import TextOutput\nfrom PIL import Image\n\nclass MyModelTextExtractor(BaseTextExtractor):\n    \"\"\"MyModel text extractor (PyTorch only).\"\"\"\n\n    def __init__(self, config: MyModelConfig):\n        self.config = config\n        self._load_model()\n\n    def _load_model(self):\n        \"\"\"Load model with PyTorch.\"\"\"\n        import torch\n        from transformers import AutoModelForCausalLM, AutoProcessor\n\n        self.processor = AutoProcessor.from_pretrained(\n            self.config.model,\n            trust_remote_code=True,\n        )\n\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.config.model,\n            trust_remote_code=True,\n            torch_dtype=torch.bfloat16 if self.config.torch_dtype == \"bfloat16\" else torch.float16,\n            device_map=self.config.device,\n        ).eval()\n\n    def extract(\n        self,\n        image: Image.Image,\n        output_format: str = \"markdown\",\n        **kwargs\n    ) -&gt; TextOutput:\n        \"\"\"Extract text from image.\"\"\"\n        import torch\n\n        prompt = self._get_prompt(output_format)\n\n        inputs = self.processor(\n            text=prompt,\n            images=image,\n            return_tensors=\"pt\"\n        ).to(self.model.device)\n\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=self.config.max_new_tokens,\n            )\n\n        raw_output = self.processor.decode(outputs[0], skip_special_tokens=True)\n\n        return TextOutput(\n            content=raw_output,\n            format=output_format,\n            raw_output=raw_output,\n        )\n\n    def _get_prompt(self, output_format: str) -&gt; str:\n        \"\"\"Get extraction prompt.\"\"\"\n        if output_format == \"markdown\":\n            return \"Extract text in markdown format...\"\n        elif output_format == \"html\":\n            return \"Extract text in HTML format...\"\n        else:\n            return \"Extract all text...\"\n</code></pre>"},{"location":"contributing/adding-models/#multi-backend-extractor","title":"Multi-Backend Extractor","text":"<pre><code># omnidocs/tasks/text_extraction/mymodel.py\n\nfrom typing import Union, Optional\nfrom .base import BaseTextExtractor\nfrom .models import TextOutput\nfrom omnidocs.tasks.text_extraction.mymodel import (\n    MyModelPyTorchConfig,\n    MyModelVLLMConfig,\n    MyModelAPIConfig,\n)\n\nMyModelBackendConfig = Union[\n    MyModelPyTorchConfig,\n    MyModelVLLMConfig,\n    MyModelAPIConfig,\n]\n\nclass MyModelTextExtractor(BaseTextExtractor):\n    \"\"\"MyModel text extractor with multi-backend support.\"\"\"\n\n    def __init__(self, backend: MyModelBackendConfig):\n        self.backend_config = backend\n        self._backend = self._create_backend()\n\n    def _create_backend(self):\n        \"\"\"Create appropriate backend based on config type.\"\"\"\n        if isinstance(self.backend_config, MyModelPyTorchConfig):\n            try:\n                from omnidocs.inference.pytorch import PyTorchInference\n            except ImportError:\n                raise ImportError(\n                    \"PyTorch backend requires torch and transformers. \"\n                    \"Install with: pip install omnidocs[pytorch]\"\n                )\n            return PyTorchInference(self.backend_config)\n\n        elif isinstance(self.backend_config, MyModelVLLMConfig):\n            try:\n                from omnidocs.inference.vllm import VLLMInference\n            except ImportError:\n                raise ImportError(\n                    \"VLLM backend requires vllm. \"\n                    \"Install with: pip install omnidocs[vllm]\"\n                )\n            return VLLMInference(self.backend_config)\n\n        elif isinstance(self.backend_config, MyModelAPIConfig):\n            try:\n                from omnidocs.inference.api import APIInference\n            except ImportError:\n                raise ImportError(\n                    \"API backend requires openai. \"\n                    \"Install with: pip install omnidocs[api]\"\n                )\n            return APIInference(self.backend_config)\n\n        else:\n            raise TypeError(f\"Unknown backend: {type(self.backend_config)}\")\n\n    def extract(\n        self,\n        image: Image.Image,\n        output_format: str = \"markdown\",\n        **kwargs\n    ) -&gt; TextOutput:\n        \"\"\"Extract text from image.\"\"\"\n        prompt = self._get_prompt(output_format)\n        raw_output = self._backend.infer(image, prompt)\n\n        return TextOutput(\n            content=raw_output,\n            format=output_format,\n            raw_output=raw_output,\n        )\n</code></pre>"},{"location":"contributing/adding-models/#step-5-update-package-exports","title":"Step 5: Update Package Exports","text":"<p>Edit <code>omnidocs/tasks/text_extraction/__init__.py</code>:</p> <pre><code># Existing imports\nfrom .base import BaseTextExtractor\nfrom .models import TextOutput\n\n# New imports - single backend\nfrom .mymodel import MyModelTextExtractor, MyModelConfig\n\n# OR - multi-backend\nfrom .mymodel import MyModelTextExtractor\nfrom .mymodel import (\n    MyModelPyTorchConfig,\n    MyModelVLLMConfig,\n    MyModelAPIConfig,\n)\n\n__all__ = [\n    # Base\n    \"BaseTextExtractor\",\n    \"TextOutput\",\n\n    # Existing\n    \"QwenTextExtractor\",\n    \"DotsOCRTextExtractor\",\n\n    # New\n    \"MyModelTextExtractor\",\n    \"MyModelConfig\",  # or MyModelPyTorchConfig, MyModelVLLMConfig, etc.\n]\n</code></pre>"},{"location":"contributing/adding-models/#step-6-add-dependencies","title":"Step 6: Add Dependencies","text":"<pre><code>cd Omnidocs/\n\n# Add model-specific package\nuv add --group pytorch mymodel-package\n\n# Sync virtual environment\nuv sync\n</code></pre>"},{"location":"contributing/adding-models/#step-7-write-tests","title":"Step 7: Write Tests","text":"<p>Create <code>Omnidocs/tests/tasks/text_extraction/test_mymodel.py</code>:</p> <pre><code>import pytest\nfrom omnidocs.tasks.text_extraction import MyModelTextExtractor, MyModelConfig\n\nclass TestMyModelConfig:\n    \"\"\"Test configuration validation.\"\"\"\n\n    def test_valid_config(self):\n        config = MyModelConfig(model=\"vendor/mymodel\")\n        assert config.device == \"cuda\"\n        assert config.torch_dtype == \"bfloat16\"\n\n    def test_invalid_dtype(self):\n        with pytest.raises(ValueError):\n            MyModelConfig(model=\"vendor/mymodel\", torch_dtype=\"invalid\")\n\n    def test_extra_fields_forbidden(self):\n        with pytest.raises(ValueError):\n            MyModelConfig(model=\"vendor/mymodel\", invalid_param=\"value\")\n\nclass TestMyModelExtractor:\n    \"\"\"Test extractor functionality.\"\"\"\n\n    @pytest.fixture\n    def sample_image(self):\n        from PIL import Image\n        return Image.new(\"RGB\", (800, 600), color=\"white\")\n\n    def test_extract_markdown(self, sample_image):\n        config = MyModelConfig(model=\"vendor/mymodel\", device=\"cpu\")\n        extractor = MyModelTextExtractor(config=config)\n\n        result = extractor.extract(sample_image, output_format=\"markdown\")\n\n        assert result.format.value == \"markdown\"\n        assert isinstance(result.content, str)\n</code></pre> <p>Run tests: <pre><code>cd Omnidocs/\nuv run pytest tests/tasks/text_extraction/test_mymodel.py -v\n</code></pre></p>"},{"location":"contributing/adding-models/#step-8-integration-test","title":"Step 8: Integration Test","text":"<p>Create <code>scripts/text_extract_omnidocs/modal_mymodel_text_hf.py</code>:</p> <pre><code>\"\"\"Test MyModel through Omnidocs package on Modal.\"\"\"\n\nimport modal\n\napp = modal.App(\"test-mymodel-omnidocs\")\n\n# Standard image with Omnidocs\nOMNIDOCS_IMAGE = (\n    modal.Image.from_registry(\"nvidia/cuda:12.4.0-devel-ubuntu22.04\", add_python=\"3.12\")\n    .apt_install(\"libgl1-mesa-glx\", \"libglib2.0-0\")\n    .uv_pip_install(\"torch\", \"transformers\", \"pillow\")\n    .run_commands(\"uv pip install -e /pkg/Omnidocs --system\")\n    .env({\"HF_HOME\": \"/data/.cache\"})\n)\n\nvolume = modal.Volume.from_name(\"omnidocs\", create_if_missing=True)\nsecret = modal.Secret.from_name(\"adithya-hf-wandb\")\npkg_mount = modal.Mount.from_local_dir(\n    Path(__file__).parent.parent.parent / \"Omnidocs\",\n    remote_path=\"/pkg/Omnidocs\"\n)\n\n@app.function(\n    image=OMNIDOCS_IMAGE,\n    gpu=\"A10G:1\",\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    mounts=[pkg_mount],\n    timeout=600,\n)\ndef test_omnidocs_mymodel():\n    \"\"\"Test MyModel through Omnidocs.\"\"\"\n    from omnidocs.tasks.text_extraction import MyModelTextExtractor\n    from omnidocs.tasks.text_extraction.mymodel import MyModelPyTorchConfig\n    from PIL import Image\n\n    extractor = MyModelTextExtractor(\n        config=MyModelPyTorchConfig(\n            model=\"vendor/mymodel\",\n            device=\"cuda\",\n        )\n    )\n\n    test_image = Image.new(\"RGB\", (800, 600), color=\"white\")\n    result = extractor.extract(test_image, output_format=\"markdown\")\n\n    assert result.format.value == \"markdown\"\n    assert len(result.content) &gt; 0\n\n    return {\"success\": True, \"length\": len(result.content)}\n\n@app.local_entrypoint()\ndef main():\n    result = test_omnidocs_mymodel.remote()\n    print(f\"Test passed: {result}\")\n</code></pre>"},{"location":"contributing/adding-models/#step-9-lint-checks","title":"Step 9: Lint Checks","text":"<pre><code>cd Omnidocs/\n\n# Format code\nuv run black omnidocs/\n\n# Type checking\nuv run mypy omnidocs/\n</code></pre>"},{"location":"contributing/adding-models/#step-10-submit-pr","title":"Step 10: Submit PR","text":"<p>Follow Workflow - Phase 5: Pull Request.</p>"},{"location":"contributing/adding-models/#checklist","title":"Checklist","text":"<ul> <li>[ ] Experiment script created and tested</li> <li>[ ] Backend type chosen (single or multi)</li> <li>[ ] Config classes written with validation</li> <li>[ ] Extractor class implemented</li> <li>[ ] Package exports updated</li> <li>[ ] Dependencies added (uv add)</li> <li>[ ] Unit tests &gt;80% coverage</li> <li>[ ] Integration test passing on Modal</li> <li>[ ] Black formatting applied</li> <li>[ ] Mypy checks passing</li> <li>[ ] PR created and reviewed</li> </ul>"},{"location":"contributing/adding-models/#next-steps","title":"Next Steps","text":"<p>After PR approval and merge, follow Workflow - Phase 6: Release to publish the new version.</p>"},{"location":"contributing/style-guide/","title":"Style Guide","text":"<p>OmniDocs follows consistent code and documentation standards to maintain quality and clarity.</p>"},{"location":"contributing/style-guide/#code-style","title":"Code Style","text":""},{"location":"contributing/style-guide/#type-hints","title":"Type Hints","text":"<p>All public APIs must have complete type hints:</p> <pre><code># \u2705 GOOD\ndef extract(self, image: Image.Image, output_format: str = \"markdown\") -&gt; TextOutput:\n    \"\"\"Extract text from image.\"\"\"\n    pass\n\n# \u274c BAD\ndef extract(self, image, output_format=\"markdown\"):\n    \"\"\"Extract text from image.\"\"\"\n    pass\n</code></pre>"},{"location":"contributing/style-guide/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings for all public classes and methods:</p> <pre><code>def extract(\n    self,\n    image: Image.Image,\n    output_format: str = \"markdown\",\n    include_layout: bool = False,\n) -&gt; TextOutput:\n    \"\"\"Extract text from document image.\n\n    Converts document images to formatted text using the configured model\n    and backend.\n\n    Args:\n        image: PIL Image to extract text from.\n        output_format: Output format (\"markdown\" or \"html\").\n        include_layout: Include layout information in output.\n\n    Returns:\n        TextOutput containing extracted content.\n\n    Raises:\n        ValueError: If output_format is invalid.\n        RuntimeError: If model is not loaded.\n\n    Example:\n        &gt;&gt;&gt; extractor = QwenTextExtractor(backend=config)\n        &gt;&gt;&gt; result = extractor.extract(image, output_format=\"markdown\")\n        &gt;&gt;&gt; print(result.content)\n    \"\"\"\n</code></pre>"},{"location":"contributing/style-guide/#pydantic-configs","title":"Pydantic Configs","text":"<p>All config classes must follow these rules:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Literal, Optional\n\nclass MyConfig(BaseModel):\n    \"\"\"Clear docstring explaining purpose.\"\"\"\n\n    # Required parameters\n    required_param: str = Field(\n        ...,  # Indicates required\n        description=\"Description of parameter\"\n    )\n\n    # Optional with defaults\n    optional_param: str = Field(\n        default=\"default_value\",\n        description=\"Description\"\n    )\n\n    # Enum/Literal for fixed choices\n    dtype: Literal[\"float16\", \"bfloat16\", \"float32\"] = Field(\n        default=\"bfloat16\",\n        description=\"Data type\"\n    )\n\n    # Numeric with bounds\n    confidence: float = Field(\n        default=0.5,\n        ge=0.0,\n        le=1.0,\n        description=\"Confidence threshold\"\n    )\n\n    # Optional nullable\n    cache_dir: Optional[str] = Field(\n        default=None,\n        description=\"Optional cache directory\"\n    )\n\n    class Config:\n        extra = \"forbid\"  # CRITICAL: Catch typos\n</code></pre> <p>Rules: - \u2705 All parameters use <code>Field(...)</code> - \u2705 All parameters have descriptions - \u2705 Type hints for everything - \u2705 Validation rules (ge, le, Literal) - \u2705 <code>extra = \"forbid\"</code> to catch mistakes - \u2705 Class-level docstring</p>"},{"location":"contributing/style-guide/#error-handling","title":"Error Handling","text":"<p>Provide informative error messages with installation instructions:</p> <pre><code># \u2705 GOOD\nif isinstance(self.backend_config, QwenPyTorchConfig):\n    try:\n        from omnidocs.inference.pytorch import PyTorchInference\n    except ImportError:\n        raise ImportError(\n            \"PyTorch backend requires torch and transformers. \"\n            \"Install with: pip install omnidocs[pytorch]\"\n        )\n    return PyTorchInference(self.backend_config)\n\n# \u274c BAD\nfrom omnidocs.inference.pytorch import PyTorchInference  # Hard requirement\n</code></pre>"},{"location":"contributing/style-guide/#testing-standards","title":"Testing Standards","text":""},{"location":"contributing/style-guide/#test-structure","title":"Test Structure","text":"<p>Use pytest fixtures and clear test names:</p> <pre><code>import pytest\nfrom unittest.mock import Mock, patch\n\nclass TestMyExtractor:\n    \"\"\"Test suite for MyExtractor.\"\"\"\n\n    @pytest.fixture\n    def sample_image(self):\n        \"\"\"Create sample test image.\"\"\"\n        from PIL import Image\n        return Image.new(\"RGB\", (800, 600), color=\"white\")\n\n    @pytest.fixture\n    def config(self):\n        \"\"\"Create test config.\"\"\"\n        return MyConfig(model=\"test-model\", device=\"cpu\")\n\n    def test_valid_config(self, config):\n        \"\"\"Test that valid config initializes.\"\"\"\n        assert config.device == \"cpu\"\n        assert config.model == \"test-model\"\n\n    def test_invalid_param_raises(self):\n        \"\"\"Test that invalid params raise ValidationError.\"\"\"\n        with pytest.raises(ValueError):\n            MyConfig(model=\"test\", invalid_param=\"value\")\n\n    def test_extract_returns_output(self, sample_image, config):\n        \"\"\"Test that extract method returns Output.\"\"\"\n        extractor = MyExtractor(config=config)\n        result = extractor.extract(sample_image)\n\n        assert isinstance(result, MyOutput)\n        assert len(result.content) &gt; 0\n\n    @pytest.mark.skipif(\n        not torch.cuda.is_available(),\n        reason=\"Requires GPU\"\n    )\n    def test_extract_on_gpu(self, sample_image, config):\n        \"\"\"Test GPU extraction.\"\"\"\n        config.device = \"cuda\"\n        extractor = MyExtractor(config=config)\n        result = extractor.extract(sample_image)\n        assert result is not None\n</code></pre>"},{"location":"contributing/style-guide/#test-coverage","title":"Test Coverage","text":"<p>Target &gt;80% coverage for all code:</p> <pre><code># Run tests with coverage\nuv run pytest tests/ --cov=omnidocs --cov-report=html\n\n# Check coverage report\nopen htmlcov/index.html\n</code></pre>"},{"location":"contributing/style-guide/#assertion-best-practices","title":"Assertion Best Practices","text":"<pre><code># \u2705 GOOD - Specific assertions\nassert result.format.value == \"markdown\"\nassert len(result.content) &gt; 100\nassert result.content_length == len(result.content)\n\n# \u274c BAD - Generic assertions\nassert result is not None\nassert bool(result)\n</code></pre>"},{"location":"contributing/style-guide/#documentation-standards","title":"Documentation Standards","text":""},{"location":"contributing/style-guide/#markdown-style","title":"Markdown Style","text":"<pre><code># Main Title (H1)\n\nBrief introductory paragraph explaining purpose.\n\n## Section (H2)\n\nSubsection content.\n\n### Subsection (H3)\n\nUse code blocks for examples:\n\n\\`\\`\\`python\n# Python code example\nresult = extractor.extract(image)\n\\`\\`\\`\n\n### Lists\n\n- Bullet point 1\n- Bullet point 2\n  - Nested point\n  - Another nested\n\n1. Numbered item 1\n2. Numbered item 2\n\n### Tables\n\n| Header 1 | Header 2 |\n|----------|----------|\n| Cell 1   | Cell 2   |\n\n### Links\n\n[Link text](../path/to/file.md)\n</code></pre>"},{"location":"contributing/style-guide/#code-examples","title":"Code Examples","text":"<p>All code examples must be: 1. Complete - Can be copy-pasted and run 2. Runnable - All imports included 3. Correct - Verified against actual API 4. Documented - Comments explaining key parts</p> <pre><code>### Example: Extract Text\n\nHere's a complete example:\n\n\\`\\`\\`python\nfrom omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n\n# Load document\ndoc = Document.from_pdf(\"document.pdf\")\n\n# Create extractor\nextractor = QwenTextExtractor(\n    backend=QwenPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n    )\n)\n\n# Extract text from first page\nresult = extractor.extract(\n    doc.get_page(0),\n    output_format=\"markdown\"\n)\n\n# Print result\nprint(result.content)\n\\`\\`\\`\n</code></pre>"},{"location":"contributing/style-guide/#documentation-requirements","title":"Documentation Requirements","text":"<p>Every user-facing documentation file must include:</p> <ol> <li>Title (H1) - Clear, descriptive</li> <li>Summary (1-2 paragraphs) - What is this doc about?</li> <li>Table of Contents - For long docs (&gt;2000 words)</li> <li>Main Content - Progressive disclosure (simple \u2192 complex)</li> <li>Code Examples - 3-5 complete, runnable examples</li> <li>Comparison Table - When/what/why decisions</li> <li>Troubleshooting - Common issues and solutions</li> <li>See Also - Links to related docs</li> <li>YAML Frontmatter - For AI-friendly parsing</li> </ol> <pre><code>---\ntitle: \"Task Name\"\ndescription: \"Short description\"\ncategory: \"guides\"\ndifficulty: \"intermediate\"\ntime_estimate: \"15 minutes\"\nkeywords: [\"text extraction\", \"markdown\", \"pdf\"]\n---\n\n# Task Name\n\nBrief intro (1-2 sentences).\n\n## When to Use This Guide\n\n## Quick Example\n\n## Main Content\n\n## Advanced Features\n\n## Troubleshooting\n\n## Next Steps\n</code></pre>"},{"location":"contributing/style-guide/#git-commit-standards","title":"Git &amp; Commit Standards","text":""},{"location":"contributing/style-guide/#branch-naming","title":"Branch Naming","text":"<pre><code>feature/add-qwen-support\nbugfix/fix-memory-leak\ndocs/update-installation-guide\nrefactor/simplify-backend-system\n</code></pre>"},{"location":"contributing/style-guide/#commit-messages","title":"Commit Messages","text":"<p>Use imperative mood, no capitals, no periods:</p> <pre><code># \u2705 GOOD\nfeat: add Qwen text extraction support\nfix: prevent GPU memory leak in batch processing\ndocs: update installation guide\nrefactor: simplify backend selection logic\n\n# \u274c BAD\nAdded Qwen support\nFix memory leak\nUpdate docs\nRefactored backend code\n</code></pre>"},{"location":"contributing/style-guide/#commit-content","title":"Commit Content","text":"<ul> <li>One logical change per commit</li> <li>All tests passing</li> <li>Code properly formatted</li> <li>No debugging code or comments</li> </ul>"},{"location":"contributing/style-guide/#python-code-formatting","title":"Python Code Formatting","text":""},{"location":"contributing/style-guide/#use-black","title":"Use Black","text":"<pre><code>uv run black omnidocs/ tests/\n</code></pre> <p>Black settings (in <code>pyproject.toml</code>): <pre><code>[tool.black]\nline-length = 100\ntarget-version = [\"py310\", \"py311\", \"py312\"]\n</code></pre></p>"},{"location":"contributing/style-guide/#use-mypy-for-type-checking","title":"Use MyPy for Type Checking","text":"<pre><code>uv run mypy omnidocs/\n</code></pre> <p>MyPy settings (in <code>pyproject.toml</code>): <pre><code>[tool.mypy]\npython_version = \"3.10\"\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = false\ndisallow_incomplete_defs = true\n</code></pre></p>"},{"location":"contributing/style-guide/#import-ordering","title":"Import Ordering","text":"<p>Use isort (configured in pyproject.toml):</p> <pre><code># Standard library\nimport os\nimport sys\nfrom pathlib import Path\n\n# Third party\nimport numpy as np\nimport torch\nfrom transformers import AutoModel\nfrom pydantic import BaseModel\n\n# Local\nfrom omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\n</code></pre>"},{"location":"contributing/style-guide/#quality-checklist","title":"Quality Checklist","text":"<p>Before submitting code:</p> <ul> <li>[ ] All tests pass (<code>uv run pytest</code>)</li> <li>[ ] Coverage &gt;80% (<code>uv run pytest --cov</code>)</li> <li>[ ] Code formatted (<code>uv run black omnidocs/</code>)</li> <li>[ ] Type checked (<code>uv run mypy omnidocs/</code>)</li> <li>[ ] No hard-coded paths or secrets</li> <li>[ ] Docstrings complete</li> <li>[ ] Error messages helpful</li> <li>[ ] Examples runnable</li> <li>[ ] No debug print statements</li> <li>[ ] Git history clean</li> </ul>"},{"location":"contributing/style-guide/#common-patterns","title":"Common Patterns","text":""},{"location":"contributing/style-guide/#backend-selection-pattern","title":"Backend Selection Pattern","text":"<pre><code>def _create_backend(self):\n    \"\"\"Create appropriate backend.\"\"\"\n    if isinstance(self.backend_config, MyModelPyTorchConfig):\n        from omnidocs.inference.pytorch import PyTorchInference\n        return PyTorchInference(self.backend_config)\n    # ... other backends\n    else:\n        raise TypeError(f\"Unknown backend: {type(self.backend_config)}\")\n</code></pre>"},{"location":"contributing/style-guide/#config-validation-pattern","title":"Config Validation Pattern","text":"<pre><code>class MyConfig(BaseModel):\n    param: str = Field(..., description=\"...\")\n\n    @validator(\"param\")\n    def validate_param(cls, v):\n        if v not in [\"valid1\", \"valid2\"]:\n            raise ValueError(f\"Invalid param: {v}\")\n        return v\n\n    class Config:\n        extra = \"forbid\"\n</code></pre>"},{"location":"contributing/style-guide/#test-fixture-pattern","title":"Test Fixture Pattern","text":"<pre><code>@pytest.fixture\ndef config(self):\n    \"\"\"Create test config.\"\"\"\n    return MyConfig(\n        model=\"test-model\",\n        device=\"cpu\",  # Use CPU for tests\n    )\n\n@pytest.fixture\ndef extractor(self, config):\n    \"\"\"Create test extractor.\"\"\"\n    return MyExtractor(config=config)\n</code></pre>"},{"location":"contributing/style-guide/#when-in-doubt","title":"When in Doubt","text":"<ul> <li>Check existing code in <code>/omnidocs/</code> - follow established patterns</li> <li>Read Concepts - understand architecture</li> <li>Check tests in <code>/tests/</code> - see testing patterns</li> <li>Run <code>black</code> and <code>mypy</code> - follow their output</li> </ul> <p>Questions? Open an issue or check CLAUDE.md for development guide.</p>"},{"location":"contributing/workflow/","title":"Implementation Workflow","text":"<p>This guide walks through the complete 6-phase workflow for adding new features to OmniDocs.</p>"},{"location":"contributing/workflow/#overview","title":"Overview","text":"<pre><code>Issue &amp; Planning\n    \u2193\nExperimentation (scripts/)\n    \u2193\nIntegration (Omnidocs/)\n    \u2193\nTesting &amp; Validation\n    \u2193\nPull Request &amp; Review\n    \u2193\nVersion Release\n</code></pre>"},{"location":"contributing/workflow/#phase-1-issue-planning","title":"Phase 1: Issue &amp; Planning","text":""},{"location":"contributing/workflow/#create-github-issue","title":"Create GitHub Issue","text":"<p>Create a new issue with this template:</p> <pre><code>**Title**: Add [Model/Task Name] Support\n\n**Description**:\n- **Task Type**: Text Extraction / Layout Analysis / OCR / etc.\n- **Model**: [Model name]\n- **Backends**: PyTorch / VLLM / MLX / API\n- **Use Case**: [Brief description]\n\n**References**:\n- Model Card: [HuggingFace link]\n- Paper: [arXiv link if applicable]\n\n**Checklist**:\n- [ ] Create implementation plan\n- [ ] Experiment in scripts/\n- [ ] Integrate into Omnidocs/\n- [ ] Write tests\n- [ ] Pass lint checks\n- [ ] Create PR\n</code></pre>"},{"location":"contributing/workflow/#read-design-documents","title":"Read Design Documents","text":"<p>Before implementing ANYTHING, read these:</p> <ol> <li><code>IMPLEMENTATION_PLAN/BACKEND_ARCHITECTURE.md</code> - Backend system design</li> <li><code>IMPLEMENTATION_PLAN/DEVEX.md</code> - API design principles</li> <li><code>CLAUDE.md</code> - Development guide and standards</li> </ol>"},{"location":"contributing/workflow/#write-implementation-plan","title":"Write Implementation Plan","text":"<p>Add a comment to your issue with:</p> <p><pre><code>## Implementation Plan\n\n### Architecture\n- Single-backend or multi-backend?\n- Which backends to support?\n- Config class names?\n\n### File Structure\n</code></pre> omnidocs/tasks/text_extraction/ \u251c\u2500\u2500 mymodel.py           # Main extractor \u2514\u2500\u2500 mymodel/             # Configs (if multi-backend)     \u251c\u2500\u2500 pytorch.py     \u251c\u2500\u2500 vllm.py     \u2514\u2500\u2500 api.py <pre><code>### Dependencies\nList all new packages to add\n\n### Timeline\nEstimate effort for each phase\n</code></pre></p>"},{"location":"contributing/workflow/#phase-2-experimentation-scripts","title":"Phase 2: Experimentation (scripts/)","text":""},{"location":"contributing/workflow/#create-experiment-scripts","title":"Create Experiment Scripts","text":"<p>For GPU models, create <code>scripts/text_extract/modal_mymodel_pytorch.py</code>:</p> <pre><code>import modal\n\napp = modal.App(\"omnidocs-mymodel-test\")\n\ncuda_version = \"12.4.0\"\nflavor = \"devel\"\noperating_sys = \"ubuntu22.04\"\ntag = f\"{cuda_version}-{flavor}-{operating_sys}\"\n\n# Base image (cached)\nBASE_IMAGE = (\n    modal.Image.from_registry(f\"nvidia/cuda:{tag}\", add_python=\"3.12\")\n    .apt_install(\"libgl1-mesa-glx\", \"libglib2.0-0\")\n    .uv_pip_install(\"torch\", \"transformers\", \"pillow\")\n    .env({\"HF_HOME\": \"/data/.cache\"})\n)\n\n# Model-specific layer\nIMAGE = BASE_IMAGE.uv_pip_install(\"mymodel-package\")\n\nvolume = modal.Volume.from_name(\"omnidocs\", create_if_missing=True)\nsecret = modal.Secret.from_name(\"adithya-hf-wandb\")\n\n@app.function(image=IMAGE, gpu=\"A10G:1\", volumes={\"/data\": volume}, secrets=[secret])\ndef test_inference():\n    # Test code here\n    pass\n\n@app.local_entrypoint()\ndef main():\n    test_inference.remote()\n</code></pre> <p>Naming Convention: - <code>modal_{model}_{backend}.py</code> - GPU-based (PyTorch/VLLM) - <code>litellm_{model}_{task}.py</code> - API-based (local) - <code>mlx_{model}_{task}.py</code> - Apple Silicon (local)</p>"},{"location":"contributing/workflow/#run-and-validate","title":"Run and Validate","text":"<pre><code>cd scripts/text_extract/\nmodal run modal_mymodel_pytorch.py\n</code></pre> <p>Validation Checklist: - [ ] Model loads successfully - [ ] Inference produces expected output - [ ] Performance is acceptable - [ ] Error handling works - [ ] Different input types work</p>"},{"location":"contributing/workflow/#document-findings","title":"Document Findings","text":"<p>In the GitHub issue, comment with: - Model size and memory requirements - Inference speed (tokens/sec or pages/sec) - Quality observations - Recommended configurations</p>"},{"location":"contributing/workflow/#phase-3-integration-omnidocs","title":"Phase 3: Integration (Omnidocs/)","text":""},{"location":"contributing/workflow/#step-1-determine-backend-support","title":"Step 1: Determine Backend Support","text":"<p>Is this single-backend or multi-backend?</p> <p>Single-Backend (e.g., DocLayoutYOLO): - One config class in same file as extractor - Use <code>config=</code> parameter</p> <p>Multi-Backend (e.g., Qwen): - Multiple config classes in subfolder - Use <code>backend=</code> parameter with Union type</p>"},{"location":"contributing/workflow/#step-2-create-config-classes","title":"Step 2: Create Config Classes","text":"<p>Single-Backend (in <code>omnidocs/tasks/text_extraction/mymodel.py</code>):</p> <pre><code>from pydantic import BaseModel, Field\n\nclass MyModelConfig(BaseModel):\n    \"\"\"Configuration for MyModel.\"\"\"\n\n    model: str = Field(default=\"vendor/mymodel\", description=\"Model ID\")\n    device: str = Field(default=\"cuda\", description=\"Device\")\n    dtype: str = Field(default=\"bfloat16\", description=\"Data type\")\n\n    class Config:\n        extra = \"forbid\"\n</code></pre> <p>Multi-Backend (in <code>omnidocs/tasks/text_extraction/mymodel/pytorch.py</code>):</p> <pre><code>class MyModelPyTorchConfig(BaseModel):\n    \"\"\"PyTorch backend config.\"\"\"\n    model: str = Field(..., description=\"Model ID\")\n    device: str = Field(default=\"cuda\")\n    torch_dtype: str = Field(default=\"bfloat16\")\n    class Config:\n        extra = \"forbid\"\n</code></pre> <p>Config Rules: - \u2705 All parameters with <code>Field(...)</code> - \u2705 All parameters have descriptions - \u2705 Type hints for everything - \u2705 Validation rules (ge, le, Literal) - \u2705 <code>extra = \"forbid\"</code> to catch typos - \u2705 Pydantic docstring for class</p>"},{"location":"contributing/workflow/#step-3-create-extractor-class","title":"Step 3: Create Extractor Class","text":"<pre><code>from typing import Union, Optional\nfrom .base import BaseTextExtractor\nfrom .models import TextOutput\n\nclass MyModelTextExtractor(BaseTextExtractor):\n    \"\"\"MyModel text extractor.\"\"\"\n\n    def __init__(self, backend: MyModelBackendConfig):\n        self.backend_config = backend\n        self._backend = self._create_backend()\n\n    def _create_backend(self):\n        \"\"\"Create backend based on config type.\"\"\"\n        if isinstance(self.backend_config, MyModelPyTorchConfig):\n            from omnidocs.inference.pytorch import PyTorchInference\n            return PyTorchInference(self.backend_config)\n        # ... other backends\n\n    def extract(self, image, output_format=\"markdown\", **kwargs) -&gt; TextOutput:\n        \"\"\"Extract text from image.\"\"\"\n        # Implementation\n        pass\n</code></pre>"},{"location":"contributing/workflow/#step-4-update-exports","title":"Step 4: Update Exports","text":"<p>Edit <code>omnidocs/tasks/text_extraction/__init__.py</code>:</p> <pre><code>from .mymodel import MyModelTextExtractor\nfrom .mymodel import (\n    MyModelPyTorchConfig,\n    MyModelVLLMConfig,\n    # ...\n)\n\n__all__ = [\n    \"MyModelTextExtractor\",\n    \"MyModelPyTorchConfig\",\n    # ...\n]\n</code></pre>"},{"location":"contributing/workflow/#step-5-add-dependencies","title":"Step 5: Add Dependencies","text":"<pre><code>cd Omnidocs/\nuv add --group pytorch mymodel-package\nuv sync\n</code></pre>"},{"location":"contributing/workflow/#phase-4-testing-validation","title":"Phase 4: Testing &amp; Validation","text":""},{"location":"contributing/workflow/#write-unit-tests","title":"Write Unit Tests","text":"<p>Create <code>Omnidocs/tests/tasks/text_extraction/test_mymodel.py</code>:</p> <pre><code>import pytest\nfrom omnidocs.tasks.text_extraction import MyModelTextExtractor\nfrom omnidocs.tasks.text_extraction.mymodel import MyModelPyTorchConfig\n\nclass TestMyModelConfig:\n    \"\"\"Test config validation.\"\"\"\n\n    def test_valid_config(self):\n        config = MyModelPyTorchConfig(model=\"vendor/mymodel\")\n        assert config.device == \"cuda\"\n\n    def test_invalid_param(self):\n        with pytest.raises(ValueError):\n            MyModelPyTorchConfig(\n                model=\"vendor/mymodel\",\n                invalid_param=\"value\"  # Should raise error\n            )\n\nclass TestMyModelExtractor:\n    \"\"\"Test extractor functionality.\"\"\"\n\n    @pytest.fixture\n    def sample_image(self):\n        from PIL import Image\n        return Image.new(\"RGB\", (800, 600))\n\n    def test_extract_markdown(self, sample_image):\n        config = MyModelPyTorchConfig(model=\"vendor/mymodel\", device=\"cuda\")\n        extractor = MyModelTextExtractor(backend=config)\n        result = extractor.extract(sample_image, output_format=\"markdown\")\n\n        assert result.format.value == \"markdown\"\n        assert len(result.content) &gt; 0\n</code></pre> <p>Coverage Target: &gt;80%</p>"},{"location":"contributing/workflow/#integration-test","title":"Integration Test","text":"<p>Create <code>scripts/text_extract_omnidocs/modal_mymodel_text_hf.py</code>:</p> <pre><code>\"\"\"Test MyModel through Omnidocs package on Modal.\"\"\"\n\n@app.function(image=OMNIDOCS_IMAGE, gpu=\"A10G:1\", ...)\ndef test_omnidocs_mymodel():\n    from omnidocs.tasks.text_extraction import MyModelTextExtractor\n    from omnidocs.tasks.text_extraction.mymodel import MyModelPyTorchConfig\n\n    extractor = MyModelTextExtractor(\n        backend=MyModelPyTorchConfig(model=\"vendor/mymodel\", device=\"cuda\")\n    )\n\n    result = extractor.extract(image, output_format=\"markdown\")\n    assert result.format.value == \"markdown\"\n    return {\"success\": True, \"length\": len(result.content)}\n</code></pre>"},{"location":"contributing/workflow/#lint-checks","title":"Lint Checks","text":"<pre><code>cd Omnidocs/\n\n# Format code\nuv run black omnidocs/ tests/\n\n# Type checking\nuv run mypy omnidocs/\n\n# Fix any issues before proceeding\n</code></pre>"},{"location":"contributing/workflow/#phase-5-pull-request","title":"Phase 5: Pull Request","text":""},{"location":"contributing/workflow/#create-feature-branch","title":"Create Feature Branch","text":"<pre><code>git checkout main\ngit pull origin main\ngit checkout -b feature/add-mymodel-support\n</code></pre>"},{"location":"contributing/workflow/#commit-changes","title":"Commit Changes","text":"<pre><code>git add Omnidocs/omnidocs/tasks/text_extraction/mymodel.py\ngit add Omnidocs/omnidocs/tasks/text_extraction/mymodel/\ngit add Omnidocs/tests/tasks/text_extraction/test_mymodel.py\ngit add Omnidocs/pyproject.toml\ngit add scripts/text_extract/modal_mymodel_pytorch.py\ngit add scripts/text_extract_omnidocs/test_mymodel.py\n\ngit commit -m \"$(cat &lt;&lt;'EOF'\nfeat: add MyModel text extraction support\n\nAdds MyModelTextExtractor with PyTorch backend:\n- Complete text extraction to Markdown/HTML\n- Pydantic config validation\n- Integration with Document class\n- Unit tests (&gt;80% coverage)\n- Modal deployment scripts\n\nTesting:\n- Unit tests passing\n- Integration tests on Modal verified\n- Black formatting applied\n- Mypy type checks passing\nEOF\n)\"\n</code></pre> <p>Important: - NO <code>Co-Authored-By</code> attribution - NO AI/Claude mentions - Commits attributed to repository owner only</p>"},{"location":"contributing/workflow/#push-and-create-pr","title":"Push and Create PR","text":"<pre><code>git push origin feature/add-mymodel-support\n\ngh pr create \\\n  --title \"Add MyModel Text Extraction\" \\\n  --body \"Adds MyModel support with PyTorch backend.\n\n## Changes\n- MyModelTextExtractor class\n- PyTorch configuration\n- Unit and integration tests\n- Modal deployment scripts\n\n## Testing\n- [x] Unit tests passing (&gt;80%)\n- [x] Integration tests on Modal\n- [x] Lint checks (black, mypy)\n- [x] Documentation updated\"\n</code></pre>"},{"location":"contributing/workflow/#iterate-on-review","title":"Iterate on Review","text":"<ol> <li>Address reviewer feedback</li> <li>Push updates to same branch</li> <li>Re-run lint and tests</li> <li>Request re-review</li> </ol>"},{"location":"contributing/workflow/#phase-6-version-release","title":"Phase 6: Version &amp; Release","text":""},{"location":"contributing/workflow/#update-version","title":"Update Version","text":"<p>Edit <code>Omnidocs/pyproject.toml</code>:</p> <pre><code>[project]\nname = \"omnidocs\"\nversion = \"2.2.0\"  # Increment MINOR for new feature\n</code></pre>"},{"location":"contributing/workflow/#update-changelog","title":"Update Changelog","text":"<p>Edit <code>Omnidocs/CHANGELOG.md</code>:</p> <pre><code>## [2.2.0] - 2026-02-15\n\n### Added\n- **MyModel Text Extraction**: PyTorch backend for efficient text extraction\n  - Markdown and HTML output formats\n  - Integration with Document class\n  - Unit tests with &gt;80% coverage\n</code></pre>"},{"location":"contributing/workflow/#create-git-tag","title":"Create Git Tag","text":"<pre><code>git checkout main\ngit pull origin main\ngit tag -a v2.2.0 -m \"Release v2.2.0: Add MyModel text extraction\"\ngit push origin v2.2.0\n</code></pre>"},{"location":"contributing/workflow/#build-and-publish","title":"Build and Publish","text":"<pre><code>cd Omnidocs/\n\n# Build distribution\nuv build\n\n# Publish to PyPI\nuv publish\n</code></pre>"},{"location":"contributing/workflow/#create-github-release","title":"Create GitHub Release","text":"<pre><code>gh release create v2.2.0 \\\n  --title \"v2.2.0: MyModel Text Extraction\" \\\n  --notes \"Added MyModel support for efficient text extraction\"\n</code></pre>"},{"location":"contributing/workflow/#summary-checklist","title":"Summary Checklist","text":""},{"location":"contributing/workflow/#planning","title":"Planning","text":"<ul> <li>[ ] GitHub issue created</li> <li>[ ] Design docs read</li> <li>[ ] Implementation plan written</li> </ul>"},{"location":"contributing/workflow/#experimentation","title":"Experimentation","text":"<ul> <li>[ ] Experiment script in scripts/</li> <li>[ ] Modal/local execution successful</li> <li>[ ] Findings documented</li> </ul>"},{"location":"contributing/workflow/#integration","title":"Integration","text":"<ul> <li>[ ] Config classes created</li> <li>[ ] Extractor class implemented</li> <li>[ ] init.py exports updated</li> <li>[ ] Dependencies added (uv add)</li> </ul>"},{"location":"contributing/workflow/#testing","title":"Testing","text":"<ul> <li>[ ] Unit tests &gt;80% coverage</li> <li>[ ] Integration test in *_omnidocs/</li> <li>[ ] Modal test successful</li> <li>[ ] Lint checks passing (black, mypy)</li> </ul>"},{"location":"contributing/workflow/#pr","title":"PR","text":"<ul> <li>[ ] Feature branch created</li> <li>[ ] Changes committed</li> <li>[ ] PR created with description</li> <li>[ ] CI/CD checks pass</li> <li>[ ] Feedback addressed</li> </ul>"},{"location":"contributing/workflow/#release","title":"Release","text":"<ul> <li>[ ] Version bumped</li> <li>[ ] Changelog updated</li> <li>[ ] Git tag created</li> <li>[ ] Package published to PyPI</li> <li>[ ] GitHub release created</li> </ul>"},{"location":"contributing/workflow/#next-see-adding-models-for-step-by-step-model-integration","title":"Next: See Adding Models for step-by-step model integration.","text":""},{"location":"getting-started/","title":"Getting Started with OmniDocs","text":"<p>Welcome! OmniDocs is a unified Python toolkit for visual document processing. This section will get you up and running in minutes.</p>"},{"location":"getting-started/#choose-your-path","title":"\ud83d\udcd6 Choose Your Path","text":""},{"location":"getting-started/#path-1-just-show-me-the-code-5-minutes","title":"Path 1: \"Just Show Me the Code\" (5 minutes)","text":"<p>Go straight to Quickstart for a copy-paste working example: - Load a PDF - Extract text - Get the output</p> <p>Best for: Experienced developers, quick prototyping.</p>"},{"location":"getting-started/#path-2-let-me-understand-this-first-30-minutes","title":"Path 2: \"Let Me Understand This First\" (30 minutes)","text":"<p>Follow this learning path in order:</p> <ol> <li>Installation Guide (5-10 min)</li> <li>Install OmniDocs for your system</li> <li>Choose between PyTorch, VLLM, MLX, or API backend</li> <li> <p>Verify installation works</p> </li> <li> <p>Quickstart (5 min)</p> </li> <li>Run a minimal working example</li> <li> <p>Understand the basic API</p> </li> <li> <p>First Document (10-15 min)</p> </li> <li>Load documents (PDF, URL, images)</li> <li>Access pages and metadata</li> <li> <p>Learn memory management</p> </li> <li> <p>Choosing Backends (5-10 min)</p> </li> <li>Understand tradeoffs between backends</li> <li>Make informed backend choice</li> <li>See performance comparisons</li> </ol> <p>Best for: New users, learning the system deeply.</p>"},{"location":"getting-started/#path-3-i-have-a-specific-use-case-targeted","title":"Path 3: \"I Have a Specific Use Case\" (Targeted)","text":"<p>Need to process PDFs on your Mac? \u2192 Installation: MLX Backend + Quickstart</p> <p>Need fast batch processing? \u2192 Choosing Backends: VLLM + First Document: Batch Processing</p> <p>Need to process without GPU? \u2192 Installation: API Backend + Choosing Backends: API</p> <p>Need simple one-off extraction? \u2192 Quickstart</p>"},{"location":"getting-started/#5-minute-quick-reference","title":"\ud83c\udfaf 5-Minute Quick Reference","text":"<pre><code># 1. Load document (3 seconds)\nfrom omnidocs import Document\ndoc = Document.from_pdf(\"document.pdf\")\n\n# 2. Initialize extractor (30 seconds on first run)\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(device=\"cuda\")\n)\n\n# 3. Extract text (1-2 seconds per page)\nresult = extractor.extract(doc.get_page(0), output_format=\"markdown\")\n\n# 4. Use the output\nprint(result.content)  # Extracted markdown text\n</code></pre>"},{"location":"getting-started/#what-each-guide-covers","title":"\ud83d\udccb What Each Guide Covers","text":""},{"location":"getting-started/#installation-guide","title":"Installation Guide","text":"<ul> <li>System requirements - What you need to get started</li> <li>Backend selection - PyTorch vs VLLM vs MLX vs API</li> <li>Step-by-step installation - Install commands for each backend</li> <li>Verification steps - Make sure everything works</li> <li>Troubleshooting - Common issues and fixes</li> </ul> <p>Read this if: You haven't installed OmniDocs yet, or need to switch backends.</p>"},{"location":"getting-started/#quickstart","title":"Quickstart","text":"<ul> <li>5-minute tutorial - Get working code immediately</li> <li>Load a PDF - <code>Document.from_pdf()</code></li> <li>Extract text - <code>QwenTextExtractor.extract()</code></li> <li>Get output - <code>result.content</code></li> <li>Complete examples - Copy-paste ready code</li> <li>Common tasks - Batch processing, different formats</li> </ul> <p>Read this if: You want to see working code right away, or get started quickly.</p>"},{"location":"getting-started/#first-document","title":"First Document","text":"<ul> <li>Document class deep dive - How documents work</li> <li>Loading methods - PDF, URL, images, bytes</li> <li>Page access - Get single pages, iterate, batch process</li> <li>Metadata - Access document information</li> <li>Memory management - Cache behavior, clearing cache</li> <li>Performance tips - Optimize for speed</li> <li>Real-world examples - Complete use case implementations</li> </ul> <p>Read this if: You want to understand document loading deeply, or need to optimize for performance.</p>"},{"location":"getting-started/#choosing-backends","title":"Choosing Backends","text":"<ul> <li>Decision tree - Which backend for my use case?</li> <li>Detailed profiles - Strengths, limitations, performance of each backend</li> <li>Comparison tables - Speed, cost, setup time</li> <li>Migration guide - How to switch between backends</li> <li>Troubleshooting - Backend selection issues</li> </ul> <p>Read this if: You're unsure which backend to use, or want to understand the tradeoffs.</p>"},{"location":"getting-started/#common-questions-answered","title":"\u26a1 Common Questions Answered","text":"<p>Q: Which backend should I use? A: Start with PyTorch for development (simplest). Switch to VLLM if you need 100+ docs/day. Use MLX on Mac, API if you don't have GPU. See Choosing Backends for details.</p> <p>Q: How long does extraction take? A: ~1 second per page on modern GPU (PyTorch), ~0.1 second with VLLM, ~2-3 seconds on Mac (MLX), ~5 seconds with API (includes network latency).</p> <p>Q: What file formats are supported? A: PDF, PNG, JPG, GIF, BMP, TIFF, WebP. Can load from files, URLs, or raw bytes.</p> <p>Q: Can I use this on a Mac? A: Yes! Use MLX backend for native performance optimization.</p> <p>Q: Can I use this without a GPU? A: Yes, three options: 1. Use API backend (costs money, easiest) 2. Use PyTorch on CPU (free, very slow, ~20-30 sec per page) 3. Use MLX on Mac (good Mac performance, free)</p> <p>Q: What's the difference between backends? A: See Choosing Backends for full comparison. TL;DR: PyTorch is easiest, VLLM is fastest, MLX is for Mac, API is simplest.</p> <p>Q: How much GPU VRAM do I need? A: ~4-8 GB for 2B models, ~8-16 GB for 8B models. VLLM needs 24+ GB.</p> <p>Q: Can I process multiple documents in parallel? A: With PyTorch, one at a time. With VLLM, can batch multiple images. See First Document: Batch Processing.</p>"},{"location":"getting-started/#get-started-now","title":"\ud83d\ude80 Get Started Now","text":""},{"location":"getting-started/#fastest-path-right-now","title":"Fastest Path (Right Now)","text":"<p><pre><code>pip install omnidocs[pytorch]\npython -c \"from omnidocs import Document; print('Ready to go!')\"\n</code></pre> Then go to Quickstart for working code.</p>"},{"location":"getting-started/#recommended-path-takes-30-min","title":"Recommended Path (Takes 30 min)","text":"<ol> <li>Installation - Install for your system</li> <li>Quickstart - See a working example</li> <li>First Document - Understand the API</li> <li>Choosing Backends - Pick the best backend</li> </ol>"},{"location":"getting-started/#learning-path-deep-dive","title":"Learning Path (Deep Dive)","text":"<ol> <li>Installation - Understand all backend options</li> <li>First Document - Master document loading</li> <li>Quickstart - Learn extraction</li> <li>Choosing Backends - Understand performance tradeoffs</li> </ol>"},{"location":"getting-started/#next-steps-after-getting-started","title":"\ud83d\udcda Next Steps After Getting Started","text":"<ul> <li>Guides - Task-oriented tutorials</li> <li>Concepts - Architecture deep-dives</li> <li>Models - Model reference</li> <li>API Reference - Complete API documentation</li> <li>Contributing - Contribute to OmniDocs</li> </ul>"},{"location":"getting-started/#tips-for-success","title":"\ud83d\udca1 Tips for Success","text":"<ol> <li>Start simple - Use 2B Qwen model for fast feedback, switch to 8B for production</li> <li>Use PyTorch first - Easiest to learn, then optimize later</li> <li>Process one page at a time - Avoid memory issues with <code>iter_pages()</code></li> <li>Save results immediately - Don't accumulate in memory, save to disk</li> <li>Clear cache regularly - Free GPU memory with <code>doc.clear_cache()</code></li> <li>Use page ranges - <code>page_range=(0, 100)</code> for faster loading of partial docs</li> </ol>"},{"location":"getting-started/#getting-help","title":"\ud83c\udd98 Getting Help","text":"<ul> <li>Documentation - This guide + full API docs</li> <li>GitHub Issues - Report bugs or request features</li> <li>GitHub Discussions - Ask questions and chat</li> <li>Stack Overflow - Tag questions with <code>omnidocs</code></li> </ul>"},{"location":"getting-started/#learning-paths-at-a-glance","title":"\ud83d\udcca Learning Paths at a Glance","text":"Your Background Recommended Path Time New to Python Path 2: Full Learning 30 min Experienced Python Path 2: Full Learning 20 min Experienced ML engineer Quickstart + Choosing Backends 10 min Just want code Quickstart 5 min <p>Ready? Pick a guide above and start building!</p> <p>For the impatient: Go to Quickstart For the thorough: Start with Installation For the strategic: Jump to Choosing Backends</p>"},{"location":"getting-started/choosing-backends/","title":"Choosing the Right Backend","text":"<p>OmniDocs supports 4 inference backends. This guide helps you choose the right one for your use case.</p>"},{"location":"getting-started/choosing-backends/#quick-decision-tree","title":"Quick Decision Tree","text":"<pre><code>START HERE\n\nDo you have a Mac with Apple Silicon (M1/M2/M3+)?\n\u251c\u2500 YES \u2192 MLX backend (Best choice)\n\u2514\u2500 NO\n   \u251c\u2500 Do you need to process 100+ documents per day?\n   \u2502  \u251c\u2500 YES \u2192 VLLM backend (Fastest, GPU required)\n   \u2502  \u2514\u2500 NO\n   \u2502     \u251c\u2500 Do you have a GPU (NVIDIA/AMD)?\n   \u2502     \u2502  \u251c\u2500 YES \u2192 PyTorch backend (Recommended, free)\n   \u2502     \u2502  \u2514\u2500 NO\n   \u2502     \u2502     \u2514\u2500 API backend (Easy, no setup)\n</code></pre>"},{"location":"getting-started/choosing-backends/#backend-comparison","title":"Backend Comparison","text":"Feature PyTorch VLLM MLX API Setup Time 5 min 10 min 5 min 2 min Speed 1x (baseline) 10x 2x 0.5x* Cost Free Free Free $0.01-0.10/doc GPU Required Optional Yes (NVIDIA) No No Batch Processing Good Excellent Good Fair Model Size 2B-8B 2B-32B 2B-7B 2B-8B Memory 4-16 GB 24-80 GB 8-16 GB Minimal Best For Development Production scale Mac users Quick testing <p>*API speed depends on network latency</p>"},{"location":"getting-started/choosing-backends/#detailed-backend-profiles","title":"Detailed Backend Profiles","text":""},{"location":"getting-started/choosing-backends/#pytorch-recommended-for-most-users","title":"PyTorch (Recommended for Most Users)","text":"<p>Best for: Local development, prototyping, single GPU inference.</p> <p>Installation: <pre><code>pip install omnidocs[pytorch]\n</code></pre></p> <p>Quick Setup: <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-2B-Instruct\",\n        device=\"cuda\",  # or \"cpu\" or \"mps\" (Mac)\n        torch_dtype=\"bfloat16\",\n    )\n)\n\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n</code></pre></p> <p>Strengths: - Standard PyTorch/HuggingFace ecosystem - Easy to install and use - Excellent documentation and community support - Works on NVIDIA, AMD, and Apple Silicon - Good for experimentation and debugging</p> <p>Limitations: - Slower than VLLM for batch processing (1 image at a time) - Requires GPU for fast inference - Higher memory usage per inference</p> <p>Performance Expectations: - NVIDIA A100 (40 GB): ~0.5-1 sec per page - NVIDIA RTX 4090 (24 GB): ~1-2 sec per page - Mac M3 Max: ~3-5 sec per page (CPU optimization) - CPU-only: ~20-30 sec per page (very slow)</p> <p>Configuration Options:</p> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nconfig = QwenTextPyTorchConfig(\n    # Model selection\n    model=\"Qwen/Qwen3-VL-2B-Instruct\",  # Small, fast (default)\n    # model=\"Qwen/Qwen3-VL-8B-Instruct\",  # Large, more accurate\n    # model=\"Qwen/Qwen3-VL-32B-Instruct\",  # Huge, slowest\n\n    # Hardware\n    device=\"cuda\",              # NVIDIA/AMD GPU (recommended)\n    # device=\"mps\",             # Apple Silicon (native)\n    # device=\"cpu\",             # CPU-only (very slow)\n\n    # Data precision\n    torch_dtype=\"bfloat16\",     # Fast, good quality (recommended)\n    # torch_dtype=\"float16\",    # Faster, slightly lower quality\n    # torch_dtype=\"float32\",    # Slowest, highest quality\n    # torch_dtype=\"auto\",       # Let model decide\n\n    # Advanced\n    device_map=\"auto\",          # Automatic memory optimization\n    trust_remote_code=True,     # Allow custom model code\n    use_flash_attention=False,  # Flash attention (experimental)\n)\n</code></pre> <p>When to Use PyTorch: - Learning OmniDocs (simplest setup) - Processing documents one at a time - Experimenting with different models - Don't need maximum throughput - Prefer free, no external API dependencies</p> <p>When NOT to Use: - Need to process 100+ documents per hour - Working with very large documents (&gt;100 pages) - Must minimize GPU memory usage</p>"},{"location":"getting-started/choosing-backends/#vllm-production-throughput","title":"VLLM (Production Throughput)","text":"<p>Best for: Production systems, batch processing, 100+ documents per day.</p> <p>Installation: <pre><code>pip install omnidocs[vllm]\n</code></pre></p> <p>Quick Setup: <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\n\nextractor = QwenTextExtractor(\n    backend=QwenTextVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=1,      # Devices to split model across\n        gpu_memory_utilization=0.9,  # Optimize for throughput\n    )\n)\n\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n</code></pre></p> <p>Strengths: - 10x faster throughput than PyTorch - Optimized tensor parallelism (split across multiple GPUs) - Intelligent batching and memory management - Built for production inference - Lower latency per request</p> <p>Limitations: - NVIDIA GPU required (no CPU or Mac support) - Requires 24+ GB VRAM (A40, A100, RTX 4090, H100, etc.) - Steeper learning curve - Requires separate VLLM server setup</p> <p>Performance Expectations: - NVIDIA H100 (80 GB): ~100-150 images/hour - NVIDIA A100 (40 GB): ~50-80 images/hour - NVIDIA RTX 4090 (24 GB): ~30-50 images/hour - Tensor parallel (2x A100): ~150+ images/hour</p> <p>Configuration Options:</p> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\n\nconfig = QwenTextVLLMConfig(\n    # Model selection\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n\n    # Parallelization\n    tensor_parallel_size=1,         # Single GPU (default)\n    # tensor_parallel_size=2,       # Split across 2 GPUs\n    # tensor_parallel_size=4,       # Split across 4 GPUs\n\n    # Memory optimization\n    gpu_memory_utilization=0.9,     # 90% utilization (aggressive)\n    # gpu_memory_utilization=0.6,   # 60% utilization (conservative)\n\n    # Performance tuning\n    max_model_len=8192,             # Max tokens (truncate if needed)\n    dtype=\"auto\",                   # Auto-select dtype\n)\n</code></pre> <p>Multi-GPU Tensor Parallelism:</p> <pre><code># Distribute model across 2 GPUs for massive throughput\nconfig = QwenTextVLLMConfig(\n    model=\"Qwen/Qwen3-VL-32B-Instruct\",\n    tensor_parallel_size=2,  # Split across GPUs 0, 1\n    gpu_memory_utilization=0.95,\n)\n\n# Model is automatically split:\n# GPU 0: 50% of model weights\n# GPU 1: 50% of model weights\n# Inference uses both GPUs in parallel\n</code></pre> <p>When to Use VLLM: - Processing 100+ documents per day - Need consistent inference throughput - Have multiple GPUs available - Building production inference service - Cost per inference matters</p> <p>When NOT to Use: - Learning OmniDocs (use PyTorch first) - Don't have NVIDIA GPU - Need quick one-off processing - Working on Mac</p>"},{"location":"getting-started/choosing-backends/#mlx-apple-silicon","title":"MLX (Apple Silicon)","text":"<p>Best for: M1/M2/M3 Mac users, no external dependencies.</p> <p>Installation: <pre><code>pip install omnidocs[mlx]\n</code></pre></p> <p>Quick Setup: <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextMLXConfig\n\nextractor = QwenTextExtractor(\n    backend=QwenTextMLXConfig(\n        model=\"Qwen/Qwen3-VL-2B-Instruct\",\n        # Configuration options shown below\n    )\n)\n\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n</code></pre></p> <p>Strengths: - Native optimization for Apple Silicon - No NVIDIA GPU required - Simple setup, minimal dependencies - Good performance on Mac hardware - Efficient memory management (unified memory)</p> <p>Limitations: - Mac only (M1/M2/M3 required) - Smaller model selection than PyTorch - Slightly slower than VLLM - Less community support than PyTorch</p> <p>Performance Expectations: - M3 Max (128 GB unified): ~2-3 sec per page - M2 Pro (16 GB unified): ~3-5 sec per page - M1 (8 GB unified): ~5-10 sec per page</p> <p>Configuration Options:</p> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextMLXConfig\n\nconfig = QwenTextMLXConfig(\n    # Model selection (MLX-optimized models)\n    model=\"Qwen/Qwen3-VL-2B-Instruct\",\n\n    # Precision (auto-optimal for Apple Silicon)\n    dtype=\"auto\",  # MLX picks the best dtype\n\n    # Optimization\n    quantization=\"4bit\",  # 4-bit quantization for speed\n    # quantization=None,   # Full precision (slower but more accurate)\n\n    # Max tokens\n    max_model_len=8192,\n)\n</code></pre> <p>When to Use MLX: - Developing on Mac with Apple Silicon - Want native performance optimization - Prefer not to deal with NVIDIA drivers - Building Mac-native applications - Prefer simple, no-hassle setup</p> <p>When NOT to Use: - Need VLLM production performance - Don't have Apple Silicon Mac - Need many different model choices - Working in cloud environment</p>"},{"location":"getting-started/choosing-backends/#api-backend-cloud-based","title":"API Backend (Cloud-Based)","text":"<p>Best for: Quick testing, no GPU setup, cloud-first applications.</p> <p>Installation: <pre><code>pip install omnidocs[api]\nexport OPENAI_API_KEY=\"sk-...\"\n</code></pre></p> <p>Quick Setup: <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextAPIConfig\n\nextractor = QwenTextExtractor(\n    backend=QwenTextAPIConfig(\n        model=\"gpt-4-vision\",\n        api_key=\"sk-...\",  # Or use OPENAI_API_KEY env var\n    )\n)\n\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n</code></pre></p> <p>Strengths: - Zero GPU setup required - Works anywhere with internet - No local hardware needed - Easy to switch models - Excellent for prototyping</p> <p>Limitations: - Costs money ($0.01-0.10 per document) - Network latency - Depends on API availability - Limited model selection - Privacy concerns (data sent to external service)</p> <p>Pricing Estimates: - Small documents (&lt; 1 MB): $0.01-0.05 - Medium documents (1-10 MB): $0.05-0.15 - Large documents (&gt; 10 MB): $0.15-0.50</p> <p>For 1000 documents: - PyTorch: $0 (after initial setup) - VLLM: $0 (after server setup) - MLX: $0 (Mac only) - API: $50-100 (at $0.05-0.10 per doc)</p> <p>Configuration Options:</p> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextAPIConfig\n\nconfig = QwenTextAPIConfig(\n    # Model selection\n    model=\"gpt-4-vision\",           # Most capable\n    # model=\"gpt-4-turbo-vision\",    # Faster\n    # model=\"gpt-3.5-vision\",        # Cheapest\n\n    # Authentication\n    api_key=\"sk-...\",               # Or use env var\n    base_url=\"https://api.openai.com/v1\",  # Custom endpoint\n\n    # Rate limiting\n    rate_limit=10,                  # Requests per minute\n)\n</code></pre> <p>When to Use API: - Prototyping quickly - Don't have GPU - Occasional document processing (&lt; 50/month) - Data privacy not a concern - Building SaaS with usage-based pricing</p> <p>When NOT to Use: - High volume (100+ documents/day) - too expensive - Sensitive data (privacy concerns) - No internet access - Need offline capability - Want total cost predictability</p>"},{"location":"getting-started/choosing-backends/#performance-comparison","title":"Performance Comparison","text":""},{"location":"getting-started/choosing-backends/#throughput-documents-per-hour","title":"Throughput (Documents per Hour)","text":"<pre><code>PyTorch (2B model, RTX 4090): 30-50 docs/hour\nPyTorch (8B model, A100):     40-60 docs/hour\nVLLM (8B model, RTX 4090):    300-500 docs/hour\nVLLM (8B model, A100):        600-1000 docs/hour\nMLX (2B model, M3 Max):       60-80 docs/hour\nAPI (via OpenAI):             10-20 docs/hour (network limited)\n</code></pre>"},{"location":"getting-started/choosing-backends/#quality-accuracy","title":"Quality (Accuracy)","text":"<p>Approximately equal across all backends if using same model:</p> <pre><code>2B models:     85-90% accuracy (fast, good for most documents)\n8B models:     92-96% accuracy (slower, better for complex docs)\n32B models:    96-98% accuracy (slowest, for critical documents)\n</code></pre> <p>Quality depends more on model size than backend choice.</p>"},{"location":"getting-started/choosing-backends/#cost-analysis","title":"Cost Analysis","text":"<p>Scenario 1: Process 100 documents once - PyTorch: $0 (one-time setup ~$100 GPU) - VLLM: $0 (one-time setup ~$500-5000 GPU) - MLX: $0 (Mac-only, built-in) - API: $5-10</p> <p>Scenario 2: Process 10,000 documents - PyTorch: $0 (amortized: $0.01 per doc for GPU) - VLLM: $0 (amortized: $0.05-0.20 per doc for GPU) - MLX: $0 (Mac-only) - API: $500-1000</p> <p>Scenario 3: Process 100,000+ documents - PyTorch: $0 amortized, but need to manage GPU scaling - VLLM: $0 amortized, designed for this scale - MLX: Not suitable (Mac only) - API: $5,000-10,000 (prohibitive)</p>"},{"location":"getting-started/choosing-backends/#migration-between-backends","title":"Migration Between Backends","text":"<p>OmniDocs is designed for easy backend switching - code changes minimally:</p>"},{"location":"getting-started/choosing-backends/#switch-from-pytorch-to-vllm","title":"Switch from PyTorch to VLLM","text":"<pre><code># PyTorch version\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nbackend = QwenTextPyTorchConfig(device=\"cuda\")\n\n# VLLM version (same interface, different config)\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\nbackend = QwenTextVLLMConfig(tensor_parallel_size=1)\n\n# Rest of code stays the same!\n</code></pre>"},{"location":"getting-started/choosing-backends/#switch-from-vllm-to-pytorch","title":"Switch from VLLM to PyTorch","text":"<pre><code># VLLM version\nbackend = QwenTextVLLMConfig(tensor_parallel_size=2)\n\n# PyTorch version\nbackend = QwenTextPyTorchConfig(device=\"cuda\", torch_dtype=\"bfloat16\")\n\n# Same .extract() interface works for both\n</code></pre>"},{"location":"getting-started/choosing-backends/#troubleshooting-backend-selection","title":"Troubleshooting Backend Selection","text":""},{"location":"getting-started/choosing-backends/#i-get-outofmemory-errors","title":"\"I get OutOfMemory errors\"","text":"<p>Option 1: Use smaller model <pre><code># Instead of 8B\nbackend = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-2B-Instruct\"\n)\n</code></pre></p> <p>Option 2: Use MLX (better memory management) <pre><code>pip install omnidocs[mlx]\n</code></pre></p> <p>Option 3: Use API (no local GPU memory) <pre><code>pip install omnidocs[api]\n</code></pre></p>"},{"location":"getting-started/choosing-backends/#my-gpu-is-slow","title":"\"My GPU is slow\"","text":"<p>Option 1: Use VLLM for better throughput <pre><code>pip install omnidocs[vllm]\n</code></pre></p> <p>Option 2: Check CUDA version <pre><code>nvcc --version  # Should be 12.1+\n</code></pre></p>"},{"location":"getting-started/choosing-backends/#i-only-have-a-mac","title":"\"I only have a Mac\"","text":"<p>Use MLX (best for Mac): <pre><code>pip install omnidocs[mlx]\n</code></pre></p> <p>Or use API (if MLX not suitable): <pre><code>pip install omnidocs[api]\n</code></pre></p>"},{"location":"getting-started/choosing-backends/#i-dont-have-a-gpu","title":"\"I don't have a GPU\"","text":"<p>Option 1: Use API backend <pre><code>pip install omnidocs[api]\n</code></pre></p> <p>Option 2: Use PyTorch on CPU (very slow) <pre><code>backend = QwenTextPyTorchConfig(device=\"cpu\")\n</code></pre></p>"},{"location":"getting-started/choosing-backends/#decision-matrix-by-use-case","title":"Decision Matrix by Use Case","text":"Use Case Recommended Alternative Avoid Learning OmniDocs PyTorch API - Development/Prototyping PyTorch MLX (Mac) VLLM Mac development MLX PyTorch - 100+ docs/day VLLM PyTorch API (expensive) Production service VLLM - PyTorch No GPU setup API - PyTorch, VLLM Cost sensitive PyTorch VLLM API Quick one-off API PyTorch VLLM High accuracy critical 8B+ model - 2B model Batch processing VLLM PyTorch API"},{"location":"getting-started/choosing-backends/#summary-and-recommendations","title":"Summary and Recommendations","text":"<p>For Most Users: Start with PyTorch backend - Simple setup - Free (after initial GPU cost) - Flexible - Excellent documentation</p> <p>For Scale: Upgrade to VLLM backend - When processing 100+ documents per day - When cost of GPU amortization matters - When throughput is critical</p> <p>For Mac Users: Use MLX backend - Native optimization - Simple setup - Good performance</p> <p>For Quick Testing: Use API backend - Minimal setup - No GPU needed - Good for prototyping - Acceptable for low volume</p> <p>Next: Quickstart to start extracting text!</p>"},{"location":"getting-started/first-document/","title":"First Document: Loading and Processing","text":"<p>Learn how to load documents, access pages, and work with document metadata.</p>"},{"location":"getting-started/first-document/#understanding-the-document-class","title":"Understanding the Document Class","text":"<p>The <code>Document</code> class is OmniDocs' central abstraction for working with PDF and image data. Key design principles:</p> <ul> <li>Stateless: Documents contain only source data, not analysis results</li> <li>Lazy Loading: Pages are rendered only when accessed (memory efficient)</li> <li>Cached: Once rendered, pages are cached to avoid re-rendering</li> <li>Memory Safe: Clear cache to free GPU/CPU memory as needed</li> </ul> <pre><code>from omnidocs import Document\n\n# This is fast - does NOT render pages yet\ndoc = Document.from_pdf(\"large_document.pdf\")\n# Takes &lt; 1 second even for 1000-page PDFs\n\n# Pages only render when you access them\npage = doc.get_page(0)  # Renders on demand\npage = doc.get_page(1)  # Cached, returns instantly\n</code></pre>"},{"location":"getting-started/first-document/#loading-documents","title":"Loading Documents","text":"<p>OmniDocs supports multiple input formats:</p>"},{"location":"getting-started/first-document/#from-pdf-file","title":"From PDF File","text":"<pre><code>from omnidocs import Document\n\n# Simple case\ndoc = Document.from_pdf(\"document.pdf\")\nprint(f\"Loaded: {doc.page_count} pages\")\n\n# With custom DPI (resolution)\ndoc = Document.from_pdf(\"document.pdf\", dpi=300)\n# Higher DPI = higher quality but slower rendering\n# Default: 150 DPI (good balance)\n\n# Load only specific pages\ndoc = Document.from_pdf(\"document.pdf\", page_range=(0, 10))\n# Loads pages 0-10 (inclusive, 0-indexed)\n# Useful for working with partial PDFs\n\n# Context manager (auto-closes document)\nwith Document.from_pdf(\"document.pdf\") as doc:\n    page = doc.get_page(0)\n    # Document automatically closed after use\n</code></pre> <p>Error Handling: <pre><code>from omnidocs import Document\nfrom omnidocs.document import DocumentLoadError, UnsupportedFormatError\n\ntry:\n    doc = Document.from_pdf(\"missing.pdf\")\nexcept DocumentLoadError as e:\n    print(f\"File not found: {e}\")\nexcept UnsupportedFormatError as e:\n    print(f\"Wrong format: {e}\")\n</code></pre></p>"},{"location":"getting-started/first-document/#from-url","title":"From URL","text":"<pre><code>from omnidocs import Document\n\n# Download and load from URL\ndoc = Document.from_url(\"https://example.com/document.pdf\")\n\n# With timeout (default: 30 seconds)\ndoc = Document.from_url(\n    \"https://example.com/large_file.pdf\",\n    timeout=60  # Wait up to 60 seconds\n)\n\n# Error handling for downloads\nfrom omnidocs.document import URLDownloadError\n\ntry:\n    doc = Document.from_url(\"https://bad.url/doc.pdf\")\nexcept URLDownloadError as e:\n    print(f\"Download failed: {e}\")\n</code></pre>"},{"location":"getting-started/first-document/#from-raw-bytes","title":"From Raw Bytes","text":"<pre><code>from omnidocs import Document\n\n# Useful for PDFs from databases or APIs\npdf_bytes = b\"%PDF-1.4...\"  # Raw PDF bytes\n\ndoc = Document.from_bytes(\n    pdf_bytes,\n    filename=\"document.pdf\"  # Optional, for metadata\n)\n\n# Real-world example: Download with requests\nimport requests\n\nresponse = requests.get(\"https://example.com/doc.pdf\")\ndoc = Document.from_bytes(\n    response.content,\n    filename=\"downloaded.pdf\"\n)\n</code></pre>"},{"location":"getting-started/first-document/#from-images","title":"From Images","text":"<p>OmniDocs can treat images as single or multi-page documents:</p> <pre><code>from omnidocs import Document\n\n# Single image\ndoc = Document.from_image(\"page.png\")\nprint(doc.page_count)  # Always 1\n\n# Multiple images (treated as multi-page document)\ndoc = Document.from_images([\n    \"page1.png\",\n    \"page2.png\",\n    \"page3.png\"\n])\nprint(doc.page_count)  # 3\n</code></pre> <p>Supported Image Formats: - PNG, JPG, JPEG, GIF, BMP, TIFF, WebP</p>"},{"location":"getting-started/first-document/#accessing-pages","title":"Accessing Pages","text":""},{"location":"getting-started/first-document/#get-single-page","title":"Get Single Page","text":"<pre><code>from omnidocs import Document\n\ndoc = Document.from_pdf(\"document.pdf\")\n\n# 0-indexed (first page is 0)\nfirst_page = doc.get_page(0)\nlast_page = doc.get_page(doc.page_count - 1)\n\n# Pages are PIL Images - compatible with all image tools\nprint(f\"Image size: {first_page.size}\")  # (width, height)\n\n# Save page as image\nfirst_page.save(\"page_1.png\")\n</code></pre>"},{"location":"getting-started/first-document/#iterate-over-all-pages","title":"Iterate Over All Pages","text":"<p>For large documents, iterate instead of loading all at once:</p> <pre><code>from omnidocs import Document\n\ndoc = Document.from_pdf(\"document.pdf\")\n\n# Memory efficient - one page at a time\nfor i, page in enumerate(doc.iter_pages()):\n    print(f\"Page {i + 1}\")\n    # Process page\n    # Page is cleared from memory before next iteration\n</code></pre> <p>Why Iterate? - Memory Safe: Only one page in memory at a time - Progress Tracking: Easy to show progress for large docs - Early Exit: Can stop processing early - Cache Control: Can clear cache between pages</p>"},{"location":"getting-started/first-document/#load-all-pages-at-once","title":"Load All Pages at Once","text":"<pre><code>from omnidocs import Document\n\ndoc = Document.from_pdf(\"document.pdf\")\n\n# Load all pages into memory\nall_pages = doc.pages  # List of PIL Images\n\n# Only use this for small documents (&lt; 50 pages)\n# For larger documents, use iter_pages() instead\n</code></pre>"},{"location":"getting-started/first-document/#get-page-properties","title":"Get Page Properties","text":"<pre><code>from omnidocs import Document\n\ndoc = Document.from_pdf(\"document.pdf\")\n\n# Get page dimensions WITHOUT rendering\nwidth, height = doc.get_page_size(0)\n# Fast - doesn't need to render the full page\n\n# Get page as PIL Image (renders on demand)\npage = doc.get_page(0)\nprint(f\"Page: {page.size}\")  # (width, height)\nprint(f\"Mode: {page.mode}\")  # Color mode (RGB, RGBA, etc.)\n</code></pre>"},{"location":"getting-started/first-document/#accessing-text-content","title":"Accessing Text Content","text":""},{"location":"getting-started/first-document/#get-page-text","title":"Get Page Text","text":"<pre><code>from omnidocs import Document\n\ndoc = Document.from_pdf(\"document.pdf\")\n\n# Get text from specific page (1-indexed, like PDF viewers)\ntext = doc.get_page_text(1)  # First page\nprint(text[:100])  # First 100 characters\n\n# Note: get_page(0) uses 0-indexing, get_page_text(1) uses 1-indexing\n# get_page_text is slower (extracts text using pypdfium2/pdfplumber)\n</code></pre>"},{"location":"getting-started/first-document/#get-full-document-text","title":"Get Full Document Text","text":"<pre><code>from omnidocs import Document\n\ndoc = Document.from_pdf(\"document.pdf\")\n\n# Get all text (lazy, cached)\nfull_text = doc.text\nprint(f\"Total: {len(full_text)} characters\")\n\n# Uses pypdfium2 (fast), falls back to pdfplumber if needed\n# Cached after first access\n</code></pre> <p>Warning: PDF text extraction can fail for: - Scanned PDFs (images, no selectable text) - Encrypted PDFs - PDFs with unusual encodings</p> <p>For reliable text extraction, always use a Vision-Language Model (Qwen, DotsOCR, etc.), not PDF text extraction.</p>"},{"location":"getting-started/first-document/#understanding-document-metadata","title":"Understanding Document Metadata","text":""},{"location":"getting-started/first-document/#access-metadata","title":"Access Metadata","text":"<pre><code>from omnidocs import Document\n\ndoc = Document.from_pdf(\"document.pdf\")\n\n# Get metadata object\nmetadata = doc.metadata\nprint(metadata.page_count)      # Number of pages\nprint(metadata.source_type)     # \"file\", \"url\", \"bytes\", \"image\"\nprint(metadata.source_path)     # File path or URL\nprint(metadata.file_name)       # Filename\nprint(metadata.file_size)       # Size in bytes\nprint(metadata.format)          # \"pdf\", \"png\", \"jpg\", etc.\nprint(metadata.image_dpi)       # DPI for rendering (150, 300, etc.)\nprint(metadata.loaded_at)       # ISO timestamp\n\n# For PDFs, also includes PDF metadata if available\nif metadata.pdf_metadata:\n    print(metadata.pdf_metadata)\n    # {'Title': '...', 'Author': '...', 'Subject': '...'}\n</code></pre>"},{"location":"getting-started/first-document/#convert-to-dictionary","title":"Convert to Dictionary","text":"<pre><code>from omnidocs import Document\n\ndoc = Document.from_pdf(\"document.pdf\")\n\n# Convert metadata to dict for serialization\ndata = doc.to_dict()\nprint(data)\n\n# Output:\n# {\n#     'source_type': 'file',\n#     'source_path': '/path/to/document.pdf',\n#     'file_name': 'document.pdf',\n#     'file_size': 12345,\n#     'page_count': 50,\n#     'format': 'pdf',\n#     'image_dpi': 150,\n#     'loaded_at': '2024-02-01T12:34:56.123456'\n# }\n\n# Save metadata as JSON\nimport json\nwith open(\"metadata.json\", \"w\") as f:\n    json.dump(data, f, indent=2)\n</code></pre>"},{"location":"getting-started/first-document/#memory-management","title":"Memory Management","text":""},{"location":"getting-started/first-document/#cache-behavior","title":"Cache Behavior","text":"<p>OmniDocs caches rendered pages to avoid re-rendering:</p> <pre><code>from omnidocs import Document\n\ndoc = Document.from_pdf(\"document.pdf\")\n\n# First access: renders page (slow)\npage1 = doc.get_page(0)  # ~200ms\n\n# Second access: returns cached copy (instant)\npage1_again = doc.get_page(0)  # &lt;1ms\n</code></pre>"},{"location":"getting-started/first-document/#clear-cache","title":"Clear Cache","text":"<p>For large batch processing, clear cache to free memory:</p> <pre><code>from omnidocs import Document\n\ndoc = Document.from_pdf(\"document.pdf\")\n\n# Clear specific page\ndoc.clear_cache(0)\n\n# Clear all pages\ndoc.clear_cache()\n\n# Good practice when processing many documents\nfor pdf_file in pdf_files:\n    doc = Document.from_pdf(pdf_file)\n    for page in doc.iter_pages():\n        # Process page\n        pass\n    doc.clear_cache()  # Free memory before next document\n</code></pre>"},{"location":"getting-started/first-document/#close-document","title":"Close Document","text":"<pre><code>from omnidocs import Document\n\ndoc = Document.from_pdf(\"document.pdf\")\n\n# Explicitly close and free resources\ndoc.close()\n\n# Or use context manager (recommended)\nwith Document.from_pdf(\"document.pdf\") as doc:\n    # Work with document\n    pass\n# Automatically closed\n</code></pre>"},{"location":"getting-started/first-document/#performance-tips","title":"Performance Tips","text":""},{"location":"getting-started/first-document/#optimize-dpi-for-speed","title":"Optimize DPI for Speed","text":"<pre><code>from omnidocs import Document\n\n# Default: 150 DPI (good balance)\ndoc_balanced = Document.from_pdf(\"doc.pdf\", dpi=150)\n\n# Faster but lower quality (50 DPI)\ndoc_fast = Document.from_pdf(\"doc.pdf\", dpi=100)\n\n# Higher quality but slower (300 DPI)\ndoc_hq = Document.from_pdf(\"doc.pdf\", dpi=300)\n</code></pre> DPI Speed Quality Use Case 72 10x Poor Text only, OCR 100 5x Fair Quick scan 150 1x Good Default 200 0.5x Very Good Important docs 300 0.2x Excellent Archives, legal"},{"location":"getting-started/first-document/#process-large-documents-efficiently","title":"Process Large Documents Efficiently","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\ndoc = Document.from_pdf(\"1000_pages.pdf\", dpi=150)\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(device=\"cuda\")\n)\n\n# Pattern: Process one page at a time, save immediately\nresults = []\nfor i, page in enumerate(doc.iter_pages()):\n    result = extractor.extract(page, output_format=\"markdown\")\n\n    # Save page result immediately (don't accumulate in memory)\n    with open(f\"page_{i+1:04d}.md\", \"w\") as f:\n        f.write(result.content)\n\n    # Clear cache every N pages to free memory\n    if (i + 1) % 10 == 0:\n        doc.clear_cache()\n        print(f\"Processed {i + 1}/{doc.page_count} pages\")\n\nprint(\"Done!\")\n</code></pre>"},{"location":"getting-started/first-document/#working-with-page-range","title":"Working with Page Range","text":"<pre><code>from omnidocs import Document\n\n# Load only pages 50-100 (saves memory)\ndoc = Document.from_pdf(\n    \"1000_pages.pdf\",\n    page_range=(49, 99)  # 0-indexed, inclusive\n)\n\nprint(doc.page_count)  # 51 (pages 50-100 inclusive)\n\n# Now access as normal\nfor page in doc.iter_pages():\n    # Only processes the 51-page range\n    pass\n</code></pre>"},{"location":"getting-started/first-document/#batch-processing-multiple-files","title":"Batch Processing Multiple Files","text":"<pre><code>from pathlib import Path\nfrom omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize extractor once (expensive, don't repeat)\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(device=\"cuda\")\n)\n\n# Find all PDFs\npdf_files = sorted(Path(\"documents/\").glob(\"*.pdf\"))\n\n# Process each\nfor pdf_path in pdf_files:\n    print(f\"Processing {pdf_path.name}...\")\n\n    with Document.from_pdf(str(pdf_path)) as doc:\n        for i, page in enumerate(doc.iter_pages()):\n            result = extractor.extract(page)\n            # Save result\n            output_file = pdf_path.stem / f\"page_{i+1}.md\"\n            with open(output_file, \"w\") as f:\n                f.write(result.content)\n\nprint(f\"Processed {len(pdf_files)} documents\")\n</code></pre>"},{"location":"getting-started/first-document/#real-world-examples","title":"Real-World Examples","text":""},{"location":"getting-started/first-document/#example-1-extract-and-save-first-page","title":"Example 1: Extract and Save First Page","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Load document\ndoc = Document.from_pdf(\"report.pdf\")\n\n# Extract first page\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(device=\"cuda\")\n)\nresult = extractor.extract(doc.get_page(0), output_format=\"markdown\")\n\n# Save\nwith open(\"first_page.md\", \"w\") as f:\n    f.write(result.content)\n</code></pre>"},{"location":"getting-started/first-document/#example-2-extract-table-of-contents","title":"Example 2: Extract Table of Contents","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\ndoc = Document.from_pdf(\"book.pdf\")\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(device=\"cuda\")\n)\n\n# Extract first few pages (usually TOC is at start)\ntoc_pages = []\nfor i in range(min(5, doc.page_count)):\n    page = doc.get_page(i)\n    result = extractor.extract(page)\n    toc_pages.append(result.content)\n\n# Combine and save TOC\nwith open(\"table_of_contents.md\", \"w\") as f:\n    f.write(\"\\n\\n\".join(toc_pages))\n</code></pre>"},{"location":"getting-started/first-document/#example-3-document-summary","title":"Example 3: Document Summary","text":"<pre><code>from omnidocs import Document\n\ndoc = Document.from_pdf(\"document.pdf\")\n\n# Print document info\nprint(f\"File: {doc.metadata.file_name}\")\nprint(f\"Size: {doc.metadata.file_size / (1024**2):.2f} MB\")\nprint(f\"Pages: {doc.page_count}\")\nprint(f\"Loaded: {doc.metadata.loaded_at}\")\n\nif doc.metadata.pdf_metadata:\n    print(f\"Title: {doc.metadata.pdf_metadata.get('Title', 'N/A')}\")\n    print(f\"Author: {doc.metadata.pdf_metadata.get('Author', 'N/A')}\")\n</code></pre>"},{"location":"getting-started/first-document/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/first-document/#page-out-of-range","title":"\"Page out of range\"","text":"<pre><code># Always check bounds\nif page_num &lt; 0 or page_num &gt;= doc.page_count:\n    print(\"Invalid page number\")\nelse:\n    page = doc.get_page(page_num)\n</code></pre>"},{"location":"getting-started/first-document/#document-takes-too-long-to-load","title":"Document takes too long to load","text":"<pre><code># Increase DPI if rendering is slow\ndoc = Document.from_pdf(\"doc.pdf\", dpi=100)  # Lower quality, faster\n\n# Or use page range\ndoc = Document.from_pdf(\"doc.pdf\", page_range=(0, 10))  # First 10 pages\n</code></pre>"},{"location":"getting-started/first-document/#out-of-memory-during-iteration","title":"\"Out of memory\" during iteration","text":"<pre><code># Clear cache more frequently\nfor i, page in enumerate(doc.iter_pages()):\n    # Process page\n    if i % 5 == 0:  # Every 5 pages\n        doc.clear_cache()\n</code></pre>"},{"location":"getting-started/first-document/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart - Jump to extracting text</li> <li>Choosing Backends - Select the right inference backend</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"getting-started/first-document/#summary","title":"Summary","text":"<pre><code># Load document (4 ways)\ndoc = Document.from_pdf(\"file.pdf\")\ndoc = Document.from_url(\"https://example.com/doc.pdf\")\ndoc = Document.from_bytes(pdf_bytes)\ndoc = Document.from_image(\"page.png\")\n\n# Access pages\npage = doc.get_page(0)           # Single page (0-indexed)\nfor page in doc.iter_pages():    # Iterate efficiently\n    pass\nwidth, height = doc.get_page_size(0)  # Get size without rendering\n\n# Access metadata\nprint(doc.page_count)\nprint(doc.metadata.file_name)\nprint(doc.to_dict())\n\n# Memory management\ndoc.clear_cache()                # Free memory\ndoc.clear_cache(0)               # Clear specific page\ndoc.close()                      # Close document\n</code></pre> <p>Happy processing!</p>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>Welcome to OmniDocs! This guide walks you through installing OmniDocs and choosing the right backend for your use case.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"<p>Before installing, ensure you have:</p> <ul> <li>Python: 3.10, 3.11, or 3.12</li> <li>pip or uv (recommended for faster installation)</li> <li>4 GB RAM minimum (8 GB+ recommended for large documents)</li> <li>GPU (optional, but highly recommended for fast inference)</li> </ul>"},{"location":"getting-started/installation/#quick-install","title":"Quick Install","text":"<p>The fastest way to get started:</p> <pre><code># Basic installation (PyTorch backend, most common)\npip install omnidocs[pytorch]\n\n# Or with uv (faster)\nuv pip install omnidocs[pytorch]\n</code></pre> <p>This installs OmniDocs with PyTorch support for local GPU inference on NVIDIA/AMD GPUs and Apple Silicon.</p>"},{"location":"getting-started/installation/#choosing-your-backend","title":"Choosing Your Backend","text":"<p>OmniDocs supports 4 inference backends. Choose one based on your needs:</p> Backend Best For GPU Required Setup Time Cost PyTorch Development, local testing, single GPU Optional (faster with GPU) ~5 min Free VLLM Production, high throughput, batch processing Yes (NVIDIA) ~10 min Free MLX Apple Silicon (M1/M2/M3+), no GPU needed No (optimized for Mac) ~5 min Free API Cloud-based, no GPU setup, pay-per-use No ~2 min $0.01-0.10/request"},{"location":"getting-started/installation/#backend-decision-tree","title":"Backend Decision Tree","text":"<pre><code>Do you have a Mac with Apple Silicon (M1/M2/M3+)?\n\u251c\u2500 YES \u2192 Use MLX backend\n\u2514\u2500 NO\n   \u251c\u2500 Do you need to process 100+ documents quickly?\n   \u2502  \u251c\u2500 YES \u2192 Use VLLM backend\n   \u2502  \u2514\u2500 NO\n   \u2502     \u251c\u2500 Do you have a GPU (NVIDIA, AMD, or other)?\n   \u2502     \u2502  \u251c\u2500 YES \u2192 Use PyTorch backend (recommended)\n   \u2502     \u2502  \u2514\u2500 NO\n   \u2502     \u2502     \u2514\u2500 Use API backend\n   \u2514\u2500 Or just prototyping/learning?\n      \u2514\u2500 Use PyTorch backend\n</code></pre>"},{"location":"getting-started/installation/#installation-instructions","title":"Installation Instructions","text":""},{"location":"getting-started/installation/#option-1-pytorch-recommended-for-most-users","title":"Option 1: PyTorch (Recommended for Most Users)","text":"<p>Best for local development and GPU inference on NVIDIA/AMD GPUs.</p> <pre><code># Install with pip\npip install omnidocs[pytorch]\n\n# Or with uv\nuv pip install omnidocs[pytorch]\n\n# Verify installation\npython -c \"from omnidocs import Document; print('PyTorch backend ready!')\"\n</code></pre> <p>What's Installed: - <code>torch</code> - Deep learning framework - <code>transformers</code> - HuggingFace model support - <code>accelerate</code> - Multi-GPU support - All core OmniDocs dependencies</p> <p>Requirements: - NVIDIA GPU: CUDA 12.1+ (install from nvidia.com) - AMD GPU: ROCm support (install from rocmdocs.amd.com) - Apple Silicon: Works out of the box (will use CPU, slower)</p> <p>GPU Check: <pre><code>import torch\nprint(f\"GPU Available: {torch.cuda.is_available()}\")\nprint(f\"GPU Count: {torch.cuda.device_count()}\")\nprint(f\"GPU Name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n</code></pre></p>"},{"location":"getting-started/installation/#option-2-vllm-high-throughput-production","title":"Option 2: VLLM (High-Throughput Production)","text":"<p>Best for processing many documents or running as a service.</p> <pre><code># Install with pip\npip install omnidocs[vllm]\n\n# Or with uv\nuv pip install omnidocs[vllm]\n\n# Verify installation\npython -c \"from omnidocs.tasks.text_extraction import QwenTextExtractor; print('VLLM backend ready!')\"\n</code></pre> <p>What's Installed: - <code>vllm</code> - High-throughput inference engine - All PyTorch dependencies (automatically included)</p> <p>Requirements: - NVIDIA GPU with 24+ GB VRAM (A40, A100, H100, RTX 4090, etc.) - CUDA 12.1+ installed</p> <p>Why VLLM? - 10x faster throughput than PyTorch for batch processing - Optimized tensor parallelism across multiple GPUs - Built-in request batching and memory optimization</p>"},{"location":"getting-started/installation/#option-3-mlx-apple-silicon","title":"Option 3: MLX (Apple Silicon)","text":"<p>Best for M1/M2/M3 Macs without external GPU dependencies.</p> <pre><code># Install with pip\npip install omnidocs[mlx]\n\n# Or with uv\nuv pip install omnidocs[mlx]\n\n# Verify installation\npython -c \"from omnidocs.tasks.text_extraction.qwen import QwenTextMLXConfig; print('MLX backend ready!')\"\n</code></pre> <p>What's Installed: - <code>mlx</code> - Apple Silicon machine learning framework - <code>mlx-vlm</code> - Vision-language model support for MLX</p> <p>Requirements: - Mac with Apple Silicon (M1, M1 Pro, M1 Max, M2, M3, etc.) - macOS 12+ - 8 GB+ unified memory (RAM)</p> <p>Why MLX? - Native Apple Silicon optimization (2-3x faster than generic Python) - No need for NVIDIA GPU drivers or CUDA - Automatic unified memory management</p> <p>Check Your Mac: <pre><code># See your chip type\nsysctl -a | grep machdep.cpu.brand_string\n\n# Output: Apple M3 Max \u2192 You can use MLX!\n</code></pre></p>"},{"location":"getting-started/installation/#option-4-api-cloud-based-no-setup","title":"Option 4: API (Cloud-Based, No Setup)","text":"<p>Best for quick testing without GPU, or outsourcing inference costs.</p> <pre><code># Install with pip\npip install omnidocs[api]\n\n# Set up API key\nexport OPENAI_API_KEY=\"sk-...\"  # Your API key\n\n# Or use in code\nimport os\nos.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n\n# Verify installation\npython -c \"from omnidocs.tasks.text_extraction.qwen import QwenTextAPIConfig; print('API backend ready!')\"\n</code></pre> <p>What's Installed: - <code>openai</code> - API client library</p> <p>Requirements: - API key (from OpenAI or compatible provider) - Internet connection - ~$0.01-0.10 per document</p> <p>Cost Estimation: - Small documents (&lt; 1 MB): $0.01-0.05 - Medium documents (1-10 MB): $0.05-0.15 - Large documents (&gt; 10 MB): $0.15-0.50</p>"},{"location":"getting-started/installation/#advanced-installations","title":"Advanced Installations","text":""},{"location":"getting-started/installation/#install-everything-all-backends","title":"Install Everything (All Backends)","text":"<pre><code>pip install omnidocs[all]\n</code></pre> <p>This installs PyTorch, VLLM, MLX, and API backends. Useful for organizations with mixed infrastructure.</p>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>For contributing to OmniDocs:</p> <pre><code># Clone repository\ngit clone https://github.com/adithya-s-k/OmniDocs.git\ncd OmniDocs\n\n# Install with development tools\nuv sync --group dev\n</code></pre>"},{"location":"getting-started/installation/#using-uv-recommended-for-performance","title":"Using UV (Recommended for Performance)","text":"<p>UV is 10-100x faster than pip for dependency resolution:</p> <pre><code># Install uv (one time)\npip install uv\n\n# Use uv instead of pip\nuv pip install omnidocs[pytorch]\n\n# Or create a virtual environment\nuv venv\nsource .venv/bin/activate\nuv pip install omnidocs[pytorch]\n</code></pre>"},{"location":"getting-started/installation/#verification-steps","title":"Verification Steps","text":""},{"location":"getting-started/installation/#verify-core-installation","title":"Verify Core Installation","text":"<pre><code># Test basic import\nfrom omnidocs import Document\nprint(\"OmniDocs installed successfully!\")\n\n# Check version\nimport omnidocs\nprint(f\"Version: {omnidocs.__version__}\")\n</code></pre>"},{"location":"getting-started/installation/#verify-backend-specific-installation","title":"Verify Backend-Specific Installation","text":"<p>PyTorch: <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nprint(\"PyTorch backend available!\")\n</code></pre></p> <p>VLLM: <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\nprint(\"VLLM backend available!\")\n</code></pre></p> <p>MLX: <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextMLXConfig\nprint(\"MLX backend available!\")\n</code></pre></p> <p>API: <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextAPIConfig\nprint(\"API backend available!\")\n</code></pre></p>"},{"location":"getting-started/installation/#full-test","title":"Full Test","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom PIL import Image\n\n# Create a test image\nimg = Image.new('RGB', (400, 300), color='white')\n\n# Initialize extractor\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-2B-Instruct\",  # Small model for testing\n        device=\"cpu\",  # Test on CPU first\n    )\n)\n\n# Extract text\nresult = extractor.extract(img, output_format=\"markdown\")\nprint(f\"Success! Extracted {len(result.content)} characters\")\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#modulenotfounderror-no-module-named-torch","title":"\"ModuleNotFoundError: No module named 'torch'\"","text":"<p>Problem: PyTorch backend not installed.</p> <p>Solution: <pre><code>pip install omnidocs[pytorch]\n</code></pre></p>"},{"location":"getting-started/installation/#cuda-out-of-memory","title":"\"CUDA out of memory\"","text":"<p>Problem: GPU doesn't have enough memory for the model.</p> <p>Solutions: 1. Use a smaller model:    <pre><code>config = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-2B-Instruct\"  # 2B instead of 8B\n)\n</code></pre></p> <ol> <li> <p>Reduce batch size (if processing multiple images)</p> </li> <li> <p>Clear GPU memory:    <pre><code>import torch\ntorch.cuda.empty_cache()\n</code></pre></p> </li> </ol>"},{"location":"getting-started/installation/#no-gpu-detected","title":"\"No GPU detected\"","text":"<p>Problem: PyTorch installed but GPU not recognized.</p> <p>Solution: <pre><code># Check CUDA installation\nnvidia-smi  # For NVIDIA GPUs\n\n# Reinstall torch with CUDA support\npip uninstall torch torchvision\npip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n</code></pre></p>"},{"location":"getting-started/installation/#importerror-urllib3-not-available","title":"\"ImportError: urllib3 not available\"","text":"<p>Problem: Dependency conflict with requests library.</p> <p>Solution: <pre><code>pip install --upgrade requests urllib3\n</code></pre></p>"},{"location":"getting-started/installation/#vllm-requires-cuda","title":"\"VLLM requires CUDA\"","text":"<p>Problem: VLLM installed on non-NVIDIA system.</p> <p>Solution: Use PyTorch backend instead: <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nconfig = QwenTextPyTorchConfig(...)\n</code></pre></p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>After installation, proceed to:</p> <ol> <li>Quickstart - Get running in 5 minutes</li> <li>First Document - Load and process your first PDF</li> <li>Choosing Backends - Understand backend tradeoffs</li> </ol>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<ul> <li>Documentation: Visit omnidocs.readthedocs.io</li> <li>GitHub Issues: Report bugs at github.com/adithya-s-k/OmniDocs/issues</li> <li>Discussions: Ask questions at github.com/adithya-s-k/OmniDocs/discussions</li> </ul>"},{"location":"getting-started/installation/#summary","title":"Summary","text":"Use Case Install Command Backend Approx Time Quick prototyping <code>pip install omnidocs[pytorch]</code> PyTorch 5 min Production scale <code>pip install omnidocs[vllm]</code> VLLM 10 min Mac development <code>pip install omnidocs[mlx]</code> MLX 5 min No GPU available <code>pip install omnidocs[api]</code> API 2 min All options <code>pip install omnidocs[all]</code> All 15 min <p>Happy documenting!</p>"},{"location":"getting-started/quickstart/","title":"Quickstart: 5 Minutes to Your First Extraction","text":"<p>Get OmniDocs running in 5 minutes with this minimal working example.</p>"},{"location":"getting-started/quickstart/#1-install-1-minute","title":"1. Install (1 minute)","text":"<pre><code>pip install omnidocs[pytorch]\n</code></pre> <p>If you don't have a GPU, use the API backend instead: <pre><code>pip install omnidocs[api]\nexport OPENAI_API_KEY=\"sk-...\"\n</code></pre></p>"},{"location":"getting-started/quickstart/#2-load-a-document-30-seconds","title":"2. Load a Document (30 seconds)","text":"<p>OmniDocs can load PDFs, images, or URLs:</p> <pre><code>from omnidocs import Document\n\n# Load from PDF file\ndoc = Document.from_pdf(\"example.pdf\")\nprint(f\"Loaded {doc.page_count} pages\")\n\n# Or load from image\ndoc = Document.from_image(\"page.png\")\n\n# Or load from URL\ndoc = Document.from_url(\"https://example.com/document.pdf\")\n</code></pre>"},{"location":"getting-started/quickstart/#3-extract-text-2-minutes","title":"3. Extract Text (2 minutes)","text":"<p>Extract all text from the document in Markdown format:</p> <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize the extractor (loads model on first use)\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-2B-Instruct\",  # Fast, small model\n        device=\"cuda\",  # Use \"cpu\" if no GPU\n    )\n)\n\n# Get the first page\nfirst_page = doc.get_page(0)\n\n# Extract text as Markdown\nresult = extractor.extract(\n    first_page,\n    output_format=\"markdown\"\n)\n\n# Access the extracted content\nprint(result.content)\n</code></pre>"},{"location":"getting-started/quickstart/#4-get-the-output-30-seconds","title":"4. Get the Output (30 seconds)","text":"<pre><code># The result object has these properties:\nprint(result.content)           # The extracted text (markdown)\nprint(result.format)            # Output format (markdown, html)\nprint(result.content_length)    # Number of characters\nprint(result.has_layout)        # Whether layout was included\n</code></pre> <p>Example output: <pre><code># Document Title\n\nThis is the first paragraph extracted from your document.\n\n## Section Heading\n\n- Bullet point 1\n- Bullet point 2\n\n**Bold text** and *italic text* are preserved.\n</code></pre></p>"},{"location":"getting-started/quickstart/#complete-example-process-an-entire-document","title":"Complete Example: Process an Entire Document","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Load document\ndoc = Document.from_pdf(\"research_paper.pdf\")\nprint(f\"Processing {doc.page_count} pages...\")\n\n# Initialize extractor\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-2B-Instruct\",\n        device=\"cuda\",\n    )\n)\n\n# Process all pages and collect results\nall_content = []\nfor page_num in range(doc.page_count):\n    page = doc.get_page(page_num)\n    result = extractor.extract(page, output_format=\"markdown\")\n    all_content.append(result.content)\n    print(f\"Processed page {page_num + 1}/{doc.page_count}\")\n\n# Save combined output\nfull_text = \"\\n\\n---\\n\\n\".join(all_content)\nwith open(\"output.md\", \"w\") as f:\n    f.write(full_text)\n\nprint(\"Saved extracted text to output.md\")\n</code></pre>"},{"location":"getting-started/quickstart/#faster-processing-with-efficient-iteration","title":"Faster Processing with Efficient Iteration","text":"<p>For large documents, process pages efficiently:</p> <pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Load document\ndoc = Document.from_pdf(\"large_document.pdf\", dpi=150)\n\n# Initialize extractor once\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-2B-Instruct\",\n        device=\"cuda\",\n    )\n)\n\n# Process pages one at a time (memory efficient)\nfor i, page in enumerate(doc.iter_pages()):\n    result = extractor.extract(page, output_format=\"markdown\")\n\n    # Save each page immediately to avoid memory buildup\n    with open(f\"page_{i+1:03d}.md\", \"w\") as f:\n        f.write(result.content)\n\n    # Clear cache to free GPU memory\n    if i % 5 == 0:\n        doc.clear_cache()\n\nprint(f\"Saved all {doc.page_count} pages to output files\")\n</code></pre>"},{"location":"getting-started/quickstart/#choose-your-backend","title":"Choose Your Backend","text":"<p>Different backends for different needs:</p>"},{"location":"getting-started/quickstart/#pytorch-default-recommended","title":"PyTorch (Default, Recommended)","text":"<p>Best for local GPU inference on NVIDIA/AMD GPUs.</p> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-2B-Instruct\",\n    device=\"cuda\",\n    torch_dtype=\"bfloat16\",  # Faster with less precision\n)\n</code></pre>"},{"location":"getting-started/quickstart/#vllm-high-throughput","title":"VLLM (High Throughput)","text":"<p>Best for processing 100+ documents quickly.</p> <pre><code>pip install omnidocs[vllm]\n</code></pre> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\n\nconfig = QwenTextVLLMConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    tensor_parallel_size=2,  # Multi-GPU\n    gpu_memory_utilization=0.9,\n)\n</code></pre>"},{"location":"getting-started/quickstart/#mlx-apple-silicon","title":"MLX (Apple Silicon)","text":"<p>Best for Mac development.</p> <pre><code>pip install omnidocs[mlx]\n</code></pre> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextMLXConfig\n\nconfig = QwenTextMLXConfig(\n    model=\"Qwen/Qwen3-VL-2B-Instruct\",\n)\n</code></pre>"},{"location":"getting-started/quickstart/#api-cloud-based","title":"API (Cloud-Based)","text":"<p>Best for no GPU setup.</p> <pre><code>pip install omnidocs[api]\nexport OPENAI_API_KEY=\"sk-...\"\n</code></pre> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextAPIConfig\n\nconfig = QwenTextAPIConfig(\n    model=\"qwen3-vl-8b\",\n    api_key=\"sk-...\",\n)\n</code></pre>"},{"location":"getting-started/quickstart/#common-tasks","title":"Common Tasks","text":""},{"location":"getting-started/quickstart/#extract-from-pdf-and-save-to-file","title":"Extract from PDF and Save to File","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\ndoc = Document.from_pdf(\"input.pdf\")\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(device=\"cuda\")\n)\n\n# Extract first page\nresult = extractor.extract(doc.get_page(0))\n\n# Save to file\nwith open(\"output.md\", \"w\") as f:\n    f.write(result.content)\n</code></pre>"},{"location":"getting-started/quickstart/#extract-with-layout-information","title":"Extract with Layout Information","text":"<pre><code># Include structure and layout in output\nresult = extractor.extract(\n    page,\n    output_format=\"markdown\",\n    include_layout=True  # Adds structure information\n)\n\n# Access layout information\nif result.layout:\n    print(f\"Found {len(result.layout)} layout elements\")\n    for elem in result.layout:\n        print(f\"- {elem.category}: {elem.content[:50]}...\")\n</code></pre>"},{"location":"getting-started/quickstart/#batch-process-multiple-pdfs","title":"Batch Process Multiple PDFs","text":"<pre><code>from pathlib import Path\nfrom omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Find all PDFs in a directory\npdf_dir = Path(\"documents/\")\npdf_files = list(pdf_dir.glob(\"*.pdf\"))\n\n# Initialize extractor once\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(device=\"cuda\")\n)\n\n# Process each PDF\nfor pdf_path in pdf_files:\n    print(f\"Processing {pdf_path.name}...\")\n    doc = Document.from_pdf(str(pdf_path))\n\n    for i, page in enumerate(doc.iter_pages()):\n        result = extractor.extract(page)\n\n        # Save output\n        output_path = pdf_path.stem / f\"page_{i+1}.md\"\n        with open(output_path, \"w\") as f:\n            f.write(result.content)\n</code></pre>"},{"location":"getting-started/quickstart/#output-formats","title":"Output Formats","text":"<p>OmniDocs can extract to different formats:</p>"},{"location":"getting-started/quickstart/#markdown-default","title":"Markdown (Default)","text":"<p>Preserves formatting with Markdown syntax:</p> <pre><code>result = extractor.extract(page, output_format=\"markdown\")\n# Output: # Heading\\n\\nParagraph text\\n\\n- List items\n</code></pre>"},{"location":"getting-started/quickstart/#html","title":"HTML","text":"<p>Preserves formatting with HTML tags:</p> <pre><code>result = extractor.extract(page, output_format=\"html\")\n# Output: &lt;h1&gt;Heading&lt;/h1&gt;&lt;p&gt;Paragraph text&lt;/p&gt;&lt;ul&gt;&lt;li&gt;List items&lt;/li&gt;&lt;/ul&gt;\n</code></pre>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":"<ul> <li>First Document Guide - Deep dive into loading and accessing documents</li> <li>Backend Selection - Understand PyTorch vs VLLM vs MLX vs API</li> <li>Full API Reference - Complete API documentation</li> </ul>"},{"location":"getting-started/quickstart/#quick-reference","title":"Quick Reference","text":"<pre><code># Load document\ndoc = Document.from_pdf(\"file.pdf\")\ndoc = Document.from_image(\"file.png\")\ndoc = Document.from_url(\"https://example.com/doc.pdf\")\n\n# Access pages\npage = doc.get_page(0)           # Single page (0-indexed)\nfor page in doc.iter_pages():    # Iterate efficiently\n    pass\n\n# Extract text\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(device=\"cuda\")\n)\n\nresult = extractor.extract(page, output_format=\"markdown\")\n\n# Access results\nprint(result.content)            # Extracted text\nprint(result.format)             # Output format\nprint(result.has_layout)         # Has layout info\n</code></pre>"},{"location":"getting-started/quickstart/#troubleshooting-quick-fixes","title":"Troubleshooting Quick Fixes","text":"<p>\"CUDA out of memory\": <pre><code># Use a smaller model\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-2B-Instruct\"  # Instead of 8B\n)\n</code></pre></p> <p>\"Model download is slow\": <pre><code># Models download to HuggingFace cache directory\n# First run takes longer (~10 min), subsequent runs are instant\n# Check progress in terminal\n</code></pre></p> <p>\"I don't have a GPU\": <pre><code># Use API backend instead\npip install omnidocs[api]\n</code></pre></p> <p>Happy extracting! For more advanced usage, see the complete documentation.</p>"},{"location":"guides/","title":"OmniDocs Task-Oriented Guides","text":"<p>Practical, copy-paste-ready guides for common OmniDocs tasks. Each guide includes real-world examples, best practices, and troubleshooting.</p>"},{"location":"guides/#quick-navigation","title":"Quick Navigation","text":""},{"location":"guides/#by-task","title":"By Task","text":"<p>Extract text from documents: - Text Extraction Guide - Convert documents to Markdown/HTML (2000 words, 4 examples) - Choose model: Qwen3-VL (recommended), DotsOCR (technical docs), Nanonets (coming soon) - Output formats: Markdown, HTML, plain text - Advanced: Custom prompts, include_layout, temperature control</p> <p>Detect document structure: - Layout Analysis Guide - Find elements and their locations (2000 words, 4 examples) - Choose model: DocLayoutYOLO (fast), RT-DETR (accurate), QwenLayoutDetector (flexible) - Features: Fixed labels, custom labels, confidence filtering, visualization</p> <p>Extract text with locations: - OCR Extraction Guide - Get text + bounding boxes (1800 words, 4 examples) - Choose model: Tesseract (CPU), EasyOCR (accurate), PaddleOCR (fast) - Features: Multi-language, confidence filtering, region filtering - Granularities: Character, word, line, block</p> <p>Process many documents: - Batch Processing Guide - Handle 100+ documents efficiently (1600 words) - Patterns: Sequential, batched, PDF pages, parallel preprocessing - Memory optimization: Streaming, garbage collection, monitoring - Progress tracking with tqdm, ETA estimation</p> <p>Deploy to GPU cloud: - Modal Deployment Guide - Scale with serverless GPUs (1800 words, 2 examples) - Cost: $0.30-1.00 per hour of GPU - Patterns: Single-GPU, multi-GPU, batch, scheduled, webhooks - Multi-GPU: Tensor parallelism, load balancing</p>"},{"location":"guides/#which-guide-to-read-first","title":"Which Guide to Read First","text":""},{"location":"guides/#i-want-to","title":"I want to...","text":"<p>...extract text from a document 1. Read Text Extraction Guide - Basic Usage section 2. Choose model (Table: Qwen3-VL recommended) 3. Copy Example 1: Simple Markdown Extraction 4. Run on your document</p> <p>...build a document processing pipeline 1. Read Layout Analysis Guide - Basic Usage 2. Read Text Extraction Guide - Advanced Features 3. Combine: Layout \u2192 Filter \u2192 Text extraction 4. See Batch Processing Guide for multiple documents</p> <p>...find precise text locations 1. Read OCR Extraction Guide - Basic Usage 2. Choose model (Table: EasyOCR for accuracy, PaddleOCR for speed) 3. Copy Example 1: Simple Word-Level OCR 4. Filter results by confidence/region as needed</p> <p>...process 100+ documents 1. Read Batch Processing Guide - Processing Patterns 2. Choose pattern (Sequential recommended for learning) 3. Add progress tracking (tqdm) 4. Monitor GPU memory 5. See Deployment Guide for scale</p> <p>...deploy on GPU cloud 1. Read Deployment Guide - Standard Setup 2. Set up Modal (token, volume, secret) 3. Copy Example 1: Simple Text Extraction 4. Deploy with <code>modal run script.py</code> 5. Scale with Example 2: Multi-GPU VLLM</p>"},{"location":"guides/#guide-comparison","title":"Guide Comparison","text":"Guide Focus Typical Task Time Difficulty Text Extraction Content conversion \"Convert PDF to Markdown\" 2-5s/page Beginner Layout Analysis Structure detection \"Find all tables in doc\" 0.5-1s/page Beginner OCR Extraction Text with locations \"Extract word coordinates\" 1-2s/page Intermediate Batch Processing Scale &amp; efficiency \"Process 1000 documents\" Variable Intermediate Deployment Cloud infrastructure \"Run 24/7 API service\" Setup: 5min Advanced"},{"location":"guides/#common-workflows","title":"Common Workflows","text":""},{"location":"guides/#workflow-1-simple-document-parsing","title":"Workflow 1: Simple Document Parsing","text":"<p>Convert documents to readable Markdown.</p> <p>Tools: Text Extraction only Steps: 1. Load document with Document.from_pdf() 2. Use QwenTextExtractor with PyTorch backend 3. Extract to Markdown format 4. Save to file</p> <p>Guide: Text Extraction - Example 3 (PDF with Multiple Pages)</p>"},{"location":"guides/#workflow-2-table-and-figure-extraction","title":"Workflow 2: Table and Figure Extraction","text":"<p>Find all tables and figures with their content.</p> <p>Tools: Layout Analysis + Text Extraction Steps: 1. Run layout detection (find all elements) 2. Filter for 'table' and 'figure' labels 3. Crop to bounding box 4. Extract text/tables from each region 5. Combine results</p> <p>Guides: - Layout Analysis - Filtering and Analyzing Results - Text Extraction - Basic Usage Example 2 (Layout Information)</p>"},{"location":"guides/#workflow-3-handwriting-localization","title":"Workflow 3: Handwriting Localization","text":"<p>Find coordinates of all handwritten text.</p> <p>Tools: OCR Extraction (EasyOCR or PaddleOCR) Steps: 1. Load image 2. Run OCR extraction 3. Filter by confidence (handwriting has lower confidence) 4. Get bounding boxes for each word 5. Save coordinates</p> <p>Guide: OCR Extraction - Extracting Bounding Boxes</p>"},{"location":"guides/#workflow-4-batch-document-processing","title":"Workflow 4: Batch Document Processing","text":"<p>Process 100+ documents efficiently.</p> <p>Tools: Text Extraction + Batch Processing Steps: 1. Load all images from directory 2. Initialize extractor once 3. Process sequentially or batched 4. Stream results to disk (JSONL) 5. Track progress with tqdm</p> <p>Guides: - Batch Processing - Processing Patterns Example 1 - Text Extraction - Example 4 (Batch with Progress)</p>"},{"location":"guides/#workflow-5-scale-to-1000-documents","title":"Workflow 5: Scale to 1000+ Documents","text":"<p>Deploy batch processing on GPU cloud.</p> <p>Tools: Batch Processing + Modal Deployment Steps: 1. Write batch function 2. Deploy to Modal 3. Submit batch jobs 4. Monitor with logging 5. Optimize costs with spot instances</p> <p>Guides: - Batch Processing - All sections - Deployment - Production Patterns</p>"},{"location":"guides/#api-quick-reference","title":"API Quick Reference","text":""},{"location":"guides/#text-extraction","title":"Text Extraction","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nconfig = QwenTextPyTorchConfig(device=\"cuda\")\nextractor = QwenTextExtractor(backend=config)\n\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)  # Formatted text\n</code></pre>"},{"location":"guides/#layout-analysis","title":"Layout Analysis","text":"<pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\nconfig = DocLayoutYOLOConfig(device=\"cuda\")\ndetector = DocLayoutYOLO(config=config)\n\nresult = detector.extract(image)\nfor elem in result.elements:\n    print(f\"{elem.label} @ {elem.bbox}\")\n</code></pre>"},{"location":"guides/#ocr-extraction","title":"OCR Extraction","text":"<pre><code>from omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\nconfig = EasyOCRConfig(languages=[\"en\"], gpu=True)\nocr = EasyOCR(config=config)\n\nresult = ocr.extract(image)\nfor block in result.text_blocks:\n    print(f\"{block.text} @ {block.bbox}\")\n</code></pre>"},{"location":"guides/#batch-processing","title":"Batch Processing","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom pathlib import Path\n\nconfig = QwenTextPyTorchConfig(device=\"cuda\")\nextractor = QwenTextExtractor(backend=config)\n\nfor image_path in Path(\"images/\").glob(\"*.png\"):\n    image = Image.open(image_path)\n    result = extractor.extract(image)\n    # Process...\n</code></pre>"},{"location":"guides/#modal-deployment","title":"Modal Deployment","text":"<pre><code>import modal\n\n@app.function(gpu=\"A10G:1\")\ndef extract(image_bytes: bytes):\n    # Extract text...\n    return result\n\n# Deploy: modal run script.py\n# Or: modal deploy script.py\n</code></pre>"},{"location":"guides/#performance-expectations","title":"Performance Expectations","text":""},{"location":"guides/#per-page-latency","title":"Per-Page Latency","text":"Task Model Device Time Text Extraction Qwen3-VL-8B A10G GPU 2-3s Text Extraction Qwen3-VL-8B CPU 15-30s Layout Detection DocLayoutYOLO A10G GPU 0.5-1s OCR EasyOCR A10G GPU 1-2s OCR Tesseract CPU 0.5-1s"},{"location":"guides/#100-document-processing","title":"100-Document Processing","text":"Setup Tool Time Single A10G QwenTextExtractor ~4-5 min 2x A10G (VLLM) QwenTextExtractor ~2-3 min CPU Tesseract OCR ~50-100 min Modal batch QwenTextExtractor ~$0.30-0.50 cost"},{"location":"guides/#troubleshooting-quick-links","title":"Troubleshooting Quick Links","text":"<p>Model not found: - Text Extraction Troubleshooting - Model Download Issues</p> <p>Out of memory: - Text Extraction - OOM Errors - Batch Processing - OOM During Batch</p> <p>Slow inference: - Text Extraction - Backend Optimization - Batch Processing - Slow Processing</p> <p>Poor accuracy: - OCR Extraction - Low Accuracy - Layout Analysis - Missing Elements</p> <p>Deployment issues: - Modal Deployment - All common issues</p>"},{"location":"guides/#learn-more","title":"Learn More","text":"<ul> <li>OmniDocs API Reference: See <code>Omnidocs/omnidocs/</code> source code</li> <li>Modal Documentation: https://modal.com/docs</li> <li>HuggingFace Models: https://huggingface.co/models</li> <li>GitHub Issues: Report bugs or request features</li> </ul>"},{"location":"guides/#examples-repository","title":"Examples Repository","text":"<p>All code examples in these guides are copy-paste ready. For working end-to-end examples with test data, see: - <code>scripts/text_extract_omnidocs/</code> - Text extraction examples - <code>scripts/layout_omnidocs/</code> - Layout analysis examples - <code>scripts/ocr_omnidocs/</code> - OCR examples - <code>Omnidocs/tests/</code> - Unit tests with examples</p> <p>Start with: Text Extraction Guide \u2192 Example 1</p> <p>Happy extracting! \ud83d\ude80</p>"},{"location":"guides/batch-processing/","title":"Batch Processing Guide","text":"<p>Process multiple documents efficiently at scale. This guide covers batch loading, processing patterns, memory optimization, progress tracking, and GPU deployment.</p>"},{"location":"guides/batch-processing/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Batch Loading</li> <li>Processing Patterns</li> <li>Memory Optimization</li> <li>Progress Tracking</li> <li>Error Handling</li> <li>Performance Benchmarks</li> <li>Troubleshooting</li> </ul>"},{"location":"guides/batch-processing/#batch-loading","title":"Batch Loading","text":""},{"location":"guides/batch-processing/#load-from-directory","title":"Load from Directory","text":"<p>Load all images or PDFs from a directory.</p> <pre><code>from pathlib import Path\nfrom omnidocs import Document\nfrom PIL import Image\n\n# Find all image files\nimage_dir = Path(\"documents/images\")\nimage_paths = sorted(\n    list(image_dir.glob(\"*.png\")) +\n    list(image_dir.glob(\"*.jpg\")) +\n    list(image_dir.glob(\"*.jpeg\"))\n)\n\nprint(f\"Found {len(image_paths)} images\")\n\n# Load as PIL Images\nimages = [Image.open(p) for p in image_paths]\n\n# Load PDFs\npdf_dir = Path(\"documents/pdfs\")\npdf_paths = sorted(pdf_dir.glob(\"*.pdf\"))\n\ndocuments = [Document.from_pdf(p) for p in pdf_paths]\nprint(f\"Found {len(documents)} PDFs with {sum(d.page_count for d in documents)} total pages\")\n</code></pre>"},{"location":"guides/batch-processing/#lazy-loading-for-large-batches","title":"Lazy Loading for Large Batches","text":"<p>Don't load all images upfront - load as needed to save memory.</p> <pre><code>from pathlib import Path\nfrom PIL import Image\n\nimage_dir = Path(\"documents/\")\nimage_paths = sorted(image_dir.glob(\"*.png\"))\n\n# Generator: loads images on-demand\ndef image_generator(paths):\n    \"\"\"Generator that yields images one at a time.\"\"\"\n    for path in paths:\n        yield Image.open(path)\n\n# Usage: iterate without loading all at once\nfor idx, image in enumerate(image_generator(image_paths)):\n    print(f\"Processing image {idx+1}/{len(image_paths)}\")\n    # Process one image, then load next\n    # image is garbage collected automatically\n</code></pre>"},{"location":"guides/batch-processing/#load-with-metadata","title":"Load with Metadata","text":"<p>Track source information for each batch item.</p> <pre><code>from pathlib import Path\nfrom PIL import Image\nfrom dataclasses import dataclass\nfrom typing import Dict, Any\n\n@dataclass\nclass BatchItem:\n    \"\"\"Container for batch item with metadata.\"\"\"\n    path: Path\n    image: Image.Image\n    metadata: Dict[str, Any]\n\n# Load with metadata\nitems = []\nfor image_path in image_paths:\n    image = Image.open(image_path)\n    item = BatchItem(\n        path=image_path,\n        image=image,\n        metadata={\n            \"filename\": image_path.name,\n            \"size_bytes\": image_path.stat().st_size,\n            \"dimensions\": image.size,\n            \"format\": image.format,\n        }\n    )\n    items.append(item)\n\nprint(f\"Loaded {len(items)} items with metadata\")\n</code></pre>"},{"location":"guides/batch-processing/#processing-patterns","title":"Processing Patterns","text":""},{"location":"guides/batch-processing/#pattern-1-simple-loop","title":"Pattern 1: Simple Loop","text":"<p>Process items sequentially (smallest memory footprint).</p> <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom pathlib import Path\nfrom PIL import Image\nimport time\n\n# Initialize extractor once\nconfig = QwenTextPyTorchConfig(device=\"cuda\")\nextractor = QwenTextExtractor(backend=config)\n\n# Load image paths\nimages = sorted(Path(\"images/\").glob(\"*.png\"))\n\n# Process sequentially\nresults = []\nstart = time.time()\n\nfor idx, image_path in enumerate(images):\n    image = Image.open(image_path)\n    result = extractor.extract(image, output_format=\"markdown\")\n    results.append({\n        \"path\": str(image_path),\n        \"content_length\": result.content_length,\n        \"word_count\": result.word_count,\n    })\n\nelapsed = time.time() - start\nprint(f\"Processed {len(results)} images in {elapsed:.1f}s\")\nprint(f\"Average: {elapsed/len(images):.2f}s per image\")\n</code></pre>"},{"location":"guides/batch-processing/#pattern-2-batched-processing","title":"Pattern 2: Batched Processing","text":"<p>Group images into batches (more efficient for VLLM).</p> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\nfrom pathlib import Path\nfrom PIL import Image\n\n# Use VLLM for better batch efficiency\nconfig = QwenTextVLLMConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    max_tokens=4096,\n)\nextractor = QwenTextExtractor(backend=config)\n\n# Load images\nimages = [Image.open(p) for p in sorted(Path(\"images/\").glob(\"*.png\"))]\n\n# Process in batches\nbatch_size = 4\nresults = []\n\nfor batch_idx in range(0, len(images), batch_size):\n    batch = images[batch_idx:batch_idx + batch_size]\n    print(f\"Processing batch {batch_idx//batch_size + 1}\")\n\n    for image in batch:\n        result = extractor.extract(image, output_format=\"markdown\")\n        results.append(result)\n\nprint(f\"Processed {len(results)} images\")\n</code></pre>"},{"location":"guides/batch-processing/#pattern-3-pdf-with-multiple-pages","title":"Pattern 3: PDF with Multiple Pages","text":"<p>Process all pages of multiple PDFs.</p> <pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom pathlib import Path\n\n# Initialize\nconfig = QwenTextPyTorchConfig(device=\"cuda\")\nextractor = QwenTextExtractor(backend=config)\n\n# Load PDFs\npdf_files = sorted(Path(\"pdfs/\").glob(\"*.pdf\"))\n\n# Process all pages\nall_results = []\n\nfor pdf_path in pdf_files:\n    print(f\"Processing {pdf_path.name}\")\n    doc = Document.from_pdf(pdf_path)\n\n    for page_idx in range(doc.page_count):\n        page_image = doc.get_page(page_idx)\n        result = extractor.extract(page_image, output_format=\"markdown\")\n\n        all_results.append({\n            \"pdf\": pdf_path.name,\n            \"page\": page_idx + 1,\n            \"word_count\": result.word_count,\n            \"content\": result.content,\n        })\n\nprint(f\"Processed {sum(d['page_count'] for d in documents)} pages total\")\n</code></pre>"},{"location":"guides/batch-processing/#pattern-4-parallel-processing-per-document","title":"Pattern 4: Parallel Processing (Per-Document)","text":"<p>Use multiprocessing for CPU-bound preprocessing.</p> <pre><code>from multiprocessing import Pool\nfrom PIL import Image\nfrom pathlib import Path\n\ndef preprocess_image(image_path):\n    \"\"\"Preprocess a single image.\"\"\"\n    image = Image.open(image_path)\n\n    # Resize if needed\n    if image.width &lt; 1024:\n        image = image.resize((image.width * 2, image.height * 2))\n\n    # Convert to RGB\n    if image.mode != \"RGB\":\n        image = image.convert(\"RGB\")\n\n    return image_path, image\n\n# Parallel preprocessing\nimage_paths = sorted(Path(\"images/\").glob(\"*.png\"))\n\nwith Pool(4) as pool:  # 4 processes\n    results = pool.map(preprocess_image, image_paths)\n\nprint(f\"Preprocessed {len(results)} images\")\n\n# Then process with GPU (sequential, since we only have 1 GPU)\nconfig = QwenTextPyTorchConfig(device=\"cuda\")\nextractor = QwenTextExtractor(backend=config)\n\nfor path, image in results:\n    result = extractor.extract(image, output_format=\"markdown\")\n    # Process...\n</code></pre>"},{"location":"guides/batch-processing/#memory-optimization","title":"Memory Optimization","text":""},{"location":"guides/batch-processing/#monitor-gpu-memory","title":"Monitor GPU Memory","text":"<pre><code>import torch\n\nprint(\"GPU Memory:\")\nprint(f\"  Allocated: {torch.cuda.memory_allocated()/1e9:.1f}GB\")\nprint(f\"  Reserved: {torch.cuda.memory_reserved()/1e9:.1f}GB\")\nprint(f\"  Available: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB\")\n\n# Clear cache between batches\ntorch.cuda.empty_cache()\nprint(\"Cache cleared\")\n</code></pre>"},{"location":"guides/batch-processing/#optimize-model-configuration","title":"Optimize Model Configuration","text":"<pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Memory-optimized configuration\nconfig = QwenTextPyTorchConfig(\n    device=\"cuda\",\n    torch_dtype=\"float16\",  # Half precision (less memory)\n    max_new_tokens=2048,  # Smaller context (less memory)\n)\n</code></pre>"},{"location":"guides/batch-processing/#process-in-streaming-fashion","title":"Process in Streaming Fashion","text":"<p>Never keep all results in memory - stream to disk.</p> <pre><code>import json\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom pathlib import Path\nfrom PIL import Image\n\nconfig = QwenTextPyTorchConfig(device=\"cuda\")\nextractor = QwenTextExtractor(backend=config)\n\nimages = sorted(Path(\"images/\").glob(\"*.png\"))\n\n# Stream results to JSON Lines file\noutput_file = \"results.jsonl\"\n\nwith open(output_file, \"w\") as f:\n    for image_path in images:\n        image = Image.open(image_path)\n        result = extractor.extract(image, output_format=\"markdown\")\n\n        # Write immediately (don't accumulate in memory)\n        record = {\n            \"path\": str(image_path),\n            \"content_length\": result.content_length,\n            \"word_count\": result.word_count,\n        }\n        f.write(json.dumps(record) + \"\\n\")\n\n# Results are on disk, not in memory\nprint(f\"Streamed results to {output_file}\")\n\n# Read results later\nresults = []\nwith open(output_file) as f:\n    for line in f:\n        results.append(json.loads(line))\n</code></pre>"},{"location":"guides/batch-processing/#garbage-collection","title":"Garbage Collection","text":"<p>Explicitly free memory between batches.</p> <pre><code>import gc\nimport torch\n\nfor batch_idx, images in enumerate(batches):\n    # Process batch\n    for image in images:\n        result = extractor.extract(image)\n\n    # Free memory\n    del images\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    print(f\"Batch {batch_idx + 1} complete, memory freed\")\n</code></pre>"},{"location":"guides/batch-processing/#progress-tracking","title":"Progress Tracking","text":""},{"location":"guides/batch-processing/#simple-counter","title":"Simple Counter","text":"<pre><code>images = sorted(Path(\"images/\").glob(\"*.png\"))\ntotal = len(images)\n\nfor idx, image_path in enumerate(images, 1):\n    image = Image.open(image_path)\n    result = extractor.extract(image)\n\n    # Print progress\n    print(f\"[{idx}/{total}] {image_path.name}\", end=\" \")\n    print(f\"\u2713 {result.word_count} words\")\n</code></pre> <p>Output: <pre><code>[1/100] document_1.png \u2713 245 words\n[2/100] document_2.png \u2713 312 words\n[3/100] document_3.png \u2713 189 words\n</code></pre></p>"},{"location":"guides/batch-processing/#progress-bar-with-tqdm","title":"Progress Bar with tqdm","text":"<pre><code>from tqdm import tqdm\nfrom pathlib import Path\nfrom PIL import Image\n\nimages = sorted(Path(\"images/\").glob(\"*.png\"))\n\nfor image_path in tqdm(images, desc=\"Processing\"):\n    image = Image.open(image_path)\n    result = extractor.extract(image)\n    # Process...\n</code></pre> <p>Output: <pre><code>Processing: 45%|\u2588\u2588\u2588\u2588\u258c     | 45/100 [5:23&lt;6:32, 8.22s/it]\n</code></pre></p>"},{"location":"guides/batch-processing/#detailed-progress-with-eta","title":"Detailed Progress with ETA","text":"<pre><code>import time\nfrom pathlib import Path\nfrom PIL import Image\n\nimages = sorted(Path(\"images/\").glob(\"*.png\"))\ntotal = len(images)\n\nstart_time = time.time()\n\nfor idx, image_path in enumerate(images, 1):\n    image = Image.open(image_path)\n    result = extractor.extract(image)\n\n    # Calculate metrics\n    elapsed = time.time() - start_time\n    avg_time = elapsed / idx\n    remaining = (total - idx) * avg_time\n    remaining_mins = remaining / 60\n\n    # Print progress\n    percent = 100 * idx / total\n    print(f\"[{idx:3d}/{total}] {percent:5.1f}% \"\n          f\"{image_path.name:20} \"\n          f\"ETA: {remaining_mins:5.1f}min\")\n</code></pre> <p>Output: <pre><code>[  1/100]   1.0% document_1.png         ETA:  8.2min\n[ 10/100]  10.0% document_10.png        ETA:  7.4min\n[ 50/100]  50.0% document_50.png        ETA:  3.7min\n[100/100] 100.0% document_100.png       ETA:  0.0min\n</code></pre></p>"},{"location":"guides/batch-processing/#save-progress-periodically","title":"Save Progress Periodically","text":"<pre><code>import json\nfrom pathlib import Path\nfrom PIL import Image\n\nimages = sorted(Path(\"images/\").glob(\"*.png\"))\ncheckpoint_file = \"progress.json\"\n\n# Load existing progress\nif checkpoint_file.exists():\n    with open(checkpoint_file) as f:\n        completed = set(json.load(f).get(\"completed\", []))\nelse:\n    completed = set()\n\nresults = []\n\nfor image_path in images:\n    if str(image_path) in completed:\n        print(f\"Skipping {image_path.name} (already processed)\")\n        continue\n\n    image = Image.open(image_path)\n    result = extractor.extract(image)\n    results.append({\n        \"path\": str(image_path),\n        \"word_count\": result.word_count,\n    })\n\n    # Save progress periodically\n    completed.add(str(image_path))\n    if len(results) % 10 == 0:\n        with open(checkpoint_file, \"w\") as f:\n            json.dump({\"completed\": list(completed)}, f)\n        print(f\"Saved progress: {len(completed)}/{len(images)} completed\")\n\n# Final save\nwith open(checkpoint_file, \"w\") as f:\n    json.dump({\"completed\": list(completed)}, f)\n</code></pre>"},{"location":"guides/batch-processing/#error-handling","title":"Error Handling","text":""},{"location":"guides/batch-processing/#graceful-degradation","title":"Graceful Degradation","text":"<pre><code>from pathlib import Path\nfrom PIL import Image\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nimages = sorted(Path(\"images/\").glob(\"*.png\"))\nresults = []\nerrors = []\n\nfor image_path in images:\n    try:\n        image = Image.open(image_path)\n        result = extractor.extract(image)\n        results.append({\"path\": str(image_path), \"success\": True})\n\n    except torch.cuda.OutOfMemoryError:\n        logger.error(f\"OOM on {image_path.name}\")\n        errors.append({\"path\": str(image_path), \"error\": \"OOM\"})\n        torch.cuda.empty_cache()\n\n    except Exception as e:\n        logger.error(f\"Error on {image_path.name}: {e}\")\n        errors.append({\"path\": str(image_path), \"error\": str(e)})\n\nprint(f\"\\nResults: {len(results)} succeeded, {len(errors)} failed\")\n\nif errors:\n    print(\"\\nFailed items:\")\n    for error in errors:\n        print(f\"  {error['path']}: {error['error']}\")\n</code></pre>"},{"location":"guides/batch-processing/#retry-on-error","title":"Retry on Error","text":"<pre><code>from tenacity import retry, stop_after_attempt, wait_exponential\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=2, max=10),\n)\ndef extract_with_retry(extractor, image):\n    \"\"\"Extract with automatic retry on failure.\"\"\"\n    try:\n        return extractor.extract(image)\n    except Exception as e:\n        logger.warning(f\"Extraction failed: {e}, retrying...\")\n        raise\n\n# Use in batch processing\nfor image_path in images:\n    try:\n        image = Image.open(image_path)\n        result = extract_with_retry(extractor, image)\n        results.append(result)\n    except Exception as e:\n        logger.error(f\"Failed after retries: {image_path}: {e}\")\n</code></pre>"},{"location":"guides/batch-processing/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"guides/batch-processing/#typical-performance","title":"Typical Performance","text":"<p>Processing a standard page (300 DPI, ~2000x3000px):</p> <p>PyTorch (Single GPU): - Model load: ~2-3 seconds (one-time) - Per-page latency: ~2-3 seconds - Throughput: ~1 page/second - GPU Memory: ~16GB</p> <p>VLLM (Single GPU): - Model load: ~5-8 seconds (one-time) - Per-page latency: ~2-3 seconds - Throughput: ~1-2 pages/second (batched) - GPU Memory: ~20GB</p> <p>Multi-GPU VLLM: - Model load: ~8-12 seconds - Per-page latency: ~1-2 seconds - Throughput: ~2-4 pages/second (batched) - GPU Memory: ~10GB per GPU</p>"},{"location":"guides/batch-processing/#100-document-benchmark","title":"100-Document Benchmark","text":"<p>Processing 100 pages (typical):</p> <pre><code>import time\n\nimages = [...] # 100 images\n\n# PyTorch\nconfig = QwenTextPyTorchConfig(device=\"cuda\")\nextractor = QwenTextExtractor(backend=config)\n\nstart = time.time()\nfor image in images:\n    result = extractor.extract(image)\nelapsed = time.time() - start\n\nprint(f\"PyTorch: {elapsed:.1f}s ({elapsed/100:.2f}s per page)\")\n# Expected: ~3-4 minutes total\n\n# VLLM\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\n\nconfig = QwenTextVLLMConfig(\n    tensor_parallel_size=1,\n    max_tokens=4096,\n)\nextractor = QwenTextExtractor(backend=config)\n\nstart = time.time()\nfor image in images:\n    result = extractor.extract(image)\nelapsed = time.time() - start\n\nprint(f\"VLLM: {elapsed:.1f}s ({elapsed/100:.2f}s per page)\")\n# Expected: ~2-3 minutes total\n</code></pre>"},{"location":"guides/batch-processing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/batch-processing/#out-of-memory-during-batch-processing","title":"Out of Memory During Batch Processing","text":"<p>Problem: CUDA OOM after processing several documents.</p> <p>Solutions: 1. Reduce batch size 2. Process one item at a time 3. Use smaller model 4. Clear cache between items</p> <pre><code># Solution 1: Reduce batch\nfor batch in batches:\n    for image in batch[:2]:  # Process 2 at a time instead of 4\n        result = extractor.extract(image)\n\n# Solution 2: Clear cache\ntorch.cuda.empty_cache()\ngc.collect()\n\n# Solution 3: Use smaller model\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-7B-Instruct\",  # Smaller\n    device=\"cuda\",\n)\n\n# Solution 4: Lower token limit\nconfig = QwenTextPyTorchConfig(\n    device=\"cuda\",\n    max_new_tokens=2048,  # Reduced\n)\n</code></pre>"},{"location":"guides/batch-processing/#very-slow-processing","title":"Very Slow Processing","text":"<p>Problem: Processing taking much longer than expected.</p> <p>Solutions: 1. Check GPU utilization 2. Use VLLM instead of PyTorch 3. Reduce image resolution 4. Verify model is on GPU</p> <pre><code>import torch\nimport subprocess\n\n# Check GPU usage\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=utilization.gpu\", \"--format=csv,noheader\"],\n    capture_output=True, text=True\n)\ngpu_util = result.stdout.strip()\nprint(f\"GPU Utilization: {gpu_util}%\")\n\nif gpu_util &lt; \"50%\":\n    # GPU not being fully used - try VLLM\n    from omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\n    config = QwenTextVLLMConfig()\n    extractor = QwenTextExtractor(backend=config)\n\n# Verify model on GPU\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Current device: {torch.cuda.current_device()}\")\n</code></pre>"},{"location":"guides/batch-processing/#variable-processing-times","title":"Variable Processing Times","text":"<p>Problem: Some documents take much longer to process.</p> <p>Solutions: 1. Check image sizes 2. Set token limit 3. Log processing times</p> <pre><code>import time\n\nfor image_path in images:\n    start = time.time()\n    image = Image.open(image_path)\n\n    # Check size\n    if image.size[0] &gt; 4000:\n        print(f\"Warning: Large image {image.size}, may be slow\")\n\n    result = extractor.extract(image)\n    elapsed = time.time() - start\n\n    # Flag slow items\n    if elapsed &gt; 5:\n        print(f\"Slow: {image_path.name} took {elapsed:.1f}s\")\n\n    # Limit tokens for very large documents\n    if result.word_count &gt; 5000:\n        print(f\"Very long output: {result.word_count} words\")\n</code></pre>"},{"location":"guides/batch-processing/#failed-documents","title":"Failed Documents","text":"<p>Problem: Some documents fail to process.</p> <p>Solutions: 1. Check file integrity 2. Try with different model 3. Check image format</p> <pre><code>from PIL import Image\nimport traceback\n\nfor image_path in images:\n    try:\n        # Verify image\n        image = Image.open(image_path)\n        image.verify()\n\n        # Reload (verify closes the file)\n        image = Image.open(image_path)\n\n        # Try extraction\n        result = extractor.extract(image)\n\n    except Exception as e:\n        print(f\"Failed {image_path.name}:\")\n        traceback.print_exc()\n\n        # Try alternative\n        try:\n            # Fallback to Tesseract (simple OCR)\n            from omnidocs.tasks.ocr_extraction import Tesseract\n            ocr = Tesseract()\n            result = ocr.extract(image)\n            print(\"  Fallback succeeded with Tesseract\")\n        except:\n            print(\"  Fallback also failed\")\n</code></pre> <p>Next Steps: - See Text Extraction Guide for extraction configuration - See Deployment Guide for scaling batches on GPU - See OCR Guide for text with locations</p>"},{"location":"guides/deployment-modal/","title":"Modal Deployment Guide","text":"<p>Deploy OmniDocs inference at scale on Modal serverless GPUs. This guide covers setup, configuration, deployment patterns, and cost optimization.</p>"},{"location":"guides/deployment-modal/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Why Modal for OmniDocs</li> <li>Standard Setup</li> <li>Basic Deployment</li> <li>Multi-GPU Deployment</li> <li>Production Patterns</li> <li>Monitoring &amp; Logging</li> <li>Cost Optimization</li> <li>Troubleshooting</li> </ul>"},{"location":"guides/deployment-modal/#why-modal-for-omnidocs","title":"Why Modal for OmniDocs","text":"<p>Modal is ideal for OmniDocs because:</p> <ol> <li>No Infrastructure Management - Modal handles GPU provisioning, networking, and scaling</li> <li>Pay Per Use - Only pay for actual GPU time, not idle time</li> <li>Automatic Scaling - Handle traffic spikes without manual scaling</li> <li>Pre-built GPU Images - CUDA, drivers, PyTorch pre-installed</li> <li>Distributed Processing - Process multiple documents in parallel</li> <li>Easy CLI - Deploy with single <code>modal run</code> command</li> </ol> <p>Cost Comparison: - Self-managed GPU: $500-2000/month (always on) - Modal (batch processing): $0.30-1.00 per hour of GPU time - For 100 documents (3 hours GPU time): ~$1.00</p>"},{"location":"guides/deployment-modal/#standard-setup","title":"Standard Setup","text":""},{"location":"guides/deployment-modal/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Install Modal CLI: <pre><code>pip install modal\n</code></pre></p> </li> <li> <p>Authenticate: <pre><code>modal token new\n# Or use existing token\nmodal token set\n</code></pre></p> </li> <li> <p>Create Modal workspace (optional): <pre><code>modal workspace create my-workspace\nmodal workspace use my-workspace\n</code></pre></p> </li> </ol>"},{"location":"guides/deployment-modal/#standard-configuration","title":"Standard Configuration","text":"<p>Every OmniDocs Modal script uses this standard setup:</p> <pre><code>import modal\nfrom pathlib import Path\n\n# ============= Configuration =============\n\n# Model settings\nMODEL_NAME = \"Qwen/Qwen3-VL-8B-Instruct\"  # or other models\nGPU_CONFIG = \"A10G:1\"  # GPU type and count\n\n# Cache directories\nMODEL_CACHE_DIR = \"/data/omnidocs_models\"\n\n# CUDA settings (keep consistent across all scripts)\ncuda_version = \"12.4.0\"\nflavor = \"devel\"\noperating_sys = \"ubuntu22.04\"\ntag = f\"{cuda_version}-{flavor}-{operating_sys}\"\n\n# ============= Build Modal Image =============\n\nIMAGE = (\n    modal.Image.from_registry(f\"nvidia/cuda:{tag}\", add_python=\"3.12\")\n    .apt_install(\"libgl1-mesa-glx\", \"libglib2.0-0\")\n    # Base dependencies first (gets cached)\n    .uv_pip_install(\n        \"torch\",\n        \"torchvision\",\n        \"torchaudio\",\n        \"transformers\",\n        \"pillow\",\n        \"numpy\",\n        \"pydantic\",\n        \"huggingface_hub\",\n        \"hf_transfer\",\n        \"accelerate\",\n    )\n    # Model-specific dependencies\n    .uv_pip_install(\"qwen-vl-utils\")\n    .env({\n        \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",\n        \"HF_HOME\": \"/data/.cache\",\n        \"OMNIDOCS_MODEL_CACHE\": MODEL_CACHE_DIR,\n    })\n)\n\n# ============= Modal Setup =============\n\nvolume = modal.Volume.from_name(\"omnidocs\", create_if_missing=True)\nhuggingface_secret = modal.Secret.from_name(\"adithya-hf-wandb\")\n\napp = modal.App(\"omnidocs-deployment\")\n</code></pre> <p>Key Points: - Volume Name: Always use <code>\"omnidocs\"</code> for consistency - Secret Name: Always use <code>\"adithya-hf-wandb\"</code> (contains HF token) - Python Version: <code>3.12</code> for latest compatibility - GPU: <code>A10G:1</code> is standard (adjust as needed) - Timeout: Default 600s (10 min), increase for long documents</p>"},{"location":"guides/deployment-modal/#environment-variables-for-deployment","title":"Environment Variables for Deployment","text":"<p>Set up required secrets:</p> <pre><code># Create HuggingFace secret (one-time)\nmodal secret create adithya-hf-wandb \\\n  --key HF_TOKEN \\\n  --value \"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n\n# Create volume for model caching (one-time)\nmodal volume create omnidocs\n</code></pre>"},{"location":"guides/deployment-modal/#basic-deployment","title":"Basic Deployment","text":""},{"location":"guides/deployment-modal/#example-1-simple-text-extraction","title":"Example 1: Simple Text Extraction","text":"<p>Deploy a basic text extraction function.</p> <pre><code>import modal\nfrom typing import Dict, Any\nfrom pathlib import Path\n\n# ============= Configuration =============\ncuda_version = \"12.4.0\"\nflavor = \"devel\"\noperating_sys = \"ubuntu22.04\"\ntag = f\"{cuda_version}-{flavor}-{operating_sys}\"\n\nIMAGE = (\n    modal.Image.from_registry(f\"nvidia/cuda:{tag}\", add_python=\"3.12\")\n    .apt_install(\"libgl1-mesa-glx\", \"libglib2.0-0\")\n    .uv_pip_install(\n        \"torch\", \"torchvision\", \"transformers\", \"pillow\", \"numpy\",\n        \"pydantic\", \"huggingface_hub\", \"accelerate\",\n    )\n    .uv_pip_install(\"qwen-vl-utils\")\n    .env({\n        \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",\n        \"HF_HOME\": \"/data/.cache\",\n    })\n)\n\nvolume = modal.Volume.from_name(\"omnidocs\", create_if_missing=True)\nsecret = modal.Secret.from_name(\"adithya-hf-wandb\")\n\napp = modal.App(\"omnidocs-text-extraction\")\n\n# ============= Modal Function =============\n\n@app.function(\n    image=IMAGE,\n    gpu=\"A10G:1\",\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=600,\n)\ndef extract_text(image_bytes: bytes) -&gt; Dict[str, Any]:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image_bytes: Image file contents (PNG/JPG)\n\n    Returns:\n        Dict with extracted text and metadata\n    \"\"\"\n    from omnidocs.tasks.text_extraction import QwenTextExtractor\n    from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n    from PIL import Image\n    import io\n\n    # Load image\n    image = Image.open(io.BytesIO(image_bytes))\n\n    # Initialize extractor\n    config = QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n    )\n    extractor = QwenTextExtractor(backend=config)\n\n    # Extract\n    result = extractor.extract(image, output_format=\"markdown\")\n\n    return {\n        \"success\": True,\n        \"content_length\": result.content_length,\n        \"word_count\": result.word_count,\n        \"content\": result.content,\n    }\n\n# ============= Local Entrypoint =============\n\n@app.local_entrypoint()\ndef main():\n    \"\"\"Test the deployment.\"\"\"\n    from pathlib import Path\n\n    # Test with a sample image\n    test_image_path = \"test_document.png\"\n    with open(test_image_path, \"rb\") as f:\n        image_bytes = f.read()\n\n    # Run extraction\n    result = extract_text.remote(image_bytes)\n\n    print(f\"Extraction succeeded: {result['success']}\")\n    print(f\"Content length: {result['content_length']} chars\")\n    print(f\"Word count: {result['word_count']}\")\n    print(f\"\\nContent preview:\")\n    print(result['content'][:500])\n</code></pre> <p>Deploy: <pre><code># Test locally\npython script.py\n\n# Or run on Modal GPU\nmodal run script.py\n</code></pre></p>"},{"location":"guides/deployment-modal/#example-2-batch-processing-with-progress","title":"Example 2: Batch Processing with Progress","text":"<p>Deploy a batch processor with progress tracking.</p> <pre><code>import modal\nfrom typing import List, Dict, Any\nimport time\n\n# ... (image and app setup as above)\n\n@app.function(\n    image=IMAGE,\n    gpu=\"A10G:1\",\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=1800,  # 30 min for large batches\n)\ndef process_batch(image_bytes_list: List[bytes]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Process a batch of images.\n\n    Args:\n        image_bytes_list: List of image byte strings\n\n    Returns:\n        Processing results and statistics\n    \"\"\"\n    from omnidocs.tasks.text_extraction import QwenTextExtractor\n    from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n    from PIL import Image\n    import io\n\n    # Initialize once (expensive)\n    config = QwenTextPyTorchConfig(device=\"cuda\")\n    extractor = QwenTextExtractor(backend=config)\n\n    results = []\n    start_time = time.time()\n\n    for idx, image_bytes in enumerate(image_bytes_list, 1):\n        try:\n            image = Image.open(io.BytesIO(image_bytes))\n            result = extractor.extract(image, output_format=\"markdown\")\n\n            results.append({\n                \"index\": idx,\n                \"success\": True,\n                \"word_count\": result.word_count,\n                \"content_length\": result.content_length,\n            })\n\n            # Progress\n            elapsed = time.time() - start_time\n            avg_time = elapsed / idx\n            remaining = (len(image_bytes_list) - idx) * avg_time\n            print(f\"[{idx}/{len(image_bytes_list)}] {remaining/60:.1f}min remaining\")\n\n        except Exception as e:\n            results.append({\n                \"index\": idx,\n                \"success\": False,\n                \"error\": str(e),\n            })\n\n    total_time = time.time() - start_time\n\n    return {\n        \"total_time\": total_time,\n        \"num_images\": len(image_bytes_list),\n        \"succeeded\": sum(1 for r in results if r[\"success\"]),\n        \"failed\": sum(1 for r in results if not r[\"success\"]),\n        \"results\": results,\n    }\n\n@app.local_entrypoint()\ndef main():\n    \"\"\"Test batch processing.\"\"\"\n    from pathlib import Path\n\n    # Load images\n    image_dir = Path(\"test_images/\")\n    image_paths = sorted(image_dir.glob(\"*.png\"))[:5]  # Test with 5\n\n    image_bytes_list = [\n        open(p, \"rb\").read()\n        for p in image_paths\n    ]\n\n    # Process batch\n    result = process_batch.remote(image_bytes_list)\n\n    print(f\"\\nResults:\")\n    print(f\"  Succeeded: {result['succeeded']}/{result['num_images']}\")\n    print(f\"  Failed: {result['failed']}/{result['num_images']}\")\n    print(f\"  Total time: {result['total_time']:.1f}s\")\n    print(f\"  Average: {result['total_time']/result['num_images']:.2f}s per image\")\n</code></pre>"},{"location":"guides/deployment-modal/#multi-gpu-deployment","title":"Multi-GPU Deployment","text":""},{"location":"guides/deployment-modal/#example-1-vllm-with-tensor-parallelism","title":"Example 1: VLLM with Tensor Parallelism","text":"<p>Use VLLM to distribute inference across multiple GPUs.</p> <pre><code>import modal\n\n# Use larger GPU for tensor parallelism\nGPU_CONFIG = \"A10G:2\"  # 2 GPUs\n\nIMAGE = (\n    modal.Image.from_registry(f\"nvidia/cuda:12.4.0-devel-ubuntu22.04\", add_python=\"3.12\")\n    .apt_install(\"libopenmpi-dev\", \"libnuma-dev\", \"libgl1-mesa-glx\", \"libglib2.0-0\")\n    .uv_pip_install(\n        \"torch\", \"transformers\", \"pillow\", \"pydantic\",\n        \"huggingface_hub\", \"accelerate\",\n    )\n    # VLLM for multi-GPU inference\n    .uv_pip_install(\"vllm\")\n    .run_commands(\"uv pip install flash-attn --no-build-isolation --system\")\n    .env({\n        \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",\n        \"HF_HOME\": \"/data/.cache\",\n    })\n)\n\nvolume = modal.Volume.from_name(\"omnidocs\", create_if_missing=True)\nsecret = modal.Secret.from_name(\"adithya-hf-wandb\")\n\napp = modal.App(\"omnidocs-vllm-2gpu\")\n\n@app.function(\n    image=IMAGE,\n    gpu=GPU_CONFIG,  # 2 GPUs\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=600,\n)\ndef extract_vllm_2gpu(image_bytes: bytes) -&gt; Dict[str, Any]:\n    \"\"\"\n    Extract using VLLM with 2-GPU tensor parallelism.\n    \"\"\"\n    from omnidocs.tasks.text_extraction import QwenTextExtractor\n    from omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\n    from PIL import Image\n    import io\n\n    image = Image.open(io.BytesIO(image_bytes))\n\n    # Configure for 2 GPUs\n    config = QwenTextVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,  # Distribute across 2 GPUs\n        gpu_memory_utilization=0.9,\n        max_tokens=4096,\n    )\n    extractor = QwenTextExtractor(backend=config)\n\n    result = extractor.extract(image, output_format=\"markdown\")\n\n    return {\n        \"success\": True,\n        \"word_count\": result.word_count,\n        \"model\": \"VLLM (2-GPU tensor parallel)\",\n    }\n</code></pre>"},{"location":"guides/deployment-modal/#example-2-multi-function-with-load-balancing","title":"Example 2: Multi-Function with Load Balancing","text":"<p>Deploy multiple functions to handle parallel requests.</p> <pre><code>import modal\nfrom typing import Dict, Any\n\n# ... (image and app setup)\n\n# Create 3 independent extract functions\nfor func_idx in range(3):\n    @app.function(\n        image=IMAGE,\n        gpu=\"A10G:1\",\n        volumes={\"/data\": volume},\n        secrets=[secret],\n        timeout=600,\n        name=f\"extract_{func_idx}\",\n    )\n    def extract_text(image_bytes: bytes, _func_idx=func_idx) -&gt; Dict[str, Any]:\n        # Same implementation as before\n        # Modal will create 3 independent functions\n        from omnidocs.tasks.text_extraction import QwenTextExtractor\n        from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n        from PIL import Image\n        import io\n\n        image = Image.open(io.BytesIO(image_bytes))\n        config = QwenTextPyTorchConfig(device=\"cuda\")\n        extractor = QwenTextExtractor(backend=config)\n        result = extractor.extract(image, output_format=\"markdown\")\n\n        return {\n            \"success\": True,\n            \"word_count\": result.word_count,\n            \"worker\": _func_idx,\n        }\n\n@app.local_entrypoint()\ndef main():\n    \"\"\"Process 3 images in parallel.\"\"\"\n    import concurrent.futures\n\n    # Get function references\n    extract_0 = modal.Function.lookup(\"omnidocs-multiworker\", \"extract_0\")\n    extract_1 = modal.Function.lookup(\"omnidocs-multiworker\", \"extract_1\")\n    extract_2 = modal.Function.lookup(\"omnidocs-multiworker\", \"extract_2\")\n\n    functions = [extract_0, extract_1, extract_2]\n\n    # Prepare 3 test images\n    image_bytes_list = [\n        open(f\"test_{i}.png\", \"rb\").read()\n        for i in range(3)\n    ]\n\n    # Process in parallel\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        futures = [\n            executor.submit(func.remote, img_bytes)\n            for func, img_bytes in zip(functions, image_bytes_list)\n        ]\n        results = [f.result() for f in futures]\n\n    print(f\"Processed {len(results)} images in parallel\")\n    for result in results:\n        print(f\"  Worker {result['worker']}: {result['word_count']} words\")\n</code></pre>"},{"location":"guides/deployment-modal/#production-patterns","title":"Production Patterns","text":""},{"location":"guides/deployment-modal/#scheduled-processing","title":"Scheduled Processing","text":"<p>Run batch processing on a schedule.</p> <pre><code>import modal\nfrom datetime import datetime\n\n# ... (image and app setup)\n\n@app.function(\n    image=IMAGE,\n    gpu=\"A10G:1\",\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=3600,\n    schedule=modal.Period(days=1),  # Daily at midnight UTC\n)\ndef daily_batch_processing():\n    \"\"\"Process accumulated documents daily.\"\"\"\n    from pathlib import Path\n    from omnidocs.tasks.text_extraction import QwenTextExtractor\n    from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n    from PIL import Image\n    import json\n\n    # Find new documents\n    inbox_dir = Path(\"/data/inbox\")\n    processed_dir = Path(\"/data/processed\")\n    results_dir = Path(\"/data/results\")\n\n    results_dir.mkdir(exist_ok=True)\n\n    # Initialize extractor\n    config = QwenTextPyTorchConfig(device=\"cuda\")\n    extractor = QwenTextExtractor(backend=config)\n\n    # Process documents\n    for image_path in inbox_dir.glob(\"*.png\"):\n        image = Image.open(image_path)\n        result = extractor.extract(image, output_format=\"markdown\")\n\n        # Save result\n        result_file = results_dir / f\"{image_path.stem}.json\"\n        with open(result_file, \"w\") as f:\n            json.dump({\n                \"filename\": image_path.name,\n                \"word_count\": result.word_count,\n                \"content_length\": result.content_length,\n                \"processed_at\": datetime.now().isoformat(),\n            }, f)\n\n        # Move to processed\n        image_path.rename(processed_dir / image_path.name)\n\n    print(f\"Daily processing complete\")\n</code></pre>"},{"location":"guides/deployment-modal/#webhook-handler","title":"Webhook Handler","text":"<p>Accept requests from an external service.</p> <pre><code>import modal\nfrom typing import Dict, Any\nfrom fastapi import FastAPI\n\n# ... (image setup)\n\nweb_app = FastAPI()\n\n# Create Modal web endpoint\n@app.function(\n    image=IMAGE,\n    gpu=\"A10G:1\",\n    volumes={\"/data\": volume},\n    secrets=[secret],\n)\n@modal.web_endpoint(method=\"POST\")\ndef extract_from_url(request: Dict[str, str]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Accept image URL and extract text.\n\n    POST /extract_from_url\n    {\"image_url\": \"https://example.com/doc.png\"}\n    \"\"\"\n    import requests\n    from omnidocs.tasks.text_extraction import QwenTextExtractor\n    from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n    from PIL import Image\n    import io\n\n    # Download image\n    response = requests.get(request[\"image_url\"], timeout=30)\n    image = Image.open(io.BytesIO(response.content))\n\n    # Extract\n    config = QwenTextPyTorchConfig(device=\"cuda\")\n    extractor = QwenTextExtractor(backend=config)\n    result = extractor.extract(image, output_format=\"markdown\")\n\n    return {\n        \"success\": True,\n        \"word_count\": result.word_count,\n        \"content\": result.content,\n    }\n</code></pre> <p>Deploy and test: <pre><code># Deploy\nmodal deploy script.py\n\n# Get URL\nmodal app list\n\n# Test\ncurl -X POST https://your-workspace.modal.run/extract_from_url \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"image_url\":\"https://example.com/doc.png\"}'\n</code></pre></p>"},{"location":"guides/deployment-modal/#monitoring-logging","title":"Monitoring &amp; Logging","text":""},{"location":"guides/deployment-modal/#log-extraction-progress","title":"Log Extraction Progress","text":"<pre><code>import modal\nimport logging\nfrom typing import List\n\n# ... (image and app setup)\n\n@app.function(\n    image=IMAGE,\n    gpu=\"A10G:1\",\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=1800,\n)\ndef process_with_logging(image_bytes_list: List[bytes]) -&gt; Dict[str, Any]:\n    \"\"\"Process with detailed logging.\"\"\"\n    import logging\n\n    # Configure logging\n    logging.basicConfig(\n        level=logging.INFO,\n        format='[%(asctime)s] %(levelname)s: %(message)s',\n    )\n    logger = logging.getLogger(__name__)\n\n    from omnidocs.tasks.text_extraction import QwenTextExtractor\n    from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n    from PIL import Image\n    import io\n    import time\n    import torch\n\n    logger.info(f\"Starting batch processing: {len(image_bytes_list)} images\")\n    logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\n    config = QwenTextPyTorchConfig(device=\"cuda\")\n    extractor = QwenTextExtractor(backend=config)\n\n    results = []\n    start_time = time.time()\n\n    for idx, image_bytes in enumerate(image_bytes_list, 1):\n        try:\n            iter_start = time.time()\n\n            image = Image.open(io.BytesIO(image_bytes))\n            result = extractor.extract(image, output_format=\"markdown\")\n\n            iter_time = time.time() - iter_start\n\n            logger.info(f\"[{idx}/{len(image_bytes_list)}] \"\n                       f\"Processed in {iter_time:.2f}s, \"\n                       f\"{result.word_count} words\")\n\n            results.append({\"success\": True})\n\n        except Exception as e:\n            logger.error(f\"[{idx}] Failed: {e}\")\n            results.append({\"success\": False, \"error\": str(e)})\n\n    total_time = time.time() - start_time\n    logger.info(f\"Batch complete: {total_time:.1f}s total, \"\n               f\"{total_time/len(image_bytes_list):.2f}s per image\")\n\n    return {\"results\": results}\n</code></pre>"},{"location":"guides/deployment-modal/#monitor-gpu-memory","title":"Monitor GPU Memory","text":"<pre><code>@app.function(\n    image=IMAGE,\n    gpu=\"A10G:1\",\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=600,\n)\ndef extract_with_memory_monitoring(image_bytes: bytes):\n    \"\"\"Extract with GPU memory monitoring.\"\"\"\n    import torch\n\n    def log_memory(label):\n        allocated = torch.cuda.memory_allocated() / 1e9\n        reserved = torch.cuda.memory_reserved() / 1e9\n        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n        print(f\"{label}: allocated={allocated:.1f}GB, \"\n              f\"reserved={reserved:.1f}GB, total={total:.1f}GB\")\n\n    log_memory(\"Initial\")\n\n    # Load model\n    from omnidocs.tasks.text_extraction import QwenTextExtractor\n    from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n    config = QwenTextPyTorchConfig(device=\"cuda\")\n    extractor = QwenTextExtractor(backend=config)\n\n    log_memory(\"After model load\")\n\n    # Extract\n    from PIL import Image\n    import io\n\n    image = Image.open(io.BytesIO(image_bytes))\n    result = extractor.extract(image, output_format=\"markdown\")\n\n    log_memory(\"After extraction\")\n\n    return {\"success\": True}\n</code></pre>"},{"location":"guides/deployment-modal/#cost-optimization","title":"Cost Optimization","text":""},{"location":"guides/deployment-modal/#gpu-selection","title":"GPU Selection","text":"GPU $/hour Ideal For A10G $0.35 General purpose, fast A40 $1.10 Large models, high VRAM T4 $0.15 Budget processing L40S $1.25 High-end graphics <p>Recommendation: A10G (sweet spot of price/performance)</p>"},{"location":"guides/deployment-modal/#cost-calculation","title":"Cost Calculation","text":"<pre><code>Cost per document = (Model load time + Processing time) \u00d7 $/hour GPU\n\nExample (A10G @ $0.35/hour):\n- Model load: 2 seconds (one-time amortized across batch)\n- Per-image: 3 seconds\n- Batch of 100: (2 + 100*3) / 3600 hours \u00d7 $0.35/hour \u2248 $0.03 per image\n- 100 documents: ~$3 total GPU cost\n</code></pre>"},{"location":"guides/deployment-modal/#batch-size-optimization","title":"Batch Size Optimization","text":"<p>Larger batches = lower per-item cost (amortize model load).</p> <pre><code># Calculation\nmodel_load_time = 2  # seconds\nper_image_time = 3   # seconds\ngpu_cost_per_hour = 0.35  # dollars\nbatch_sizes = [1, 5, 10, 50, 100]\n\nprint(\"Batch Size | Total Time | Cost per Image\")\nprint(\"-\" * 40)\n\nfor batch_size in batch_sizes:\n    total_time = (model_load_time + batch_size * per_image_time) / 3600\n    cost_per_image = total_time * gpu_cost_per_hour / batch_size\n    print(f\"{batch_size:10} | {total_time*3600:9.0f}s | ${cost_per_image:.4f}\")\n\n# Output:\n# Batch Size | Total Time | Cost per Image\n# ----------------------------------------\n#          1 |          5s | $0.0005\n#          5 |         17s | $0.0012\n#         10 |         32s | $0.0031\n#         50 |        152s | $0.0149\n#        100 |        302s | $0.0293\n</code></pre> <p>Takeaway: Batch size of ~50 is optimal (amortizes load, avoids timeout issues)</p>"},{"location":"guides/deployment-modal/#spot-instances","title":"Spot Instances","text":"<p>Use cheaper Spot instances for non-critical batches.</p> <pre><code>import modal\n\n@app.function(\n    image=IMAGE,\n    gpu=modal.gpu.A10G(count=1, spot=True),  # Use Spot instance\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=600,\n)\ndef extract_spot(image_bytes: bytes):\n    # Same as regular extraction\n    pass\n\n# Cost: ~60% cheaper than on-demand\n</code></pre>"},{"location":"guides/deployment-modal/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/deployment-modal/#model-download-stuck","title":"Model Download Stuck","text":"<p>Problem: Model stuck downloading.</p> <p>Solution: Set HF token and increase timeout.</p> <pre><code># Ensure HF token is set\nmodal secret create adithya-hf-wandb \\\n  --key HF_TOKEN \\\n  --value \"your-token\"\n\n# Increase timeout for initial runs\n@app.function(\n    ...,\n    timeout=1800,  # 30 minutes for first run\n)\n</code></pre>"},{"location":"guides/deployment-modal/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<p>Problem: CUDA OOM during extraction.</p> <p>Solution: Use larger GPU or reduce model size.</p> <pre><code># Option 1: Use larger GPU\ngpu=modal.gpu.A40(count=1)  # 48GB VRAM vs 24GB on A10G\n\n# Option 2: Use smaller model\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-7B-Instruct\",  # 7B instead of 8B\n)\n\n# Option 3: Use VLLM with tensor parallelism\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\nconfig = QwenTextVLLMConfig(\n    tensor_parallel_size=2,  # Split across 2 GPUs\n)\ngpu=modal.gpu.A10G(count=2)\n</code></pre>"},{"location":"guides/deployment-modal/#timeout-errors","title":"Timeout Errors","text":"<p>Problem: Function times out.</p> <p>Solution: Increase timeout or reduce batch size.</p> <pre><code>@app.function(\n    ...,\n    timeout=1800,  # Increase to 30 minutes\n)\n\n# Or reduce batch size\nbatch_size = 10  # Process 10 at a time instead of 100\n</code></pre>"},{"location":"guides/deployment-modal/#network-errors","title":"Network Errors","text":"<p>Problem: Intermittent network failures.</p> <p>Solution: Add retry logic.</p> <pre><code>from tenacity import retry, stop_after_attempt, wait_exponential\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=2, max=10),\n)\ndef download_with_retry(image_url: str):\n    import requests\n    response = requests.get(image_url, timeout=30)\n    response.raise_for_status()\n    return response.content\n</code></pre> <p>Next Steps: - See Batch Processing Guide for local batch patterns - See Text Extraction Guide for model configuration - Modal docs: https://modal.com/docs</p>"},{"location":"guides/layout-analysis/","title":"Layout Analysis Guide","text":"<p>Detect document structure and element boundaries using layout detection models. This guide explains how to extract layout information, work with labels, and build advanced document processing pipelines.</p>"},{"location":"guides/layout-analysis/#table-of-contents","title":"Table of Contents","text":"<ul> <li>What is Layout Detection</li> <li>Available Models</li> <li>Fixed vs Custom Labels</li> <li>Basic Usage</li> <li>Filtering and Analyzing Results</li> <li>Visualization</li> <li>Advanced: Custom Labels</li> <li>Troubleshooting</li> </ul>"},{"location":"guides/layout-analysis/#what-is-layout-detection","title":"What is Layout Detection","text":"<p>Layout detection identifies document regions (elements) and classifies them by type. Unlike OCR (which extracts text) or text extraction (which formats content), layout detection provides structural information.</p> <p>Output: List of bounding boxes with labels and confidence scores.</p> <p>Use Cases: - Document structure analysis - Segmentation for downstream processing - Building multi-stage pipelines (layout \u2192 text \u2192 output) - Understanding document semantics - Filtering unwanted elements (headers, footers, artifacts)</p>"},{"location":"guides/layout-analysis/#available-models","title":"Available Models","text":""},{"location":"guides/layout-analysis/#1-doclayoutyolo-fast-fixed-labels","title":"1. DocLayoutYOLO (Fast, fixed labels)","text":"<p>YOLO-based detector optimized for speed and accuracy on document layouts.</p> <p>Strengths: - Extremely fast (~0.3-0.5 sec per page) - High accuracy on standard document elements - Single-backend (PyTorch only) - Low memory requirements</p> <p>Weaknesses: - Fixed labels only (no custom categories) - Less accurate on irregular documents - May struggle with non-English text</p> <p>Fixed Labels: - <code>title</code> - Document/section titles - <code>text</code> - Body paragraphs - <code>list</code> - Bullet or numbered lists - <code>table</code> - Data tables - <code>figure</code> - Images, diagrams, charts - <code>caption</code> - Figure/table captions - <code>formula</code> - Mathematical equations - <code>footnote</code> - Footnotes - <code>page_header</code> - Page headers - <code>page_footer</code> - Page footers - <code>unknown</code> - Unclassifiable elements</p>"},{"location":"guides/layout-analysis/#2-rt-detr-accuracy-focused-fixed-labels","title":"2. RT-DETR (Accuracy-focused, fixed labels)","text":"<p>High-accuracy detector with stronger backbone, but slower than YOLO.</p> <p>Strengths: - Higher accuracy than YOLO - Good on challenging document types - Handles small elements better</p> <p>Weaknesses: - Slower (~1-2 sec per page) - Higher memory requirements - Fixed labels only</p> <p>Same fixed labels as DocLayoutYOLO.</p>"},{"location":"guides/layout-analysis/#3-qwenlayoutdetector-flexible-custom-labels","title":"3. QwenLayoutDetector (Flexible, custom labels)","text":"<p>Vision-language model supporting custom layout labels.</p> <p>Strengths: - Flexible custom labels (define your own) - Strong on diverse document types - Multi-backend support (PyTorch, VLLM, MLX, API) - Better on irregular layouts</p> <p>Weaknesses: - Slower than YOLO (~2-3 sec per page) - Higher VRAM requirements - Requires more GPU memory</p> <p>Supports: - Standard LayoutLabel enum - Custom labels (unlimited) - Mixed standard + custom labels</p>"},{"location":"guides/layout-analysis/#fixed-vs-custom-labels","title":"Fixed vs Custom Labels","text":""},{"location":"guides/layout-analysis/#fixed-labels-doclayoutyolo-rt-detr","title":"Fixed Labels (DocLayoutYOLO, RT-DETR)","text":"<p>Predefined categories that the model recognizes during training.</p> <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\n# Fixed labels are built-in and cannot be changed\nconfig = DocLayoutYOLOConfig(device=\"cuda\")\ndetector = DocLayoutYOLO(config=config)\n\nresult = detector.extract(image)\n\n# Result contains elements with fixed labels\nfor element in result.elements:\n    print(f\"{element.label}: {element.bbox}\")\n    # Labels will always be from: title, text, list, table, figure, caption, formula, etc.\n</code></pre> <p>Available Fixed Labels:</p> Label Use Case Typical Content <code>title</code> Document/section heading \"Chapter 1\", \"Introduction\" <code>text</code> Body paragraphs Main content text <code>list</code> Bullet or numbered lists \"- Item 1\", \"1. Point A\" <code>table</code> Data tables and grids Tabular data <code>figure</code> Images, diagrams, charts Photos, graphics, plots <code>caption</code> Figure/table descriptions \"Figure 1: Title\" <code>formula</code> Mathematical equations LaTeX, equations <code>footnote</code> Bottom-of-page notes Footnotes, endnotes <code>page_header</code> Page header regions Running headers <code>page_footer</code> Page footer regions Running footers <code>unknown</code> Unclassifiable elements Artifacts, noise"},{"location":"guides/layout-analysis/#custom-labels-qwenlayoutdetector-only","title":"Custom Labels (QwenLayoutDetector only)","text":"<p>Define your own label categories for domain-specific documents.</p> <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\nfrom omnidocs.tasks.layout_extraction import CustomLabel\n\n# Define custom labels\ncustom_labels = [\n    CustomLabel(name=\"code_block\", description=\"Code snippets\"),\n    CustomLabel(name=\"sidebar\", description=\"Sidebar content\"),\n    CustomLabel(name=\"pull_quote\", description=\"Quoted text\"),\n]\n\nconfig = QwenLayoutPyTorchConfig(device=\"cuda\")\ndetector = QwenLayoutDetector(backend=config)\n\nresult = detector.extract(image, custom_labels=custom_labels)\n\n# Result contains elements with your custom labels\nfor element in result.elements:\n    print(f\"{element.label}: {element.bbox}\")\n    # Labels will be: code_block, sidebar, pull_quote, or standard labels\n</code></pre>"},{"location":"guides/layout-analysis/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/layout-analysis/#example-1-fast-layout-detection-doclayoutyolo","title":"Example 1: Fast Layout Detection (DocLayoutYOLO)","text":"<p>Detect layout using the fast YOLO-based model.</p> <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs import Document\nfrom PIL import Image\n\n# Load image\nimage = Image.open(\"document_page.png\")\n\n# Initialize detector (fastest option)\nconfig = DocLayoutYOLOConfig(\n    device=\"cuda\",  # or \"cpu\"\n    img_size=1024,\n    confidence=0.25,  # Detection confidence threshold\n)\ndetector = DocLayoutYOLO(config=config)\n\n# Extract layout\nresult = detector.extract(image)\n\nprint(f\"Detected {len(result.elements)} layout elements\")\n\n# Display results\nfor element in result.elements:\n    print(f\"  {element.label:12} @ {element.bbox} (confidence: {element.confidence:.2f})\")\n\n# Count by label\nfrom collections import Counter\nlabel_counts = Counter(e.label for e in result.elements)\nprint(f\"\\nSummary: {dict(label_counts)}\")\n</code></pre> <p>Output Example: <pre><code>Detected 12 layout elements\n  title         @ [50, 20, 500, 60] (confidence: 0.98)\n  text          @ [50, 80, 950, 300] (confidence: 0.95)\n  figure        @ [50, 320, 450, 650] (confidence: 0.92)\n  caption       @ [50, 660, 450, 700] (confidence: 0.88)\n  text          @ [480, 320, 950, 600] (confidence: 0.94)\n  table         @ [50, 720, 950, 1000] (confidence: 0.91)\n\nSummary: {'title': 1, 'text': 2, 'figure': 1, 'caption': 1, 'table': 1, ...}\n</code></pre></p>"},{"location":"guides/layout-analysis/#example-2-extract-from-pdf-with-multiple-pages","title":"Example 2: Extract from PDF with Multiple Pages","text":"<p>Process all pages of a PDF document.</p> <pre><code>from omnidocs import Document\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom pathlib import Path\n\n# Load PDF\ndoc = Document.from_pdf(\"document.pdf\")\nprint(f\"Loaded {doc.page_count} pages\")\n\n# Initialize detector\nconfig = DocLayoutYOLOConfig(device=\"cuda\")\ndetector = DocLayoutYOLO(config=config)\n\n# Process all pages\npage_layouts = []\nfor page_idx in range(doc.page_count):\n    page_image = doc.get_page(page_idx)\n    result = detector.extract(page_image)\n    page_layouts.append({\n        \"page\": page_idx + 1,\n        \"num_elements\": len(result.elements),\n        \"elements\": result.elements,\n    })\n    print(f\"Page {page_idx + 1}: {len(result.elements)} elements\")\n\n# Summary statistics\ntotal_elements = sum(p[\"num_elements\"] for p in page_layouts)\nprint(f\"\\nTotal: {total_elements} elements across {doc.page_count} pages\")\n</code></pre>"},{"location":"guides/layout-analysis/#example-3-high-accuracy-detection-rt-detr","title":"Example 3: High-Accuracy Detection (RT-DETR)","text":"<p>Use the more accurate RT-DETR model for challenging documents.</p> <pre><code>from omnidocs.tasks.layout_extraction import RTDETRLayoutDetector, RTDETRLayoutConfig\nfrom PIL import Image\n\nimage = Image.open(\"complex_document.png\")\n\n# Use RT-DETR for better accuracy (slower)\nconfig = RTDETRLayoutConfig(\n    device=\"cuda\",\n    confidence=0.3,  # Lower confidence threshold for more detections\n)\ndetector = RTDETRLayoutDetector(config=config)\n\nresult = detector.extract(image)\n\n# Filter by confidence\nhigh_confidence = [e for e in result.elements if e.confidence &gt;= 0.9]\nprint(f\"High confidence detections: {len(high_confidence)}/{len(result.elements)}\")\n\nfor element in high_confidence:\n    print(f\"  {element.label:12} {element.bbox} (conf: {element.confidence:.3f})\")\n</code></pre>"},{"location":"guides/layout-analysis/#example-4-custom-labels-with-qwen","title":"Example 4: Custom Labels with Qwen","text":"<p>Use Qwen to detect custom document elements.</p> <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\nfrom omnidocs.tasks.layout_extraction import CustomLabel\nfrom PIL import Image\n\nimage = Image.open(\"technical_document.png\")\n\n# Define domain-specific labels\ncustom_labels = [\n    CustomLabel(name=\"code_block\", description=\"Syntax-highlighted code\"),\n    CustomLabel(name=\"api_doc\", description=\"API documentation/reference\"),\n    CustomLabel(name=\"note_box\", description=\"Important note or warning\"),\n    CustomLabel(name=\"example\", description=\"Code example or usage\"),\n]\n\n# Initialize with PyTorch backend\nconfig = QwenLayoutPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cuda\",\n)\ndetector = QwenLayoutDetector(backend=config)\n\n# Extract with custom labels\nresult = detector.extract(image, custom_labels=custom_labels)\n\n# Analyze custom labels\ncustom_elements = [e for e in result.elements if e.label in [l.name for l in custom_labels]]\nprint(f\"Found {len(custom_elements)} domain-specific elements\")\n\nfor element in custom_elements:\n    print(f\"  {element.label:12} {element.bbox}\")\n</code></pre>"},{"location":"guides/layout-analysis/#filtering-and-analyzing-results","title":"Filtering and Analyzing Results","text":""},{"location":"guides/layout-analysis/#filter-by-label","title":"Filter by Label","text":"<p>Extract only specific types of elements.</p> <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\ndetector = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = detector.extract(image)\n\n# Get only text elements\ntext_elements = [e for e in result.elements if e.label == \"text\"]\nprint(f\"Text blocks: {len(text_elements)}\")\n\n# Get tables and figures\nvisual_elements = [e for e in result.elements if e.label in [\"table\", \"figure\"]]\nprint(f\"Visual elements: {len(visual_elements)}\")\n\n# Exclude page artifacts (headers, footers)\ncontent_elements = [\n    e for e in result.elements\n    if e.label not in [\"page_header\", \"page_footer\", \"unknown\"]\n]\nprint(f\"Main content elements: {len(content_elements)}\")\n</code></pre>"},{"location":"guides/layout-analysis/#filter-by-confidence","title":"Filter by Confidence","text":"<p>Exclude low-confidence detections.</p> <pre><code># Keep only high-confidence detections\nmin_confidence = 0.8\nfiltered = [e for e in result.elements if e.confidence &gt;= min_confidence]\n\nprint(f\"Original: {len(result.elements)} elements\")\nprint(f\"Filtered (confidence &gt;= {min_confidence}): {len(filtered)} elements\")\n\n# Analyze confidence distribution\nconfidences = [e.confidence for e in result.elements]\nprint(f\"Confidence range: {min(confidences):.2f} - {max(confidences):.2f}\")\nprint(f\"Average: {sum(confidences)/len(confidences):.2f}\")\n</code></pre>"},{"location":"guides/layout-analysis/#filter-by-bounding-box","title":"Filter by Bounding Box","text":"<p>Find elements in specific image regions.</p> <pre><code># Find elements in top half of page\nimage_height = image.height\ntop_half_elements = [\n    e for e in result.elements\n    if e.bbox[1] &lt; image_height / 2  # y1 &lt; height/2\n]\nprint(f\"Elements in top half: {len(top_half_elements)}\")\n\n# Find elements in specific region (e.g., sidebar)\ndef in_region(bbox, region):\n    \"\"\"Check if element overlaps with region.\"\"\"\n    x1, y1, x2, y2 = bbox\n    rx1, ry1, rx2, ry2 = region\n    # Check overlap\n    return not (x2 &lt; rx1 or x1 &gt; rx2 or y2 &lt; ry1 or y1 &gt; ry2)\n\nsidebar_region = (0, 0, 200, 1024)  # Left 200px\nsidebar_elements = [e for e in result.elements if in_region(e.bbox, sidebar_region)]\nprint(f\"Elements in sidebar region: {len(sidebar_elements)}\")\n</code></pre>"},{"location":"guides/layout-analysis/#analyze-element-sizes","title":"Analyze Element Sizes","text":"<p>Check element dimensions for quality control.</p> <pre><code># Calculate element sizes\nelement_sizes = []\nfor element in result.elements:\n    bbox = element.bbox  # [x1, y1, x2, y2]\n    width = bbox[2] - bbox[0]\n    height = bbox[3] - bbox[1]\n    area = width * height\n    element_sizes.append({\n        \"label\": element.label,\n        \"width\": width,\n        \"height\": height,\n        \"area\": area,\n    })\n\n# Find largest elements\nlargest = sorted(element_sizes, key=lambda e: e[\"area\"], reverse=True)[:5]\nprint(\"Largest elements:\")\nfor elem in largest:\n    print(f\"  {elem['label']}: {elem['width']:.0f}x{elem['height']:.0f} ({elem['area']:.0f} px\u00b2)\")\n\n# Find anomalies (very small or very large)\navg_area = sum(e[\"area\"] for e in element_sizes) / len(element_sizes)\nanomalies = [e for e in element_sizes if e[\"area\"] &lt; avg_area * 0.1 or e[\"area\"] &gt; avg_area * 10]\nprint(f\"Anomalous elements: {len(anomalies)}\")\n</code></pre>"},{"location":"guides/layout-analysis/#visualization","title":"Visualization","text":""},{"location":"guides/layout-analysis/#visualize-detections","title":"Visualize Detections","text":"<p>Draw bounding boxes on the image.</p> <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom PIL import Image, ImageDraw\nimport random\n\nimage = Image.open(\"document.png\")\ndetector = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = detector.extract(image)\n\n# Create visualization\nvis_image = image.copy()\ndraw = ImageDraw.Draw(vis_image)\n\n# Color map for labels\ncolors = {\n    \"title\": \"#FF0000\",\n    \"text\": \"#00FF00\",\n    \"table\": \"#0000FF\",\n    \"figure\": \"#FFFF00\",\n    \"list\": \"#FF00FF\",\n    \"caption\": \"#00FFFF\",\n}\n\n# Draw bounding boxes\nfor element in result.elements:\n    bbox = element.bbox\n    label = element.label\n    color = colors.get(label, \"#FFFFFF\")\n\n    # Draw rectangle\n    draw.rectangle(bbox, outline=color, width=2)\n\n    # Draw label\n    draw.text((bbox[0], bbox[1]-10), label, fill=color)\n\n# Save visualization\nvis_image.save(\"layout_visualization.png\")\nprint(\"Saved visualization to layout_visualization.png\")\n</code></pre>"},{"location":"guides/layout-analysis/#use-built-in-visualization-if-available","title":"Use Built-in Visualization (if available)","text":"<p>Some models provide built-in visualization.</p> <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\ndetector = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = detector.extract(image)\n\n# If the model supports visualization\nif hasattr(result, 'visualize'):\n    vis_image = result.visualize(image)\n    vis_image.save(\"layout_visualization.png\")\nelse:\n    print(\"Visualization not available for this model\")\n</code></pre>"},{"location":"guides/layout-analysis/#create-mask-images","title":"Create Mask Images","text":"<p>Generate segmentation masks for each label.</p> <pre><code>from PIL import Image, ImageDraw\nimport numpy as np\n\n# Label to color mapping\nlabel_colors = {\n    \"title\": (255, 0, 0),\n    \"text\": (0, 255, 0),\n    \"table\": (0, 0, 255),\n    \"figure\": (255, 255, 0),\n    \"list\": (255, 0, 255),\n    \"caption\": (0, 255, 255),\n}\n\n# Create mask image\nmask = Image.new(\"RGB\", image.size, color=(255, 255, 255))\ndraw = ImageDraw.Draw(mask)\n\n# Draw filled rectangles\nfor element in result.elements:\n    color = label_colors.get(element.label, (128, 128, 128))\n    draw.rectangle(element.bbox, fill=color)\n\n# Save and display\nmask.save(\"layout_mask.png\")\n\n# Create overlay\noverlay = Image.blend(image, mask, alpha=0.5)\noverlay.save(\"layout_overlay.png\")\nprint(\"Saved mask and overlay\")\n</code></pre>"},{"location":"guides/layout-analysis/#advanced-custom-labels","title":"Advanced: Custom Labels","text":""},{"location":"guides/layout-analysis/#multi-stage-pipeline-with-custom-labels","title":"Multi-Stage Pipeline with Custom Labels","text":"<p>Detect custom elements, then extract text from specific types.</p> <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\nfrom omnidocs.tasks.layout_extraction import CustomLabel\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom PIL import Image, ImageDraw\nimport numpy as np\n\nimage = Image.open(\"document.png\")\n\n# Stage 1: Layout detection with custom labels\ncustom_labels = [\n    CustomLabel(name=\"code_snippet\"),\n    CustomLabel(name=\"data_table\"),\n    CustomLabel(name=\"api_reference\"),\n]\n\nlayout_config = QwenLayoutPyTorchConfig(device=\"cuda\")\nlayout_detector = QwenLayoutDetector(backend=layout_config)\nlayout_result = layout_detector.extract(image, custom_labels=custom_labels)\n\n# Group elements by type\ncode_snippets = [e for e in layout_result.elements if e.label == \"code_snippet\"]\ndata_tables = [e for e in layout_result.elements if e.label == \"data_table\"]\napi_docs = [e for e in layout_result.elements if e.label == \"api_reference\"]\n\nprint(f\"Found: {len(code_snippets)} code snippets, \"\n      f\"{len(data_tables)} tables, \"\n      f\"{len(api_docs)} API references\")\n\n# Stage 2: Extract text from specific regions\ntext_config = QwenTextPyTorchConfig(device=\"cuda\")\ntext_extractor = QwenTextExtractor(backend=text_config)\n\n# Process code snippets with special handling\nfor snippet in code_snippets:\n    x1, y1, x2, y2 = snippet.bbox\n    snippet_img = image.crop((x1, y1, x2, y2))\n\n    # Use specialized prompt for code\n    code_prompt = \"Extract the code from this code snippet. Format as a code block with language identifier.\"\n    result = text_extractor.extract(\n        snippet_img,\n        output_format=\"markdown\",\n        custom_prompt=code_prompt,\n    )\n    print(f\"Code snippet:\\n{result.content}\\n\")\n</code></pre>"},{"location":"guides/layout-analysis/#custom-labels-with-constraints","title":"Custom Labels with Constraints","text":"<p>Add validation and constraints to custom labels.</p> <pre><code>from omnidocs.tasks.layout_extraction import CustomLabel\n\n# Create labels with metadata\nlabels = [\n    CustomLabel(\n        name=\"warning_box\",\n        description=\"Important warning or alert\",\n        color=\"#FF0000\",  # Custom metadata\n    ),\n    CustomLabel(\n        name=\"tip_box\",\n        description=\"Helpful tip or best practice\",\n        color=\"#00FF00\",\n    ),\n    CustomLabel(\n        name=\"example_code\",\n        description=\"Code example or snippet\",\n        color=\"#0000FF\",\n    ),\n]\n\n# Use in extraction\ndetector = QwenLayoutDetector(backend=config)\nresult = detector.extract(image, custom_labels=labels)\n\n# Group by custom category\nwarnings = [e for e in result.elements if e.label == \"warning_box\"]\ntips = [e for e in result.elements if e.label == \"tip_box\"]\nexamples = [e for e in result.elements if e.label == \"example_code\"]\n\nprint(f\"Warnings: {len(warnings)}, Tips: {len(tips)}, Examples: {len(examples)}\")\n</code></pre>"},{"location":"guides/layout-analysis/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/layout-analysis/#missing-elements","title":"Missing Elements","text":"<p>Problem: Some elements not detected.</p> <p>Solutions: 1. Lower <code>confidence</code> threshold 2. Try different model (RT-DETR instead of YOLO) 3. Resize image if very small 4. Check image quality</p> <pre><code># Solution 1: Lower confidence threshold\nconfig = DocLayoutYOLOConfig(device=\"cuda\", confidence=0.15)  # Default: 0.25\n\n# Solution 2: Try RT-DETR\nfrom omnidocs.tasks.layout_extraction import RTDETRLayoutDetector\n\ndetector = RTDETRLayoutDetector(config=RTDETRLayoutConfig(device=\"cuda\"))\nresult = detector.extract(image)\n\n# Solution 3: Resize image\nif image.width &lt; 512:\n    image = image.resize((image.width * 2, image.height * 2))\n\n# Solution 4: Check image quality\nprint(f\"Image size: {image.size}\")\nprint(f\"Image mode: {image.mode}\")\nif image.mode == \"RGBA\":\n    # Convert RGBA to RGB\n    image = image.convert(\"RGB\")\n</code></pre>"},{"location":"guides/layout-analysis/#false-positives","title":"False Positives","text":"<p>Problem: Detecting too many incorrect elements.</p> <p>Solutions: 1. Increase <code>confidence</code> threshold 2. Post-filter by size or region 3. Use different model</p> <pre><code># Solution 1: Increase confidence threshold\nconfig = DocLayoutYOLOConfig(device=\"cuda\", confidence=0.5)  # Higher threshold\n\nresult = detector.extract(image)\n\n# Solution 2: Filter by size\nfiltered = [\n    e for e in result.elements\n    if (e.bbox[2] - e.bbox[0]) &gt; 50  # Width &gt; 50px\n    and (e.bbox[3] - e.bbox[1]) &gt; 20  # Height &gt; 20px\n]\n\n# Solution 3: Try different model\n# If YOLO has too many false positives, try RT-DETR or Qwen\n</code></pre>"},{"location":"guides/layout-analysis/#overlapping-detections","title":"Overlapping Detections","text":"<p>Problem: Elements overlap incorrectly.</p> <p>Solutions: 1. Post-process to remove overlaps 2. Use different confidence threshold 3. Try different model</p> <pre><code>def remove_overlaps(elements, overlap_threshold=0.5):\n    \"\"\"Remove overlapping detections.\"\"\"\n    if not elements:\n        return []\n\n    # Sort by confidence (descending)\n    sorted_elements = sorted(elements, key=lambda e: e.confidence, reverse=True)\n\n    # Keep non-overlapping elements\n    kept = []\n    for elem in sorted_elements:\n        overlaps = False\n        for kept_elem in kept:\n            # Calculate intersection over union\n            # ... (implementation details)\n            pass\n        if not overlaps:\n            kept.append(elem)\n\n    return kept\n\nfiltered = remove_overlaps(result.elements)\nprint(f\"Removed {len(result.elements) - len(filtered)} overlapping detections\")\n</code></pre>"},{"location":"guides/layout-analysis/#slow-inference","title":"Slow Inference","text":"<p>Problem: Layout detection too slow.</p> <p>Solutions: 1. Use faster model (YOLO instead of RT-DETR) 2. Reduce image resolution 3. Use VLLM for Qwen</p> <pre><code># Solution 1: Use YOLO (fastest)\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO\ndetector = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\n\n# Solution 2: Resize image\noriginal_size = image.size\nimage = image.resize((image.width // 2, image.height // 2))\nresult = detector.extract(image)\n# Scale bboxes back\nfor elem in result.elements:\n    elem.bbox = [x * 2 for x in elem.bbox]\n\n# Solution 3: Use VLLM for Qwen\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutVLLMConfig\nconfig = QwenLayoutVLLMConfig(tensor_parallel_size=1)\n</code></pre> <p>Next Steps: - See Text Extraction Guide for extracting content from detected regions - See OCR Extraction Guide for character-level extraction - See Batch Processing Guide for processing many documents</p>"},{"location":"guides/ocr-extraction/","title":"OCR Extraction Guide","text":"<p>Extract text with precise bounding boxes at character, word, line, or block level. This guide covers when to use OCR, available models, multi-language support, and performance considerations.</p>"},{"location":"guides/ocr-extraction/#table-of-contents","title":"Table of Contents","text":"<ul> <li>OCR vs Text Extraction vs Layout</li> <li>Available Models</li> <li>Basic Usage</li> <li>Extracting Bounding Boxes</li> <li>Filtering Results</li> <li>Multi-Language Support</li> <li>Performance Comparison</li> <li>Troubleshooting</li> </ul>"},{"location":"guides/ocr-extraction/#ocr-vs-text-extraction-vs-layout","title":"OCR vs Text Extraction vs Layout","text":"Feature OCR Text Extraction Layout Detection Returns Text + bounding boxes Formatted text only Bounding boxes only Granularity Character/word/line Full document Element-level Location Info Yes (precise) No Yes (element regions) Output Type List of text blocks Single formatted string List of elements Use Case Word spotting, re-OCR, handwriting Document parsing Structure analysis Latency 1-2 sec per page 2-5 sec per page 0.5-1 sec per page Example Output <code>[{\"text\": \"Hello\", \"bbox\": [10, 20, 50, 35]}]</code> <code>\"# Hello\\n\\nWorld\"</code> <code>[{\"label\": \"title\", \"bbox\": [...]}]</code> <p>Choose OCR when: - You need precise character locations - Building re-OCR or correction pipelines - Extracting structured data from tables (get cell coordinates first) - Analyzing handwriting - Building word spotting systems</p> <p>Choose Text Extraction when: - Converting documents to readable format - Extracting full document content - Building markdown/HTML outputs - Focus on content quality over location</p> <p>Choose Layout Detection when: - Understanding document structure - Filtering unwanted elements - Multi-stage processing</p>"},{"location":"guides/ocr-extraction/#available-models","title":"Available Models","text":""},{"location":"guides/ocr-extraction/#model-comparison","title":"Model Comparison","text":"Model Speed Accuracy Languages GPU Req Best For Tesseract \u2b50\u2b50\u2b50\u2b50\u2b50 (Fast) \u2b50\u2b50\u2b50 (Good) 100+ None Legacy, CPU-only EasyOCR \u2b50\u2b50\u2b50 (Medium) \u2b50\u2b50\u2b50\u2b50 (Very Good) 80+ Optional Production use PaddleOCR \u2b50\u2b50\u2b50\u2b50 (Very Fast) \u2b50\u2b50\u2b50\u2b50 (Very Good) 11 Optional Speed-critical, Asian text CRAFT \u2b50\u2b50\u2b50 (Medium) \u2b50\u2b50\u2b50\u2b50 (Very Good) English Optional Scene text detection"},{"location":"guides/ocr-extraction/#1-tesseract-cpu-only","title":"1. Tesseract (CPU-only)","text":"<p>Traditional OCR engine, excellent for clean printed text.</p> <p>Strengths: - No GPU required, CPU-only - Extremely fast - Supports 100+ languages - Proven and reliable - Opensource (Apache 2.0)</p> <p>Weaknesses: - Lower accuracy on complex layouts - Struggles with handwriting - Needs training data for custom fonts</p> <p>When to use: - CPU-only systems (Raspberry Pi, servers) - Clean printed documents - Cost-sensitive applications - Multi-language documents</p> <p>Languages: 100+ (English, Chinese, Arabic, Hindi, etc.)</p>"},{"location":"guides/ocr-extraction/#2-easyocr-gpu-recommended","title":"2. EasyOCR (GPU-recommended)","text":"<p>Deep learning OCR with excellent accuracy.</p> <p>Strengths: - Very high accuracy on diverse text - Supports 80+ languages - Works with or without GPU - Easy API - Good on real-world documents</p> <p>Weaknesses: - Slower than PaddleOCR - Higher memory usage - Requires downloading large models</p> <p>When to use: - High accuracy needed - Mixed language documents - Production systems - Irregular text layouts</p> <p>Languages: English, Chinese, Japanese, Korean, Arabic, Hindi, etc. (80+ total)</p>"},{"location":"guides/ocr-extraction/#3-paddleocr-fastest-with-gpu","title":"3. PaddleOCR (Fastest with GPU)","text":"<p>Lightweight OCR optimized for speed.</p> <p>Strengths: - Fastest inference speed - Small model size - Excellent Asian language support - Works on CPU and GPU - Very efficient</p> <p>Weaknesses: - Fewer languages than EasyOCR - Slightly lower accuracy on English - Limited handwriting support</p> <p>When to use: - Performance-critical applications - Asian language documents - Resource-constrained environments - High-throughput pipelines</p> <p>Languages: English, Chinese, Japanese, Korean, Arabic (main languages)</p>"},{"location":"guides/ocr-extraction/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/ocr-extraction/#example-1-simple-word-level-ocr","title":"Example 1: Simple Word-Level OCR","text":"<p>Extract text with word-level bounding boxes.</p> <pre><code>from omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\nfrom PIL import Image\n\nimage = Image.open(\"document_page.png\")\n\n# Initialize EasyOCR for high accuracy\nconfig = EasyOCRConfig(\n    languages=[\"en\"],  # English only (faster)\n    gpu=True,  # Use GPU if available\n)\nocr = EasyOCR(config=config)\n\n# Extract text with bounding boxes\nresult = ocr.extract(image)\n\nprint(f\"Extracted {len(result.text_blocks)} text blocks\")\n\n# Access text and locations\nfor block in result.text_blocks:\n    print(f\"Text: '{block.text}'\")\n    print(f\"Bbox: {block.bbox}\")\n    print(f\"Confidence: {block.confidence:.2f}\")\n    print()\n</code></pre> <p>Output Example: <pre><code>Extracted 5 text blocks\nText: 'Document'\nBbox: BoundingBox(x1=10, y1=5, x2=120, y2=30)\nConfidence: 0.98\n\nText: 'Title'\nBbox: BoundingBox(x1=10, y1=35, x2=100, y2=55)\nConfidence: 0.97\n\n...\n</code></pre></p>"},{"location":"guides/ocr-extraction/#example-2-fast-cpu-only-ocr-tesseract","title":"Example 2: Fast CPU-Only OCR (Tesseract)","text":"<p>Use Tesseract for fast CPU-only extraction.</p> <pre><code>from omnidocs.tasks.ocr_extraction import Tesseract, TesseractConfig\nfrom PIL import Image\n\nimage = Image.open(\"document_page.png\")\n\n# Initialize Tesseract (CPU only, no GPU)\nconfig = TesseractConfig(\n    language=\"eng\",  # Single language for speed\n    config=\"--psm 3\",  # Page segmentation mode\n)\nocr = Tesseract(config=config)\n\n# Extract\nresult = ocr.extract(image)\n\nprint(f\"Found {len(result.text_blocks)} words\")\n\n# Display results with confidence\nhigh_confidence = [b for b in result.text_blocks if b.confidence &gt; 0.9]\nprint(f\"High confidence blocks: {len(high_confidence)}\")\n\n# Get plain text\nprint(\"\\nExtracted text:\")\nprint(\" \".join(block.text for block in result.text_blocks))\n</code></pre>"},{"location":"guides/ocr-extraction/#example-3-multi-language-ocr","title":"Example 3: Multi-Language OCR","text":"<p>Extract from documents with multiple languages.</p> <pre><code>from omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\nfrom PIL import Image\n\nimage = Image.open(\"multilingual_document.png\")\n\n# Support multiple languages\nconfig = EasyOCRConfig(\n    languages=[\"en\", \"zh\", \"ar\"],  # English, Chinese, Arabic\n    gpu=True,\n)\nocr = EasyOCR(config=config)\n\nresult = ocr.extract(image)\n\n# Group by detected language (if available)\nfor block in result.text_blocks:\n    print(f\"[{block.language}] {block.text}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#example-4-pdf-with-character-level-extraction","title":"Example 4: PDF with Character-Level Extraction","text":"<p>Extract at character granularity from PDF.</p> <pre><code>from omnidocs import Document\nfrom omnidocs.tasks.ocr_extraction import PaddleOCR, PaddleOCRConfig\n\n# Load PDF\ndoc = Document.from_pdf(\"document.pdf\")\n\n# Initialize PaddleOCR for character-level extraction\nconfig = PaddleOCRConfig(\n    languages=[\"en\", \"ch\"],  # English and Chinese\n    gpu=True,\n)\nocr = PaddleOCR(config=config)\n\n# Process first page\npage_image = doc.get_page(0)\nresult = ocr.extract(page_image, granularity=\"character\")\n\n# Access character-level data\nchar_count = len(result.text_blocks)\nprint(f\"Extracted {char_count} characters\")\n\n# Find coordinates of specific character\nfor block in result.text_blocks:\n    if block.text == \"A\":\n        print(f\"Found 'A' at {block.bbox}\")\n        break\n</code></pre>"},{"location":"guides/ocr-extraction/#extracting-bounding-boxes","title":"Extracting Bounding Boxes","text":""},{"location":"guides/ocr-extraction/#get-text-blocks-with-coordinates","title":"Get Text Blocks with Coordinates","text":"<pre><code>from omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\nconfig = EasyOCRConfig(languages=[\"en\"], gpu=True)\nocr = EasyOCR(config=config)\n\nresult = ocr.extract(image)\n\n# Print detailed block information\nfor block in result.text_blocks:\n    x1, y1, x2, y2 = block.bbox.x1, block.bbox.y1, block.bbox.x2, block.bbox.y2\n    width = x2 - x1\n    height = y2 - y1\n\n    print(f\"'{block.text}' @ ({x1:.0f}, {y1:.0f}) \"\n          f\"size: {width:.0f}x{height:.0f} \"\n          f\"conf: {block.confidence:.2f}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#convert-to-normalized-coordinates","title":"Convert to Normalized Coordinates","text":"<p>Convert pixel coordinates to 0-1024 normalized range.</p> <pre><code># Normalize bounding boxes to 0-1024 range\nimage_width, image_height = image.size\nnormalized_blocks = result.get_normalized_blocks()\n\nfor block in normalized_blocks:\n    # Coordinates now in 0-1024 range\n    print(f\"'{block.text}' @ {block.bbox} (normalized)\")\n\n# Manual normalization\nNORM_SIZE = 1024\n\ndef normalize_bbox(bbox, image_size):\n    \"\"\"Convert pixel bbox to normalized 0-1024.\"\"\"\n    img_w, img_h = image_size\n    x1 = int(bbox.x1 * NORM_SIZE / img_w)\n    y1 = int(bbox.y1 * NORM_SIZE / img_h)\n    x2 = int(bbox.x2 * NORM_SIZE / img_w)\n    y2 = int(bbox.y2 * NORM_SIZE / img_h)\n    return (x1, y1, x2, y2)\n\nfor block in result.text_blocks:\n    norm_bbox = normalize_bbox(block.bbox, (image_width, image_height))\n    print(f\"Normalized: {norm_bbox}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#extract-from-specific-regions","title":"Extract from Specific Regions","text":"<p>Get OCR results from a cropped region.</p> <pre><code># Crop image to specific region\nregion_bbox = (100, 100, 500, 400)  # x1, y1, x2, y2\ncropped = image.crop(region_bbox)\n\n# Run OCR on crop\nresult_crop = ocr.extract(cropped)\n\n# Adjust bboxes back to original image coordinates\nx1_offset, y1_offset = region_bbox[0], region_bbox[1]\n\nfor block in result_crop.text_blocks:\n    # Shift coordinates\n    adjusted_bbox = (\n        block.bbox.x1 + x1_offset,\n        block.bbox.y1 + y1_offset,\n        block.bbox.x2 + x1_offset,\n        block.bbox.y2 + y1_offset,\n    )\n    print(f\"'{block.text}' @ {adjusted_bbox}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#filtering-results","title":"Filtering Results","text":""},{"location":"guides/ocr-extraction/#filter-by-confidence","title":"Filter by Confidence","text":"<p>Keep only high-confidence extractions.</p> <pre><code># Filter by confidence threshold\nmin_confidence = 0.85\nconfident_blocks = [\n    b for b in result.text_blocks\n    if b.confidence &gt;= min_confidence\n]\n\nprint(f\"Original: {len(result.text_blocks)} blocks\")\nprint(f\"Filtered (conf &gt;= {min_confidence}): {len(confident_blocks)} blocks\")\n\n# Display confidence distribution\nconfidences = [b.confidence for b in result.text_blocks]\nprint(f\"Confidence range: {min(confidences):.2f} - {max(confidences):.2f}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#filter-by-region","title":"Filter by Region","text":"<p>Extract OCR results from specific image regions.</p> <pre><code>def is_in_region(bbox, region):\n    \"\"\"Check if bbox overlaps with region.\"\"\"\n    rx1, ry1, rx2, ry2 = region\n    return not (bbox.x2 &lt; rx1 or bbox.x1 &gt; rx2 or\n                bbox.y2 &lt; ry1 or bbox.y1 &gt; ry2)\n\n# Top-left region\ntop_left = (0, 0, image.width//2, image.height//2)\ntop_left_blocks = [b for b in result.text_blocks if is_in_region(b.bbox, top_left)]\n\n# Sidebar region\nsidebar = (0, 0, 200, image.height)\nsidebar_blocks = [b for b in result.text_blocks if is_in_region(b.bbox, sidebar)]\n\nprint(f\"Top-left blocks: {len(top_left_blocks)}\")\nprint(f\"Sidebar blocks: {len(sidebar_blocks)}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#filter-by-text-content","title":"Filter by Text Content","text":"<p>Find blocks matching patterns.</p> <pre><code>import re\n\n# Find numbers\nnumber_blocks = [\n    b for b in result.text_blocks\n    if re.match(r'^\\d+$', b.text.strip())\n]\n\n# Find email addresses\nemail_blocks = [\n    b for b in result.text_blocks\n    if re.match(r'^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$', b.text.strip())\n]\n\n# Find specific phrases\nphrase_blocks = [\n    b for b in result.text_blocks\n    if \"important\" in b.text.lower()\n]\n\nprint(f\"Numbers: {len(number_blocks)}\")\nprint(f\"Emails: {len(email_blocks)}\")\nprint(f\"'Important' mentions: {len(phrase_blocks)}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#filter-by-size","title":"Filter by Size","text":"<p>Exclude very small or very large blocks.</p> <pre><code># Calculate block dimensions\ndef get_size(bbox):\n    return (bbox.x2 - bbox.x1, bbox.y2 - bbox.y1)\n\n# Keep medium-sized blocks\nmedium_blocks = []\nfor b in result.text_blocks:\n    width, height = get_size(b.bbox)\n    if 30 &lt; width &lt; 500 and 10 &lt; height &lt; 100:\n        medium_blocks.append(b)\n\nprint(f\"Medium-sized blocks: {len(medium_blocks)}/{len(result.text_blocks)}\")\n\n# Analyze size distribution\nsizes = [get_size(b.bbox) for b in result.text_blocks]\navg_width = sum(w for w, h in sizes) / len(sizes)\navg_height = sum(h for w, h in sizes) / len(sizes)\nprint(f\"Average block size: {avg_width:.0f}x{avg_height:.0f}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#multi-language-support","title":"Multi-Language Support","text":""},{"location":"guides/ocr-extraction/#auto-detect-language","title":"Auto-Detect Language","text":"<p>EasyOCR can auto-detect language.</p> <pre><code>from omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\n# Auto-detect (leave empty or None)\nconfig = EasyOCRConfig(\n    languages=None,  # Auto-detect all languages\n    gpu=True,\n)\nocr = EasyOCR(config=config)\n\nresult = ocr.extract(image)\n\n# Check detected languages\ndetected_langs = set()\nfor block in result.text_blocks:\n    if hasattr(block, 'language'):\n        detected_langs.add(block.language)\n\nprint(f\"Detected languages: {detected_langs}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#process-mixed-language-documents","title":"Process Mixed-Language Documents","text":"<p>Handle documents with multiple languages.</p> <pre><code># Support common languages\nconfig = EasyOCRConfig(\n    languages=[\"en\", \"zh\", \"ar\", \"hi\", \"ja\"],  # English, Chinese, Arabic, Hindi, Japanese\n    gpu=True,\n)\nocr = EasyOCR(config=config)\n\nresult = ocr.extract(image)\n\n# Group results by language\nfrom collections import defaultdict\nby_language = defaultdict(list)\n\nfor block in result.text_blocks:\n    lang = getattr(block, 'language', 'unknown')\n    by_language[lang].append(block)\n\nfor lang, blocks in by_language.items():\n    print(f\"\\n{lang.upper()} ({len(blocks)} blocks):\")\n    for block in blocks[:3]:  # Show first 3\n        print(f\"  {block.text}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#language-specific-optimization","title":"Language-Specific Optimization","text":"<p>Different languages need different models.</p> <pre><code># For English only (fastest)\nconfig_en = EasyOCRConfig(languages=[\"en\"], gpu=True)\n\n# For Asian languages (use PaddleOCR)\nfrom omnidocs.tasks.ocr_extraction import PaddleOCR, PaddleOCRConfig\nconfig_cn = PaddleOCRConfig(languages=[\"ch\"], gpu=True)  # Chinese\n\n# For Arabic/Hebrew (right-to-left)\nconfig_rtl = EasyOCRConfig(languages=[\"ar\"], gpu=True)\n\n# For handwriting\nconfig_hw = EasyOCRConfig(languages=[\"en\"], gpu=True)\n# Note: Most OCR models struggle with handwriting\n</code></pre>"},{"location":"guides/ocr-extraction/#performance-comparison","title":"Performance Comparison","text":""},{"location":"guides/ocr-extraction/#speed-benchmarks","title":"Speed Benchmarks","text":"<p>Processing a typical page (300 DPI, ~2000x3000px):</p> Model CPU GPU Latency Memory Tesseract 0.5-1.0s N/A Very Fast ~100MB PaddleOCR 1-2s 0.3-0.5s Fast ~500MB EasyOCR 2-4s 0.5-1.0s Medium ~1GB"},{"location":"guides/ocr-extraction/#choose-by-speed-requirements","title":"Choose by Speed Requirements","text":"Requirement Model &lt;200ms per page Tesseract or PaddleOCR (GPU) &lt;500ms per page PaddleOCR (GPU) or Tesseract &lt;1s per page EasyOCR (GPU) or PaddleOCR (CPU) 1-2s acceptable EasyOCR (GPU) Accuracy paramount EasyOCR"},{"location":"guides/ocr-extraction/#optimization-for-speed","title":"Optimization for Speed","text":"<pre><code>import time\n\n# Fast configuration\nconfig_fast = PaddleOCRConfig(\n    languages=[\"en\"],  # Single language\n    gpu=True,\n)\nocr = PaddleOCR(config=config_fast)\n\n# Benchmark\nimages = [Image.open(f\"doc{i}.png\") for i in range(5)]\n\nstart = time.time()\nfor img in images:\n    result = ocr.extract(img)\nelapsed = time.time() - start\n\nprint(f\"Processed {len(images)} images in {elapsed:.1f}s\")\nprint(f\"Average: {elapsed/len(images):.2f}s per image\")\n</code></pre>"},{"location":"guides/ocr-extraction/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/ocr-extraction/#low-accuracy","title":"Low Accuracy","text":"<p>Problem: OCR results have many errors.</p> <p>Solutions: 1. Try different model (EasyOCR typically better) 2. Improve image quality 3. Try single language (faster + more accurate)</p> <pre><code># Solution 1: Use EasyOCR (more accurate)\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\nconfig = EasyOCRConfig(languages=[\"en\"], gpu=True)\n\n# Solution 2: Improve image quality\nfrom PIL import Image, ImageEnhance\n\nimg = Image.open(\"noisy_scan.png\")\n\n# Increase contrast\nenhancer = ImageEnhance.Contrast(img)\nimg = enhancer.enhance(1.5)  # 50% more contrast\n\n# Increase sharpness\nenhancer = ImageEnhance.Sharpness(img)\nimg = enhancer.enhance(2.0)  # 2x sharpness\n\n# Resize if too small\nif img.width &lt; 1024:\n    img = img.resize((img.width * 2, img.height * 2), Image.Resampling.LANCZOS)\n\nresult = ocr.extract(img)\n\n# Solution 3: Single language\nconfig = EasyOCRConfig(languages=[\"en\"], gpu=True)  # Just English\n</code></pre>"},{"location":"guides/ocr-extraction/#missing-text","title":"Missing Text","text":"<p>Problem: Some text not detected.</p> <p>Solutions: 1. Check image quality 2. Try lower confidence threshold 3. Use different OCR model</p> <pre><code># Solution 1: Check image\nprint(f\"Image size: {image.size}\")\nprint(f\"Image mode: {image.mode}\")\n\n# Solution 2: Lower confidence\nall_blocks = result.text_blocks  # Includes low confidence\n\nconfidence_dist = [b.confidence for b in result.text_blocks]\nprint(f\"Confidence range: {min(confidence_dist):.2f}-{max(confidence_dist):.2f}\")\n\n# Get even low-confidence blocks\nlow_conf_blocks = [b for b in result.text_blocks if b.confidence &lt; 0.5]\nprint(f\"Low confidence blocks: {len(low_conf_blocks)}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#false-detections","title":"False Detections","text":"<p>Problem: Non-text detected as text.</p> <p>Solutions: 1. Increase confidence threshold 2. Filter by text length 3. Manual post-processing</p> <pre><code># Solution 1: Increase confidence\nhigh_conf = [b for b in result.text_blocks if b.confidence &gt; 0.95]\n\n# Solution 2: Filter short blocks (likely noise)\nMIN_CHARS = 2\nvalid_blocks = [b for b in result.text_blocks if len(b.text) &gt;= MIN_CHARS]\n\n# Solution 3: Remove non-alphabetic text\nimport string\nalpha_blocks = [\n    b for b in result.text_blocks\n    if any(c.isalpha() for c in b.text)\n]\n</code></pre>"},{"location":"guides/ocr-extraction/#slow-performance","title":"Slow Performance","text":"<p>Problem: OCR taking too long.</p> <p>Solutions: 1. Use faster model (PaddleOCR or Tesseract) 2. Reduce image resolution 3. Use GPU 4. Single language</p> <pre><code># Solution 1: Use PaddleOCR (faster)\nfrom omnidocs.tasks.ocr_extraction import PaddleOCR\nocr = PaddleOCR(gpu=True)  # Fastest on GPU\n\n# Solution 2: Reduce resolution\nimage = image.resize((image.width // 2, image.height // 2))\n\n# Solution 3: Ensure GPU enabled\nimport torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\n# Solution 4: Single language\nconfig = PaddleOCRConfig(languages=[\"en\"], gpu=True)\n</code></pre> <p>Next Steps: - See Text Extraction Guide for formatted document output - See Layout Analysis Guide for document structure - See Batch Processing Guide for processing many documents</p>"},{"location":"guides/text-extraction/","title":"Text Extraction Guide","text":"<p>Extract formatted text content (Markdown/HTML) from document images using vision-language models. This guide covers when to use text extraction, available models, output formats, and practical examples.</p>"},{"location":"guides/text-extraction/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Quick Comparison: Text Extraction vs OCR vs Layout</li> <li>Available Models</li> <li>Basic Usage</li> <li>Output Formats</li> <li>Advanced Features</li> <li>Performance Optimization</li> <li>Troubleshooting</li> </ul>"},{"location":"guides/text-extraction/#quick-comparison","title":"Quick Comparison","text":"Feature Text Extraction OCR Layout Detection Output Formatted text (MD/HTML) Text + bounding boxes Element bounding boxes Use Case Document parsing, markdown export Word/character localization Document structure analysis Models Qwen3-VL, DotsOCR, Nanonets Tesseract, EasyOCR, PaddleOCR DocLayoutYOLO, Qwen-Layout Latency ~2-5 sec per page ~1-2 sec per page ~0.5-1 sec per page Output Type Single string List of text blocks List of bounding boxes Layout Info Optional (DotsOCR only) No Yes (with labels) <p>Choose Text Extraction when: - Converting documents to Markdown/HTML - Extracting complete page content as formatted text - Working with complex documents (multi-column, figures, tables) - You need readable output for downstream processing</p> <p>Choose OCR when: - You need precise character/word locations - Building re-OCR pipelines (e.g., for correction) - Requiring character-level accuracy metrics</p> <p>Choose Layout Detection when: - You need document structure without text content - Building advanced pipelines (layout + text) - Analyzing document semantics</p>"},{"location":"guides/text-extraction/#available-models","title":"Available Models","text":""},{"location":"guides/text-extraction/#1-qwen3-vl-recommended-for-most-cases","title":"1. Qwen3-VL (Recommended for most cases)","text":"<p>High-quality general-purpose vision-language model.</p> <p>Strengths: - Best output quality across diverse documents - Multi-backend support (PyTorch, VLLM, MLX, API) - Consistent Markdown/HTML output - Good at handling complex layouts</p> <p>Backends: - PyTorch: Local GPU inference (single GPU) - VLLM: High-throughput serving (multiple GPUs) - MLX: Apple Silicon (local) - API: Hosted models (cloud)</p> <p>Model Variants: - <code>Qwen/Qwen3-VL-8B-Instruct</code>: Recommended (8B parameters) - <code>Qwen/Qwen3-VL-32B-Instruct</code>: Higher quality (32B, slower, more VRAM)</p>"},{"location":"guides/text-extraction/#2-dotsocr-best-for-technical-documents","title":"2. DotsOCR (Best for technical documents)","text":"<p>Optimized for complex technical documents with precise layout preservation.</p> <p>Strengths: - Layout-aware extraction with bounding boxes - Specialized formatting for tables (HTML) and formulas (LaTeX) - Reading order preservation - 11-category layout detection</p> <p>Weaknesses: - Slower than Qwen (requires layout analysis) - Higher VRAM requirements</p> <p>Backends: - PyTorch: Local GPU inference - VLLM: High-throughput serving - API: Hosted models</p> <p>Output Types: - Structured JSON with layout information - Markdown with coordinate annotations - HTML with bbox attributes</p>"},{"location":"guides/text-extraction/#3-nanonets-coming-soon","title":"3. Nanonets (Coming soon)","text":"<p>Specialized for OCR-quality text extraction.</p>"},{"location":"guides/text-extraction/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/text-extraction/#example-1-simple-markdown-extraction","title":"Example 1: Simple Markdown Extraction","text":"<p>Extract a document page to Markdown using PyTorch backend.</p> <pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom PIL import Image\n\n# Load a single image\nimage = Image.open(\"document_page.png\")\n\n# Initialize extractor with PyTorch backend\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cuda\",  # or \"cpu\"\n    torch_dtype=\"auto\",  # Automatic dtype selection\n)\nextractor = QwenTextExtractor(backend=config)\n\n# Extract text in Markdown format\nresult = extractor.extract(image, output_format=\"markdown\")\n\n# Access the extracted content\nprint(result.content)  # Formatted Markdown text\nprint(result.word_count)  # Number of words\nprint(f\"Model: {result.model_name}\")\n</code></pre>"},{"location":"guides/text-extraction/#example-2-extract-with-layout-information","title":"Example 2: Extract with Layout Information","text":"<p>Use DotsOCR to get text plus layout annotations.</p> <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\nfrom PIL import Image\nimport json\n\nimage = Image.open(\"complex_document.png\")\n\n# Initialize DotsOCR with layout detection\nconfig = DotsOCRPyTorchConfig(\n    device=\"cuda\",\n    max_new_tokens=8192,  # Higher for complex documents\n)\nextractor = DotsOCRTextExtractor(backend=config)\n\n# Extract with layout information\nresult = extractor.extract(image, include_layout=True)\n\n# Access layout elements\nprint(f\"Found {result.num_layout_elements} layout elements\")\nprint(f\"Content length: {result.content_length} characters\")\n\n# Iterate through layout elements\nfor element in result.layout:\n    print(f\"[{element.category}] @{element.bbox}: {element.text[:50]}...\")\n\n# Save layout information to JSON\nlayout_json = [elem.model_dump() for elem in result.layout]\nwith open(\"layout.json\", \"w\") as f:\n    json.dump(layout_json, f, indent=2)\n</code></pre>"},{"location":"guides/text-extraction/#example-3-extract-pdf-document","title":"Example 3: Extract PDF Document","text":"<p>Process multiple pages of a PDF document.</p> <pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom pathlib import Path\n\n# Load PDF document\ndoc = Document.from_pdf(\"multi_page_document.pdf\")\nprint(f\"Loaded PDF with {doc.page_count} pages\")\n\n# Initialize extractor\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cuda\",\n)\nextractor = QwenTextExtractor(backend=config)\n\n# Extract text from all pages\nall_text = []\nfor page_idx in range(min(3, doc.page_count)):  # First 3 pages\n    page_image = doc.get_page(page_idx)\n    result = extractor.extract(page_image, output_format=\"markdown\")\n    all_text.append(result.content)\n    print(f\"Page {page_idx + 1}: {result.word_count} words\")\n\n# Combine results\nfull_document = \"\\n\\n---\\n\\n\".join(all_text)\nprint(f\"\\nTotal content: {len(full_document)} characters\")\n\n# Save to file\nwith open(\"extracted_document.md\", \"w\") as f:\n    f.write(full_document)\n</code></pre>"},{"location":"guides/text-extraction/#example-4-batch-processing-with-progress-tracking","title":"Example 4: Batch Processing with Progress Tracking","text":"<p>Process multiple documents with progress reporting.</p> <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom pathlib import Path\nfrom PIL import Image\nimport time\n\n# Find all image files\nimage_dir = Path(\"documents/\")\nimage_files = list(image_dir.glob(\"*.png\")) + list(image_dir.glob(\"*.jpg\"))\nprint(f\"Found {len(image_files)} images to process\")\n\n# Initialize extractor\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cuda\",\n    max_new_tokens=4096,\n)\nextractor = QwenTextExtractor(backend=config)\n\n# Process with progress tracking\nresults = {}\nstart_time = time.time()\n\nfor idx, image_path in enumerate(image_files, 1):\n    print(f\"[{idx}/{len(image_files)}] Processing {image_path.name}...\", end=\" \")\n\n    try:\n        image = Image.open(image_path)\n        result = extractor.extract(image, output_format=\"markdown\")\n        results[str(image_path)] = {\n            \"content_length\": result.content_length,\n            \"word_count\": result.word_count,\n        }\n        print(f\"\u2713 ({result.word_count} words)\")\n    except Exception as e:\n        print(f\"\u2717 Error: {e}\")\n        results[str(image_path)] = {\"error\": str(e)}\n\n# Summary\nelapsed = time.time() - start_time\nprint(f\"\\nCompleted in {elapsed:.1f}s ({elapsed/len(image_files):.2f}s per image)\")\nprint(f\"Successful: {sum(1 for r in results.values() if 'error' not in r)}\")\n</code></pre>"},{"location":"guides/text-extraction/#output-formats","title":"Output Formats","text":""},{"location":"guides/text-extraction/#markdown-format","title":"Markdown Format","text":"<p>Human-readable format with standard Markdown syntax. Best for documentation and web publishing.</p> <pre><code>result = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n\n# Example output:\n# # Document Title\n#\n# This is the main content with **bold** and *italic* text.\n#\n# ## Section 1\n#\n# - Bullet point 1\n# - Bullet point 2\n#\n# | Column 1 | Column 2 |\n# |----------|----------|\n# | Cell 1   | Cell 2   |\n</code></pre> <p>Advantages: - Human-readable - Git-friendly (version control) - Easy to edit - Good for documentation</p> <p>Limitations: - Loses some layout information - Tables converted to Markdown tables (may lose formatting) - No bounding box information</p>"},{"location":"guides/text-extraction/#html-format","title":"HTML Format","text":"<p>Structured HTML with semantic tags. Better for preserving layout in web contexts.</p> <pre><code>result = extractor.extract(image, output_format=\"html\")\nprint(result.content)\n\n# Example output:\n# &lt;div class=\"document\"&gt;\n#   &lt;h1&gt;Document Title&lt;/h1&gt;\n#   &lt;p&gt;This is the main content with &lt;b&gt;bold&lt;/b&gt; and &lt;i&gt;italic&lt;/i&gt; text.&lt;/p&gt;\n#   &lt;h2&gt;Section 1&lt;/h2&gt;\n#   &lt;ul&gt;\n#     &lt;li&gt;Bullet point 1&lt;/li&gt;\n#     &lt;li&gt;Bullet point 2&lt;/li&gt;\n#   &lt;/ul&gt;\n#   &lt;table&gt;...&lt;/table&gt;\n# &lt;/div&gt;\n</code></pre> <p>Advantages: - Structured and semantic - Better layout preservation - Good for web rendering - Supports nested elements</p> <p>Limitations: - More verbose - Requires HTML parser for processing - Layout information may still be approximate</p>"},{"location":"guides/text-extraction/#plain-text-fallback","title":"Plain Text (Fallback)","text":"<p>Extract plain text without any formatting.</p> <pre><code># Get plain text version\nplain_text = result.plain_text\nprint(plain_text)\n\n# Also available as property:\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\n# ... after extraction ...\nprint(result.plain_text)  # No formatting, just raw text\n</code></pre>"},{"location":"guides/text-extraction/#dotsocr-json-format","title":"DotsOCR JSON Format","text":"<p>Structured JSON with layout information (DotsOCR only).</p> <pre><code>result = extractor.extract(image, output_format=\"json\", include_layout=True)\n\n# Result includes:\n# {\n#   \"content\": \"Full text...\",\n#   \"layout\": [\n#     {\n#       \"bbox\": [100, 50, 400, 80],\n#       \"category\": \"Title\",\n#       \"text\": \"Document Title\"\n#     },\n#     ...\n#   ]\n# }\n</code></pre>"},{"location":"guides/text-extraction/#advanced-features","title":"Advanced Features","text":""},{"location":"guides/text-extraction/#custom-prompts","title":"Custom Prompts","text":"<p>Override the default extraction prompt for specialized use cases.</p> <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\nconfig = QwenTextPyTorchConfig(device=\"cuda\")\nextractor = QwenTextExtractor(backend=config)\n\n# Custom prompt for extractive summarization\ncustom_prompt = \"\"\"\nExtract the most important information from this document image.\nFocus on key facts, numbers, and action items.\nFormat as a concise Markdown list.\n\"\"\"\n\nresult = extractor.extract(\n    image,\n    output_format=\"markdown\",\n    custom_prompt=custom_prompt,\n)\n\nprint(result.content)\n</code></pre>"},{"location":"guides/text-extraction/#temperature-control-pytorch-only","title":"Temperature Control (PyTorch only)","text":"<p>Adjust model creativity/determinism via temperature parameter.</p> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Lower temperature = more deterministic (better for factual extraction)\nconfig = QwenTextPyTorchConfig(\n    device=\"cuda\",\n    temperature=0.1,  # Default: 0.1 (deterministic)\n)\n\n# Higher temperature = more creative (for summarization, etc.)\nconfig_creative = QwenTextPyTorchConfig(\n    device=\"cuda\",\n    temperature=0.7,\n)\n</code></pre>"},{"location":"guides/text-extraction/#backend-switching","title":"Backend Switching","text":"<p>Easily switch between backends without changing extraction code.</p> <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenTextPyTorchConfig,\n    QwenTextVLLMConfig,\n    QwenTextMLXConfig,\n    QwenTextAPIConfig,\n)\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\n\n# Use PyTorch for single-GPU inference\npytorch_extractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(device=\"cuda\")\n)\nresult1 = pytorch_extractor.extract(image, output_format=\"markdown\")\n\n# Use VLLM for high-throughput inference\nvllm_extractor = QwenTextExtractor(\n    backend=QwenTextVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=1,\n    )\n)\nresult2 = vllm_extractor.extract(image, output_format=\"markdown\")\n\n# Use MLX for Apple Silicon\nmlx_extractor = QwenTextExtractor(\n    backend=QwenTextMLXConfig(device=\"gpu\")\n)\nresult3 = mlx_extractor.extract(image, output_format=\"markdown\")\n\n# Use API for hosted models\napi_extractor = QwenTextExtractor(\n    backend=QwenTextAPIConfig(\n        model=\"qwen3-vl-8b\",\n        api_key=\"your-api-key\",\n        base_url=\"https://api.example.com/v1\",\n    )\n)\nresult4 = api_extractor.extract(image, output_format=\"markdown\")\n\nprint(f\"PyTorch: {result1.word_count} words\")\nprint(f\"VLLM: {result2.word_count} words\")\nprint(f\"MLX: {result3.word_count} words\")\nprint(f\"API: {result4.word_count} words\")\n</code></pre>"},{"location":"guides/text-extraction/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/text-extraction/#model-selection","title":"Model Selection","text":"Model Latency Quality VRAM Speed Qwen3-VL-8B 2-3 sec Excellent 16GB Fast Qwen3-VL-32B 5-8 sec Outstanding 32GB Slow DotsOCR 3-5 sec Very Good (technical) 20GB Medium <p>Recommendation: Start with Qwen3-VL-8B (best quality/speed tradeoff).</p>"},{"location":"guides/text-extraction/#backend-optimization","title":"Backend Optimization","text":"<p>PyTorch (Single GPU): - Best for development and small batches - Load time: ~2-3 seconds - Per-image latency: ~2-3 seconds</p> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cuda\",\n    torch_dtype=\"auto\",  # Let PyTorch choose optimal dtype\n    max_new_tokens=4096,  # Reduce for faster inference\n)\n</code></pre> <p>VLLM (Multi-GPU): - Best for batch processing / high throughput - Load time: ~5-8 seconds (slightly slower but amortizes) - Throughput: 2-4x better than PyTorch for multiple requests</p> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\n\nconfig = QwenTextVLLMConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    tensor_parallel_size=2,  # Use 2 GPUs\n    gpu_memory_utilization=0.9,  # Use 90% of VRAM\n    max_tokens=4096,\n)\n</code></pre> <p>MLX (Apple Silicon): - Best for MacBook development - No GPU-related issues - Slightly slower than VRAM-constrained models</p> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextMLXConfig\n\nconfig = QwenTextMLXConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct-MLX\",\n    device=\"gpu\",\n    quantization=\"4bit\",  # Quantization reduces VRAM\n)\n</code></pre>"},{"location":"guides/text-extraction/#batch-processing-strategy","title":"Batch Processing Strategy","text":"<p>For processing many documents, batch requests to amortize model loading.</p> <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\nfrom pathlib import Path\nfrom PIL import Image\nimport time\n\n# Initialize once (expensive)\nconfig = QwenTextVLLMConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    tensor_parallel_size=1,\n    gpu_memory_utilization=0.85,\n    max_tokens=4096,\n)\nextractor = QwenTextExtractor(backend=config)\n\n# Process many documents (cheap)\nimage_paths = list(Path(\"documents/\").glob(\"*.png\"))\nresults = []\n\nstart = time.time()\nfor image_path in image_paths:\n    image = Image.open(image_path)\n    result = extractor.extract(image, output_format=\"markdown\")\n    results.append(result)\n\nelapsed = time.time() - start\nprint(f\"Processed {len(results)} images in {elapsed:.1f}s\")\nprint(f\"Average: {elapsed/len(results):.2f}s per image\")\n</code></pre>"},{"location":"guides/text-extraction/#token-limit-tuning","title":"Token Limit Tuning","text":"<p>Adjust <code>max_new_tokens</code> based on expected output length.</p> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# For short documents (&lt; 1000 words)\nconfig_short = QwenTextPyTorchConfig(\n    device=\"cuda\",\n    max_new_tokens=2048,  # Faster\n)\n\n# For medium documents (1000-5000 words)\nconfig_medium = QwenTextPyTorchConfig(\n    device=\"cuda\",\n    max_new_tokens=4096,  # Default\n)\n\n# For long documents (&gt; 5000 words)\nconfig_long = QwenTextPyTorchConfig(\n    device=\"cuda\",\n    max_new_tokens=8192,  # Slower but handles longer docs\n)\n</code></pre>"},{"location":"guides/text-extraction/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/text-extraction/#out-of-memory-oom-errors","title":"Out of Memory (OOM) Errors","text":"<p>Problem: CUDA out of memory during inference.</p> <p>Solutions: 1. Reduce <code>max_new_tokens</code> 2. Use smaller model variant (8B instead of 32B) 3. Switch to VLLM with <code>tensor_parallel_size &gt; 1</code> 4. Use quantization (if available)</p> <pre><code># Option 1: Reduce max_new_tokens\nconfig = QwenTextPyTorchConfig(\n    device=\"cuda\",\n    max_new_tokens=2048,  # Reduced from 4096\n)\n\n# Option 2: Smaller model\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",  # Instead of 32B\n    device=\"cuda\",\n)\n\n# Option 3: VLLM with tensor parallelism\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\nconfig = QwenTextVLLMConfig(\n    tensor_parallel_size=2,  # Distribute across 2 GPUs\n    max_tokens=4096,\n)\n</code></pre>"},{"location":"guides/text-extraction/#slow-inference","title":"Slow Inference","text":"<p>Problem: Text extraction takes too long.</p> <p>Solutions: 1. Check GPU utilization (should be &gt;80%) 2. Reduce <code>max_new_tokens</code> 3. Use VLLM instead of PyTorch 4. Use VLLM tensor parallelism</p> <pre><code>import subprocess\n\n# Check GPU usage during extraction\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=utilization.gpu\", \"--format=csv,noheader\"],\n    capture_output=True,\n    text=True\n)\nprint(f\"GPU Utilization: {result.stdout.strip()}%\")\n\n# If &lt;50%, increase batch size or use VLLM\n</code></pre>"},{"location":"guides/text-extraction/#incorrect-or-garbled-output","title":"Incorrect or Garbled Output","text":"<p>Problem: Extracted text is incomplete or corrupted.</p> <p>Solutions: 1. Check image quality (min 1024px width recommended) 2. Verify model downloaded correctly 3. Try with explicit output format</p> <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\n\n# Check image size\nprint(f\"Image size: {image.size}\")  # Should be at least (1024, 768)\n\n# Resize if too small\nif image.width &lt; 1024:\n    image = image.resize((image.width * 2, image.height * 2))\n\n# Try extraction\nconfig = QwenTextPyTorchConfig(device=\"cuda\")\nextractor = QwenTextExtractor(backend=config)\nresult = extractor.extract(image, output_format=\"markdown\")\n\n# Check result\nif len(result.content) &lt; 10:\n    print(\"Warning: Very short output, may indicate extraction failure\")\n    print(f\"Raw output: {result.raw_output}\")\n</code></pre>"},{"location":"guides/text-extraction/#model-download-issues","title":"Model Download Issues","text":"<p>Problem: Model fails to download or load.</p> <p>Solutions: 1. Check internet connection 2. Verify HuggingFace token 3. Set custom cache directory</p> <pre><code>import os\n\n# Set HuggingFace token\nos.environ[\"HF_TOKEN\"] = \"your-token-here\"\n\n# Set custom cache directory\nos.environ[\"HF_HOME\"] = \"/large/disk/hf_cache\"\n\n# Verify download by loading model explicitly\nfrom transformers import AutoTokenizer, AutoModel\n\nmodel_id = \"Qwen/Qwen3-VL-8B-Instruct\"\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    print(f\"\u2713 Model {model_id} loaded successfully\")\nexcept Exception as e:\n    print(f\"\u2717 Failed to load model: {e}\")\n</code></pre>"},{"location":"guides/text-extraction/#api-backend-timeouts","title":"API Backend Timeouts","text":"<p>Problem: API requests timeout or fail.</p> <p>Solutions: 1. Increase timeout value 2. Check API credentials 3. Reduce batch size</p> <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextAPIConfig\n\nconfig = QwenTextAPIConfig(\n    model=\"qwen3-vl-8b\",\n    api_key=\"your-api-key\",\n    base_url=\"https://api.example.com/v1\",\n    timeout=60,  # Increase timeout\n    rate_limit=5,  # Reduce concurrent requests\n)\nextractor = QwenTextExtractor(backend=config)\n</code></pre> <p>Next Steps: - See Batch Processing Guide for processing many documents - See Deployment Guide for scaling on Modal - See Layout Analysis Guide for structure-aware extraction</p>"},{"location":"models/","title":"OmniDocs Model Documentation","text":"<p>Complete reference documentation for all models available in OmniDocs, including configuration, usage, and performance characteristics.</p>"},{"location":"models/#quick-navigation","title":"Quick Navigation","text":""},{"location":"models/#text-extraction","title":"Text Extraction","text":"<p>Models for extracting document text content with optional formatting.</p> <ul> <li>Qwen3-VL Text Extraction - High-quality VLM with multi-backend support</li> <li>Best for: Quality, multilingual documents, multiple backends</li> <li>Variants: 2B, 4B, 8B, 32B</li> <li>Backends: PyTorch, VLLM, MLX, API</li> <li> <p>Speed: 50-150 tok/s (PyTorch), 200-400 tok/s (VLLM)</p> </li> <li> <p>DotsOCR Text Extraction - Layout-aware text extraction</p> </li> <li>Best for: Layout analysis with text, fast inference</li> <li>Layout Categories: 11 fixed categories with bounding boxes</li> <li>Backends: PyTorch, VLLM</li> <li>Speed: 80-120 tok/s (PyTorch), 200-300 tok/s (VLLM)</li> </ul>"},{"location":"models/#layout-analysis","title":"Layout Analysis","text":"<p>Models for detecting document structure and element regions.</p> <ul> <li>DocLayout-YOLO Layout Detection - Fast YOLO-based detection</li> <li>Best for: Speed, batch processing, fixed categories</li> <li>Categories: 10 fixed layout types</li> <li>Backend: PyTorch only</li> <li> <p>Speed: 0.1-0.2s per page (very fast)</p> </li> <li> <p>Qwen3-VL Layout Detection - Flexible VLM-based detection</p> </li> <li>Best for: Custom categories, semantic understanding</li> <li>Categories: Unlimited (define custom labels)</li> <li>Backends: PyTorch, VLLM, MLX, API</li> <li>Speed: 2-5s per page (PyTorch), 0.5-1.5s (VLLM)</li> </ul>"},{"location":"models/#ocr-extraction","title":"OCR Extraction","text":"<p>Models for character-level text recognition with bounding boxes.</p> <ul> <li>Tesseract OCR - Open-source OCR engine</li> <li>Best for: Printed text, CPU-only deployment</li> <li>Languages: 100+</li> <li>Speed: 2-5s per page (CPU)</li> <li>Accuracy: 95-99% for printed English</li> </ul>"},{"location":"models/#model-comparison","title":"Model Comparison","text":"<p>Quick Comparison Table - See full details in Comparison Guide</p> Model Task Speed Quality Backends Memory Qwen3-VL Text Medium Excellent 4 4-40 GB DotsOCR Text + Layout Fast Very Good 2 16 GB DocLayout-YOLO Layout Very Fast Good 1 2-4 GB Qwen Layout Layout (Custom) Medium Excellent 4 8-16 GB Tesseract OCR Slow (CPU) Excellent CPU Minimal"},{"location":"models/#choosing-the-right-model","title":"Choosing the Right Model","text":""},{"location":"models/#by-task","title":"By Task","text":"<p>Text Extraction - High quality: Qwen3-VL (any variant) - Fast + Layout: DotsOCR - Multilingual: Qwen3-VL - Apple Silicon: Qwen3-VL (MLX backend)</p> <p>Layout Detection - Speed: DocLayout-YOLO (0.1s/page) - Accuracy: RT-DETR or Qwen Layout - Custom categories: Qwen Layout - Production pipeline: DocLayout-YOLO</p> <p>OCR (Character Recognition) - Printed text: Tesseract (free, CPU) - Handwriting: Surya - Speed: PaddleOCR - Accuracy: Surya</p>"},{"location":"models/#by-constraint","title":"By Constraint","text":"<p>Limited GPU Memory (4 GB) - DocLayout-YOLO (2-4 GB) - Qwen3-VL-2B (4 GB) - Tesseract (CPU)</p> <p>No GPU Available - Tesseract (CPU OCR) - Qwen3-VL (API backend)</p> <p>Need Fast Processing (sub-second) - DocLayout-YOLO (0.1-0.2s) - PaddleOCR (0.3-1s)</p> <p>Multilingual Documents - Qwen3-VL (25+ languages) - Tesseract (100+ languages) - PaddleOCR (80+ languages)</p> <p>Custom Layout Categories - Qwen Layout (unlimited custom labels)</p>"},{"location":"models/#recommended-pipelines","title":"Recommended Pipelines","text":""},{"location":"models/#academic-paper-processing","title":"Academic Paper Processing","text":"<pre><code>1. Layout: DocLayoutYOLO (0.1-0.2s)\n2. Text: Qwen3-VL-8B (2-4s)\nResult: Structure + high-quality content\n</code></pre>"},{"location":"models/#batch-document-processing-1000s","title":"Batch Document Processing (1000s)","text":"<pre><code>1. Layout: DocLayoutYOLO (for structure)\n2. Text: DotsOCR + VLLM (2-4 GPUs)\nResult: 5-10k docs/hour\n</code></pre>"},{"location":"models/#handwritten-document-ocr","title":"Handwritten Document OCR","text":"<pre><code>1. OCR: Surya (handwriting)\n2. Layout: Qwen Layout (if needed)\nResult: 85%+ handwriting accuracy\n</code></pre>"},{"location":"models/#form-field-extraction","title":"Form Field Extraction","text":"<pre><code>1. Layout: Qwen Layout (custom field labels)\n2. OCR: Tesseract or EasyOCR per field\nResult: Automated form parsing\n</code></pre>"},{"location":"models/#cloud-deployment-no-gpu","title":"Cloud Deployment (No GPU)","text":"<pre><code>1. Text: Qwen3-VL (API backend)\n2. OCR: Tesseract (CPU)\nResult: Serverless processing via API\n</code></pre>"},{"location":"models/#performance-summary","title":"Performance Summary","text":""},{"location":"models/#speed-rankings-fastest-to-slowest","title":"Speed Rankings (Fastest to Slowest)","text":"<p>Text Extraction 1. Nanonuts (200+ tok/s) 2. DotsOCR (120 tok/s) 3. Qwen3-VL-2B (150 tok/s) 4. Qwen3-VL-8B (100 tok/s) 5. Qwen3-VL-32B (40 tok/s)</p> <p>Layout Detection 1. DocLayout-YOLO (0.1-0.2s/page) 2. RT-DETR (0.3-0.5s/page) 3. Qwen Layout (2-5s/page)</p> <p>OCR 1. PaddleOCR (0.3-1s/page) 2. EasyOCR (1-2s/page) 3. Surya (1-3s/page) 4. Tesseract (2-5s/page, CPU)</p>"},{"location":"models/#memory-rankings-smallest-to-largest","title":"Memory Rankings (Smallest to Largest)","text":"<ol> <li>Tesseract (CPU, minimal)</li> <li>DocLayout-YOLO (2-4 GB)</li> <li>PaddleOCR (2-4 GB)</li> <li>Qwen3-VL-2B (4 GB)</li> <li>Qwen3-VL-8B (16 GB)</li> <li>Qwen3-VL-32B (40 GB)</li> </ol>"},{"location":"models/#quality-rankings-best-to-good","title":"Quality Rankings (Best to Good)","text":"<p>Text: Qwen3-VL-32B &gt; Qwen3-VL-8B &gt; DotsOCR &gt; Nanonuts Layout: Qwen Layout = RT-DETR &gt; DocLayout-YOLO Handwriting: Surya &gt; PaddleOCR &gt; EasyOCR &gt; Tesseract Multilingual: Qwen3-VL &gt; Tesseract &gt; PaddleOCR</p>"},{"location":"models/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"models/#quick-start","title":"Quick Start","text":"<pre><code># Text extraction with PyTorch (most common)\npip install omnidocs[pytorch]\n\n# Layout detection (included with pytorch)\npip install omnidocs[pytorch]\n\n# OCR with Tesseract\npip install omnidocs[pytorch]\nbrew install tesseract  # macOS, or apt-get on Linux\n\n# All backends\npip install omnidocs[all]\n</code></pre>"},{"location":"models/#verify-installation","title":"Verify Installation","text":"<pre><code># Check available models\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO\nfrom omnidocs.tasks.ocr_extraction import TesseractOCR\n\nprint(\"\u2713 OmniDocs models available\")\n</code></pre>"},{"location":"models/#faq","title":"FAQ","text":"<p>Q: Which model should I use? A: See Comparison Guide for detailed decision matrix</p> <p>Q: How do I handle documents in multiple languages? A: Use Qwen3-VL (25+ languages) or Tesseract (100+ languages)</p> <p>Q: Can I run without GPU? A: Yes - Tesseract (OCR) and API backends (text) work on CPU</p> <p>Q: What's the fastest option? A: DocLayout-YOLO (0.1s) for layout, PaddleOCR (0.3s) for OCR</p> <p>Q: How much GPU memory do I need? A: 2-4 GB for layout only, 8-16 GB for text extraction, 16+ GB for batch processing</p> <p>Q: Can I use custom layout categories? A: Yes - with Qwen Layout (define unlimited custom labels)</p> <p>Q: Which model is best for production? A: DocLayout-YOLO + Qwen3-VL-8B on 2x A10 GPU</p>"},{"location":"models/#documentation-structure","title":"Documentation Structure","text":"<p>Each model documentation includes: - Model Overview - Architecture, capabilities, limitations - Installation &amp; Configuration - Setup and config parameters - Usage Examples - Copy-paste ready code for common tasks - Performance - Speed, memory, accuracy characteristics - Troubleshooting - Solutions for common issues - API Reference - Complete function and parameter documentation</p>"},{"location":"models/#related-documentation","title":"Related Documentation","text":"<ul> <li>OmniDocs Architecture</li> <li>Developer Guide</li> <li>API Reference</li> </ul>"},{"location":"models/#version-information","title":"Version Information","text":"<p>Documentation updated: January 2026 Models supported: - Qwen3-VL (all variants) - DotsOCR (latest) - DocLayout-YOLO-DocStructBench - RT-DETR (via references) - Tesseract (5.x+)</p>"},{"location":"models/#quick-links","title":"Quick Links","text":"<ul> <li>Text Extraction Qwen - Start here for high-quality text</li> <li>Layout Detection DocLayout-YOLO - Start here for speed</li> <li>Model Comparison - Compare all models</li> <li>Tesseract OCR - Open-source OCR option</li> </ul>"},{"location":"models/comparison/","title":"Model Comparison &amp; Selection Guide","text":"<p>A comprehensive comparison of all models available in OmniDocs to help you choose the right tool for your use case.</p>"},{"location":"models/comparison/#text-extraction-models","title":"Text Extraction Models","text":"<p>Models for extracting text content with optional formatting (Markdown/HTML/JSON).</p>"},{"location":"models/comparison/#feature-comparison","title":"Feature Comparison","text":"Feature Qwen3-VL DotsOCR Nanonuts Model Size 2B-32B ~7B ~7B Text Quality Excellent Very Good Very Good Layout Info Basic Detailed (11 cats) Not included Speed Medium Fast Fast Memory 4-40 GB 16 GB 12 GB Multilingual Yes (25+) Limited English-focused Backends PyTorch, VLLM, MLX, API PyTorch, VLLM PyTorch, VLLM Output Formats Markdown, HTML Markdown (with JSON layout) Markdown License Apache 2.0 Open Apache 2.0"},{"location":"models/comparison/#decision-matrix-text-extraction","title":"Decision Matrix: Text Extraction","text":"Use Case Best Choice Why High-quality multilingual docs Qwen3-VL Best text quality, many languages Need layout + text DotsOCR Detailed layout categories with text Fast, English docs Nanonuts Fastest, good quality for English Batch processing DotsOCR + VLLM Good speed with detailed output Cloud/API deployment Qwen3-VL (API) Only option with API backend Apple Silicon only Qwen3-VL (MLX) Only VLM with MLX support"},{"location":"models/comparison/#performance-comparison","title":"Performance Comparison","text":"Model Speed (tok/s) Quality Cost Qwen3-VL-2B 100-150 Good Low (small) Qwen3-VL-8B 50-100 Excellent Medium Qwen3-VL-32B 20-40 Outstanding High DotsOCR 80-120 Very Good Medium Nanonuts 150-200 Good Medium"},{"location":"models/comparison/#layout-analysis-models","title":"Layout Analysis Models","text":"<p>Models for detecting document structure and element regions.</p>"},{"location":"models/comparison/#feature-comparison_1","title":"Feature Comparison","text":"Feature DocLayout-YOLO RT-DETR Qwen Layout Architecture YOLOv10 DETR Vision-Language Speed Very Fast Fast Medium Categories 10 (fixed) 12+ (fixed) Unlimited (custom) Accuracy Good Excellent Excellent Memory 2-4 GB 4-8 GB 8-16 GB Backends PyTorch PyTorch PyTorch, VLLM, MLX, API Custom Labels No No Yes GPU Required Yes Yes Yes (practical) Best For Speed Accuracy Flexibility"},{"location":"models/comparison/#fixed-categories-comparison","title":"Fixed Categories Comparison","text":"<p>DocLayout-YOLO (10): Title, Plain text, Figure, Figure caption, Table, Table caption, Table footnote, Formula, Formula caption, Abandon</p> <p>RT-DETR (12+): Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title, (+ Extended: Document Index, Code, Checkboxes, Forms)</p> <p>Qwen Layout: Standard labels (10) + unlimited custom labels per use case</p>"},{"location":"models/comparison/#decision-matrix-layout-analysis","title":"Decision Matrix: Layout Analysis","text":"Use Case Best Choice Why Batch processing, speed critical DocLayout-YOLO Fastest (0.1-0.2s/page) Academic papers, high precision RT-DETR Excellent accuracy on papers Custom layout categories needed Qwen Layout Only option for custom labels Web page layout Qwen Layout Better understanding of semantic regions Form field detection Qwen Layout Can detect custom field types Production pipeline DocLayout-YOLO Proven, fast, stable"},{"location":"models/comparison/#speed-comparison","title":"Speed Comparison","text":"Model Per-Page Speed Device DocLayout-YOLO 0.1-0.2s Single A10 GPU RT-DETR 0.3-0.5s Single A10 GPU Qwen Layout 2-5s Single A10 GPU Qwen Layout (VLLM) 0.5-1.5s 2x A10 GPU"},{"location":"models/comparison/#ocr-models","title":"OCR Models","text":"<p>Models for extracting text with character/word-level bounding boxes.</p>"},{"location":"models/comparison/#feature-comparison_2","title":"Feature Comparison","text":"Feature Tesseract EasyOCR PaddleOCR Surya Type Traditional Deep Learning Deep Learning Deep Learning Speed Slow (CPU) Medium (GPU) Very Fast Medium Languages 100+ 80+ 80+ Multi Handwriting Poor Medium Medium Excellent GPU Required No Yes Yes Yes Memory CPU 4-6 GB 2-4 GB 6-8 GB Setup System install Python Python Python Accuracy High (printed) Good Excellent (CJK) Best overall"},{"location":"models/comparison/#character-detection-accuracy","title":"Character Detection Accuracy","text":"Model Latin Asian Handwriting Tesseract 95-99% 70-80% 30-50% EasyOCR 90-96% 85-92% 60-70% PaddleOCR 92-97% 94-99% 70-80% Surya 94-98% 88-95% 85-90%"},{"location":"models/comparison/#decision-matrix-ocr","title":"Decision Matrix: OCR","text":"Use Case Best Choice Why Printed English docs Tesseract Fastest (CPU), excellent accuracy Mixed scripts/languages PaddleOCR Best for Asian languages Handwritten documents Surya Best handwriting support Cloud deployment EasyOCR or PaddleOCR Easier setup than Tesseract No GPU available Tesseract Only CPU option Real-time processing PaddleOCR Fastest GPU inference"},{"location":"models/comparison/#performance-comparison_1","title":"Performance Comparison","text":"Model Speed Accuracy Cost Tesseract 2-5s (CPU) 95-99% (printed) Free EasyOCR 1-2s (GPU) 90-96% Free PaddleOCR 0.3-1s (GPU) 92-99% Free Surya 1-3s (GPU) 94-98% Free"},{"location":"models/comparison/#task-specific-recommendations","title":"Task-Specific Recommendations","text":""},{"location":"models/comparison/#use-case-academic-paper-processing","title":"Use Case: Academic Paper Processing","text":"<p>Goal: Extract text and layout from research papers</p> <p>Recommended Pipeline: 1. Layout: DocLayout-YOLO (fast, accurate for papers) 2. Text: Qwen3-VL-8B (high quality, multilingual) 3. Optional: DotsOCR if detailed layout needed</p> <p>Configuration: <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Fast layout detection\nlayout = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\n\n# High-quality text extraction\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n)\n\n# Process\nlayout_result = layout.extract(image)\ntext_result = extractor.extract(image)\n</code></pre></p> <p>Estimated Performance: 2-3 seconds per page, high accuracy</p>"},{"location":"models/comparison/#use-case-document-batch-processing","title":"Use Case: Document Batch Processing","text":"<p>Goal: Extract text from 1000s of documents quickly</p> <p>Recommended Pipeline: 1. Layout: DocLayout-YOLO (for batching) 2. Text: DotsOCR with VLLM (good quality, fast)</p> <p>Configuration: <pre><code># VLLM for batching\nfrom omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nextractor = DotsOCRTextExtractor(\n    backend=DotsOCRVLLMConfig(\n        tensor_parallel_size=2,  # 2 GPUs\n        gpu_memory_utilization=0.85,\n    )\n)\n\n# Process 100+ documents per hour\n</code></pre></p> <p>Estimated Performance: 5-10k documents/hour on 2x A10 GPU</p>"},{"location":"models/comparison/#use-case-handwritten-document-ocr","title":"Use Case: Handwritten Document OCR","text":"<p>Goal: Extract text from handwritten documents</p> <p>Recommended Pipeline: 1. OCR: Surya (best for handwriting) 2. Layout: Qwen Layout with custom labels (if needed)</p> <p>Configuration: <pre><code>from omnidocs.tasks.ocr_extraction import SuryaOCR, SuryaOCRConfig\n\nocr = SuryaOCR(config=SuryaOCRConfig(\n    languages=[\"en\"],\n    det_model=\"en\",\n))\n\nresult = ocr.extract(image)\n</code></pre></p> <p>Estimated Performance: 85%+ handwriting accuracy</p>"},{"location":"models/comparison/#use-case-form-field-extraction","title":"Use Case: Form Field Extraction","text":"<p>Goal: Extract text from form documents with custom field types</p> <p>Recommended Pipeline: 1. Layout: Qwen Layout with custom labels for field types 2. OCR: Tesseract or EasyOCR per field</p> <p>Configuration: <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\ncustom_labels = [\n    CustomLabel(name=\"text_field\"),\n    CustomLabel(name=\"checkbox\"),\n    CustomLabel(name=\"signature_line\"),\n]\n\ndetector = QwenLayoutDetector(\n    backend=QwenLayoutPyTorchConfig(device=\"cuda\")\n)\n\nresult = detector.extract(image, custom_labels=custom_labels)\n</code></pre></p> <p>Estimated Performance: Form processing in 5-10 seconds</p>"},{"location":"models/comparison/#use-case-multilingual-document-processing","title":"Use Case: Multilingual Document Processing","text":"<p>Goal: Process documents in 20+ languages</p> <p>Recommended Pipeline: 1. Text: Qwen3-VL-8B (25+ languages) 2. Fallback: PaddleOCR (80+ languages) for Asian scripts</p> <p>Configuration: <pre><code># Qwen handles most languages\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n)\n</code></pre></p> <p>Supported: English, French, German, Spanish, Russian, Chinese, Japanese, Korean, Arabic, Hindi, Portuguese, Dutch, Polish, Turkish, Greek, Thai, Vietnamese, and more.</p>"},{"location":"models/comparison/#use-case-real-time-document-processing","title":"Use Case: Real-Time Document Processing","text":"<p>Goal: Process documents with &lt;1 second latency</p> <p>Recommended Pipeline: 1. Layout: DocLayout-YOLO (0.1-0.2s) 2. Text: Fast OCR or small VLM</p> <p>Configuration: <pre><code># Fastest layout detection\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\nextractor = DocLayoutYOLO(config=DocLayoutYOLOConfig(\n    device=\"cuda\",\n    img_size=768,  # Smaller for speed\n))\n\nresult = extractor.extract(image)  # &lt;200ms\n</code></pre></p> <p>Estimated Performance: 0.2-1 second per page with layout only</p>"},{"location":"models/comparison/#use-case-cloud-deployment-no-gpu","title":"Use Case: Cloud Deployment (No GPU)","text":"<p>Goal: Deploy document processing in serverless environment</p> <p>Recommended Pipeline: 1. Layout: Not practical (needs GPU) 2. Text: Use API backend or Tesseract (CPU) 3. OCR: Tesseract (CPU-friendly)</p> <p>Configuration: <pre><code># API-based for cloud\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextAPIConfig\n\nextractor = QwenTextExtractor(\n    backend=QwenTextAPIConfig(\n        model=\"qwen3-vl-8b\",\n        api_key=os.getenv(\"QWEN_API_KEY\"),\n    )\n)\n\n# CPU-based OCR\nfrom omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\n\nocr = TesseractOCR(config=TesseractOCRConfig(languages=[\"eng\"]))\n</code></pre></p> <p>Estimated Cost: $0.01-0.10 per document via API</p>"},{"location":"models/comparison/#performance-summary-table","title":"Performance Summary Table","text":""},{"location":"models/comparison/#text-extraction-speed-larger-batch","title":"Text Extraction Speed (larger batch)","text":"Model 1 GPU 2 GPUs (VLLM) Tokens/Sec Qwen3-VL-2B 100-150 250-350 Per-doc Qwen3-VL-8B 50-100 150-250 Per-doc DotsOCR 80-120 200-300 Per-doc Nanonuts 150-200 400-500 Per-doc"},{"location":"models/comparison/#layout-detection-speed","title":"Layout Detection Speed","text":"Model Speed Device DocLayout-YOLO 0.1-0.2s A10 GPU RT-DETR 0.3-0.5s A10 GPU Qwen Layout (PyTorch) 2-5s A10 GPU Qwen Layout (VLLM) 0.5-1.5s 2x A10 GPU"},{"location":"models/comparison/#ocr-speed","title":"OCR Speed","text":"Model Speed (1024x768) Device Tesseract 2-3s CPU EasyOCR 1-2s GPU PaddleOCR 0.3-1s GPU Surya 1-3s GPU"},{"location":"models/comparison/#memory-requirements-summary","title":"Memory Requirements Summary","text":""},{"location":"models/comparison/#vram-requirements","title":"VRAM Requirements","text":"Task Minimal Recommended Optimal Text (Qwen-8B) 8 GB 16 GB 24 GB Layout (DocLayout) 2 GB 4 GB 8 GB Layout (Qwen) 8 GB 16 GB 24 GB OCR (GPU-based) 2 GB 4 GB 8 GB Multi-task pipeline 16 GB 32 GB 40 GB"},{"location":"models/comparison/#cpu-requirements","title":"CPU Requirements","text":"Model CPU Load Parallelization Tesseract Medium Thread-based (4+ cores) EasyOCR Light Not parallelizable DotsOCR Light GPU-bound"},{"location":"models/comparison/#cost-analysis","title":"Cost Analysis","text":""},{"location":"models/comparison/#deployment-costs-per-million-documents","title":"Deployment Costs (per million documents)","text":"Strategy GPU Cost Model Cost Total Self-hosted (PyTorch) $500/month Free $6k/year Self-hosted (VLLM batch) $1000/month Free $12k/year API-based None $1-2/doc $1-2M Hybrid (API + cached) Minimal $0.1-0.5/doc $100k-500k"},{"location":"models/comparison/#development-time","title":"Development Time","text":"Task Effort Models Needed Simple extraction 1 hour 1 (any VLM) Layout + text 2-4 hours 2 (layout + text) Custom layout 4-8 hours Qwen layout + fine-tuning Production pipeline 1-2 weeks 3+ with batching, caching"},{"location":"models/comparison/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"models/comparison/#q-which-model-is-fastest","title":"Q: Which model is fastest?","text":"<p>A: DocLayout-YOLO for layout (0.1-0.2s), PaddleOCR for OCR (0.3-1s), Nanonuts for text (50-80 tok/s)</p>"},{"location":"models/comparison/#q-which-is-most-accurate","title":"Q: Which is most accurate?","text":"<p>A: Qwen3-VL-32B for text, Surya for handwriting, RT-DETR for layout</p>"},{"location":"models/comparison/#q-which-requires-least-gpu","title":"Q: Which requires least GPU?","text":"<p>A: DocLayout-YOLO (2-4 GB), Tesseract (CPU-only)</p>"},{"location":"models/comparison/#q-which-supports-most-languages","title":"Q: Which supports most languages?","text":"<p>A: Tesseract (100+), Qwen (25+), PaddleOCR (80+)</p>"},{"location":"models/comparison/#q-which-is-cheapest-to-run","title":"Q: Which is cheapest to run?","text":"<p>A: Tesseract (free, CPU), DocLayout-YOLO (small GPU model)</p>"},{"location":"models/comparison/#q-best-for-real-time-sub-second","title":"Q: Best for real-time (sub-second)?","text":"<p>A: DocLayout-YOLO for layout only, or PaddleOCR for OCR</p>"},{"location":"models/comparison/#q-best-for-batch-processing","title":"Q: Best for batch processing?","text":"<p>A: DotsOCR or Qwen with VLLM (2-4 GPUs)</p>"},{"location":"models/comparison/#q-can-i-run-without-gpu","title":"Q: Can I run without GPU?","text":"<p>A: Yes - Tesseract (OCR) and API backends (text)</p>"},{"location":"models/comparison/#q-which-is-easiest-to-set-up","title":"Q: Which is easiest to set up?","text":"<p>A: Qwen with PyTorch (single pip install)</p>"},{"location":"models/comparison/#q-production-recommendation","title":"Q: Production recommendation?","text":"<p>A: DocLayout-YOLO + Qwen3-VL-8B on 2x A10 GPU</p>"},{"location":"models/comparison/#migration-guide","title":"Migration Guide","text":""},{"location":"models/comparison/#from-tesseract-to-modern-ocr","title":"From Tesseract to Modern OCR","text":"<pre><code># Old: Tesseract only\nfrom omnidocs.tasks.ocr_extraction import TesseractOCR\n\n# New: Choose based on use case\nfrom omnidocs.tasks.ocr_extraction import (\n    TesseractOCR,  # Printed, many languages\n    PaddleOCR,     # Speed, Asian languages\n    SuryaOCR,      # Handwriting\n)\n</code></pre>"},{"location":"models/comparison/#from-single-model-to-pipeline","title":"From Single-Model to Pipeline","text":"<pre><code># Old: Text extraction only\ntext = extract_text(image)\n\n# New: Layout + text pipeline\nlayout = detect_layout(image)  # Understand structure\ntext = extract_text(image)     # Extract content\n# Combine results for better processing\n</code></pre>"},{"location":"models/comparison/#from-cpu-to-gpu","title":"From CPU to GPU","text":"<pre><code># Old: CPU-based\nocr = TesseractOCR()  # 2-3s per page\n\n# New: GPU-accelerated\nocr = PaddleOCR()     # 0.3-1s per page (10x faster)\n</code></pre>"},{"location":"models/comparison/#see-also","title":"See Also","text":"<ul> <li>Qwen Text Extraction</li> <li>DotsOCR Text Extraction</li> <li>DocLayout-YOLO</li> <li>Qwen Layout Detection</li> <li>Tesseract OCR</li> </ul>"},{"location":"models/layout-analysis/doclayout-yolo/","title":"DocLayout-YOLO Layout Detection","text":""},{"location":"models/layout-analysis/doclayout-yolo/#model-overview","title":"Model Overview","text":"<p>DocLayout-YOLO is a YOLO-based (You Only Look Once) object detector specifically optimized for document layout analysis. It's the fastest layout detection model in OmniDocs, making it ideal for processing large document collections.</p> <p>Model ID: juliozhao/DocLayout-YOLO-DocStructBench Architecture: YOLOv10 (object detection) Training Focus: Academic papers, technical documents, arXiv papers Framework: PyTorch only (no other backends)</p>"},{"location":"models/layout-analysis/doclayout-yolo/#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Fast Inference: 0.1-0.3s per page (fastest in OmniDocs)</li> <li>10 Layout Categories: Title, text, figures, tables, formulas, captions, etc.</li> <li>Fixed Labels: Standardized output across all documents</li> <li>Document-Optimized: Trained on 100K+ academic papers</li> <li>Confidence Scores: Per-detection confidence for filtering</li> </ul>"},{"location":"models/layout-analysis/doclayout-yolo/#limitations","title":"Limitations","text":"<ul> <li>PyTorch only: No VLLM, MLX, or API backends</li> <li>GPU required: No CPU inference (YOLO needs GPU)</li> <li>Fixed categories: Cannot customize labels</li> <li>English-focused: Optimized for English documents</li> <li>Specialized: Best for academic/technical documents</li> <li>Layout only: Does not extract text content (use with OCR/VLM)</li> </ul>"},{"location":"models/layout-analysis/doclayout-yolo/#installation-configuration","title":"Installation &amp; Configuration","text":""},{"location":"models/layout-analysis/doclayout-yolo/#basic-installation","title":"Basic Installation","text":"<pre><code># Install with layout analysis support\npip install omnidocs[pytorch]\n\n# Specifically install doclayout-yolo\npip install doclayout-yolo\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#configuration","title":"Configuration","text":"<pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\nconfig = DocLayoutYOLOConfig(\n    device=\"cuda\",           # GPU required\n    model_path=None,         # Auto-download from HuggingFace\n    img_size=1024,           # Input image size\n    confidence=0.25,         # Detection confidence threshold\n)\n\nextractor = DocLayoutYOLO(config=config)\n</code></pre> <p>Config Parameters:</p> Parameter Type Default Description <code>device</code> str \"cuda\" Device: \"cuda\", \"mps\", \"cpu\" (GPU required) <code>model_path</code> str None Path to model weights, or None for auto-download <code>img_size</code> int 1024 Input size for inference (320-1920) <code>confidence</code> float 0.25 Confidence threshold (0-1, higher = stricter)"},{"location":"models/layout-analysis/doclayout-yolo/#layout-categories-10-fixed","title":"Layout Categories (10 Fixed)","text":"<p>DocLayout-YOLO detects exactly 10 layout element types:</p> Category Description Common Content Title Document/section title \"Introduction\", \"Methods\" Plain text Body paragraph Main content paragraphs Figure Image/diagram (content region) Graphs, plots, photos Figure caption Caption for figures \"Fig. 1: System Overview\" Table Tabular data (content region) Data tables, matrices Table caption Caption for tables \"Table 2: Performance Results\" Table footnote Notes under tables Footnotes, explanations Formula Isolated equation Display math: $E=mc^2$ Formula caption Caption for formulas \"Equation 3.1: Distance metric\" Abandon Elements to ignore Watermarks, page numbers, artifacts"},{"location":"models/layout-analysis/doclayout-yolo/#usage-examples","title":"Usage Examples","text":""},{"location":"models/layout-analysis/doclayout-yolo/#basic-layout-detection","title":"Basic Layout Detection","text":"<pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom PIL import Image\n\n# Initialize\nconfig = DocLayoutYOLOConfig(device=\"cuda\", confidence=0.3)\nextractor = DocLayoutYOLO(config=config)\n\n# Load image\nimage = Image.open(\"document.png\")\n\n# Extract layout\nresult = extractor.extract(image)\n\n# Access results\nprint(f\"Found {result.element_count} elements\")\nprint(f\"Labels: {result.labels_found}\")\n\nfor box in result.bboxes:\n    print(f\"  {box.label.value}: confidence={box.confidence:.2f}\")\n    print(f\"    bbox={box.bbox.to_list()}\")\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#filter-by-confidence","title":"Filter by Confidence","text":"<pre><code># Keep only high-confidence detections\nhigh_conf = result.filter_by_confidence(min_confidence=0.5)\n\nfor box in high_conf:\n    print(f\"{box.label.value} ({box.confidence:.2%})\")\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#filter-by-label","title":"Filter by Label","text":"<pre><code>from omnidocs.tasks.layout_extraction import LayoutLabel\n\n# Extract only text regions\ntext_boxes = result.filter_by_label(LayoutLabel.TEXT)\nprint(f\"Found {len(text_boxes)} text blocks\")\n\n# Extract only figures\nfigures = result.filter_by_label(LayoutLabel.FIGURE)\nfor fig in figures:\n    x1, y1, x2, y2 = fig.bbox.to_xyxy()\n    width = x2 - x1\n    height = y2 - y1\n    print(f\"Figure: {width}x{height} at ({x1}, {y1})\")\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#normalized-coordinates","title":"Normalized Coordinates","text":"<pre><code># Get bounding boxes normalized to 0-1024 scale\nnormalized = result.get_normalized_bboxes()\n\nfor box_dict in normalized:\n    print(f\"{box_dict['label']}:\")\n    print(f\"  bbox (0-1024): {box_dict['bbox']}\")\n    print(f\"  confidence: {box_dict['confidence']:.2f}\")\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#visualization","title":"Visualization","text":"<pre><code>from PIL import Image\n\n# Load original image\nimage = Image.open(\"document.png\")\n\n# Create visualization with bounding boxes\nviz = result.visualize(\n    image,\n    output_path=\"layout_visualization.png\",\n    show_labels=True,\n    show_confidence=True,\n    line_width=2,\n)\n\n# Display\nviz.show()\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#batch-processing","title":"Batch Processing","text":"<pre><code>from pathlib import Path\nimport json\n\n# Process multiple documents\ndoc_dir = Path(\"documents/\")\nresults = {}\n\nfor img_path in sorted(doc_dir.glob(\"*.png\")):\n    print(f\"Processing {img_path.name}...\")\n    image = Image.open(img_path)\n    layout = extractor.extract(image)\n\n    results[img_path.name] = layout.to_dict()\n\n# Save results\nwith open(\"layout_results.json\", \"w\") as f:\n    json.dump(results, f, indent=2)\n\n# Summary statistics\ntotal_elements = sum(r[\"element_count\"] for r in results.values())\navg_elements = total_elements / len(results)\nprint(f\"Total elements: {total_elements}\")\nprint(f\"Average per document: {avg_elements:.1f}\")\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"models/layout-analysis/doclayout-yolo/#speed-comparison","title":"Speed Comparison","text":"Device Image Size Time A10G GPU 1024x1024 0.1-0.2s A10G GPU 2048x2048 0.3-0.5s CPU 1024x1024 5-10s CPU 2048x2048 15-30s"},{"location":"models/layout-analysis/doclayout-yolo/#memory-requirements","title":"Memory Requirements","text":"Batch Size VRAM Device 1 (single) 2-4 GB A10G 2-4 4-8 GB A10G 1 1-2 GB A100"},{"location":"models/layout-analysis/doclayout-yolo/#typical-detection-counts","title":"Typical Detection Counts","text":"Document Type Elements Speed Single page paper 10-30 0.1s Research paper (5pp) 50-150 0.5s Scanned book page 20-40 0.15s"},{"location":"models/layout-analysis/doclayout-yolo/#troubleshooting","title":"Troubleshooting","text":""},{"location":"models/layout-analysis/doclayout-yolo/#model-download-issues","title":"Model Download Issues","text":"<p>Symptom: Model fails to download on first run</p> <p>Solution:</p> <pre><code># Set cache directory\nimport os\nos.environ[\"HF_HOME\"] = \"/path/to/cache\"\n\n# Pre-download the model\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\"juliozhao/DocLayout-YOLO-DocStructBench\")\n\n# Now use extractor (will use cached model)\nextractor = DocLayoutYOLO(config=config)\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#confidence-threshold-tuning","title":"Confidence Threshold Tuning","text":"<p>Symptom: Too many false positives OR missing real elements</p> <p>Solutions:</p> <pre><code># Too many false positives \u2192 increase confidence\nconfig = DocLayoutYOLOConfig(confidence=0.5)  # Stricter\n\n# Missing elements \u2192 decrease confidence\nconfig = DocLayoutYOLOConfig(confidence=0.1)  # More lenient\n\n# Find optimal threshold\nfrom PIL import Image\nimage = Image.open(\"test.png\")\n\nfor conf in [0.1, 0.25, 0.5, 0.75]:\n    config = DocLayoutYOLOConfig(confidence=conf)\n    extractor = DocLayoutYOLO(config=config)\n    result = extractor.extract(image)\n    print(f\"Confidence {conf}: {result.element_count} elements\")\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#image-size-issues","title":"Image Size Issues","text":"<p>Symptom: Poor detection on very large or small images</p> <p>Solutions:</p> <pre><code>from PIL import Image\n\nimage = Image.open(\"document.png\")\nprint(f\"Original size: {image.size}\")\n\n# Resize to standard size for better detection\ntarget_size = 1024\nimage.thumbnail((target_size, target_size), Image.Resampling.LANCZOS)\n\nresult = extractor.extract(image)\nprint(f\"Found {result.element_count} elements\")\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#model-selection-guide","title":"Model Selection Guide","text":""},{"location":"models/layout-analysis/doclayout-yolo/#when-to-use-doclayout-yolo","title":"When to Use DocLayout-YOLO","text":"<p>Best for: - Fast batch processing of large document collections - Academic papers and technical documents - When speed is critical (real-time requirements) - You need layout detection only (will add OCR separately)</p> <p>Not ideal for: - Extracting actual text (use Qwen or DotsOCR) - Complex/unusual layouts (use Qwen layout detector) - Handwritten documents (use Surya) - When you need custom layout categories (use Qwen)</p>"},{"location":"models/layout-analysis/doclayout-yolo/#doclayout-yolo-vs-other-layout-models","title":"DocLayout-YOLO vs Other Layout Models","text":"Feature DocLayout-YOLO RT-DETR Qwen Layout Speed Very Fast Fast Medium Categories 10 (fixed) 12+ (fixed) Unlimited (custom) Backend PyTorch only PyTorch Multi-backend Memory 2-4 GB 4-8 GB 8-16 GB Quality Good Excellent Excellent Use Case Fast detection Precision Flexibility <p>Choose DocLayout-YOLO if: You need fast detection for batch processing Choose Qwen Layout if: You need flexible categories or better quality</p>"},{"location":"models/layout-analysis/doclayout-yolo/#api-reference","title":"API Reference","text":""},{"location":"models/layout-analysis/doclayout-yolo/#doclayoutyoloextract","title":"DocLayoutYOLO.extract()","text":"<pre><code>def extract(image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#layoutoutput-properties","title":"LayoutOutput Properties","text":"<pre><code>result = extractor.extract(image)\n\n# Basic properties\nresult.bboxes              # List[LayoutBox] - all detections\nresult.element_count       # Number of elements\nresult.labels_found        # Set of unique labels\nresult.image_width         # Source image width\nresult.image_height        # Source image height\nresult.model_name          # \"DocLayout-YOLO\"\n\n# Filter methods\nresult.filter_by_label(label)        # Filter by LayoutLabel\nresult.filter_by_confidence(min_conf) # Filter by confidence\n\n# Coordinate conversion\nresult.get_normalized_bboxes()  # Convert to 0-1024 scale\nresult.sort_by_position()       # Sort by reading order\n\n# Export\nresult.to_dict()           # Convert to dictionary\nresult.visualize(image)    # Create visualization\nresult.save_json(path)     # Save to JSON file\nresult.load_json(path)     # Load from JSON file\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#layoutbox-properties","title":"LayoutBox Properties","text":"<pre><code>for box in result.bboxes:\n    box.label             # LayoutLabel enum\n    box.bbox              # BoundingBox object\n    box.confidence        # float (0-1)\n    box.class_id          # int - YOLO class ID\n    box.original_label    # str - original YOLO label\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#boundingbox-methods","title":"BoundingBox Methods","text":"<pre><code>bbox = box.bbox\n\n# Access coordinates\nbbox.x1, bbox.y1          # Top-left corner\nbbox.x2, bbox.y2          # Bottom-right corner\nbbox.width                # Width in pixels\nbbox.height               # Height in pixels\nbbox.area                 # Area in pixels\u00b2\nbbox.center               # (center_x, center_y) tuple\n\n# Convert formats\nbbox.to_list()            # [x1, y1, x2, y2]\nbbox.to_xyxy()            # (x1, y1, x2, y2)\nbbox.to_xywh()            # (x, y, width, height)\n\n# Normalize to 0-1024 range\nnormalized = bbox.to_normalized(image_width, image_height)\n\n# Convert back to absolute\nabsolute = normalized.to_absolute(image_width, image_height)\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#advanced-usage","title":"Advanced Usage","text":""},{"location":"models/layout-analysis/doclayout-yolo/#reading-order-detection","title":"Reading Order Detection","text":"<pre><code># DocLayout-YOLO automatically sorts by position (top to bottom, left to right)\nsorted_result = result.sort_by_position(top_to_bottom=True)\n\nfor i, box in enumerate(sorted_result.bboxes):\n    print(f\"{i+1}. {box.label.value} at ({box.bbox.y1:.0f}, {box.bbox.x1:.0f})\")\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#region-based-processing","title":"Region-Based Processing","text":"<pre><code># Get all elements in upper half of page\nupper_half = [\n    box for box in result.bboxes\n    if box.bbox.y1 &lt; result.image_height // 2\n]\n\n# Get all large elements (&gt; 1/4 page width)\npage_width = result.image_width\nlarge_elements = [\n    box for box in result.bboxes\n    if box.bbox.width &gt; page_width // 4\n]\n\nprint(f\"Upper half: {len(upper_half)} elements\")\nprint(f\"Large: {len(large_elements)} elements\")\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#export-to-different-formats","title":"Export to Different Formats","text":"<pre><code># Save as JSON for downstream processing\nresult.save_json(\"layout.json\")\n\n# Convert to dict for custom serialization\nlayout_dict = result.to_dict()\n\n# Export to COCO format (for computer vision tools)\ncoco_format = {\n    \"images\": [{\n        \"id\": 0,\n        \"width\": result.image_width,\n        \"height\": result.image_height,\n        \"file_name\": \"document.png\"\n    }],\n    \"annotations\": [\n        {\n            \"id\": i,\n            \"image_id\": 0,\n            \"category_id\": box.class_id,\n            \"bbox\": list(box.bbox.to_xywh()),  # COCO format: [x, y, w, h]\n            \"area\": box.bbox.area,\n            \"iscrowd\": 0,\n        }\n        for i, box in enumerate(result.bboxes)\n    ],\n}\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#integration-with-text-extraction","title":"Integration with Text Extraction","text":""},{"location":"models/layout-analysis/doclayout-yolo/#pipeline-layout-ocr","title":"Pipeline: Layout + OCR","text":"<pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\nfrom PIL import Image\n\n# Step 1: Detect layout\nlayout_result = extractor.extract(image)\n\n# Step 2: Extract text from regions\nocr = TesseractOCR(config=TesseractOCRConfig(languages=[\"eng\"]))\n\nfor box in layout_result.bboxes:\n    if box.label.value in [\"text\", \"title\"]:\n        # Crop region\n        x1, y1, x2, y2 = box.bbox.to_xyxy()\n        region = image.crop((x1, y1, x2, y2))\n\n        # OCR the region\n        ocr_result = ocr.extract(region)\n\n        print(f\"{box.label.value}: {ocr_result.full_text}\")\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#pipeline-layout-vlm","title":"Pipeline: Layout + VLM","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Step 1: Detect layout\nlayout_result = extractor.extract(image)\n\n# Step 2: Extract text per element\nextractor_qwen = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(device=\"cuda\")\n)\n\nfor i, box in enumerate(layout_result.bboxes):\n    # Crop region\n    x1, y1, x2, y2 = box.bbox.to_xyxy()\n    region = image.crop((x1, y1, x2, y2))\n\n    # Extract with Qwen\n    result = extractor_qwen.extract(region)\n\n    print(f\"Element {i} ({box.label.value}):\")\n    print(result.content)\n    print()\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#see-also","title":"See Also","text":"<ul> <li>RT-DETR Layout Detection - Alternative DETR-based model</li> <li>Qwen Layout Detection - For custom categories</li> <li>Comparison Guide - Model selection matrix</li> <li>YOLOv10 Paper</li> </ul>"},{"location":"models/layout-analysis/qwen-layout/","title":"Qwen3-VL Layout Detection","text":""},{"location":"models/layout-analysis/qwen-layout/#model-overview","title":"Model Overview","text":"<p>Qwen3-VL is a Vision-Language Model that can perform flexible layout detection beyond fixed label sets. Unlike DocLayout-YOLO or RT-DETR, Qwen supports custom layout categories while maintaining high accuracy for standard layout analysis tasks.</p> <p>Model Family: Qwen3-VL-2B, Qwen3-VL-4B, Qwen3-VL-8B, Qwen3-VL-32B Repository: Qwen/Qwen3-VL Architecture: Vision Encoder + Language Model Key Feature: Flexible custom labels for domain-specific layout detection</p>"},{"location":"models/layout-analysis/qwen-layout/#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Custom Labels: Define unlimited layout categories beyond standard types</li> <li>VLM-Based: Understands semantic meaning of regions</li> <li>High Accuracy: Better handling of complex/unusual layouts</li> <li>Multi-Backend: PyTorch, VLLM, MLX, API support</li> <li>Confidence Scores: Per-detection confidence for filtering</li> <li>Multilingual: Works with documents in any language</li> </ul>"},{"location":"models/layout-analysis/qwen-layout/#limitations","title":"Limitations","text":"<ul> <li>Slower than YOLO: 5-10x slower than DocLayout-YOLO</li> <li>Requires GPU: No CPU inference practical</li> <li>Memory intensive: 8-16 GB VRAM minimum</li> <li>Less standardized: Labels are user-defined, not fixed enum</li> </ul>"},{"location":"models/layout-analysis/qwen-layout/#supported-backends","title":"Supported Backends","text":"<p>Qwen layout detection supports 4 inference backends:</p> Backend Use Case Speed Setup PyTorch Single document, development 20-50 tok/s Easy VLLM Batch processing 80-150 tok/s Multi-GPU MLX Apple Silicon 10-30 tok/s macOS M1/M3+ API Cloud inference Variable Hosted"},{"location":"models/layout-analysis/qwen-layout/#installation-configuration","title":"Installation &amp; Configuration","text":""},{"location":"models/layout-analysis/qwen-layout/#basic-installation","title":"Basic Installation","text":"<pre><code># Install with PyTorch backend (most common)\npip install omnidocs[pytorch]\n\n# Or with all backends\npip install omnidocs[all]\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#pytorch-backend-configuration","title":"PyTorch Backend Configuration","text":"<pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\nconfig = QwenLayoutPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cuda\",\n    torch_dtype=\"bfloat16\",\n    device_map=\"auto\",\n    max_new_tokens=4096,\n    temperature=0.1,\n)\n\ndetector = QwenLayoutDetector(backend=config)\n</code></pre> <p>PyTorch Config Parameters:</p> Parameter Type Default Description <code>model</code> str \"Qwen/Qwen3-VL-8B-Instruct\" HuggingFace model ID <code>device</code> str \"cuda\" Device: \"cuda\", \"mps\", \"cpu\" <code>torch_dtype</code> str \"auto\" Data type: \"float16\", \"bfloat16\", \"float32\", \"auto\" <code>device_map</code> str \"auto\" Model parallelism strategy <code>use_flash_attention</code> bool False Use Flash Attention 2 (if available) <code>max_new_tokens</code> int 4096 Max tokens to generate <code>temperature</code> float 0.1 Sampling temperature (deterministic output)"},{"location":"models/layout-analysis/qwen-layout/#vllm-backend-configuration","title":"VLLM Backend Configuration","text":"<pre><code>from omnidocs.tasks.layout_extraction.qwen import QwenLayoutVLLMConfig\n\nconfig = QwenLayoutVLLMConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    tensor_parallel_size=1,\n    gpu_memory_utilization=0.9,\n    max_model_len=4096,\n)\n\ndetector = QwenLayoutDetector(backend=config)\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#mlx-backend-configuration-apple-silicon","title":"MLX Backend Configuration (Apple Silicon)","text":"<pre><code>from omnidocs.tasks.layout_extraction.qwen import QwenLayoutMLXConfig\n\nconfig = QwenLayoutMLXConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct-MLX\",\n    quantization=\"4bit\",\n    max_tokens=4096,\n)\n\ndetector = QwenLayoutDetector(backend=config)\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#api-backend-configuration","title":"API Backend Configuration","text":"<pre><code>from omnidocs.tasks.layout_extraction.qwen import QwenLayoutAPIConfig\n\nconfig = QwenLayoutAPIConfig(\n    model=\"qwen3-vl-8b\",\n    api_key=\"your-api-key\",\n    base_url=\"https://api.provider.com/v1\",\n)\n\ndetector = QwenLayoutDetector(backend=config)\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#standard-label-detection","title":"Standard Label Detection","text":""},{"location":"models/layout-analysis/qwen-layout/#using-fixed-standard-labels","title":"Using Fixed Standard Labels","text":"<pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, LayoutLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\nfrom PIL import Image\n\n# Initialize\nconfig = QwenLayoutPyTorchConfig(device=\"cuda\")\ndetector = QwenLayoutDetector(backend=config)\n\n# Load image\nimage = Image.open(\"document.png\")\n\n# Detect standard layout (no custom labels)\nresult = detector.extract(image)\n\n# Access standard labels\nprint(f\"Found {result.element_count} elements\")\nfor box in result.bboxes:\n    print(f\"  {box.label.value}: confidence={box.confidence:.2f}\")\n\n# Filter by standard label type\ntitles = result.filter_by_label(LayoutLabel.TITLE)\ntext_blocks = result.filter_by_label(LayoutLabel.TEXT)\nfigures = result.filter_by_label(LayoutLabel.FIGURE)\ntables = result.filter_by_label(LayoutLabel.TABLE)\n\nprint(f\"Titles: {len(titles)}\")\nprint(f\"Text blocks: {len(text_blocks)}\")\nprint(f\"Figures: {len(figures)}\")\nprint(f\"Tables: {len(tables)}\")\n</code></pre> <p>Standard Layout Labels:</p> Label Description <code>LayoutLabel.TITLE</code> Document/section title <code>LayoutLabel.TEXT</code> Body text paragraph <code>LayoutLabel.LIST</code> Bulleted/numbered list <code>LayoutLabel.FIGURE</code> Image, diagram, plot <code>LayoutLabel.TABLE</code> Tabular data <code>LayoutLabel.CAPTION</code> Figure/table caption <code>LayoutLabel.FORMULA</code> Mathematical equation <code>LayoutLabel.FOOTNOTE</code> Footer note <code>LayoutLabel.PAGE_HEADER</code> Page header <code>LayoutLabel.PAGE_FOOTER</code> Page footer"},{"location":"models/layout-analysis/qwen-layout/#custom-label-detection","title":"Custom Label Detection","text":""},{"location":"models/layout-analysis/qwen-layout/#define-custom-labels","title":"Define Custom Labels","text":"<pre><code>from omnidocs.tasks.layout_extraction import CustomLabel\n\n# Simple custom labels\ncode_block = CustomLabel(name=\"code_block\")\nsidebar = CustomLabel(name=\"sidebar\")\nannotation = CustomLabel(name=\"annotation\")\n\n# Labels with metadata\nabstract = CustomLabel(\n    name=\"abstract\",\n    description=\"Document abstract or summary\",\n    color=\"#E8F4F8\",\n    detection_prompt=\"Look for abstract sections, usually after title\",\n)\n\nrelated_work = CustomLabel(\n    name=\"related_work\",\n    description=\"Related work or background section\",\n    color=\"#FFF3CD\",\n)\n\n# Create list of custom labels\ncustom_labels = [code_block, sidebar, abstract, related_work]\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#extract-with-custom-labels","title":"Extract with Custom Labels","text":"<pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\nfrom PIL import Image\n\ndetector = QwenLayoutDetector(\n    backend=QwenLayoutPyTorchConfig(device=\"cuda\")\n)\n\nimage = Image.open(\"document.png\")\n\n# Detect with custom labels\nresult = detector.extract(\n    image,\n    custom_labels=[code_block, sidebar, abstract],\n)\n\n# Access detections\nfor box in result.bboxes:\n    print(f\"Label: {box.label}\")  # Custom label name\n    print(f\"Bbox: {box.bbox.to_list()}\")\n    print(f\"Confidence: {box.confidence}\")\n    print()\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#mixed-standard-and-custom-labels","title":"Mixed Standard and Custom Labels","text":"<pre><code>from omnidocs.tasks.layout_extraction import LayoutLabel, CustomLabel\n\n# Combine standard and custom labels\nstandard_labels = [LayoutLabel.TITLE, LayoutLabel.TEXT]\ncustom_labels = [\n    CustomLabel(name=\"code_example\"),\n    CustomLabel(name=\"warning_box\"),\n]\n\n# Detect with both\nresult = detector.extract(\n    image,\n    custom_labels=custom_labels,  # Standard labels always included\n)\n\n# All labels present in result\nfor box in result.bboxes:\n    print(f\"Detected: {box.label}\")\n    # Could be: title, text, code_example, warning_box\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#usage-examples","title":"Usage Examples","text":""},{"location":"models/layout-analysis/qwen-layout/#example-1-academic-paper-layout","title":"Example 1: Academic Paper Layout","text":"<pre><code>from omnidocs.tasks.layout_extraction import CustomLabel\nfrom PIL import Image\n\n# Define academic paper custom labels\ncustom_labels = [\n    CustomLabel(\n        name=\"abstract\",\n        description=\"Abstract section\",\n        detection_prompt=\"Find the abstract after the title\",\n    ),\n    CustomLabel(\n        name=\"methodology\",\n        description=\"Methods and experimental setup\",\n    ),\n    CustomLabel(\n        name=\"results_table\",\n        description=\"Results presented as table\",\n    ),\n    CustomLabel(\n        name=\"reference\",\n        description=\"Bibliography and references\",\n    ),\n]\n\n# Detect layout\nresult = detector.extract(\n    image,\n    custom_labels=custom_labels,\n)\n\n# Extract sections\nfor section in custom_labels:\n    elements = [\n        box for box in result.bboxes\n        if box.label == section.name\n    ]\n    if elements:\n        print(f\"Found {section.name}: {len(elements)} element(s)\")\n        for elem in elements:\n            print(f\"  Position: {elem.bbox.to_list()}\")\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#example-2-website-layout-analysis","title":"Example 2: Website Layout Analysis","text":"<pre><code># For web page screenshots\ncustom_labels = [\n    CustomLabel(name=\"header\", description=\"Top navigation bar\"),\n    CustomLabel(name=\"sidebar\", description=\"Left/right sidebar\"),\n    CustomLabel(name=\"main_content\", description=\"Primary content area\"),\n    CustomLabel(name=\"advertisement\", description=\"Ad placement\"),\n    CustomLabel(name=\"footer\", description=\"Footer section\"),\n]\n\nresult = detector.extract(image, custom_labels=custom_labels)\n\n# Map to regions\nregions = {label.name: [] for label in custom_labels}\nfor box in result.bboxes:\n    if box.label in regions:\n        regions[box.label].append(box)\n\nfor region_name, boxes in regions.items():\n    print(f\"{region_name}: {len(boxes)} element(s)\")\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#example-3-form-field-detection","title":"Example 3: Form Field Detection","text":"<pre><code># For forms and structured documents\nform_labels = [\n    CustomLabel(name=\"text_field\", description=\"Text input field\"),\n    CustomLabel(name=\"checkbox\", description=\"Checkbox option\"),\n    CustomLabel(name=\"radio_button\", description=\"Radio button\"),\n    CustomLabel(name=\"dropdown\", description=\"Dropdown select\"),\n    CustomLabel(name=\"required_field\", description=\"Field marked as required (*)\"),\n]\n\nresult = detector.extract(image, custom_labels=form_labels)\n\n# Count field types\nfield_counts = {}\nfor box in result.bboxes:\n    label = str(box.label)\n    field_counts[label] = field_counts.get(label, 0) + 1\n\nfor field_type, count in field_counts.items():\n    print(f\"  {field_type}: {count}\")\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"models/layout-analysis/qwen-layout/#speed-comparison-with-doclayout-yolo","title":"Speed Comparison with DocLayout-YOLO","text":"Model Speed Trade-offs DocLayout-YOLO 0.1-0.2s/page Fast but fixed labels Qwen Layout (PyTorch) 2-5s/page Slower but flexible Qwen Layout (VLLM) 0.5-1.5s/page Better speed with batching"},{"location":"models/layout-analysis/qwen-layout/#memory-requirements","title":"Memory Requirements","text":"Backend Min VRAM Typical Batch PyTorch 8 GB 16 GB 1 VLLM 12 GB 24 GB 2-4 MLX 8 GB 16 GB 1"},{"location":"models/layout-analysis/qwen-layout/#troubleshooting","title":"Troubleshooting","text":""},{"location":"models/layout-analysis/qwen-layout/#custom-labels-not-detected","title":"Custom Labels Not Detected","text":"<p>Symptom: Custom labels return 0 detections</p> <p>Solutions:</p> <pre><code># 1. Provide more detailed descriptions\ncustom_labels = [\n    CustomLabel(\n        name=\"code_block\",\n        description=\"Monospaced font code/programming examples in gray background\",\n        detection_prompt=\"Look for gray-boxed code sections with monospaced text\",\n    ),\n]\n\n# 2. Reduce temperature for more confident predictions\nconfig = QwenLayoutPyTorchConfig(\n    temperature=0.0,  # Most deterministic\n)\n\n# 3. Use larger model variant\nconfig = QwenLayoutPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-32B-Instruct\",\n)\n\n# 4. Check with standard labels first (confidence building)\nresult = detector.extract(image)  # No custom labels\nprint(f\"Found {result.element_count} standard elements\")\n# Then try with custom\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#memory-issues","title":"Memory Issues","text":"<p>Symptom: CUDA out of memory</p> <p>Solutions:</p> <pre><code># Use smaller model\nconfig = QwenLayoutPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-4B-Instruct\",\n)\n\n# Reduce max_new_tokens\nconfig = QwenLayoutPyTorchConfig(\n    max_new_tokens=2048,\n)\n\n# Enable quantization\nconfig = QwenLayoutPyTorchConfig(\n    load_in_4bit=True,\n)\n\n# Use CPU (slow but works)\nconfig = QwenLayoutPyTorchConfig(\n    device=\"cpu\",\n)\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#qwen-layout-vs-doclayout-yolo","title":"Qwen Layout vs DocLayout-YOLO","text":"Aspect Qwen Layout DocLayout-YOLO Custom Labels Yes (unlimited) No (10 fixed) Speed Slower Very fast Accuracy Higher Good Memory 8-16 GB 2-4 GB Backends 4 (PyTorch, VLLM, MLX, API) 1 (PyTorch) Best For Flexibility, custom layouts Speed, batch processing <p>Choose Qwen if: You need custom layout categories or better accuracy Choose DocLayout-YOLO if: You need speed and can use fixed categories</p>"},{"location":"models/layout-analysis/qwen-layout/#api-reference","title":"API Reference","text":""},{"location":"models/layout-analysis/qwen-layout/#qwenlayoutdetectorextract","title":"QwenLayoutDetector.extract()","text":"<pre><code>def extract(\n    image: Union[Image.Image, np.ndarray, str, Path],\n    custom_labels: Optional[List[CustomLabel]] = None,\n) -&gt; LayoutOutput:\n    \"\"\"\n    Extract layout from image with optional custom labels.\n\n    Args:\n        image: Input image\n        custom_labels: List of CustomLabel objects for flexible detection\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#layoutoutput-properties-standard-labels","title":"LayoutOutput Properties (Standard Labels)","text":"<pre><code>result = detector.extract(image)\n\n# Basic info\nresult.bboxes              # List[LayoutBox]\nresult.element_count       # Total detections\nresult.labels_found        # List of detected labels\n\n# Filter by label\nresult.filter_by_label(LayoutLabel.TEXT)\n\n# Convert coordinates\nresult.get_normalized_bboxes()  # 0-1024 scale\nresult.sort_by_position()       # Reading order\n\n# Visualization\nresult.visualize(image, output_path=\"viz.png\")\n\n# Save/load\nresult.save_json(\"layout.json\")\nLayoutOutput.load_json(\"layout.json\")\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#customlabel-properties","title":"CustomLabel Properties","text":"<pre><code>label = CustomLabel(\n    name=\"code_block\",\n    description=\"Code examples\",\n    color=\"#E0E0E0\",\n    detection_prompt=\"Look for monospaced code\",\n)\n\nprint(label.name)               # \"code_block\"\nprint(label.description)        # Description text\nprint(label.color)              # \"#E0E0E0\"\nprint(label.detection_prompt)   # Custom hint\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#advanced-usage","title":"Advanced Usage","text":""},{"location":"models/layout-analysis/qwen-layout/#hierarchical-layout-detection","title":"Hierarchical Layout Detection","text":"<pre><code># Detect first pass: standard labels\nresult_std = detector.extract(image)\n\n# Second pass: custom labels on specific regions\ntext_regions = result_std.filter_by_label(LayoutLabel.TEXT)\n\ncustom_labels = [\n    CustomLabel(name=\"list_item\"),\n    CustomLabel(name=\"definition\"),\n    CustomLabel(name=\"example\"),\n]\n\n# Could refine by cropping and re-detecting each region\nfor text_box in text_regions[:1]:  # First text block\n    x1, y1, x2, y2 = text_box.bbox.to_xyxy()\n    region = image.crop((x1, y1, x2, y2))\n\n    result_detail = detector.extract(\n        region,\n        custom_labels=custom_labels,\n    )\n    print(f\"Found {result_detail.element_count} fine-grained elements\")\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#label-color-mapping-for-visualization","title":"Label Color Mapping for Visualization","text":"<pre><code>from PIL import Image, ImageDraw\n\n# Colors for different label types\nlabel_colors = {\n    \"title\": \"#E74C3C\",        # Red\n    \"text\": \"#3498DB\",         # Blue\n    \"abstract\": \"#2ECC71\",     # Green\n    \"code_block\": \"#95A5A6\",   # Gray\n    \"figure\": \"#9B59B6\",       # Purple\n    \"table\": \"#F39C12\",        # Orange\n}\n\nimage = Image.open(\"document.png\")\nviz = image.copy()\ndraw = ImageDraw.Draw(viz)\n\n# Draw with color mapping\nfor box in result.bboxes:\n    color = label_colors.get(str(box.label), \"#CCCCCC\")\n    coords = box.bbox.to_xyxy()\n    draw.rectangle(coords, outline=color, width=2)\n\nviz.save(\"layout_colored.png\")\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#see-also","title":"See Also","text":"<ul> <li>DocLayout-YOLO - Fixed label, fast detector</li> <li>Qwen Text Extraction - Text extraction</li> <li>Comparison Guide - Model selection matrix</li> </ul>"},{"location":"models/ocr-extraction/tesseract/","title":"Tesseract OCR","text":""},{"location":"models/ocr-extraction/tesseract/#model-overview","title":"Model Overview","text":"<p>Tesseract is the leading open-source Optical Character Recognition (OCR) engine, maintained by Google since 2006. It's the most widely deployed OCR solution and excels at printed text in 100+ languages.</p> <p>Project: GitHub tesseract-ocr Architecture: Traditional OCR (legacy and LSTM-based) Training Focus: Printed documents in all major languages Framework: C/C++ with Python bindings</p>"},{"location":"models/ocr-extraction/tesseract/#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Language Support: 100+ languages with high quality</li> <li>Multilingual Documents: Seamlessly handle mixed-language text</li> <li>Word-Level Bounding Boxes: Get exact position of each word</li> <li>Line-Level Grouping: Option to return line-level blocks</li> <li>CPU-Only: No GPU required, runs anywhere</li> <li>Free &amp; Open Source: No license or API costs</li> <li>Configurable: Fine-tuned via OCR engine modes and page segmentation</li> </ul>"},{"location":"models/ocr-extraction/tesseract/#limitations","title":"Limitations","text":"<ul> <li>Printed Text Only: Struggles with handwriting (see Surya for handwritten)</li> <li>CPU-Bound: Slower than GPU-based OCR (2-5 seconds per page)</li> <li>Quality Variance: Heavily dependent on image quality and preprocessing</li> <li>Skewed Documents: Needs de-skewing for rotated documents</li> <li>Low Contrast: Performs poorly on light text or images</li> <li>No Layout Analysis: Returns text only, no structural information (use DocLayout-YOLO for layout)</li> </ul>"},{"location":"models/ocr-extraction/tesseract/#system-installation","title":"System Installation","text":""},{"location":"models/ocr-extraction/tesseract/#required-system-dependencies","title":"Required System Dependencies","text":"<p>Tesseract must be installed at the operating system level before Python can use it.</p> <p>macOS (using Homebrew): <pre><code>brew install tesseract\n</code></pre></p> <p>Ubuntu/Debian: <pre><code>sudo apt-get update\nsudo apt-get install tesseract-ocr\n</code></pre></p> <p>Windows: Download and install from GitHub releases</p> <p>Verify Installation: <pre><code>tesseract --version\n# Should output version and supported languages\n</code></pre></p>"},{"location":"models/ocr-extraction/tesseract/#python-package-installation","title":"Python Package Installation","text":"<pre><code># Install OmniDocs with OCR support\npip install omnidocs[pytorch]\n\n# Or install pytesseract directly\npip install pytesseract\n\n# Verify\npython -c \"import pytesseract; print(pytesseract.get_languages())\"\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#configuration","title":"Configuration","text":""},{"location":"models/ocr-extraction/tesseract/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\n\nconfig = TesseractOCRConfig(\n    languages=[\"eng\"],           # Single language\n    oem=3,                       # OCR Engine Mode (default)\n    psm=3,                       # Page Segmentation Mode\n)\n\nocr = TesseractOCR(config=config)\n</code></pre> <p>Config Parameters:</p> Parameter Type Default Description <code>languages</code> List[str] [\"eng\"] Language codes (e.g., [\"eng\", \"fra\", \"deu\"]) <code>tessdata_dir</code> str None Custom tessdata directory path <code>oem</code> int 3 OCR Engine Mode (0-3) <code>psm</code> int 3 Page Segmentation Mode (0-13) <code>config_params</code> Dict None Additional Tesseract config options"},{"location":"models/ocr-extraction/tesseract/#available-languages","title":"Available Languages","text":"<pre><code># List all installed languages\ntesseract --list-langs\n\n# Sample common languages:\n# eng (English)        fra (French)         deu (German)\n# spa (Spanish)        ita (Italian)        por (Portuguese)\n# chi_sim (Simplified Chinese)  jpn (Japanese)  kor (Korean)\n# ara (Arabic)         rus (Russian)        hin (Hindi)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#ocr-engine-modes-oem","title":"OCR Engine Modes (OEM)","text":"OEM Name Best For Speed 0 Legacy Old documents Fast 1 LSTM Modern text Accurate 2 Legacy+LSTM Mixed quality Medium 3 Default Auto-detect Medium <p>Recommendation: Use OEM=3 (automatic, recommended for most documents)</p>"},{"location":"models/ocr-extraction/tesseract/#page-segmentation-modes-psm","title":"Page Segmentation Modes (PSM)","text":"PSM Description Use Case 0 OSD only Orientation detection only 3 Fully automatic (default) Mixed layouts, images, text 6 Uniform block Single column of text 7 Single text line Single line input 11 Sparse text Scattered text, forms 13 Raw line Treat each line as a word <p>Recommendation: Use PSM=3 for documents, PSM=11 for forms</p>"},{"location":"models/ocr-extraction/tesseract/#usage-examples","title":"Usage Examples","text":""},{"location":"models/ocr-extraction/tesseract/#basic-text-extraction","title":"Basic Text Extraction","text":"<pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\nfrom PIL import Image\n\n# Initialize\nconfig = TesseractOCRConfig(languages=[\"eng\"])\nocr = TesseractOCR(config=config)\n\n# Extract text\nimage = Image.open(\"document.png\")\nresult = ocr.extract(image)\n\n# Access results\nprint(result.full_text)           # Complete extracted text\nprint(result.text_blocks)         # List of TextBlock objects\n\nfor block in result.text_blocks:\n    print(f\"'{block.text}' @ {block.bbox.to_list()} ({block.confidence:.2%})\")\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#line-level-extraction","title":"Line-Level Extraction","text":"<pre><code># Extract at line level (grouped words)\nresult = ocr.extract_lines(image)\n\nfor block in result.text_blocks:\n    print(f\"{block.text}\")\n\n# Useful for:\n# - Preserving line breaks\n# - Document structure\n# - Form processing\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#multilingual-documents","title":"Multilingual Documents","text":"<pre><code># Extract from document with mixed languages\nconfig = TesseractOCRConfig(\n    languages=[\"eng\", \"fra\", \"deu\"],  # English, French, German\n    oem=2,  # Legacy+LSTM for better multilingual support\n)\nocr = TesseractOCR(config=config)\n\nresult = ocr.extract(image)\nprint(f\"Languages detected: {result.languages_detected}\")\nprint(f\"Text: {result.full_text}\")\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#specialized-document-configuration","title":"Specialized Document Configuration","text":"<pre><code># For forms with sparse text\nform_config = TesseractOCRConfig(\n    languages=[\"eng\"],\n    psm=11,  # Sparse text mode\n    config_params={\n        \"tessedit_char_whitelist\": \"0123456789/-.()\",  # Digits, symbols only\n    },\n)\nocr_form = TesseractOCR(config=form_config)\n\nresult = ocr_form.extract(form_image)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#batch-processing","title":"Batch Processing","text":"<pre><code>from pathlib import Path\nimport json\n\n# Process multiple images\ndoc_dir = Path(\"documents/\")\nresults = {}\n\nconfig = TesseractOCRConfig(languages=[\"eng\"])\nocr = TesseractOCR(config=config)\n\nfor img_path in sorted(doc_dir.glob(\"*.png\"))[:10]:\n    print(f\"Processing {img_path.name}...\")\n    image = Image.open(img_path)\n    result = ocr.extract(image)\n\n    results[img_path.name] = {\n        \"text\": result.full_text,\n        \"word_count\": len(result.text_blocks),\n        \"confidence\": sum(\n            b.confidence for b in result.text_blocks\n        ) / len(result.text_blocks) if result.text_blocks else 0,\n    }\n\n# Save results\nwith open(\"ocr_results.json\", \"w\") as f:\n    json.dump(results, f, indent=2)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#image-preprocessing-for-better-results","title":"Image Preprocessing for Better Results","text":"<p>OCR quality depends heavily on image quality. Pre-process images for best results:</p>"},{"location":"models/ocr-extraction/tesseract/#contrast-enhancement","title":"Contrast Enhancement","text":"<pre><code>from PIL import Image, ImageEnhance\n\nimage = Image.open(\"document.png\")\n\n# Increase contrast\nenhancer = ImageEnhance.Contrast(image)\nimage = enhancer.enhance(1.5)  # 1.5x contrast\n\nresult = ocr.extract(image)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#grayscale-conversion","title":"Grayscale Conversion","text":"<pre><code># Convert to grayscale (Tesseract prefers grayscale)\nimage = Image.open(\"document.png\").convert(\"L\")\n\nresult = ocr.extract(image)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#deskew-rotate","title":"Deskew (Rotate)","text":"<pre><code>from PIL import Image\nimport numpy as np\n\n# For skewed documents, rotate to horizontal\nimage = Image.open(\"skewed_document.png\")\n\n# Simple 90-degree rotations\nimage = image.rotate(90, expand=True)\n\n# For arbitrary angles (requires deskew library)\nfrom deskew import determine_skew\nangle = determine_skew(np.array(image))\nif angle:\n    image = image.rotate(angle, expand=True)\n\nresult = ocr.extract(image)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#upscale-small-text","title":"Upscale Small Text","text":"<pre><code>from PIL import Image\n\nimage = Image.open(\"document.png\")\n\n# If text is very small, upscale\nif image.size[0] &lt; 1000:\n    scale = 2\n    new_size = (image.size[0] * scale, image.size[1] * scale)\n    image = image.resize(new_size, Image.Resampling.LANCZOS)\n\nresult = ocr.extract(image)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#complete-preprocessing-pipeline","title":"Complete Preprocessing Pipeline","text":"<pre><code>from PIL import Image, ImageEnhance, ImageFilter\nimport numpy as np\n\ndef preprocess_image(image_path):\n    img = Image.open(image_path)\n\n    # 1. Convert to grayscale\n    img = img.convert(\"L\")\n\n    # 2. Enhance contrast\n    enhancer = ImageEnhance.Contrast(img)\n    img = enhancer.enhance(1.5)\n\n    # 3. Sharpen\n    img = img.filter(ImageFilter.SHARPEN)\n\n    # 4. Upscale if small\n    if img.size[0] &lt; 1000:\n        new_size = (img.size[0] * 2, img.size[1] * 2)\n        img = img.resize(new_size, Image.Resampling.LANCZOS)\n\n    return img\n\n# Use in OCR\npreprocessed = preprocess_image(\"low_quality.png\")\nresult = ocr.extract(preprocessed)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#performance-accuracy","title":"Performance &amp; Accuracy","text":""},{"location":"models/ocr-extraction/tesseract/#speed-characteristics","title":"Speed Characteristics","text":"Setup Image Size Speed Device Single-threaded 1024x768 2-3s CPU 4-threaded 1024x768 0.5-1s CPU (4 cores) GPU-accelerated 1024x768 0.2-0.5s GPU (if compiled)"},{"location":"models/ocr-extraction/tesseract/#accuracy-by-document-type","title":"Accuracy by Document Type","text":"Document Type Quality Accuracy Notes Printed text High 95-99% Best case scenario Scanned PDF Medium 85-95% Needs preprocessing Handwriting High 30-60% Poor, use Surya instead Low contrast Low 20-50% Needs enhancement Multiple languages Medium 80-92% OEM 2 recommended"},{"location":"models/ocr-extraction/tesseract/#language-accuracy","title":"Language Accuracy","text":"Language Accuracy Notes English (Latin) 95-99% Excellent European languages 92-98% Very good Asian languages 80-90% Good (requires language pack) Mixed script 75-85% Challenging"},{"location":"models/ocr-extraction/tesseract/#troubleshooting","title":"Troubleshooting","text":""},{"location":"models/ocr-extraction/tesseract/#installation-issues","title":"Installation Issues","text":"<p>Symptom: <code>ModuleNotFoundError: No module named 'tesseract'</code></p> <p>Solution:</p> <pre><code># Install system Tesseract first (OS-specific)\n# macOS\nbrew install tesseract\n\n# Then install Python package\npip install pytesseract\n</code></pre> <p>Symptom: Python can't find Tesseract binary</p> <p>Solution:</p> <pre><code>import pytesseract\nfrom pathlib import Path\n\n# Option 1: Specify path in code\npytesseract.pytesseract.pytesseract_cmd = r'/usr/local/bin/tesseract'\n\n# Option 2: Configure in TesseractOCRConfig\nconfig = TesseractOCRConfig(\n    tessdata_dir=\"/path/to/tessdata\",\n)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#language-pack-issues","title":"Language Pack Issues","text":"<p>Symptom: Language not found when trying to use non-English</p> <p>Solution:</p> <pre><code># Check installed languages\ntesseract --list-langs\n\n# Install additional language data (macOS)\nbrew install tesseract-lang\n\n# Verify after installation\ntesseract --list-langs | grep fra  # Check for French\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#poor-ocr-quality","title":"Poor OCR Quality","text":"<p>Symptom: Garbled or incomplete text output</p> <p>Solutions (in order of likelihood):</p> <pre><code># 1. Preprocess image (most common fix)\nimage = image.convert(\"L\")  # Grayscale\nenhancer = ImageEnhance.Contrast(image)\nimage = enhancer.enhance(1.5)\n\n# 2. Try different PSM\nconfig = TesseractOCRConfig(psm=6)  # Uniform block\n\n# 3. Try different OEM\nconfig = TesseractOCRConfig(oem=1)  # LSTM only\n\n# 4. Upscale image\nimage = image.resize(\n    (image.size[0] * 2, image.size[1] * 2),\n    Image.Resampling.LANCZOS\n)\n\n# 5. Try line-level (may preserve structure)\nresult = ocr.extract_lines(image)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#performance-issues-slow","title":"Performance Issues (Slow)","text":"<p>Symptom: OCR takes 5+ seconds per page</p> <p>Solutions:</p> <pre><code># 1. Use simpler PSM (fewer segmentation steps)\nconfig = TesseractOCRConfig(psm=6)  # Faster\n\n# 2. Reduce image size\nimage.thumbnail((2048, 2048))\n\n# 3. Use faster OEM\nconfig = TesseractOCRConfig(oem=0)  # Legacy (faster)\n\n# 4. Process on machine with more CPU cores\n# (Tesseract can use multiple cores)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#tesseract-vs-other-ocr-models","title":"Tesseract vs Other OCR Models","text":"Feature Tesseract EasyOCR PaddleOCR Surya Speed Medium Fast Very Fast Medium Language Support 100+ 80+ 80+ Multi Handwriting Poor Medium Medium Excellent GPU Required No Yes Yes Yes Setup System install Python only Python only Python only Cost Free Free Free Free Best For Printed docs General Asian languages Handwriting <p>Choose Tesseract if: - You need CPU-only processing - Processing printed text in 100+ languages - Want zero GPU dependency</p> <p>Not ideal for: - Handwritten documents (use Surya) - Real-time processing (use PaddleOCR) - Asian documents only (use PaddleOCR)</p>"},{"location":"models/ocr-extraction/tesseract/#api-reference","title":"API Reference","text":""},{"location":"models/ocr-extraction/tesseract/#tesseractocrextract","title":"TesseractOCR.extract()","text":"<pre><code>def extract(image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run word-level OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with word-level text blocks\n    \"\"\"\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#tesseractocrextract_lines","title":"TesseractOCR.extract_lines()","text":"<pre><code>def extract_lines(image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run line-level OCR on an image.\n\n    Groups words into lines based on Tesseract's line detection.\n\n    Args:\n        image: Input image\n\n    Returns:\n        OCROutput with line-level text blocks\n    \"\"\"\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#ocroutput-properties","title":"OCROutput Properties","text":"<pre><code>result = ocr.extract(image)\n\n# Text content\nresult.full_text            # Complete text (word-separated)\nresult.text_blocks          # List[TextBlock] objects\nresult.model_name           # \"tesseract\"\nresult.languages_detected   # Languages used\n\n# Image info\nresult.image_width          # Source width in pixels\nresult.image_height         # Source height in pixels\n\n# Statistics\nlen(result.text_blocks)     # Number of detected words/lines\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#textblock-properties","title":"TextBlock Properties","text":"<pre><code>for block in result.text_blocks:\n    block.text              # Word or line text\n    block.bbox              # BoundingBox object\n    block.confidence        # float (0-1)\n    block.granularity       # WORD or LINE\n    block.language          # Detected language code\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#boundingbox-methods","title":"BoundingBox Methods","text":"<pre><code>bbox = block.bbox\n\n# Access coordinates\nbbox.x1, bbox.y1           # Top-left corner\nbbox.x2, bbox.y2           # Bottom-right corner\nbbox.width                 # Width\nbbox.height                # Height\n\n# Convert formats\nbbox.to_list()             # [x1, y1, x2, y2]\nbbox.to_xyxy()             # (x1, y1, x2, y2)\nbbox.to_xywh()             # (x, y, width, height)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"models/ocr-extraction/tesseract/#custom-tesseract-parameters","title":"Custom Tesseract Parameters","text":"<pre><code># Additional config parameters\nconfig = TesseractOCRConfig(\n    languages=[\"eng\"],\n    config_params={\n        # Whitelist specific characters\n        \"tessedit_char_whitelist\": \"0123456789ABCDEFabcdef\",\n\n        # Ignore words shorter than N characters\n        \"min_characters_to_try\": 3,\n\n        # Set segmentation to all caps\n        \"tessedit_create_pdf\": 0,  # Don't create PDF\n    },\n)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#parallel-processing","title":"Parallel Processing","text":"<pre><code>from concurrent.futures import ThreadPoolExecutor\nfrom pathlib import Path\n\ndef process_image(image_path):\n    image = Image.open(image_path)\n    return ocr.extract(image)\n\n# Process multiple images in parallel\ndoc_dir = Path(\"documents/\")\nimages = list(doc_dir.glob(\"*.png\"))\n\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    results = list(executor.map(process_image, images))\n\nprint(f\"Processed {len(results)} images\")\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#see-also","title":"See Also","text":"<ul> <li>EasyOCR - GPU-based OCR</li> <li>PaddleOCR - Fast multilingual OCR</li> <li>Surya OCR - Excellent for handwriting</li> <li>OCR Comparison - Model selection matrix</li> <li>Tesseract Docs</li> </ul>"},{"location":"models/text-extraction/dotsocr/","title":"DotsOCR Text Extraction","text":""},{"location":"models/text-extraction/dotsocr/#model-overview","title":"Model Overview","text":"<p>DotsOCR (Deep Object Text Segmentation OCR) is a specialized Vision-Language Model designed specifically for document understanding with built-in layout analysis. Unlike general-purpose VLMs, DotsOCR outputs structured information about document layout while extracting text content.</p> <p>Model ID: rednote-hilab/dots.ocr Repository: DotsOCR on HuggingFace Architecture: Vision Encoder + Language Model Training Focus: Academic papers, technical documents, PDFs</p>"},{"location":"models/text-extraction/dotsocr/#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Layout-Aware Extraction: Detects 11 document element categories with bounding boxes</li> <li>Multi-Format Text: Different formats per category (Markdown, LaTeX, HTML)</li> <li>Fast Inference: 50-100% faster than general-purpose VLMs</li> <li>Normalized Coordinates: All bboxes in 0-1024 range (scale-independent)</li> <li>Reading Order: Maintains document reading order</li> <li>Format-Specific Output:</li> <li>Text/Title/Section-header: Markdown</li> <li>Formula: LaTeX</li> <li>Table: HTML</li> <li>Picture: Bounding box only (no text)</li> </ul>"},{"location":"models/text-extraction/dotsocr/#limitations","title":"Limitations","text":"<ul> <li>PyTorch and VLLM backends only (no MLX, no API)</li> <li>Optimized for academic/technical documents (less good for forms, invoices)</li> <li>Fixed layout categories (cannot add custom categories)</li> <li>Requires GPU (minimum 16GB VRAM for 8B variant)</li> <li>Output is JSON-focused (not raw markdown like Qwen)</li> </ul>"},{"location":"models/text-extraction/dotsocr/#supported-backends","title":"Supported Backends","text":"<p>DotsOCR supports 2 inference backends:</p> Backend Use Case Performance Setup PyTorch Single document, development 50-100 tok/s Simple GPU setup VLLM Batch processing, production 150-300 tok/s Multi-GPU cluster <p>No MLX or API backends available.</p>"},{"location":"models/text-extraction/dotsocr/#installation-configuration","title":"Installation &amp; Configuration","text":""},{"location":"models/text-extraction/dotsocr/#basic-installation","title":"Basic Installation","text":"<pre><code># Install with PyTorch backend\npip install omnidocs[pytorch]\n\n# Or with VLLM for batching\npip install omnidocs[vllm]\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#pytorch-backend-configuration","title":"PyTorch Backend Configuration","text":"<pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nconfig = DotsOCRPyTorchConfig(\n    model=\"rednote-hilab/dots.ocr\",\n    device=\"cuda\",\n    torch_dtype=\"bfloat16\",\n    trust_remote_code=True,\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",  # Recommended\n)\n\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre> <p>PyTorch Config Parameters:</p> Parameter Type Default Description <code>model</code> str \"rednote-hilab/dots.ocr\" HuggingFace model ID <code>device</code> str \"cuda\" Device: \"cuda\", \"mps\", \"cpu\" <code>torch_dtype</code> str \"bfloat16\" Data type: \"float16\", \"bfloat16\", \"float32\" <code>trust_remote_code</code> bool True Allow custom model code from HuggingFace <code>device_map</code> str \"auto\" Model parallelism: \"auto\", \"balanced\", \"sequential\" <code>attn_implementation</code> str \"flash_attention_2\" Attention type: \"eager\", \"flash_attention_2\", \"sdpa\""},{"location":"models/text-extraction/dotsocr/#vllm-backend-configuration","title":"VLLM Backend Configuration","text":"<pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nconfig = DotsOCRVLLMConfig(\n    model=\"rednote-hilab/dots.ocr\",\n    tensor_parallel_size=1,  # Use 2+ for large models\n    gpu_memory_utilization=0.85,\n    max_model_len=4096,\n)\n\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre> <p>VLLM Config Parameters:</p> Parameter Type Default Description <code>model</code> str Required HuggingFace model ID <code>tensor_parallel_size</code> int 1 Number of GPUs for parallelism <code>gpu_memory_utilization</code> float 0.85 GPU memory usage (0.1-1.0) <code>max_model_len</code> int None Max context length in tokens"},{"location":"models/text-extraction/dotsocr/#layout-categories-11-fixed","title":"Layout Categories (11 Fixed)","text":"<p>DotsOCR recognizes exactly 11 layout element categories:</p> Category Description Text Format Typical Content Title Document/section title Markdown \"Introduction\", \"Chapter 2\" Section-header Subsection heading Markdown \"3.1 Method Overview\" Text Body paragraph Markdown Main content paragraphs List-item Bulleted/numbered item Markdown \"1. First point\", \"\u2022 Item\" Table Tabular data HTML <code>&lt;table&gt;&lt;tr&gt;&lt;td&gt;...&lt;/td&gt;...</code> Formula Mathematical equation LaTeX <code>$E=mc^2$</code> or display math Figure Image/figure/diagram None Bounding box only Caption Figure/table caption Markdown \"Fig 1: System Overview\" Footnote Footer note Markdown Explanatory footnotes Page-header Page header text Markdown Page number, document title Page-footer Page footer text Markdown Page number, author name"},{"location":"models/text-extraction/dotsocr/#usage-examples","title":"Usage Examples","text":""},{"location":"models/text-extraction/dotsocr/#basic-layout-aware-extraction","title":"Basic Layout-Aware Extraction","text":"<pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\nfrom PIL import Image\n\n# Initialize extractor\nconfig = DotsOCRPyTorchConfig(\n    model=\"rednote-hilab/dots.ocr\",\n    device=\"cuda\",\n)\nextractor = DotsOCRTextExtractor(backend=config)\n\n# Load document\nimage = Image.open(\"paper.png\")\n\n# Extract with layout information\nresult = extractor.extract(\n    image,\n    include_layout=True,  # Returns DotsOCRTextOutput\n)\n\n# Access layout elements\nprint(f\"Found {result.num_layout_elements} layout elements\")\nfor elem in result.layout:\n    print(f\"  {elem.category} @ {elem.bbox}: {elem.text[:50]}...\")\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#output-format-examples","title":"Output Format Examples","text":"<pre><code># Default: DotsOCRTextOutput with layout\nresult = extractor.extract(image)\n\n# Access structured layout\nfor elem in result.layout:\n    category = elem.category  # \"Title\", \"Text\", \"Table\", etc.\n    bbox = elem.bbox          # [x1, y1, x2, y2] (0-1024 normalized)\n    text = elem.text          # Content (formatted per category)\n    confidence = elem.confidence  # Detection confidence\n\nprint(result.content)        # Full text (Markdown)\nprint(result.format)         # \"markdown\" (fixed)\nprint(result.has_layout)     # True\nprint(result.content_length) # Total character count\nprint(result.image_width)    # Source image width\nprint(result.image_height)   # Source image height\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#category-specific-processing","title":"Category-Specific Processing","text":"<pre><code># Extract only formulas (as LaTeX)\nformulas = [\n    elem for elem in result.layout\n    if elem.category == \"Formula\"\n]\n\nfor formula in formulas:\n    print(f\"Formula @ {formula.bbox}:\")\n    print(formula.text)  # LaTeX format\n    print()\n\n# Extract tables (as HTML)\ntables = [\n    elem for elem in result.layout\n    if elem.category == \"Table\"\n]\n\nfor table in tables:\n    print(f\"Table @ {table.bbox}:\")\n    print(table.text)  # HTML table\n    print()\n\n# Extract all text content (non-figure)\ntext_elements = [\n    elem for elem in result.layout\n    if elem.category not in [\"Figure\", \"Page-header\", \"Page-footer\"]\n]\n\nfull_text = \"\\n\".join(elem.text for elem in text_elements)\nprint(full_text)  # Cleaned text without layout markers\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#bounding-box-operations","title":"Bounding Box Operations","text":"<pre><code># Access normalized bounding boxes (0-1024 scale)\nfor elem in result.layout:\n    x1, y1, x2, y2 = elem.bbox\n    width = x2 - x1\n    height = y2 - y1\n    area = width * height\n\n    print(f\"{elem.category}: {width}x{height} at ({x1}, {y1})\")\n\n# Filter elements by region (e.g., top half of page)\ntop_half = [\n    elem for elem in result.layout\n    if elem.bbox[1] &lt; 512  # y1 &lt; midpoint\n]\n\n# Filter by size\nlarge_elements = [\n    elem for elem in result.layout\n    if (elem.bbox[2] - elem.bbox[0]) * (elem.bbox[3] - elem.bbox[1]) &gt; 102400\n]\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#batch-processing-with-vllm","title":"Batch Processing with VLLM","text":"<pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\nfrom PIL import Image\nimport json\n\n# Initialize with VLLM\nconfig = DotsOCRVLLMConfig(\n    model=\"rednote-hilab/dots.ocr\",\n    tensor_parallel_size=2,\n    gpu_memory_utilization=0.8,\n)\nextractor = DotsOCRTextExtractor(backend=config)\n\n# Process multiple documents\ndocuments = [\"doc1.png\", \"doc2.png\", \"doc3.png\"]\nresults = []\n\nfor doc_path in documents:\n    image = Image.open(doc_path)\n    result = extractor.extract(image, include_layout=True)\n\n    results.append({\n        \"file\": doc_path,\n        \"elements\": len(result.layout),\n        \"content_length\": result.content_length,\n        \"layout\": [\n            {\n                \"category\": elem.category,\n                \"bbox\": elem.bbox,\n                \"text_length\": len(elem.text) if elem.text else 0,\n            }\n            for elem in result.layout\n        ]\n    })\n\n# Save results\nwith open(\"extraction_results.json\", \"w\") as f:\n    json.dump(results, f, indent=2)\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"models/text-extraction/dotsocr/#memory-requirements","title":"Memory Requirements","text":"Model Framework VRAM Batch Size DotsOCR PyTorch 16 GB 1 (single doc) DotsOCR VLLM 20 GB 2-4 DotsOCR VLLM (2-GPU) 20 GB (per GPU) 6-10"},{"location":"models/text-extraction/dotsocr/#inference-speed","title":"Inference Speed","text":"Setup Speed Throughput PyTorch (single A10) 50-80 tok/s ~400-600 chars/s VLLM (single A10) 150-200 tok/s ~1200-1600 chars/s VLLM (2x A10) 250-350 tok/s ~2000-2800 chars/s"},{"location":"models/text-extraction/dotsocr/#typical-processing-times","title":"Typical Processing Times","text":"Document Tokens Time (PyTorch) Time (VLLM) Single page 1000-2000 12-25s 5-10s 5 pages 5000-10000 60-130s 20-40s 10 pages 10000-20000 130-260s 40-80s"},{"location":"models/text-extraction/dotsocr/#troubleshooting","title":"Troubleshooting","text":""},{"location":"models/text-extraction/dotsocr/#memory-errors","title":"Memory Errors","text":"<p>Symptom: <code>RuntimeError: CUDA out of memory</code></p> <p>Solutions:</p> <pre><code># 1. Use CPU (slow but works)\nconfig = DotsOCRPyTorchConfig(device=\"cpu\")\n\n# 2. Reduce image size before processing\nfrom PIL import Image\nimage = Image.open(\"document.png\")\nimage.thumbnail((2048, 2048))  # Resize if larger\n\n# 3. Use VLLM with memory management\nconfig = DotsOCRVLLMConfig(\n    gpu_memory_utilization=0.7,  # Reduced from 0.85\n    max_model_len=2048,  # Reduced from 4096\n)\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#layout-parsing-errors","title":"Layout Parsing Errors","text":"<p>Symptom: <code>ValueError: Invalid layout JSON structure</code></p> <p>Solution:</p> <pre><code># Check raw output for issues\nresult = extractor.extract(image, include_layout=True)\n\nif result.error:\n    print(f\"Extraction error: {result.error}\")\n    print(f\"Raw output: {result.raw_output[:500]}...\")\n\n# Ensure image is valid\nif image.size[0] &lt; 256 or image.size[1] &lt; 256:\n    print(\"Image too small for reliable layout detection\")\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#missing-layout-categories","title":"Missing Layout Categories","text":"<p>Symptom: Some expected elements not detected</p> <p>Solutions:</p> <pre><code># Check what was detected\ndetected_categories = set(\n    elem.category for elem in result.layout\n)\nprint(f\"Found: {detected_categories}\")\n\n# Element may be below confidence threshold\n# Access raw output to see low-confidence detections\nprint(result.raw_output)\n\n# Try with different preprocessing\nfrom PIL import ImageEnhance\nimage = Image.open(\"document.png\")\nenhancer = ImageEnhance.Contrast(image)\nimage = enhancer.enhance(1.3)\nresult = extractor.extract(image)\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#dotsocr-vs-other-models","title":"DotsOCR vs Other Models","text":""},{"location":"models/text-extraction/dotsocr/#dotsocr-vs-qwen3-vl","title":"DotsOCR vs Qwen3-VL","text":"Feature DotsOCR Qwen3-VL Layout Info Detailed (11 cats) Basic Output Format JSON + Markdown Markdown/HTML Speed Fast Medium Text Quality Good Excellent Multilingual Limited Excellent (25+ langs) Backends PyTorch, VLLM PyTorch, VLLM, MLX, API Best For Layout analysis Text quality <p>Choose DotsOCR if: You need precise layout information for post-processing Choose Qwen if: You need high-quality text in multiple languages</p>"},{"location":"models/text-extraction/dotsocr/#when-to-use-dotsocr","title":"When to Use DotsOCR","text":"<p>Ideal scenarios: - Academic papers with structured layouts - Technical documents with formulas and tables - Batch processing with layout analysis - When you need bounding boxes for each element</p> <p>Not ideal for: - Handwritten documents (use Surya) - Forms with complex fields (use specialized form parser) - Real-time single-document processing (overhead &gt; benefit) - Custom layout categories needed</p>"},{"location":"models/text-extraction/dotsocr/#api-reference","title":"API Reference","text":""},{"location":"models/text-extraction/dotsocr/#dotsocrtextextractorextract","title":"DotsOCRTextExtractor.extract()","text":"<pre><code>def extract(\n    image: Union[Image.Image, np.ndarray, str, Path],\n    include_layout: bool = True,\n    output_format: str = \"markdown\",\n) -&gt; DotsOCRTextOutput:\n    \"\"\"\n    Extract text with layout from document image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n        include_layout: Include layout elements with bboxes (default: True)\n        output_format: \"markdown\" or \"json\" (fixed)\n\n    Returns:\n        DotsOCRTextOutput with layout elements and text\n    \"\"\"\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#dotsocrtextoutput-properties","title":"DotsOCRTextOutput Properties","text":"<pre><code>result = extractor.extract(image)\n\n# Layout information\nresult.layout                   # List[LayoutElement]\nresult.has_layout              # True\nresult.num_layout_elements     # int\n\n# Text content\nresult.content                 # Full text (Markdown)\nresult.format                  # \"markdown\"\nresult.content_length          # Characters\n\n# Element categories\nresult.layout_categories       # List of 11 categories\n\n# Metadata\nresult.image_width            # Source image width\nresult.image_height           # Source image height\nresult.truncated              # Output hit max tokens\nresult.error                  # Error message if any\nresult.raw_output             # Raw model JSON\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#layoutelement-properties","title":"LayoutElement Properties","text":"<pre><code>for elem in result.layout:\n    elem.category        # \"Title\", \"Text\", \"Table\", etc.\n    elem.bbox            # [x1, y1, x2, y2] (0-1024)\n    elem.text            # Content (Markdown/LaTeX/HTML)\n    elem.confidence      # float (0-1) - detection confidence\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#advanced-usage","title":"Advanced Usage","text":""},{"location":"models/text-extraction/dotsocr/#post-processing-extract-figures","title":"Post-Processing: Extract Figures","text":"<pre><code># Get all figures with their captions\nfigures = {}\nfor elem in result.layout:\n    if elem.category == \"Figure\":\n        bbox = elem.bbox\n        figures[str(bbox)] = {\n            \"bbox\": bbox,\n            \"caption\": None,\n        }\n    elif elem.category == \"Caption\":\n        # Find nearest figure\n        # (could implement spatial matching here)\n        pass\n\nfor fig_bbox, fig_data in figures.items():\n    print(f\"Figure @ {fig_data['bbox']}\")\n    print(f\"  Caption: {fig_data['caption']}\")\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#export-to-structured-format","title":"Export to Structured Format","text":"<pre><code>import json\nfrom dataclasses import asdict\n\n# Convert to JSON-serializable format\noutput_data = {\n    \"document\": {\n        \"width\": result.image_width,\n        \"height\": result.image_height,\n    },\n    \"elements\": [\n        {\n            \"category\": elem.category,\n            \"bbox\": {\n                \"x1\": elem.bbox[0],\n                \"y1\": elem.bbox[1],\n                \"x2\": elem.bbox[2],\n                \"y2\": elem.bbox[3],\n            },\n            \"text\": elem.text,\n            \"confidence\": elem.confidence,\n        }\n        for elem in result.layout\n    ]\n}\n\n# Save\nwith open(\"layout_analysis.json\", \"w\") as f:\n    json.dump(output_data, f, indent=2)\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#visualization-with-bounding-boxes","title":"Visualization with Bounding Boxes","text":"<pre><code>from PIL import Image, ImageDraw\n\n# Load original image\nimage = Image.open(\"document.png\")\nimg_w, img_h = image.size\n\n# Create visualization\nviz = image.copy()\ndraw = ImageDraw.Draw(viz)\n\n# Color map for categories\ncolors = {\n    \"Title\": \"red\",\n    \"Text\": \"blue\",\n    \"Table\": \"orange\",\n    \"Formula\": \"purple\",\n    \"Figure\": \"green\",\n}\n\n# Draw bounding boxes\nfor elem in result.layout:\n    # Convert from 0-1024 to pixel coordinates\n    bbox = [\n        (elem.bbox[0] / 1024) * img_w,\n        (elem.bbox[1] / 1024) * img_h,\n        (elem.bbox[2] / 1024) * img_w,\n        (elem.bbox[3] / 1024) * img_h,\n    ]\n\n    color = colors.get(elem.category, \"gray\")\n    draw.rectangle(bbox, outline=color, width=3)\n    draw.text((bbox[0], bbox[1] - 15), elem.category, fill=color)\n\n# Save\nviz.save(\"layout_visualization.png\")\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#see-also","title":"See Also","text":"<ul> <li>Qwen3-VL Text Extraction - For pure text quality</li> <li>DotsOCR Repository</li> <li>Comparison Guide - Model selection matrix</li> </ul>"},{"location":"models/text-extraction/qwen/","title":"Qwen3-VL Text Extraction","text":""},{"location":"models/text-extraction/qwen/#model-overview","title":"Model Overview","text":"<p>Qwen3-VL is an advanced Vision-Language Model optimized for document understanding and text extraction. It excels at producing high-quality markdown and HTML output while maintaining document layout and semantic structure.</p> <p>Model Family: Qwen3-VL-2B, Qwen3-VL-4B, Qwen3-VL-8B, Qwen3-VL-32B Repository: Qwen/Qwen3-VL Recommended Variant: Qwen3-VL-8B-Instruct (best balance of quality and speed)</p>"},{"location":"models/text-extraction/qwen/#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Multi-format Output: Markdown, HTML, or custom formats</li> <li>Layout-Aware: Preserves document structure and semantic relationships</li> <li>Multilingual: Supports 25+ languages with native quality</li> <li>Document Types: PDFs, academic papers, technical docs, web pages, presentations</li> <li>Scale Support: Handles documents from single-page images to 16k+ token outputs</li> <li>Custom Prompts: Flexible prompt engineering for specialized extraction tasks</li> </ul>"},{"location":"models/text-extraction/qwen/#limitations","title":"Limitations","text":"<ul> <li>Requires GPU for inference (2B variant: 4GB VRAM, 8B: 16GB, 32B: 40GB+)</li> <li>Slower than single-task models (100-300 tokens/sec depending on backend)</li> <li>Can struggle with highly stylized or unusual layouts</li> <li>No inherent language detection (specify language in config if needed)</li> </ul>"},{"location":"models/text-extraction/qwen/#supported-backends","title":"Supported Backends","text":"<p>Qwen3-VL supports 4 inference backends, allowing you to choose the right deployment method:</p> Backend Use Case Performance Setup PyTorch Local GPU inference 50-150 tokens/sec Easy, single GPU VLLM High-throughput batching 200-400 tokens/sec Requires GPU cluster MLX Apple Silicon (native) 20-50 tokens/sec macOS M1/M2/M3+ only API Hosted inference Variable Cloud provider"},{"location":"models/text-extraction/qwen/#installation-configuration","title":"Installation &amp; Configuration","text":""},{"location":"models/text-extraction/qwen/#basic-installation","title":"Basic Installation","text":"<pre><code># Install with PyTorch backend (most common)\npip install omnidocs[pytorch]\n\n# Or install with VLLM for high throughput\npip install omnidocs[vllm]\n\n# Or install with all backends\npip install omnidocs[all]\n</code></pre>"},{"location":"models/text-extraction/qwen/#pytorch-backend-configuration","title":"PyTorch Backend Configuration","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cuda\",\n    torch_dtype=\"bfloat16\",\n    device_map=\"auto\",\n    trust_remote_code=True,\n    use_flash_attention=False,  # Set to True if flash-attn installed\n    max_new_tokens=8192,\n    temperature=0.1,\n)\n\nextractor = QwenTextExtractor(backend=config)\n</code></pre> <p>PyTorch Config Parameters:</p> Parameter Type Default Description <code>model</code> str \"Qwen/Qwen3-VL-8B-Instruct\" HuggingFace model ID <code>device</code> str \"cuda\" Device: \"cuda\", \"mps\", \"cpu\" <code>torch_dtype</code> str \"auto\" Data type: \"float16\", \"bfloat16\", \"float32\", \"auto\" <code>device_map</code> str \"auto\" Model parallelism: \"auto\", \"balanced\", \"sequential\", None <code>trust_remote_code</code> bool True Allow custom model code from HuggingFace <code>use_flash_attention</code> bool False Use Flash Attention 2 (faster, requires flash-attn) <code>max_new_tokens</code> int 8192 Max tokens to generate (256-32768) <code>temperature</code> float 0.1 Sampling temperature (0.0-2.0, lower = deterministic)"},{"location":"models/text-extraction/qwen/#vllm-backend-configuration","title":"VLLM Backend Configuration","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\n\nconfig = QwenTextVLLMConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    tensor_parallel_size=1,  # Use 2+ for large models\n    gpu_memory_utilization=0.9,\n    max_model_len=8192,\n)\n\nextractor = QwenTextExtractor(backend=config)\n</code></pre> <p>VLLM Config Parameters:</p> Parameter Type Default Description <code>model</code> str Required HuggingFace model ID <code>tensor_parallel_size</code> int 1 Number of GPUs for parallelism <code>gpu_memory_utilization</code> float 0.9 GPU memory usage (0.1-1.0) <code>max_model_len</code> int None Max context length in tokens"},{"location":"models/text-extraction/qwen/#mlx-backend-configuration-apple-silicon","title":"MLX Backend Configuration (Apple Silicon)","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextMLXConfig\n\nconfig = QwenTextMLXConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    quantization=\"4bit\",  # or \"8bit\", \"none\"\n    max_tokens=8192,\n)\n\nextractor = QwenTextExtractor(backend=config)\n</code></pre>"},{"location":"models/text-extraction/qwen/#api-backend-configuration","title":"API Backend Configuration","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextAPIConfig\n\nconfig = QwenTextAPIConfig(\n    model=\"qwen3-vl-8b\",\n    api_key=\"your-api-key\",\n    base_url=\"https://api.provider.com/v1\",\n    rate_limit=10,  # Requests per second\n)\n\nextractor = QwenTextExtractor(backend=config)\n</code></pre>"},{"location":"models/text-extraction/qwen/#usage-examples","title":"Usage Examples","text":""},{"location":"models/text-extraction/qwen/#basic-text-extraction-markdown","title":"Basic Text Extraction (Markdown)","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom omnidocs import Document\nfrom PIL import Image\n\n# Initialize extractor\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cuda\",\n)\nextractor = QwenTextExtractor(backend=config)\n\n# Load document\nimage = Image.open(\"document.png\")\n\n# Extract text in markdown\nresult = extractor.extract(\n    image,\n    output_format=\"markdown\",\n)\n\nprint(result.content)  # Clean markdown\nprint(result.word_count)  # Approximate word count\n</code></pre>"},{"location":"models/text-extraction/qwen/#multi-format-extraction","title":"Multi-Format Extraction","text":"<pre><code># HTML output (preserves more layout semantics)\nresult_html = extractor.extract(\n    image,\n    output_format=\"html\",\n)\n\n# Custom prompt for specialized extraction\ncustom_prompt = \"\"\"Extract all text as JSON with structure:\n{\n    \"title\": \"...\",\n    \"sections\": [{\"heading\": \"...\", \"content\": \"...\"}],\n    \"tables\": [...]\n}\n\"\"\"\n\nresult_custom = extractor.extract(\n    image,\n    output_format=\"markdown\",\n    custom_prompt=custom_prompt,\n)\n</code></pre>"},{"location":"models/text-extraction/qwen/#batch-processing-with-vllm","title":"Batch Processing with VLLM","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\nfrom PIL import Image\nimport time\n\n# Initialize with VLLM for high throughput\nconfig = QwenTextVLLMConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    tensor_parallel_size=2,  # Use 2 GPUs\n    gpu_memory_utilization=0.8,\n)\nextractor = QwenTextExtractor(backend=config)\n\n# Load multiple documents\nimages = [\n    Image.open(f\"doc_{i}.png\") for i in range(10)\n]\n\n# Process with streaming\nresults = []\nstart = time.time()\n\nfor i, image in enumerate(images):\n    result = extractor.extract(image, output_format=\"markdown\")\n    results.append(result)\n    elapsed = time.time() - start\n    throughput = (i + 1) / elapsed * 1000  # chars/sec\n    print(f\"[{i+1}/10] {result.content_length} chars - {throughput:.0f} chars/sec\")\n\nprint(f\"\\nTotal time: {time.time() - start:.1f}s\")\nprint(f\"Avg length: {sum(r.content_length for r in results) / len(results):.0f} chars\")\n</code></pre>"},{"location":"models/text-extraction/qwen/#layout-aware-extraction","title":"Layout-Aware Extraction","text":"<pre><code># Include layout information\nresult = extractor.extract(\n    image,\n    output_format=\"markdown\",\n    include_layout=True,\n)\n\n# Access raw output with bounding boxes\nprint(result.raw_output)  # Contains bbox annotations\n</code></pre>"},{"location":"models/text-extraction/qwen/#api-based-extraction-cloud","title":"API-Based Extraction (Cloud)","text":"<pre><code>import os\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextAPIConfig\nfrom PIL import Image\n\n# Configure API backend\nconfig = QwenTextAPIConfig(\n    model=\"qwen3-vl-8b\",\n    api_key=os.getenv(\"QWEN_API_KEY\"),\n    base_url=\"https://api.together.xyz/v1\",\n    rate_limit=5,\n)\n\nextractor = QwenTextExtractor(backend=config)\n\n# Extract from image\nimage = Image.open(\"document.png\")\nresult = extractor.extract(\n    image,\n    output_format=\"markdown\",\n)\n\nprint(result.content)\n</code></pre>"},{"location":"models/text-extraction/qwen/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"models/text-extraction/qwen/#memory-requirements-by-variant","title":"Memory Requirements by Variant","text":"Model Min VRAM Optimal VRAM Batch Size (VLLM) Qwen3-VL-2B 4 GB 8 GB 8-16 Qwen3-VL-4B 8 GB 12 GB 4-8 Qwen3-VL-8B 16 GB 24 GB 2-4 Qwen3-VL-32B 40 GB 80 GB 1"},{"location":"models/text-extraction/qwen/#inference-speed-single-document","title":"Inference Speed (Single Document)","text":"Backend Model Speed Device PyTorch 8B 50-100 tok/s Single A10 GPU VLLM 8B 200-300 tok/s 2x A10 GPU (tensor parallel) MLX 8B-quantized 20-40 tok/s M3 Max (48GB) API 8B Variable Cloud (depends on provider)"},{"location":"models/text-extraction/qwen/#typical-output-sizes","title":"Typical Output Sizes","text":"Document Type Tokens Characters Single-page document 500-2000 3-12 KB Academic paper page 1000-4000 6-24 KB Multi-page scanned doc 2000-8000 12-48 KB"},{"location":"models/text-extraction/qwen/#troubleshooting","title":"Troubleshooting","text":""},{"location":"models/text-extraction/qwen/#out-of-memory-oom","title":"Out of Memory (OOM)","text":"<p>Symptom: <code>RuntimeError: CUDA out of memory</code></p> <p>Solutions:</p> <pre><code># 1. Reduce max_new_tokens\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    max_new_tokens=4096,  # Reduced from 8192\n)\n\n# 2. Use smaller model variant\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-4B-Instruct\",  # Smaller variant\n)\n\n# 3. Enable quantization\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    load_in_4bit=True,  # Requires bitsandbytes\n)\n\n# 4. Use CPU\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cpu\",  # Slower but works with limited VRAM\n)\n</code></pre>"},{"location":"models/text-extraction/qwen/#slow-inference","title":"Slow Inference","text":"<p>Symptom: Processing takes 30+ seconds per document</p> <p>Solutions:</p> <pre><code># 1. Enable Flash Attention (requires flash-attn package)\nconfig = QwenTextPyTorchConfig(\n    use_flash_attention=True,\n)\n\n# 2. Use VLLM for batching\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\nconfig = QwenTextVLLMConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    tensor_parallel_size=2,\n)\n\n# 3. Use smaller model\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-4B-Instruct\",  # 2x faster\n)\n\n# 4. Reduce image size\nfrom PIL import Image\nimage = Image.open(\"document.png\")\nimage.thumbnail((1024, 1024))  # Resize to 1024x1024 max\n</code></pre>"},{"location":"models/text-extraction/qwen/#poor-quality-output","title":"Poor Quality Output","text":"<p>Symptom: Garbled or incomplete text extraction</p> <p>Solutions:</p> <pre><code># 1. Lower temperature for more deterministic output\nconfig = QwenTextPyTorchConfig(\n    temperature=0.01,  # Very low for consistency\n)\n\n# 2. Use larger model variant\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-32B-Instruct\",  # Better quality\n)\n\n# 3. Pre-process image (enhance contrast, de-skew)\nfrom PIL import ImageEnhance\nimage = Image.open(\"document.png\")\nenhancer = ImageEnhance.Contrast(image)\nimage = enhancer.enhance(1.5)  # Increase contrast\n\n# 4. Custom prompt for better guidance\ncustom_prompt = \"\"\"Extract all text exactly as it appears.\nPreserve formatting, structure, and special characters.\"\"\"\nresult = extractor.extract(image, custom_prompt=custom_prompt)\n</code></pre>"},{"location":"models/text-extraction/qwen/#api-rate-limiting","title":"API Rate Limiting","text":"<p>Symptom: <code>429 Too Many Requests</code> errors</p> <p>Solutions:</p> <pre><code># Reduce rate limit\nconfig = QwenTextAPIConfig(\n    model=\"qwen3-vl-8b\",\n    api_key=\"...\",\n    rate_limit=2,  # Reduced from 10\n)\n\n# Implement retry logic\nimport time\nmax_retries = 3\nfor attempt in range(max_retries):\n    try:\n        result = extractor.extract(image)\n        break\n    except Exception as e:\n        if attempt &lt; max_retries - 1:\n            wait_time = 2 ** attempt\n            print(f\"Rate limited, waiting {wait_time}s...\")\n            time.sleep(wait_time)\n        else:\n            raise\n</code></pre>"},{"location":"models/text-extraction/qwen/#model-download-issues","title":"Model Download Issues","text":"<p>Symptom: <code>ConnectionError</code> or timeout during model loading</p> <p>Solutions:</p> <pre><code># Set HuggingFace cache directory\nimport os\nos.environ[\"HF_HOME\"] = \"/path/to/cache\"\n\n# Pre-download model\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\"Qwen/Qwen3-VL-8B-Instruct\")\n\n# Use local model path\nconfig = QwenTextPyTorchConfig(\n    model=\"/local/path/to/model\",\n)\n</code></pre>"},{"location":"models/text-extraction/qwen/#model-selection-guide","title":"Model Selection Guide","text":""},{"location":"models/text-extraction/qwen/#when-to-use-qwen3-vl","title":"When to Use Qwen3-VL","text":"<p>Best for: - High-quality document extraction (academic papers, technical docs) - Multilingual documents - Complex layouts with mixed content types - Production systems needing consistent quality</p> <p>Not ideal for: - Real-time processing (see: Nanonuts OCR for speed) - Handwritten documents (see: Surya OCR) - Fixed-label layout detection (see: DocLayout-YOLO)</p>"},{"location":"models/text-extraction/qwen/#qwen-vs-dotsocr-comparison","title":"Qwen vs DotsOCR Comparison","text":"Feature Qwen3-VL DotsOCR Output Quality Excellent Very Good Layout Info Basic Detailed (11 categories) Speed Medium Fast Memory High Medium Multilingual Yes (25+ langs) Limited Model Size Options 2B-32B Single <p>Choose Qwen3-VL if: You need high-quality text and multilingual support Choose DotsOCR if: You need detailed layout information with good performance</p>"},{"location":"models/text-extraction/qwen/#api-reference","title":"API Reference","text":""},{"location":"models/text-extraction/qwen/#qwentextextractorextract","title":"QwenTextExtractor.extract()","text":"<pre><code>def extract(\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: str = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from image using Qwen3-VL.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n        output_format: \"markdown\" or \"html\"\n        include_layout: Include layout information in raw output\n        custom_prompt: Override default extraction prompt\n\n    Returns:\n        TextOutput with extracted content\n    \"\"\"\n</code></pre>"},{"location":"models/text-extraction/qwen/#textoutput-properties","title":"TextOutput Properties","text":"<pre><code>result = extractor.extract(image)\n\n# Access extracted content\nprint(result.content)        # Formatted text (markdown/html)\nprint(result.format)         # Output format\nprint(result.plain_text)     # Plain text without formatting\nprint(result.content_length) # Character count\nprint(result.word_count)     # Approximate word count\nprint(result.image_width)    # Source image width\nprint(result.image_height)   # Source image height\nprint(result.model_name)     # Model used\nprint(result.raw_output)     # Raw model output (with artifacts)\n</code></pre>"},{"location":"models/text-extraction/qwen/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"models/text-extraction/qwen/#device-map-strategies","title":"Device Map Strategies","text":"<pre><code># Auto device mapping (recommended)\ndevice_map = \"auto\"\n\n# Balanced distribution across GPUs\ndevice_map = \"balanced\"\n\n# Sequential loading (one GPU at a time)\ndevice_map = \"sequential\"\n\n# Manual: First layer on GPU0, rest on CPU\ndevice_map = {\n    \"model.layers.0\": 0,\n    \"model.layers.1-31\": \"cpu\",\n}\n</code></pre>"},{"location":"models/text-extraction/qwen/#data-type-selection","title":"Data Type Selection","text":"<pre><code># float32: Full precision (slower, more VRAM)\ntorch_dtype = \"float32\"\n\n# float16: Half precision (faster, less VRAM, less accurate)\ntorch_dtype = \"float16\"\n\n# bfloat16: Brain float (recommended for stability)\ntorch_dtype = \"bfloat16\"\n\n# auto: Let model choose based on hardware\ntorch_dtype = \"auto\"\n</code></pre>"},{"location":"models/text-extraction/qwen/#see-also","title":"See Also","text":"<ul> <li>Qwen HuggingFace Model Card</li> <li>DotsOCR Documentation - For layout-aware extraction</li> <li>Qwen Layout Detection - For layout analysis</li> <li>Comparison Guide - Model selection matrix</li> </ul>"},{"location":"reference/","title":"API Reference","text":"<p>Auto-generated documentation for the OmniDocs package.</p>"},{"location":"reference/#package-structure","title":"Package Structure","text":"<pre><code>omnidocs/\n\u251c\u2500\u2500 document.py          # Core document handling\n\u251c\u2500\u2500 tasks/               # Document processing tasks\n\u2502   \u251c\u2500\u2500 layout_analysis/ # Detect document structure\n\u2502   \u251c\u2500\u2500 text_extraction/ # Extract text (Markdown/HTML)\n\u2502   \u2514\u2500\u2500 ocr_extraction/  # Extract text with bboxes\n\u251c\u2500\u2500 inference/           # Backend implementations\n\u2502   \u251c\u2500\u2500 pytorch.py       # PyTorch/HuggingFace\n\u2502   \u251c\u2500\u2500 vllm.py          # High-throughput VLLM\n\u2502   \u251c\u2500\u2500 mlx.py           # Apple Silicon\n\u2502   \u2514\u2500\u2500 api.py           # API-based inference\n\u2514\u2500\u2500 utils/               # Utility functions\n</code></pre>"},{"location":"reference/#quick-start","title":"Quick Start","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\n# Load document\ndoc = Document.from_pdf(\"document.pdf\")\n\n# Initialize extractor\nextractor = DotsOCRTextExtractor(\n    backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n)\n\n# Extract text\nresult = extractor.extract(doc.get_page(0), output_format=\"markdown\")\nprint(result.content)\n</code></pre> <p>Browse the sections in the sidebar to explore the full API.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Document</li> <li>Tasks<ul> <li>Overview</li> <li>Layout Extraction<ul> <li>Overview</li> <li>Base</li> <li>Doc Layout YOLO</li> <li>Models</li> <li>Qwen<ul> <li>Overview</li> <li>API</li> <li>Detector</li> <li>MLX</li> <li>PyTorch</li> <li>VLLM</li> </ul> </li> <li>Rtdetr</li> </ul> </li> <li>OCR Extraction<ul> <li>Overview</li> <li>Base</li> <li>EasyOCR</li> <li>Models</li> <li>PaddleOCR</li> <li>Tesseract</li> </ul> </li> <li>Text Extraction<ul> <li>Overview</li> <li>Base</li> <li>Dots OCR<ul> <li>Overview</li> <li>API</li> <li>Extractor</li> <li>PyTorch</li> <li>VLLM</li> </ul> </li> <li>Models</li> <li>Qwen<ul> <li>Overview</li> <li>API</li> <li>Extractor</li> <li>MLX</li> <li>PyTorch</li> <li>VLLM</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/document/","title":"Document","text":"<p>OmniDocs Document Loader</p> <p>Stateless document container for loading and accessing PDF/image data. Uses pypdfium2 (Apache 2.0) for PDF rendering and pdfplumber (MIT) for text extraction.</p>"},{"location":"reference/document/#omnidocs.document.DocumentLoadError","title":"DocumentLoadError","text":"<p>               Bases: <code>Exception</code></p> <p>Failed to load document.</p>"},{"location":"reference/document/#omnidocs.document.URLDownloadError","title":"URLDownloadError","text":"<p>               Bases: <code>Exception</code></p> <p>Failed to download from URL.</p>"},{"location":"reference/document/#omnidocs.document.PageRangeError","title":"PageRangeError","text":"<p>               Bases: <code>Exception</code></p> <p>Invalid page range.</p>"},{"location":"reference/document/#omnidocs.document.UnsupportedFormatError","title":"UnsupportedFormatError","text":"<p>               Bases: <code>Exception</code></p> <p>Unsupported file format.</p>"},{"location":"reference/document/#omnidocs.document.DocumentMetadata","title":"DocumentMetadata","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata container for documents.</p>"},{"location":"reference/document/#omnidocs.document.LazyPage","title":"LazyPage","text":"<pre><code>LazyPage(\n    pdf_doc: PdfDocument, page_index: int, dpi: int = 150\n)\n</code></pre> <p>Lazy page wrapper - renders only when accessed.</p> <p>This avoids loading all pages into memory upfront for large PDFs.</p> Source code in <code>omnidocs/document.py</code> <pre><code>def __init__(self, pdf_doc: pdfium.PdfDocument, page_index: int, dpi: int = 150):\n    self._pdf_doc = pdf_doc\n    self._page_index = page_index\n    self._dpi = dpi\n    self._cached_image: Optional[Image.Image] = None\n    self._cached_text: Optional[str] = None\n</code></pre>"},{"location":"reference/document/#omnidocs.document.LazyPage.image","title":"image  <code>property</code>","text":"<pre><code>image: Image\n</code></pre> <p>Render page to PIL Image (cached after first access).</p>"},{"location":"reference/document/#omnidocs.document.LazyPage.text","title":"text  <code>property</code>","text":"<pre><code>text: str\n</code></pre> <p>Extract text from page using pypdfium2 (cached).</p>"},{"location":"reference/document/#omnidocs.document.LazyPage.size","title":"size  <code>property</code>","text":"<pre><code>size: tuple\n</code></pre> <p>Get page dimensions without full render (fast).</p>"},{"location":"reference/document/#omnidocs.document.LazyPage.clear_cache","title":"clear_cache","text":"<pre><code>clear_cache()\n</code></pre> <p>Clear cached image to free memory.</p> Source code in <code>omnidocs/document.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear cached image to free memory.\"\"\"\n    self._cached_image = None\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document","title":"Document","text":"<pre><code>Document(\n    pdf_doc: Optional[PdfDocument],\n    pdf_bytes: Optional[bytes],\n    metadata: DocumentMetadata,\n    dpi: int = 150,\n    page_range: Optional[tuple] = None,\n    preloaded_images: Optional[List[Image]] = None,\n)\n</code></pre> <p>Stateless document container for OmniDocs.</p> <p>Features: - Lazy page rendering (pages only rendered when accessed) - Page caching (rendered pages cached to avoid re-rendering) - Multiple source support (PDF file, URL, bytes, images) - Text extraction with pypdfium2 first, pdfplumber fallback - Memory efficient for large documents</p> <p>Design: - Document is SOURCE DATA only - does NOT store task results - Users manage their own analysis results and caching strategy</p> <p>Examples:</p> <pre><code># Load from file\ndoc = Document.from_pdf(\"paper.pdf\")\n\n# Access pages\npage = doc.get_page(0)  # 0-indexed\ntext = doc.get_page_text(1)  # 1-indexed for compatibility\n\n# Iterate efficiently\nfor page in doc.iter_pages():\n        result = layout.extract(page)\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>def __init__(\n    self,\n    pdf_doc: Optional[pdfium.PdfDocument],\n    pdf_bytes: Optional[bytes],\n    metadata: DocumentMetadata,\n    dpi: int = 150,\n    page_range: Optional[tuple] = None,\n    preloaded_images: Optional[List[Image.Image]] = None,\n):\n    self._pdf_doc = pdf_doc\n    self._pdf_bytes = pdf_bytes\n    self._metadata = metadata\n    self._dpi = dpi\n    self._page_range = page_range\n\n    # For image-based documents (no PDF)\n    self._preloaded_images = preloaded_images\n\n    # Lazy page wrappers\n    self._lazy_pages: Optional[List[LazyPage]] = None\n    if pdf_doc is not None:\n        start = page_range[0] if page_range else 0\n        end = page_range[1] if page_range else len(pdf_doc) - 1\n        self._lazy_pages = [LazyPage(pdf_doc, i, dpi) for i in range(start, end + 1)]\n\n    # Full text cache\n    self._full_text_cache: Optional[str] = None\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.page_count","title":"page_count  <code>property</code>","text":"<pre><code>page_count: int\n</code></pre> <p>Number of pages in document.</p>"},{"location":"reference/document/#omnidocs.document.Document.metadata","title":"metadata  <code>property</code>","text":"<pre><code>metadata: DocumentMetadata\n</code></pre> <p>Document metadata.</p>"},{"location":"reference/document/#omnidocs.document.Document.pages","title":"pages  <code>property</code>","text":"<pre><code>pages: List[Image]\n</code></pre> <p>List of all page images.</p> <p>Note: This renders ALL pages. For large documents, use get_page() or iter_pages() instead.</p> RETURNS DESCRIPTION <code>List[Image]</code> <p>List of PIL Images</p>"},{"location":"reference/document/#omnidocs.document.Document.text","title":"text  <code>property</code>","text":"<pre><code>text: str\n</code></pre> <p>Full document text (lazy, cached).</p> <p>Uses pypdfium2 first (fast), falls back to pdfplumber if needed.</p> RETURNS DESCRIPTION <code>str</code> <p>Full document text</p>"},{"location":"reference/document/#omnidocs.document.Document.from_pdf","title":"from_pdf  <code>classmethod</code>","text":"<pre><code>from_pdf(\n    path: str,\n    page_range: Optional[tuple] = None,\n    dpi: int = 150,\n) -&gt; Document\n</code></pre> <p>Load document from PDF file (lazy - pages not rendered yet).</p> PARAMETER DESCRIPTION <code>path</code> <p>Path to PDF file</p> <p> TYPE: <code>str</code> </p> <code>page_range</code> <p>Optional (start, end) tuple for page range (0-indexed, inclusive)</p> <p> TYPE: <code>Optional[tuple]</code> DEFAULT: <code>None</code> </p> <code>dpi</code> <p>Resolution for page rendering (default: 150)</p> <p> TYPE: <code>int</code> DEFAULT: <code>150</code> </p> RETURNS DESCRIPTION <code>Document</code> <p>Document instance</p> RAISES DESCRIPTION <code>DocumentLoadError</code> <p>If file not found</p> <code>UnsupportedFormatError</code> <p>If not a PDF file</p> <code>PageRangeError</code> <p>If page range is invalid</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\ndoc = Document.from_pdf(\"paper.pdf\", page_range=(0, 4))\ndoc = Document.from_pdf(\"paper.pdf\", dpi=300)\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>@classmethod\ndef from_pdf(\n    cls,\n    path: str,\n    page_range: Optional[tuple] = None,\n    dpi: int = 150,\n) -&gt; \"Document\":\n    \"\"\"\n    Load document from PDF file (lazy - pages not rendered yet).\n\n    Args:\n        path: Path to PDF file\n        page_range: Optional (start, end) tuple for page range (0-indexed, inclusive)\n        dpi: Resolution for page rendering (default: 150)\n\n    Returns:\n        Document instance\n\n    Raises:\n        DocumentLoadError: If file not found\n        UnsupportedFormatError: If not a PDF file\n        PageRangeError: If page range is invalid\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        doc = Document.from_pdf(\"paper.pdf\", page_range=(0, 4))\n        doc = Document.from_pdf(\"paper.pdf\", dpi=300)\n        ```\n    \"\"\"\n    path = Path(path)\n\n    if not path.exists():\n        raise DocumentLoadError(f\"File not found: {path}\")\n\n    if path.suffix.lower() != \".pdf\":\n        raise UnsupportedFormatError(f\"Expected PDF file, got: {path.suffix}\")\n\n    # Read file bytes\n    pdf_bytes = path.read_bytes()\n    pdf_doc = pdfium.PdfDocument(pdf_bytes)\n    total_pages = len(pdf_doc)\n\n    # Validate page range\n    if page_range:\n        start, end = page_range\n        if start &lt; 0 or end &gt;= total_pages or start &gt; end:\n            raise PageRangeError(f\"Invalid page range ({start}, {end}) for {total_pages} pages\")\n\n    # Extract metadata (fast, no rendering)\n    pdf_meta = {}\n    try:\n        meta = pdf_doc.get_metadata_dict()\n        if meta:\n            pdf_meta = {k: v for k, v in meta.items() if v}\n    except Exception:\n        pass\n\n    actual_pages = (page_range[1] - page_range[0] + 1) if page_range else total_pages\n\n    metadata = DocumentMetadata(\n        source_type=\"file\",\n        source_path=str(path.absolute()),\n        file_name=path.name,\n        file_size=len(pdf_bytes),\n        pdf_metadata=pdf_meta or None,\n        page_count=actual_pages,\n        format=\"pdf\",\n        image_dpi=dpi,\n    )\n\n    return cls(\n        pdf_doc=pdf_doc,\n        pdf_bytes=pdf_bytes,\n        metadata=metadata,\n        dpi=dpi,\n        page_range=page_range,\n    )\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.from_url","title":"from_url  <code>classmethod</code>","text":"<pre><code>from_url(\n    url: str,\n    page_range: Optional[tuple] = None,\n    dpi: int = 150,\n    timeout: int = 30,\n) -&gt; Document\n</code></pre> <p>Download and load document from URL (lazy).</p> PARAMETER DESCRIPTION <code>url</code> <p>URL to PDF file</p> <p> TYPE: <code>str</code> </p> <code>page_range</code> <p>Optional (start, end) tuple for page range</p> <p> TYPE: <code>Optional[tuple]</code> DEFAULT: <code>None</code> </p> <code>dpi</code> <p>Resolution for page rendering</p> <p> TYPE: <code>int</code> DEFAULT: <code>150</code> </p> <code>timeout</code> <p>Download timeout in seconds</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> RETURNS DESCRIPTION <code>Document</code> <p>Document instance</p> RAISES DESCRIPTION <code>URLDownloadError</code> <p>If download fails</p> <code>PageRangeError</code> <p>If page range is invalid</p> <p>Examples:</p> <pre><code>doc = Document.from_url(\"https://example.com/doc.pdf\")\ndoc = Document.from_url(\"https://example.com/doc.pdf\", timeout=60)\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>@classmethod\ndef from_url(\n    cls,\n    url: str,\n    page_range: Optional[tuple] = None,\n    dpi: int = 150,\n    timeout: int = 30,\n) -&gt; \"Document\":\n    \"\"\"\n    Download and load document from URL (lazy).\n\n    Args:\n        url: URL to PDF file\n        page_range: Optional (start, end) tuple for page range\n        dpi: Resolution for page rendering\n        timeout: Download timeout in seconds\n\n    Returns:\n        Document instance\n\n    Raises:\n        URLDownloadError: If download fails\n        PageRangeError: If page range is invalid\n\n    Examples:\n        ```python\n        doc = Document.from_url(\"https://example.com/doc.pdf\")\n        doc = Document.from_url(\"https://example.com/doc.pdf\", timeout=60)\n        ```\n    \"\"\"\n    try:\n        import requests\n    except ImportError:\n        raise ImportError(\"requests is required for URL downloads. Install with: pip install requests\")\n\n    try:\n        response = requests.get(url, timeout=timeout)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        raise URLDownloadError(f\"Failed to download: {e}\")\n\n    pdf_bytes = response.content\n    pdf_doc = pdfium.PdfDocument(pdf_bytes)\n    total_pages = len(pdf_doc)\n\n    if page_range:\n        start, end = page_range\n        if start &lt; 0 or end &gt;= total_pages or start &gt; end:\n            raise PageRangeError(\"Invalid page range\")\n\n    pdf_meta = {}\n    try:\n        meta = pdf_doc.get_metadata_dict()\n        if meta:\n            pdf_meta = {k: v for k, v in meta.items() if v}\n    except Exception:\n        pass\n\n    file_name = url.split(\"/\")[-1].split(\"?\")[0]\n    if not file_name.endswith(\".pdf\"):\n        file_name = \"downloaded.pdf\"\n\n    actual_pages = (page_range[1] - page_range[0] + 1) if page_range else total_pages\n\n    metadata = DocumentMetadata(\n        source_type=\"url\",\n        source_path=url,\n        file_name=file_name,\n        file_size=len(pdf_bytes),\n        pdf_metadata=pdf_meta or None,\n        page_count=actual_pages,\n        format=\"pdf\",\n        image_dpi=dpi,\n    )\n\n    return cls(\n        pdf_doc=pdf_doc,\n        pdf_bytes=pdf_bytes,\n        metadata=metadata,\n        dpi=dpi,\n        page_range=page_range,\n    )\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.from_bytes","title":"from_bytes  <code>classmethod</code>","text":"<pre><code>from_bytes(\n    data: bytes,\n    filename: Optional[str] = None,\n    page_range: Optional[tuple] = None,\n    dpi: int = 150,\n) -&gt; Document\n</code></pre> <p>Load document from PDF bytes (lazy).</p> PARAMETER DESCRIPTION <code>data</code> <p>PDF file bytes</p> <p> TYPE: <code>bytes</code> </p> <code>filename</code> <p>Optional filename for metadata</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>page_range</code> <p>Optional (start, end) tuple for page range</p> <p> TYPE: <code>Optional[tuple]</code> DEFAULT: <code>None</code> </p> <code>dpi</code> <p>Resolution for page rendering</p> <p> TYPE: <code>int</code> DEFAULT: <code>150</code> </p> RETURNS DESCRIPTION <code>Document</code> <p>Document instance</p> RAISES DESCRIPTION <code>PageRangeError</code> <p>If page range is invalid</p> <p>Examples:</p> <pre><code>with open(\"doc.pdf\", \"rb\") as f:\n        doc = Document.from_bytes(f.read())\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>@classmethod\ndef from_bytes(\n    cls,\n    data: bytes,\n    filename: Optional[str] = None,\n    page_range: Optional[tuple] = None,\n    dpi: int = 150,\n) -&gt; \"Document\":\n    \"\"\"\n    Load document from PDF bytes (lazy).\n\n    Args:\n        data: PDF file bytes\n        filename: Optional filename for metadata\n        page_range: Optional (start, end) tuple for page range\n        dpi: Resolution for page rendering\n\n    Returns:\n        Document instance\n\n    Raises:\n        PageRangeError: If page range is invalid\n\n    Examples:\n        ```python\n        with open(\"doc.pdf\", \"rb\") as f:\n                doc = Document.from_bytes(f.read())\n        ```\n    \"\"\"\n    pdf_doc = pdfium.PdfDocument(data)\n    total_pages = len(pdf_doc)\n\n    if page_range:\n        start, end = page_range\n        if start &lt; 0 or end &gt;= total_pages or start &gt; end:\n            raise PageRangeError(\"Invalid page range\")\n\n    pdf_meta = {}\n    try:\n        meta = pdf_doc.get_metadata_dict()\n        if meta:\n            pdf_meta = {k: v for k, v in meta.items() if v}\n    except Exception:\n        pass\n\n    actual_pages = (page_range[1] - page_range[0] + 1) if page_range else total_pages\n\n    metadata = DocumentMetadata(\n        source_type=\"bytes\",\n        file_name=filename or \"document.pdf\",\n        file_size=len(data),\n        pdf_metadata=pdf_meta or None,\n        page_count=actual_pages,\n        format=\"pdf\",\n        image_dpi=dpi,\n    )\n\n    return cls(\n        pdf_doc=pdf_doc,\n        pdf_bytes=data,\n        metadata=metadata,\n        dpi=dpi,\n        page_range=page_range,\n    )\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.from_image","title":"from_image  <code>classmethod</code>","text":"<pre><code>from_image(path: str) -&gt; Document\n</code></pre> <p>Load document from single image file.</p> PARAMETER DESCRIPTION <code>path</code> <p>Path to image file</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Document</code> <p>Document instance</p> RAISES DESCRIPTION <code>DocumentLoadError</code> <p>If file not found</p> <p>Examples:</p> <pre><code>doc = Document.from_image(\"page.png\")\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>@classmethod\ndef from_image(cls, path: str) -&gt; \"Document\":\n    \"\"\"\n    Load document from single image file.\n\n    Args:\n        path: Path to image file\n\n    Returns:\n        Document instance\n\n    Raises:\n        DocumentLoadError: If file not found\n\n    Examples:\n        ```python\n        doc = Document.from_image(\"page.png\")\n        ```\n    \"\"\"\n    path = Path(path)\n    if not path.exists():\n        raise DocumentLoadError(f\"File not found: {path}\")\n\n    img = Image.open(path).convert(\"RGB\")\n\n    metadata = DocumentMetadata(\n        source_type=\"image\",\n        source_path=str(path.absolute()),\n        file_name=path.name,\n        file_size=path.stat().st_size,\n        page_count=1,\n        format=path.suffix.lower().replace(\".\", \"\"),\n    )\n\n    return cls(\n        pdf_doc=None,\n        pdf_bytes=None,\n        metadata=metadata,\n        preloaded_images=[img],\n    )\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.from_images","title":"from_images  <code>classmethod</code>","text":"<pre><code>from_images(paths: List[str]) -&gt; Document\n</code></pre> <p>Load document from multiple images (multi-page).</p> PARAMETER DESCRIPTION <code>paths</code> <p>List of paths to image files</p> <p> TYPE: <code>List[str]</code> </p> RETURNS DESCRIPTION <code>Document</code> <p>Document instance</p> RAISES DESCRIPTION <code>DocumentLoadError</code> <p>If any file not found</p> <p>Examples:</p> <pre><code>doc = Document.from_images([\"page1.png\", \"page2.png\"])\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>@classmethod\ndef from_images(cls, paths: List[str]) -&gt; \"Document\":\n    \"\"\"\n    Load document from multiple images (multi-page).\n\n    Args:\n        paths: List of paths to image files\n\n    Returns:\n        Document instance\n\n    Raises:\n        DocumentLoadError: If any file not found\n\n    Examples:\n        ```python\n        doc = Document.from_images([\"page1.png\", \"page2.png\"])\n        ```\n    \"\"\"\n    images = []\n    total_size = 0\n\n    for p in paths:\n        path = Path(p)\n        if not path.exists():\n            raise DocumentLoadError(f\"File not found: {path}\")\n        images.append(Image.open(path).convert(\"RGB\"))\n        total_size += path.stat().st_size\n\n    metadata = DocumentMetadata(\n        source_type=\"image\",\n        file_name=f\"{len(paths)}_images\",\n        file_size=total_size,\n        page_count=len(images),\n        format=\"images\",\n    )\n\n    return cls(\n        pdf_doc=None,\n        pdf_bytes=None,\n        metadata=metadata,\n        preloaded_images=images,\n    )\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.get_page","title":"get_page","text":"<pre><code>get_page(page_num: int) -&gt; Image.Image\n</code></pre> <p>Get single page image (0-indexed).</p> <p>More memory efficient than accessing .pages for large documents.</p> PARAMETER DESCRIPTION <code>page_num</code> <p>Page number (0-indexed)</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image</p> RAISES DESCRIPTION <code>PageRangeError</code> <p>If page number out of range</p> <p>Examples:</p> <pre><code>page = doc.get_page(0)  # First page\npage = doc.get_page(doc.page_count - 1)  # Last page\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>def get_page(self, page_num: int) -&gt; Image.Image:\n    \"\"\"\n    Get single page image (0-indexed).\n\n    More memory efficient than accessing .pages for large documents.\n\n    Args:\n        page_num: Page number (0-indexed)\n\n    Returns:\n        PIL Image\n\n    Raises:\n        PageRangeError: If page number out of range\n\n    Examples:\n        ```python\n        page = doc.get_page(0)  # First page\n        page = doc.get_page(doc.page_count - 1)  # Last page\n        ```\n    \"\"\"\n    if self._preloaded_images:\n        if page_num &lt; 0 or page_num &gt;= len(self._preloaded_images):\n            raise PageRangeError(f\"Page {page_num} out of range (0-{len(self._preloaded_images) - 1})\")\n        return self._preloaded_images[page_num]\n\n    if self._lazy_pages:\n        if page_num &lt; 0 or page_num &gt;= len(self._lazy_pages):\n            raise PageRangeError(f\"Page {page_num} out of range (0-{len(self._lazy_pages) - 1})\")\n        return self._lazy_pages[page_num].image\n\n    raise PageRangeError(\"No pages available\")\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.get_page_text","title":"get_page_text","text":"<pre><code>get_page_text(page_num: int) -&gt; str\n</code></pre> <p>Get text for specific page (1-indexed for compatibility with PDF page numbers).</p> PARAMETER DESCRIPTION <code>page_num</code> <p>Page number (1-indexed, like PDF viewers)</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Page text</p> RAISES DESCRIPTION <code>PageRangeError</code> <p>If page number out of range</p> <p>Examples:</p> <pre><code>text = doc.get_page_text(1)  # First page\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>def get_page_text(self, page_num: int) -&gt; str:\n    \"\"\"\n    Get text for specific page (1-indexed for compatibility with PDF page numbers).\n\n    Args:\n        page_num: Page number (1-indexed, like PDF viewers)\n\n    Returns:\n        Page text\n\n    Raises:\n        PageRangeError: If page number out of range\n\n    Examples:\n        ```python\n        text = doc.get_page_text(1)  # First page\n        ```\n    \"\"\"\n    idx = page_num - 1  # Convert to 0-based\n\n    if self._lazy_pages:\n        if idx &lt; 0 or idx &gt;= len(self._lazy_pages):\n            raise PageRangeError(f\"Page {page_num} out of range (1-{len(self._lazy_pages)})\")\n        return self._lazy_pages[idx].text\n\n    return \"\"\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.get_page_size","title":"get_page_size","text":"<pre><code>get_page_size(page_num: int) -&gt; tuple\n</code></pre> <p>Get page dimensions without rendering (fast).</p> PARAMETER DESCRIPTION <code>page_num</code> <p>Page number (0-indexed)</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>Tuple of (width, height) in pixels</p> <p>Examples:</p> <pre><code>width, height = doc.get_page_size(0)\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>def get_page_size(self, page_num: int) -&gt; tuple:\n    \"\"\"\n    Get page dimensions without rendering (fast).\n\n    Args:\n        page_num: Page number (0-indexed)\n\n    Returns:\n        Tuple of (width, height) in pixels\n\n    Examples:\n        ```python\n        width, height = doc.get_page_size(0)\n        ```\n    \"\"\"\n    if self._lazy_pages:\n        if page_num &lt; 0 or page_num &gt;= len(self._lazy_pages):\n            raise PageRangeError(f\"Page {page_num} out of range\")\n        return self._lazy_pages[page_num].size\n\n    if self._preloaded_images:\n        if page_num &lt; 0 or page_num &gt;= len(self._preloaded_images):\n            raise PageRangeError(f\"Page {page_num} out of range\")\n        return self._preloaded_images[page_num].size\n\n    raise PageRangeError(\"No pages available\")\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.iter_pages","title":"iter_pages","text":"<pre><code>iter_pages() -&gt; Iterator[Image.Image]\n</code></pre> <p>Iterate over pages one at a time (memory efficient).</p> <p>Use this for large documents instead of .pages property.</p> YIELDS DESCRIPTION <code>Image</code> <p>PIL Images</p> <p>Examples:</p> <pre><code>for page in doc.iter_pages():\n        result = layout.extract(page)\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>def iter_pages(self) -&gt; Iterator[Image.Image]:\n    \"\"\"\n    Iterate over pages one at a time (memory efficient).\n\n    Use this for large documents instead of .pages property.\n\n    Yields:\n        PIL Images\n\n    Examples:\n        ```python\n        for page in doc.iter_pages():\n                result = layout.extract(page)\n        ```\n    \"\"\"\n    for i in range(self.page_count):\n        yield self.get_page(i)\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.clear_cache","title":"clear_cache","text":"<pre><code>clear_cache(page_num: Optional[int] = None)\n</code></pre> <p>Clear cached page images to free memory.</p> PARAMETER DESCRIPTION <code>page_num</code> <p>Specific page to clear, or None for all pages</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <p>Examples:</p> <pre><code>doc.clear_cache()  # Clear all\ndoc.clear_cache(0)  # Clear just first page\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>def clear_cache(self, page_num: Optional[int] = None):\n    \"\"\"\n    Clear cached page images to free memory.\n\n    Args:\n        page_num: Specific page to clear, or None for all pages\n\n    Examples:\n        ```python\n        doc.clear_cache()  # Clear all\n        doc.clear_cache(0)  # Clear just first page\n        ```\n    \"\"\"\n    if self._lazy_pages:\n        if page_num is not None:\n            if 0 &lt;= page_num &lt; len(self._lazy_pages):\n                self._lazy_pages[page_num].clear_cache()\n        else:\n            for lp in self._lazy_pages:\n                lp.clear_cache()\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.save_images","title":"save_images","text":"<pre><code>save_images(\n    output_dir: str,\n    prefix: str = \"page\",\n    format: str = \"PNG\",\n) -&gt; List[Path]\n</code></pre> <p>Save all pages as individual image files.</p> PARAMETER DESCRIPTION <code>output_dir</code> <p>Output directory path</p> <p> TYPE: <code>str</code> </p> <code>prefix</code> <p>Filename prefix (default: \"page\")</p> <p> TYPE: <code>str</code> DEFAULT: <code>'page'</code> </p> <code>format</code> <p>Image format (default: \"PNG\")</p> <p> TYPE: <code>str</code> DEFAULT: <code>'PNG'</code> </p> RETURNS DESCRIPTION <code>List[Path]</code> <p>List of saved file paths</p> <p>Examples:</p> <pre><code>paths = doc.save_images(\"output/\", prefix=\"doc\", format=\"PNG\")\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>def save_images(\n    self,\n    output_dir: str,\n    prefix: str = \"page\",\n    format: str = \"PNG\",\n) -&gt; List[Path]:\n    \"\"\"\n    Save all pages as individual image files.\n\n    Args:\n        output_dir: Output directory path\n        prefix: Filename prefix (default: \"page\")\n        format: Image format (default: \"PNG\")\n\n    Returns:\n        List of saved file paths\n\n    Examples:\n        ```python\n        paths = doc.save_images(\"output/\", prefix=\"doc\", format=\"PNG\")\n        ```\n    \"\"\"\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    saved = []\n    for i in range(self.page_count):\n        img = self.get_page(i)\n        file_path = output_path / f\"{prefix}_{i + 1:03d}.{format.lower()}\"\n        img.save(file_path, format=format)\n        saved.append(file_path)\n\n    return saved\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert document metadata to dictionary.</p> RETURNS DESCRIPTION <code>dict</code> <p>Dictionary of metadata</p> <p>Examples:</p> <pre><code>data = doc.to_dict()\nprint(data['page_count'])\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"\n    Convert document metadata to dictionary.\n\n    Returns:\n        Dictionary of metadata\n\n    Examples:\n        ```python\n        data = doc.to_dict()\n        print(data['page_count'])\n        ```\n    \"\"\"\n    return self._metadata.model_dump()\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close PDF document and free resources.</p> <p>Examples:</p> <pre><code>doc.close()\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>def close(self):\n    \"\"\"\n    Close PDF document and free resources.\n\n    Examples:\n        ```python\n        doc.close()\n        ```\n    \"\"\"\n    if self._pdf_doc:\n        self._pdf_doc.close()\n        self._pdf_doc = None\n    self._lazy_pages = None\n    self._pdf_bytes = None\n</code></pre>"},{"location":"reference/tasks/overview/","title":"Overview","text":"<p>OmniDocs Task Modules.</p> <p>Each task module provides extractors for specific document processing tasks.</p> Available task modules <ul> <li>layout_extraction: Detect document structure (titles, tables, figures, etc.)</li> <li>ocr_extraction: Extract text with bounding boxes from images</li> <li>text_extraction: Convert document images to HTML/Markdown</li> </ul>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction","title":"layout_extraction","text":"<p>Layout Extraction Module.</p> <p>Provides extractors for detecting document layout elements such as titles, text blocks, figures, tables, formulas, and captions.</p> Available Extractors <ul> <li>DocLayoutYOLO: YOLO-based layout detector (fast, accurate)</li> <li>RTDETRLayoutExtractor: Transformer-based detector (more categories)</li> <li>QwenLayoutDetector: VLM-based detector with custom label support (multi-backend)</li> </ul> Example <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\nextractor = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n# VLM-based detection with custom labels\nfrom omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BaseLayoutExtractor","title":"BaseLayoutExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for layout extractors.</p> <p>All layout extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyLayoutExtractor(BaseLayoutExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Load model weights\n            pass\n\n        def extract(self, image):\n            # Run extraction\n            return LayoutOutput(...)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BaseLayoutExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput containing detected layout boxes with standardized labels</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format is not supported</p> <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/layout_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n\n    Returns:\n        LayoutOutput containing detected layout boxes with standardized labels\n\n    Raises:\n        ValueError: If image format is not supported\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.DocLayoutYOLO","title":"DocLayoutYOLO","text":"<pre><code>DocLayoutYOLO(config: DocLayoutYOLOConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>DocLayout-YOLO layout extractor.</p> <p>A YOLO-based model optimized for document layout detection. Detects: title, text, figure, table, formula, captions, etc.</p> <p>This is a single-backend model (PyTorch only).</p> Example <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\nextractor = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre> <p>Initialize DocLayout-YOLO extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object with device, model_path, etc.</p> <p> TYPE: <code>DocLayoutYOLOConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/doc_layout_yolo.py</code> <pre><code>def __init__(self, config: DocLayoutYOLOConfig):\n    \"\"\"\n    Initialize DocLayout-YOLO extractor.\n\n    Args:\n        config: Configuration object with device, model_path, etc.\n    \"\"\"\n    self.config = config\n    self._model = None\n    self._device = self._resolve_device(config.device)\n    self._model_path = self._resolve_model_path(config.model_path)\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.DocLayoutYOLO.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> Source code in <code>omnidocs/tasks/layout_extraction/doc_layout_yolo.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n    if self._model is None:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    img_width, img_height = pil_image.size\n\n    # Run inference\n    results = self._model.predict(\n        pil_image,\n        imgsz=self.config.img_size,\n        conf=self.config.confidence,\n        device=self._device,\n    )\n\n    result = results[0]\n\n    # Parse detections\n    layout_boxes = []\n\n    if hasattr(result, \"boxes\") and result.boxes is not None:\n        boxes = result.boxes\n\n        for i in range(len(boxes)):\n            # Get coordinates\n            bbox_coords = boxes.xyxy[i].cpu().numpy().tolist()\n\n            # Get class and confidence\n            class_id = int(boxes.cls[i].item())\n            confidence = float(boxes.conf[i].item())\n\n            # Get original label from class names\n            original_label = DOCLAYOUT_YOLO_CLASS_NAMES.get(class_id, f\"class_{class_id}\")\n\n            # Map to standardized label\n            standard_label = DOCLAYOUT_YOLO_MAPPING.to_standard(original_label)\n\n            layout_boxes.append(\n                LayoutBox(\n                    label=standard_label,\n                    bbox=BoundingBox.from_list(bbox_coords),\n                    confidence=confidence,\n                    class_id=class_id,\n                    original_label=original_label,\n                )\n            )\n\n    # Sort by y-coordinate (top to bottom reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=img_width,\n        image_height=img_height,\n        model_name=\"DocLayout-YOLO\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.DocLayoutYOLOConfig","title":"DocLayoutYOLOConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for DocLayout-YOLO layout extractor.</p> <p>This is a single-backend model (PyTorch only).</p> Example <pre><code>config = DocLayoutYOLOConfig(device=\"cuda\", confidence=0.3)\nextractor = DocLayoutYOLO(config=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box coordinates in pixel space.</p> <p>Coordinates follow the convention: (x1, y1) is top-left, (x2, y2) is bottom-right.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x, y, width, height) format.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_xywh(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x, y, width, height) format.\"\"\"\n    return (self.x1, self.y1, self.width, self.height)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> <p>Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas. This provides consistent coordinates regardless of original image size.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Example <pre><code>bbox = BoundingBox(x1=100, y1=50, x2=500, y2=300)\nnormalized = bbox.to_normalized(1000, 800)\n# x: 100/1000*1024 = 102.4, y: 50/800*1024 = 64\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas.\n    This provides consistent coordinates regardless of original image size.\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n\n    Example:\n        ```python\n        bbox = BoundingBox(x1=100, y1=50, x2=500, y2=300)\n        normalized = bbox.to_normalized(1000, 800)\n        # x: 100/1000*1024 = 102.4, y: 50/800*1024 = 64\n        ```\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox.to_absolute","title":"to_absolute","text":"<pre><code>to_absolute(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert from normalized (0-1024) to absolute pixel coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Target image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Target image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with absolute pixel coordinates</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_absolute(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert from normalized (0-1024) to absolute pixel coordinates.\n\n    Args:\n        image_width: Target image width in pixels\n        image_height: Target image height in pixels\n\n    Returns:\n        New BoundingBox with absolute pixel coordinates\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / NORMALIZED_SIZE * image_width,\n        y1=self.y1 / NORMALIZED_SIZE * image_height,\n        x2=self.x2 / NORMALIZED_SIZE * image_width,\n        y2=self.y2 / NORMALIZED_SIZE * image_height,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.CustomLabel","title":"CustomLabel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Type-safe custom layout label definition for VLM-based models.</p> <p>VLM models like Qwen3-VL support flexible custom labels beyond the standard LayoutLabel enum. Use this class to define custom labels with validation.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import CustomLabel\n\n# Simple custom label\ncode_block = CustomLabel(name=\"code_block\")\n\n# With metadata\nsidebar = CustomLabel(\n        name=\"sidebar\",\n        description=\"Secondary content panel\",\n        color=\"#9B59B6\",\n    )\n\n# Use with QwenLayoutDetector\nresult = detector.extract(image, custom_labels=[code_block, sidebar])\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LabelMapping","title":"LabelMapping","text":"<pre><code>LabelMapping(mapping: Dict[str, LayoutLabel])\n</code></pre> <p>Base class for model-specific label mappings.</p> <p>Each model maps its native labels to standardized LayoutLabel values.</p> <p>Initialize label mapping.</p> PARAMETER DESCRIPTION <code>mapping</code> <p>Dict mapping model-specific labels to LayoutLabel enum values</p> <p> TYPE: <code>Dict[str, LayoutLabel]</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def __init__(self, mapping: Dict[str, LayoutLabel]):\n    \"\"\"\n    Initialize label mapping.\n\n    Args:\n        mapping: Dict mapping model-specific labels to LayoutLabel enum values\n    \"\"\"\n    self._mapping = {k.lower(): v for k, v in mapping.items()}\n    self._reverse_mapping = {v: k for k, v in mapping.items()}\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LabelMapping.supported_labels","title":"supported_labels  <code>property</code>","text":"<pre><code>supported_labels: List[str]\n</code></pre> <p>Get list of supported model-specific labels.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LabelMapping.standard_labels","title":"standard_labels  <code>property</code>","text":"<pre><code>standard_labels: List[LayoutLabel]\n</code></pre> <p>Get list of standard labels this mapping produces.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LabelMapping.to_standard","title":"to_standard","text":"<pre><code>to_standard(model_label: str) -&gt; LayoutLabel\n</code></pre> <p>Convert model-specific label to standardized LayoutLabel.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_standard(self, model_label: str) -&gt; LayoutLabel:\n    \"\"\"Convert model-specific label to standardized LayoutLabel.\"\"\"\n    return self._mapping.get(model_label.lower(), LayoutLabel.UNKNOWN)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LabelMapping.from_standard","title":"from_standard","text":"<pre><code>from_standard(standard_label: LayoutLabel) -&gt; Optional[str]\n</code></pre> <p>Convert standardized LayoutLabel to model-specific label.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def from_standard(self, standard_label: LayoutLabel) -&gt; Optional[str]:\n    \"\"\"Convert standardized LayoutLabel to model-specific label.\"\"\"\n    return self._reverse_mapping.get(standard_label)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutBox","title":"LayoutBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single detected layout element with label, bounding box, and confidence.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutBox.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"label\": self.label.value,\n        \"bbox\": self.bbox.to_list(),\n        \"confidence\": self.confidence,\n        \"class_id\": self.class_id,\n        \"original_label\": self.original_label,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutBox.get_normalized_bbox","title":"get_normalized_bbox","text":"<pre><code>get_normalized_bbox(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Get bounding box in normalized (0-1024) coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox with normalized coordinates</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def get_normalized_bbox(self, image_width: int, image_height: int) -&gt; BoundingBox:\n    \"\"\"\n    Get bounding box in normalized (0-1024) coordinates.\n\n    Args:\n        image_width: Original image width\n        image_height: Original image height\n\n    Returns:\n        BoundingBox with normalized coordinates\n    \"\"\"\n    return self.bbox.to_normalized(image_width, image_height)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutLabel","title":"LayoutLabel","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Standardized layout labels used across all layout extractors.</p> <p>These provide a consistent vocabulary regardless of which model is used.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput","title":"LayoutOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete layout extraction results for a single image.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.element_count","title":"element_count  <code>property</code>","text":"<pre><code>element_count: int\n</code></pre> <p>Number of detected elements.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.labels_found","title":"labels_found  <code>property</code>","text":"<pre><code>labels_found: List[str]\n</code></pre> <p>Unique labels found in detections.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.filter_by_label","title":"filter_by_label","text":"<pre><code>filter_by_label(label: LayoutLabel) -&gt; List[LayoutBox]\n</code></pre> <p>Filter boxes by label.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def filter_by_label(self, label: LayoutLabel) -&gt; List[LayoutBox]:\n    \"\"\"Filter boxes by label.\"\"\"\n    return [box for box in self.bboxes if box.label == label]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(\n    min_confidence: float,\n) -&gt; List[LayoutBox]\n</code></pre> <p>Filter boxes by minimum confidence.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def filter_by_confidence(self, min_confidence: float) -&gt; List[LayoutBox]:\n    \"\"\"Filter boxes by minimum confidence.\"\"\"\n    return [box for box in self.bboxes if box.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"bboxes\": [box.to_dict() for box in self.bboxes],\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"element_count\": self.element_count,\n        \"labels_found\": self.labels_found,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.sort_by_position","title":"sort_by_position","text":"<pre><code>sort_by_position(\n    top_to_bottom: bool = True,\n) -&gt; LayoutOutput\n</code></pre> <p>Return a new LayoutOutput with boxes sorted by position.</p> PARAMETER DESCRIPTION <code>top_to_bottom</code> <p>If True, sort by y-coordinate (reading order)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def sort_by_position(self, top_to_bottom: bool = True) -&gt; \"LayoutOutput\":\n    \"\"\"\n    Return a new LayoutOutput with boxes sorted by position.\n\n    Args:\n        top_to_bottom: If True, sort by y-coordinate (reading order)\n    \"\"\"\n    sorted_boxes = sorted(self.bboxes, key=lambda b: (b.bbox.y1, b.bbox.x1), reverse=not top_to_bottom)\n    return LayoutOutput(\n        bboxes=sorted_boxes,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        model_name=self.model_name,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.get_normalized_bboxes","title":"get_normalized_bboxes","text":"<pre><code>get_normalized_bboxes() -&gt; List[Dict]\n</code></pre> <p>Get all bounding boxes in normalized (0-1024) coordinates.</p> RETURNS DESCRIPTION <code>List[Dict]</code> <p>List of dicts with normalized bbox coordinates and metadata.</p> Example <pre><code>result = extractor.extract(image)\nnormalized = result.get_normalized_bboxes()\nfor box in normalized:\n        print(f\"{box['label']}: {box['bbox']}\")  # coords in 0-1024 range\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def get_normalized_bboxes(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all bounding boxes in normalized (0-1024) coordinates.\n\n    Returns:\n        List of dicts with normalized bbox coordinates and metadata.\n\n    Example:\n        ```python\n        result = extractor.extract(image)\n        normalized = result.get_normalized_bboxes()\n        for box in normalized:\n                print(f\"{box['label']}: {box['bbox']}\")  # coords in 0-1024 range\n        ```\n    \"\"\"\n    normalized = []\n    for box in self.bboxes:\n        norm_bbox = box.bbox.to_normalized(self.image_width, self.image_height)\n        normalized.append(\n            {\n                \"label\": box.label.value,\n                \"bbox\": norm_bbox.to_list(),\n                \"confidence\": box.confidence,\n                \"class_id\": box.class_id,\n                \"original_label\": box.original_label,\n            }\n        )\n    return normalized\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.visualize","title":"visualize","text":"<pre><code>visualize(\n    image: Image,\n    output_path: Optional[Union[str, Path]] = None,\n    show_labels: bool = True,\n    show_confidence: bool = True,\n    line_width: int = 3,\n    font_size: int = 12,\n) -&gt; Image.Image\n</code></pre> <p>Visualize layout detection results on the image.</p> <p>Draws bounding boxes with labels and confidence scores on the image. Each layout category has a distinct color for easy identification.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to draw on (will be copied, not modified)</p> <p> TYPE: <code>Image</code> </p> <code>output_path</code> <p>Optional path to save the visualization</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> <code>show_labels</code> <p>Whether to show label text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_confidence</code> <p>Whether to show confidence scores</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>line_width</code> <p>Width of bounding box lines</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>font_size</code> <p>Size of label text (note: uses default font)</p> <p> TYPE: <code>int</code> DEFAULT: <code>12</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image with visualizations drawn</p> Example <pre><code>result = extractor.extract(image)\nviz = result.visualize(image, output_path=\"layout_viz.png\")\nviz.show()  # Display in notebook/viewer\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def visualize(\n    self,\n    image: \"Image.Image\",\n    output_path: Optional[Union[str, Path]] = None,\n    show_labels: bool = True,\n    show_confidence: bool = True,\n    line_width: int = 3,\n    font_size: int = 12,\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Visualize layout detection results on the image.\n\n    Draws bounding boxes with labels and confidence scores on the image.\n    Each layout category has a distinct color for easy identification.\n\n    Args:\n        image: PIL Image to draw on (will be copied, not modified)\n        output_path: Optional path to save the visualization\n        show_labels: Whether to show label text\n        show_confidence: Whether to show confidence scores\n        line_width: Width of bounding box lines\n        font_size: Size of label text (note: uses default font)\n\n    Returns:\n        PIL Image with visualizations drawn\n\n    Example:\n        ```python\n        result = extractor.extract(image)\n        viz = result.visualize(image, output_path=\"layout_viz.png\")\n        viz.show()  # Display in notebook/viewer\n        ```\n    \"\"\"\n    from PIL import ImageDraw\n\n    # Copy image to avoid modifying original\n    viz_image = image.copy().convert(\"RGB\")\n    draw = ImageDraw.Draw(viz_image)\n\n    for box in self.bboxes:\n        # Get color for this label\n        color = LABEL_COLORS.get(box.label, \"#95A5A6\")\n\n        # Draw bounding box\n        coords = box.bbox.to_xyxy()\n        draw.rectangle(coords, outline=color, width=line_width)\n\n        # Build label text\n        if show_labels or show_confidence:\n            label_parts = []\n            if show_labels:\n                label_parts.append(box.label.value)\n            if show_confidence:\n                label_parts.append(f\"{box.confidence:.2f}\")\n            label_text = \" \".join(label_parts)\n\n            # Draw label background\n            text_bbox = draw.textbbox((coords[0], coords[1] - 20), label_text)\n            draw.rectangle(text_bbox, fill=color)\n\n            # Draw label text\n            draw.text(\n                (coords[0], coords[1] - 20),\n                label_text,\n                fill=\"white\",\n            )\n\n    # Save if path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        viz_image.save(output_path)\n\n    return viz_image\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; LayoutOutput\n</code></pre> <p>Load a LayoutOutput instance from a JSON file.</p> <p>Reads a JSON file and deserializes its contents into a LayoutOutput object. Uses Pydantic's model_validate_json for proper handling of nested objects.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to JSON file containing serialized LayoutOutput data.       Can be string or pathlib.Path object.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>Deserialized layout output instance from file.</p> <p> TYPE: <code>LayoutOutput</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>UnicodeDecodeError</code> <p>If file cannot be decoded as UTF-8.</p> <code>ValueError</code> <p>If file contents are not valid JSON.</p> <code>ValidationError</code> <p>If JSON data doesn't match LayoutOutput schema.</p> Example <p><pre><code>output = LayoutOutput.load_json('layout_results.json')\nprint(f\"Found {output.element_count} elements\")\n</code></pre> Found 5 elements</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"LayoutOutput\":\n    \"\"\"\n    Load a LayoutOutput instance from a JSON file.\n\n    Reads a JSON file and deserializes its contents into a LayoutOutput object.\n    Uses Pydantic's model_validate_json for proper handling of nested objects.\n\n    Args:\n        file_path: Path to JSON file containing serialized LayoutOutput data.\n                  Can be string or pathlib.Path object.\n\n    Returns:\n        LayoutOutput: Deserialized layout output instance from file.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n        UnicodeDecodeError: If file cannot be decoded as UTF-8.\n        ValueError: If file contents are not valid JSON.\n        ValidationError: If JSON data doesn't match LayoutOutput schema.\n\n    Example:\n        ```python\n        output = LayoutOutput.load_json('layout_results.json')\n        print(f\"Found {output.element_count} elements\")\n        ```\n        Found 5 elements\n    \"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save LayoutOutput instance to a JSON file.</p> <p>Serializes the LayoutOutput object to JSON and writes it to a file. Automatically creates parent directories if they don't exist. Uses UTF-8 encoding for compatibility and proper handling of special characters.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path where JSON file should be saved. Can be string or       pathlib.Path object. Parent directories will be created       if they don't exist.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> RAISES DESCRIPTION <code>OSError</code> <p>If file cannot be written due to permission or disk errors.</p> <code>TypeError</code> <p>If file_path is not a string or Path object.</p> Example <pre><code>output = LayoutOutput(bboxes=[], image_width=800, image_height=600)\noutput.save_json('results/layout_output.json')\n# File is created at results/layout_output.json\n# Parent 'results' directory is created if it didn't exist\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"\n    Save LayoutOutput instance to a JSON file.\n\n    Serializes the LayoutOutput object to JSON and writes it to a file.\n    Automatically creates parent directories if they don't exist. Uses UTF-8\n    encoding for compatibility and proper handling of special characters.\n\n    Args:\n        file_path: Path where JSON file should be saved. Can be string or\n                  pathlib.Path object. Parent directories will be created\n                  if they don't exist.\n\n    Returns:\n        None\n\n    Raises:\n        OSError: If file cannot be written due to permission or disk errors.\n        TypeError: If file_path is not a string or Path object.\n\n    Example:\n        ```python\n        output = LayoutOutput(bboxes=[], image_width=800, image_height=600)\n        output.save_json('results/layout_output.json')\n        # File is created at results/layout_output.json\n        # Parent 'results' directory is created if it didn't exist\n        ```\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.QwenLayoutDetector","title":"QwenLayoutDetector","text":"<pre><code>QwenLayoutDetector(backend: QwenLayoutBackendConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>Qwen3-VL Vision-Language Model layout detector.</p> <p>A flexible VLM-based layout detector that supports custom labels. Unlike fixed-label models (DocLayoutYOLO, RT-DETR), Qwen can detect any document elements specified at runtime.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\n# Initialize with PyTorch backend\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Basic extraction with default labels\nresult = detector.extract(image)\n\n# With custom labels (strings)\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n\n# With typed custom labels\nlabels = [\n        CustomLabel(name=\"code_block\", color=\"#E74C3C\"),\n        CustomLabel(name=\"sidebar\", description=\"Side panel content\"),\n    ]\nresult = detector.extract(image, custom_labels=labels)\n</code></pre> <p>Initialize Qwen layout detector.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend - QwenLayoutVLLMConfig: VLLM high-throughput backend - QwenLayoutMLXConfig: MLX backend for Apple Silicon - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenLayoutBackendConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def __init__(self, backend: QwenLayoutBackendConfig):\n    \"\"\"\n    Initialize Qwen layout detector.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenLayoutVLLMConfig: VLLM high-throughput backend\n            - QwenLayoutMLXConfig: MLX backend for Apple Silicon\n            - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.QwenLayoutDetector.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    custom_labels: Optional[\n        List[Union[str, CustomLabel]]\n    ] = None,\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout detection on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>custom_labels</code> <p>Optional custom labels to detect. Can be: - None: Use default labels (title, text, table, figure, etc.) - List[str]: Simple label names [\"code_block\", \"sidebar\"] - List[CustomLabel]: Typed labels with metadata</p> <p> TYPE: <code>Optional[List[Union[str, CustomLabel]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    custom_labels: Optional[List[Union[str, CustomLabel]]] = None,\n) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout detection on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        custom_labels: Optional custom labels to detect. Can be:\n            - None: Use default labels (title, text, table, figure, etc.)\n            - List[str]: Simple label names [\"code_block\", \"sidebar\"]\n            - List[CustomLabel]: Typed labels with metadata\n\n    Returns:\n        LayoutOutput with detected layout boxes\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Normalize labels\n    label_names = self._normalize_labels(custom_labels)\n\n    # Build prompt\n    prompt = self._build_detection_prompt(label_names)\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenLayoutPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenLayoutVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenLayoutMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenLayoutAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse detections\n    detections = self._parse_json_output(raw_output)\n\n    # Convert to LayoutOutput\n    layout_boxes = self._build_layout_boxes(detections, width, height)\n\n    # Sort by position (reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.RTDETRConfig","title":"RTDETRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for RT-DETR layout extractor.</p> <p>This is a single-backend model (PyTorch/Transformers only).</p> Example <pre><code>config = RTDETRConfig(device=\"cuda\", confidence=0.4)\nextractor = RTDETRLayoutExtractor(config=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.RTDETRLayoutExtractor","title":"RTDETRLayoutExtractor","text":"<pre><code>RTDETRLayoutExtractor(config: RTDETRConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>RT-DETR layout extractor using HuggingFace Transformers.</p> <p>A transformer-based real-time detection model for document layout. Detects: title, text, table, figure, list, formula, captions, headers, footers.</p> <p>This is a single-backend model (PyTorch/Transformers only).</p> Example <pre><code>from omnidocs.tasks.layout_extraction import RTDETRLayoutExtractor, RTDETRConfig\n\nextractor = RTDETRLayoutExtractor(config=RTDETRConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre> <p>Initialize RT-DETR layout extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object with device, model settings, etc.</p> <p> TYPE: <code>RTDETRConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/rtdetr.py</code> <pre><code>def __init__(self, config: RTDETRConfig):\n    \"\"\"\n    Initialize RT-DETR layout extractor.\n\n    Args:\n        config: Configuration object with device, model settings, etc.\n    \"\"\"\n    self.config = config\n    self._model = None\n    self._processor = None\n    self._device = self._resolve_device(config.device)\n    self._model_path = self._resolve_model_path(config.model_path)\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.RTDETRLayoutExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> Source code in <code>omnidocs/tasks/layout_extraction/rtdetr.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n    import torch\n\n    if self._model is None or self._processor is None:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    img_width, img_height = pil_image.size\n\n    # Preprocess\n    inputs = self._processor(\n        images=pil_image,\n        return_tensors=\"pt\",\n        size={\"height\": self.config.image_size, \"width\": self.config.image_size},\n    )\n\n    # Move to device\n    inputs = {k: v.to(self._device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n\n    # Run inference\n    with torch.no_grad():\n        outputs = self._model(**inputs)\n\n    # Post-process results\n    target_sizes = torch.tensor([[img_height, img_width]])\n    results = self._processor.post_process_object_detection(\n        outputs,\n        target_sizes=target_sizes,\n        threshold=self.config.confidence,\n    )[0]\n\n    # Parse detections\n    layout_boxes = []\n\n    for score, label_id, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n        confidence = float(score.item())\n        class_id = int(label_id.item())\n\n        # Get original label from model config\n        # Note: The model outputs 0-indexed class IDs, but id2label has background at index 0,\n        # so we add 1 to map correctly (e.g., model output 8 -&gt; id2label[9] = \"Table\")\n        original_label = self._model.config.id2label.get(class_id + 1, f\"class_{class_id}\")\n\n        # Map to standardized label\n        standard_label = RTDETR_MAPPING.to_standard(original_label)\n\n        # Box coordinates\n        box_coords = box.cpu().tolist()\n\n        layout_boxes.append(\n            LayoutBox(\n                label=standard_label,\n                bbox=BoundingBox.from_list(box_coords),\n                confidence=confidence,\n                class_id=class_id,\n                original_label=original_label,\n            )\n        )\n\n    # Sort by y-coordinate (top to bottom reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=img_width,\n        image_height=img_height,\n        model_name=\"RT-DETR (docling-layout)\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.base","title":"base","text":"<p>Base class for layout extractors.</p> <p>Defines the abstract interface that all layout extractors must implement.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.base.BaseLayoutExtractor","title":"BaseLayoutExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for layout extractors.</p> <p>All layout extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyLayoutExtractor(BaseLayoutExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Load model weights\n            pass\n\n        def extract(self, image):\n            # Run extraction\n            return LayoutOutput(...)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.base.BaseLayoutExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput containing detected layout boxes with standardized labels</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format is not supported</p> <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/layout_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n\n    Returns:\n        LayoutOutput containing detected layout boxes with standardized labels\n\n    Raises:\n        ValueError: If image format is not supported\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.doc_layout_yolo","title":"doc_layout_yolo","text":"<p>DocLayout-YOLO layout extractor.</p> <p>A YOLO-based model for document layout detection, optimized for academic papers and technical documents.</p> <p>Model: juliozhao/DocLayout-YOLO-DocStructBench</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.doc_layout_yolo.DocLayoutYOLOConfig","title":"DocLayoutYOLOConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for DocLayout-YOLO layout extractor.</p> <p>This is a single-backend model (PyTorch only).</p> Example <pre><code>config = DocLayoutYOLOConfig(device=\"cuda\", confidence=0.3)\nextractor = DocLayoutYOLO(config=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.doc_layout_yolo.DocLayoutYOLO","title":"DocLayoutYOLO","text":"<pre><code>DocLayoutYOLO(config: DocLayoutYOLOConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>DocLayout-YOLO layout extractor.</p> <p>A YOLO-based model optimized for document layout detection. Detects: title, text, figure, table, formula, captions, etc.</p> <p>This is a single-backend model (PyTorch only).</p> Example <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\nextractor = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre> <p>Initialize DocLayout-YOLO extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object with device, model_path, etc.</p> <p> TYPE: <code>DocLayoutYOLOConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/doc_layout_yolo.py</code> <pre><code>def __init__(self, config: DocLayoutYOLOConfig):\n    \"\"\"\n    Initialize DocLayout-YOLO extractor.\n\n    Args:\n        config: Configuration object with device, model_path, etc.\n    \"\"\"\n    self.config = config\n    self._model = None\n    self._device = self._resolve_device(config.device)\n    self._model_path = self._resolve_model_path(config.model_path)\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.doc_layout_yolo.DocLayoutYOLO.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> Source code in <code>omnidocs/tasks/layout_extraction/doc_layout_yolo.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n    if self._model is None:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    img_width, img_height = pil_image.size\n\n    # Run inference\n    results = self._model.predict(\n        pil_image,\n        imgsz=self.config.img_size,\n        conf=self.config.confidence,\n        device=self._device,\n    )\n\n    result = results[0]\n\n    # Parse detections\n    layout_boxes = []\n\n    if hasattr(result, \"boxes\") and result.boxes is not None:\n        boxes = result.boxes\n\n        for i in range(len(boxes)):\n            # Get coordinates\n            bbox_coords = boxes.xyxy[i].cpu().numpy().tolist()\n\n            # Get class and confidence\n            class_id = int(boxes.cls[i].item())\n            confidence = float(boxes.conf[i].item())\n\n            # Get original label from class names\n            original_label = DOCLAYOUT_YOLO_CLASS_NAMES.get(class_id, f\"class_{class_id}\")\n\n            # Map to standardized label\n            standard_label = DOCLAYOUT_YOLO_MAPPING.to_standard(original_label)\n\n            layout_boxes.append(\n                LayoutBox(\n                    label=standard_label,\n                    bbox=BoundingBox.from_list(bbox_coords),\n                    confidence=confidence,\n                    class_id=class_id,\n                    original_label=original_label,\n                )\n            )\n\n    # Sort by y-coordinate (top to bottom reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=img_width,\n        image_height=img_height,\n        model_name=\"DocLayout-YOLO\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models","title":"models","text":"<p>Pydantic models for layout extraction outputs.</p> <p>Defines standardized output types and label enums for layout detection.</p> Coordinate Systems <ul> <li>Absolute (default): Coordinates in pixels relative to original image size</li> <li>Normalized (0-1024): Coordinates scaled to 0-1024 range (virtual 1024x1024 canvas)</li> </ul> <p>Use <code>bbox.to_normalized(width, height)</code> or <code>output.get_normalized_bboxes()</code> to convert to normalized coordinates.</p> Example <pre><code>result = extractor.extract(image)  # Returns absolute pixel coordinates\nnormalized = result.get_normalized_bboxes()  # Returns 0-1024 normalized coords\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutLabel","title":"LayoutLabel","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Standardized layout labels used across all layout extractors.</p> <p>These provide a consistent vocabulary regardless of which model is used.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.CustomLabel","title":"CustomLabel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Type-safe custom layout label definition for VLM-based models.</p> <p>VLM models like Qwen3-VL support flexible custom labels beyond the standard LayoutLabel enum. Use this class to define custom labels with validation.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import CustomLabel\n\n# Simple custom label\ncode_block = CustomLabel(name=\"code_block\")\n\n# With metadata\nsidebar = CustomLabel(\n        name=\"sidebar\",\n        description=\"Secondary content panel\",\n        color=\"#9B59B6\",\n    )\n\n# Use with QwenLayoutDetector\nresult = detector.extract(image, custom_labels=[code_block, sidebar])\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LabelMapping","title":"LabelMapping","text":"<pre><code>LabelMapping(mapping: Dict[str, LayoutLabel])\n</code></pre> <p>Base class for model-specific label mappings.</p> <p>Each model maps its native labels to standardized LayoutLabel values.</p> <p>Initialize label mapping.</p> PARAMETER DESCRIPTION <code>mapping</code> <p>Dict mapping model-specific labels to LayoutLabel enum values</p> <p> TYPE: <code>Dict[str, LayoutLabel]</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def __init__(self, mapping: Dict[str, LayoutLabel]):\n    \"\"\"\n    Initialize label mapping.\n\n    Args:\n        mapping: Dict mapping model-specific labels to LayoutLabel enum values\n    \"\"\"\n    self._mapping = {k.lower(): v for k, v in mapping.items()}\n    self._reverse_mapping = {v: k for k, v in mapping.items()}\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LabelMapping.supported_labels","title":"supported_labels  <code>property</code>","text":"<pre><code>supported_labels: List[str]\n</code></pre> <p>Get list of supported model-specific labels.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LabelMapping.standard_labels","title":"standard_labels  <code>property</code>","text":"<pre><code>standard_labels: List[LayoutLabel]\n</code></pre> <p>Get list of standard labels this mapping produces.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LabelMapping.to_standard","title":"to_standard","text":"<pre><code>to_standard(model_label: str) -&gt; LayoutLabel\n</code></pre> <p>Convert model-specific label to standardized LayoutLabel.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_standard(self, model_label: str) -&gt; LayoutLabel:\n    \"\"\"Convert model-specific label to standardized LayoutLabel.\"\"\"\n    return self._mapping.get(model_label.lower(), LayoutLabel.UNKNOWN)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LabelMapping.from_standard","title":"from_standard","text":"<pre><code>from_standard(standard_label: LayoutLabel) -&gt; Optional[str]\n</code></pre> <p>Convert standardized LayoutLabel to model-specific label.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def from_standard(self, standard_label: LayoutLabel) -&gt; Optional[str]:\n    \"\"\"Convert standardized LayoutLabel to model-specific label.\"\"\"\n    return self._reverse_mapping.get(standard_label)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box coordinates in pixel space.</p> <p>Coordinates follow the convention: (x1, y1) is top-left, (x2, y2) is bottom-right.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x, y, width, height) format.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_xywh(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x, y, width, height) format.\"\"\"\n    return (self.x1, self.y1, self.width, self.height)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> <p>Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas. This provides consistent coordinates regardless of original image size.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Example <pre><code>bbox = BoundingBox(x1=100, y1=50, x2=500, y2=300)\nnormalized = bbox.to_normalized(1000, 800)\n# x: 100/1000*1024 = 102.4, y: 50/800*1024 = 64\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas.\n    This provides consistent coordinates regardless of original image size.\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n\n    Example:\n        ```python\n        bbox = BoundingBox(x1=100, y1=50, x2=500, y2=300)\n        normalized = bbox.to_normalized(1000, 800)\n        # x: 100/1000*1024 = 102.4, y: 50/800*1024 = 64\n        ```\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_absolute","title":"to_absolute","text":"<pre><code>to_absolute(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert from normalized (0-1024) to absolute pixel coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Target image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Target image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with absolute pixel coordinates</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_absolute(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert from normalized (0-1024) to absolute pixel coordinates.\n\n    Args:\n        image_width: Target image width in pixels\n        image_height: Target image height in pixels\n\n    Returns:\n        New BoundingBox with absolute pixel coordinates\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / NORMALIZED_SIZE * image_width,\n        y1=self.y1 / NORMALIZED_SIZE * image_height,\n        x2=self.x2 / NORMALIZED_SIZE * image_width,\n        y2=self.y2 / NORMALIZED_SIZE * image_height,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutBox","title":"LayoutBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single detected layout element with label, bounding box, and confidence.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutBox.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"label\": self.label.value,\n        \"bbox\": self.bbox.to_list(),\n        \"confidence\": self.confidence,\n        \"class_id\": self.class_id,\n        \"original_label\": self.original_label,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutBox.get_normalized_bbox","title":"get_normalized_bbox","text":"<pre><code>get_normalized_bbox(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Get bounding box in normalized (0-1024) coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox with normalized coordinates</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def get_normalized_bbox(self, image_width: int, image_height: int) -&gt; BoundingBox:\n    \"\"\"\n    Get bounding box in normalized (0-1024) coordinates.\n\n    Args:\n        image_width: Original image width\n        image_height: Original image height\n\n    Returns:\n        BoundingBox with normalized coordinates\n    \"\"\"\n    return self.bbox.to_normalized(image_width, image_height)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput","title":"LayoutOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete layout extraction results for a single image.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.element_count","title":"element_count  <code>property</code>","text":"<pre><code>element_count: int\n</code></pre> <p>Number of detected elements.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.labels_found","title":"labels_found  <code>property</code>","text":"<pre><code>labels_found: List[str]\n</code></pre> <p>Unique labels found in detections.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.filter_by_label","title":"filter_by_label","text":"<pre><code>filter_by_label(label: LayoutLabel) -&gt; List[LayoutBox]\n</code></pre> <p>Filter boxes by label.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def filter_by_label(self, label: LayoutLabel) -&gt; List[LayoutBox]:\n    \"\"\"Filter boxes by label.\"\"\"\n    return [box for box in self.bboxes if box.label == label]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(\n    min_confidence: float,\n) -&gt; List[LayoutBox]\n</code></pre> <p>Filter boxes by minimum confidence.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def filter_by_confidence(self, min_confidence: float) -&gt; List[LayoutBox]:\n    \"\"\"Filter boxes by minimum confidence.\"\"\"\n    return [box for box in self.bboxes if box.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"bboxes\": [box.to_dict() for box in self.bboxes],\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"element_count\": self.element_count,\n        \"labels_found\": self.labels_found,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.sort_by_position","title":"sort_by_position","text":"<pre><code>sort_by_position(\n    top_to_bottom: bool = True,\n) -&gt; LayoutOutput\n</code></pre> <p>Return a new LayoutOutput with boxes sorted by position.</p> PARAMETER DESCRIPTION <code>top_to_bottom</code> <p>If True, sort by y-coordinate (reading order)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def sort_by_position(self, top_to_bottom: bool = True) -&gt; \"LayoutOutput\":\n    \"\"\"\n    Return a new LayoutOutput with boxes sorted by position.\n\n    Args:\n        top_to_bottom: If True, sort by y-coordinate (reading order)\n    \"\"\"\n    sorted_boxes = sorted(self.bboxes, key=lambda b: (b.bbox.y1, b.bbox.x1), reverse=not top_to_bottom)\n    return LayoutOutput(\n        bboxes=sorted_boxes,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        model_name=self.model_name,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.get_normalized_bboxes","title":"get_normalized_bboxes","text":"<pre><code>get_normalized_bboxes() -&gt; List[Dict]\n</code></pre> <p>Get all bounding boxes in normalized (0-1024) coordinates.</p> RETURNS DESCRIPTION <code>List[Dict]</code> <p>List of dicts with normalized bbox coordinates and metadata.</p> Example <pre><code>result = extractor.extract(image)\nnormalized = result.get_normalized_bboxes()\nfor box in normalized:\n        print(f\"{box['label']}: {box['bbox']}\")  # coords in 0-1024 range\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def get_normalized_bboxes(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all bounding boxes in normalized (0-1024) coordinates.\n\n    Returns:\n        List of dicts with normalized bbox coordinates and metadata.\n\n    Example:\n        ```python\n        result = extractor.extract(image)\n        normalized = result.get_normalized_bboxes()\n        for box in normalized:\n                print(f\"{box['label']}: {box['bbox']}\")  # coords in 0-1024 range\n        ```\n    \"\"\"\n    normalized = []\n    for box in self.bboxes:\n        norm_bbox = box.bbox.to_normalized(self.image_width, self.image_height)\n        normalized.append(\n            {\n                \"label\": box.label.value,\n                \"bbox\": norm_bbox.to_list(),\n                \"confidence\": box.confidence,\n                \"class_id\": box.class_id,\n                \"original_label\": box.original_label,\n            }\n        )\n    return normalized\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.visualize","title":"visualize","text":"<pre><code>visualize(\n    image: Image,\n    output_path: Optional[Union[str, Path]] = None,\n    show_labels: bool = True,\n    show_confidence: bool = True,\n    line_width: int = 3,\n    font_size: int = 12,\n) -&gt; Image.Image\n</code></pre> <p>Visualize layout detection results on the image.</p> <p>Draws bounding boxes with labels and confidence scores on the image. Each layout category has a distinct color for easy identification.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to draw on (will be copied, not modified)</p> <p> TYPE: <code>Image</code> </p> <code>output_path</code> <p>Optional path to save the visualization</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> <code>show_labels</code> <p>Whether to show label text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_confidence</code> <p>Whether to show confidence scores</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>line_width</code> <p>Width of bounding box lines</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>font_size</code> <p>Size of label text (note: uses default font)</p> <p> TYPE: <code>int</code> DEFAULT: <code>12</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image with visualizations drawn</p> Example <pre><code>result = extractor.extract(image)\nviz = result.visualize(image, output_path=\"layout_viz.png\")\nviz.show()  # Display in notebook/viewer\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def visualize(\n    self,\n    image: \"Image.Image\",\n    output_path: Optional[Union[str, Path]] = None,\n    show_labels: bool = True,\n    show_confidence: bool = True,\n    line_width: int = 3,\n    font_size: int = 12,\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Visualize layout detection results on the image.\n\n    Draws bounding boxes with labels and confidence scores on the image.\n    Each layout category has a distinct color for easy identification.\n\n    Args:\n        image: PIL Image to draw on (will be copied, not modified)\n        output_path: Optional path to save the visualization\n        show_labels: Whether to show label text\n        show_confidence: Whether to show confidence scores\n        line_width: Width of bounding box lines\n        font_size: Size of label text (note: uses default font)\n\n    Returns:\n        PIL Image with visualizations drawn\n\n    Example:\n        ```python\n        result = extractor.extract(image)\n        viz = result.visualize(image, output_path=\"layout_viz.png\")\n        viz.show()  # Display in notebook/viewer\n        ```\n    \"\"\"\n    from PIL import ImageDraw\n\n    # Copy image to avoid modifying original\n    viz_image = image.copy().convert(\"RGB\")\n    draw = ImageDraw.Draw(viz_image)\n\n    for box in self.bboxes:\n        # Get color for this label\n        color = LABEL_COLORS.get(box.label, \"#95A5A6\")\n\n        # Draw bounding box\n        coords = box.bbox.to_xyxy()\n        draw.rectangle(coords, outline=color, width=line_width)\n\n        # Build label text\n        if show_labels or show_confidence:\n            label_parts = []\n            if show_labels:\n                label_parts.append(box.label.value)\n            if show_confidence:\n                label_parts.append(f\"{box.confidence:.2f}\")\n            label_text = \" \".join(label_parts)\n\n            # Draw label background\n            text_bbox = draw.textbbox((coords[0], coords[1] - 20), label_text)\n            draw.rectangle(text_bbox, fill=color)\n\n            # Draw label text\n            draw.text(\n                (coords[0], coords[1] - 20),\n                label_text,\n                fill=\"white\",\n            )\n\n    # Save if path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        viz_image.save(output_path)\n\n    return viz_image\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; LayoutOutput\n</code></pre> <p>Load a LayoutOutput instance from a JSON file.</p> <p>Reads a JSON file and deserializes its contents into a LayoutOutput object. Uses Pydantic's model_validate_json for proper handling of nested objects.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to JSON file containing serialized LayoutOutput data.       Can be string or pathlib.Path object.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>Deserialized layout output instance from file.</p> <p> TYPE: <code>LayoutOutput</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>UnicodeDecodeError</code> <p>If file cannot be decoded as UTF-8.</p> <code>ValueError</code> <p>If file contents are not valid JSON.</p> <code>ValidationError</code> <p>If JSON data doesn't match LayoutOutput schema.</p> Example <p><pre><code>output = LayoutOutput.load_json('layout_results.json')\nprint(f\"Found {output.element_count} elements\")\n</code></pre> Found 5 elements</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"LayoutOutput\":\n    \"\"\"\n    Load a LayoutOutput instance from a JSON file.\n\n    Reads a JSON file and deserializes its contents into a LayoutOutput object.\n    Uses Pydantic's model_validate_json for proper handling of nested objects.\n\n    Args:\n        file_path: Path to JSON file containing serialized LayoutOutput data.\n                  Can be string or pathlib.Path object.\n\n    Returns:\n        LayoutOutput: Deserialized layout output instance from file.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n        UnicodeDecodeError: If file cannot be decoded as UTF-8.\n        ValueError: If file contents are not valid JSON.\n        ValidationError: If JSON data doesn't match LayoutOutput schema.\n\n    Example:\n        ```python\n        output = LayoutOutput.load_json('layout_results.json')\n        print(f\"Found {output.element_count} elements\")\n        ```\n        Found 5 elements\n    \"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save LayoutOutput instance to a JSON file.</p> <p>Serializes the LayoutOutput object to JSON and writes it to a file. Automatically creates parent directories if they don't exist. Uses UTF-8 encoding for compatibility and proper handling of special characters.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path where JSON file should be saved. Can be string or       pathlib.Path object. Parent directories will be created       if they don't exist.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> RAISES DESCRIPTION <code>OSError</code> <p>If file cannot be written due to permission or disk errors.</p> <code>TypeError</code> <p>If file_path is not a string or Path object.</p> Example <pre><code>output = LayoutOutput(bboxes=[], image_width=800, image_height=600)\noutput.save_json('results/layout_output.json')\n# File is created at results/layout_output.json\n# Parent 'results' directory is created if it didn't exist\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"\n    Save LayoutOutput instance to a JSON file.\n\n    Serializes the LayoutOutput object to JSON and writes it to a file.\n    Automatically creates parent directories if they don't exist. Uses UTF-8\n    encoding for compatibility and proper handling of special characters.\n\n    Args:\n        file_path: Path where JSON file should be saved. Can be string or\n                  pathlib.Path object. Parent directories will be created\n                  if they don't exist.\n\n    Returns:\n        None\n\n    Raises:\n        OSError: If file cannot be written due to permission or disk errors.\n        TypeError: If file_path is not a string or Path object.\n\n    Example:\n        ```python\n        output = LayoutOutput(bboxes=[], image_width=800, image_height=600)\n        output.save_json('results/layout_output.json')\n        # File is created at results/layout_output.json\n        # Parent 'results' directory is created if it didn't exist\n        ```\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen","title":"qwen","text":"<p>Qwen3-VL backend configurations and detector for layout detection.</p> Available backends <ul> <li>QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend</li> <li>QwenLayoutVLLMConfig: VLLM high-throughput backend</li> <li>QwenLayoutMLXConfig: MLX backend for Apple Silicon</li> <li>QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</li> </ul> Example <pre><code>from omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\nconfig = QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutAPIConfig","title":"QwenLayoutAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen layout detection.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenLayoutAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutDetector","title":"QwenLayoutDetector","text":"<pre><code>QwenLayoutDetector(backend: QwenLayoutBackendConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>Qwen3-VL Vision-Language Model layout detector.</p> <p>A flexible VLM-based layout detector that supports custom labels. Unlike fixed-label models (DocLayoutYOLO, RT-DETR), Qwen can detect any document elements specified at runtime.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\n# Initialize with PyTorch backend\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Basic extraction with default labels\nresult = detector.extract(image)\n\n# With custom labels (strings)\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n\n# With typed custom labels\nlabels = [\n        CustomLabel(name=\"code_block\", color=\"#E74C3C\"),\n        CustomLabel(name=\"sidebar\", description=\"Side panel content\"),\n    ]\nresult = detector.extract(image, custom_labels=labels)\n</code></pre> <p>Initialize Qwen layout detector.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend - QwenLayoutVLLMConfig: VLLM high-throughput backend - QwenLayoutMLXConfig: MLX backend for Apple Silicon - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenLayoutBackendConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def __init__(self, backend: QwenLayoutBackendConfig):\n    \"\"\"\n    Initialize Qwen layout detector.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenLayoutVLLMConfig: VLLM high-throughput backend\n            - QwenLayoutMLXConfig: MLX backend for Apple Silicon\n            - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutDetector.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    custom_labels: Optional[\n        List[Union[str, CustomLabel]]\n    ] = None,\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout detection on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>custom_labels</code> <p>Optional custom labels to detect. Can be: - None: Use default labels (title, text, table, figure, etc.) - List[str]: Simple label names [\"code_block\", \"sidebar\"] - List[CustomLabel]: Typed labels with metadata</p> <p> TYPE: <code>Optional[List[Union[str, CustomLabel]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    custom_labels: Optional[List[Union[str, CustomLabel]]] = None,\n) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout detection on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        custom_labels: Optional custom labels to detect. Can be:\n            - None: Use default labels (title, text, table, figure, etc.)\n            - List[str]: Simple label names [\"code_block\", \"sidebar\"]\n            - List[CustomLabel]: Typed labels with metadata\n\n    Returns:\n        LayoutOutput with detected layout boxes\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Normalize labels\n    label_names = self._normalize_labels(custom_labels)\n\n    # Build prompt\n    prompt = self._build_detection_prompt(label_names)\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenLayoutPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenLayoutVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenLayoutMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenLayoutAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse detections\n    detections = self._parse_json_output(raw_output)\n\n    # Convert to LayoutOutput\n    layout_boxes = self._build_layout_boxes(detections, width, height)\n\n    # Sort by position (reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutMLXConfig","title":"QwenLayoutMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen layout detection.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenLayoutMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutPyTorchConfig","title":"QwenLayoutPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen layout detection.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutVLLMConfig","title":"QwenLayoutVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen layout detection.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.api","title":"api","text":"<p>API backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.api.QwenLayoutAPIConfig","title":"QwenLayoutAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen layout detection.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenLayoutAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.detector","title":"detector","text":"<p>Qwen3-VL layout detector.</p> <p>A Vision-Language Model for flexible layout detection with custom label support. Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = detector.extract(image)\n\n# With custom labels\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.detector.QwenLayoutDetector","title":"QwenLayoutDetector","text":"<pre><code>QwenLayoutDetector(backend: QwenLayoutBackendConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>Qwen3-VL Vision-Language Model layout detector.</p> <p>A flexible VLM-based layout detector that supports custom labels. Unlike fixed-label models (DocLayoutYOLO, RT-DETR), Qwen can detect any document elements specified at runtime.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\n# Initialize with PyTorch backend\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Basic extraction with default labels\nresult = detector.extract(image)\n\n# With custom labels (strings)\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n\n# With typed custom labels\nlabels = [\n        CustomLabel(name=\"code_block\", color=\"#E74C3C\"),\n        CustomLabel(name=\"sidebar\", description=\"Side panel content\"),\n    ]\nresult = detector.extract(image, custom_labels=labels)\n</code></pre> <p>Initialize Qwen layout detector.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend - QwenLayoutVLLMConfig: VLLM high-throughput backend - QwenLayoutMLXConfig: MLX backend for Apple Silicon - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenLayoutBackendConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def __init__(self, backend: QwenLayoutBackendConfig):\n    \"\"\"\n    Initialize Qwen layout detector.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenLayoutVLLMConfig: VLLM high-throughput backend\n            - QwenLayoutMLXConfig: MLX backend for Apple Silicon\n            - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.detector.QwenLayoutDetector.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    custom_labels: Optional[\n        List[Union[str, CustomLabel]]\n    ] = None,\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout detection on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>custom_labels</code> <p>Optional custom labels to detect. Can be: - None: Use default labels (title, text, table, figure, etc.) - List[str]: Simple label names [\"code_block\", \"sidebar\"] - List[CustomLabel]: Typed labels with metadata</p> <p> TYPE: <code>Optional[List[Union[str, CustomLabel]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    custom_labels: Optional[List[Union[str, CustomLabel]]] = None,\n) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout detection on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        custom_labels: Optional custom labels to detect. Can be:\n            - None: Use default labels (title, text, table, figure, etc.)\n            - List[str]: Simple label names [\"code_block\", \"sidebar\"]\n            - List[CustomLabel]: Typed labels with metadata\n\n    Returns:\n        LayoutOutput with detected layout boxes\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Normalize labels\n    label_names = self._normalize_labels(custom_labels)\n\n    # Build prompt\n    prompt = self._build_detection_prompt(label_names)\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenLayoutPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenLayoutVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenLayoutMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenLayoutAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse detections\n    detections = self._parse_json_output(raw_output)\n\n    # Convert to LayoutOutput\n    layout_boxes = self._build_layout_boxes(detections, width, height)\n\n    # Sort by position (reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.mlx","title":"mlx","text":"<p>MLX backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.mlx.QwenLayoutMLXConfig","title":"QwenLayoutMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen layout detection.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenLayoutMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.pytorch","title":"pytorch","text":"<p>PyTorch/HuggingFace backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.pytorch.QwenLayoutPyTorchConfig","title":"QwenLayoutPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen layout detection.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.vllm","title":"vllm","text":"<p>VLLM backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.vllm.QwenLayoutVLLMConfig","title":"QwenLayoutVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen layout detection.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.rtdetr","title":"rtdetr","text":"<p>RT-DETR layout extractor.</p> <p>A transformer-based real-time detection model for document layout detection. Uses HuggingFace Transformers implementation.</p> <p>Model: HuggingPanda/docling-layout</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.rtdetr.RTDETRConfig","title":"RTDETRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for RT-DETR layout extractor.</p> <p>This is a single-backend model (PyTorch/Transformers only).</p> Example <pre><code>config = RTDETRConfig(device=\"cuda\", confidence=0.4)\nextractor = RTDETRLayoutExtractor(config=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.rtdetr.RTDETRLayoutExtractor","title":"RTDETRLayoutExtractor","text":"<pre><code>RTDETRLayoutExtractor(config: RTDETRConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>RT-DETR layout extractor using HuggingFace Transformers.</p> <p>A transformer-based real-time detection model for document layout. Detects: title, text, table, figure, list, formula, captions, headers, footers.</p> <p>This is a single-backend model (PyTorch/Transformers only).</p> Example <pre><code>from omnidocs.tasks.layout_extraction import RTDETRLayoutExtractor, RTDETRConfig\n\nextractor = RTDETRLayoutExtractor(config=RTDETRConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre> <p>Initialize RT-DETR layout extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object with device, model settings, etc.</p> <p> TYPE: <code>RTDETRConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/rtdetr.py</code> <pre><code>def __init__(self, config: RTDETRConfig):\n    \"\"\"\n    Initialize RT-DETR layout extractor.\n\n    Args:\n        config: Configuration object with device, model settings, etc.\n    \"\"\"\n    self.config = config\n    self._model = None\n    self._processor = None\n    self._device = self._resolve_device(config.device)\n    self._model_path = self._resolve_model_path(config.model_path)\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.rtdetr.RTDETRLayoutExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> Source code in <code>omnidocs/tasks/layout_extraction/rtdetr.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n    import torch\n\n    if self._model is None or self._processor is None:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    img_width, img_height = pil_image.size\n\n    # Preprocess\n    inputs = self._processor(\n        images=pil_image,\n        return_tensors=\"pt\",\n        size={\"height\": self.config.image_size, \"width\": self.config.image_size},\n    )\n\n    # Move to device\n    inputs = {k: v.to(self._device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n\n    # Run inference\n    with torch.no_grad():\n        outputs = self._model(**inputs)\n\n    # Post-process results\n    target_sizes = torch.tensor([[img_height, img_width]])\n    results = self._processor.post_process_object_detection(\n        outputs,\n        target_sizes=target_sizes,\n        threshold=self.config.confidence,\n    )[0]\n\n    # Parse detections\n    layout_boxes = []\n\n    for score, label_id, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n        confidence = float(score.item())\n        class_id = int(label_id.item())\n\n        # Get original label from model config\n        # Note: The model outputs 0-indexed class IDs, but id2label has background at index 0,\n        # so we add 1 to map correctly (e.g., model output 8 -&gt; id2label[9] = \"Table\")\n        original_label = self._model.config.id2label.get(class_id + 1, f\"class_{class_id}\")\n\n        # Map to standardized label\n        standard_label = RTDETR_MAPPING.to_standard(original_label)\n\n        # Box coordinates\n        box_coords = box.cpu().tolist()\n\n        layout_boxes.append(\n            LayoutBox(\n                label=standard_label,\n                bbox=BoundingBox.from_list(box_coords),\n                confidence=confidence,\n                class_id=class_id,\n                original_label=original_label,\n            )\n        )\n\n    # Sort by y-coordinate (top to bottom reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=img_width,\n        image_height=img_height,\n        model_name=\"RT-DETR (docling-layout)\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction","title":"ocr_extraction","text":"<p>OCR Extraction Module.</p> <p>Provides extractors for detecting text with bounding boxes from document images. Returns text content along with spatial coordinates (unlike Text Extraction which returns formatted Markdown/HTML without coordinates).</p> Available Extractors <ul> <li>TesseractOCR: Open-source OCR (CPU, requires system Tesseract)</li> <li>EasyOCR: PyTorch-based OCR (CPU/GPU, 80+ languages)</li> <li>PaddleOCR: PaddlePaddle-based OCR (CPU/GPU, excellent CJK support)</li> </ul> Key Difference from Text Extraction <ul> <li>OCR Extraction: Text + Bounding Boxes (spatial location)</li> <li>Text Extraction: Markdown/HTML (formatted document export)</li> </ul> Example <pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\n\nocr = TesseractOCR(config=TesseractOCRConfig(languages=[\"eng\"]))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()} (conf: {block.confidence:.2f})\")\n# With EasyOCR\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\nocr = EasyOCR(config=EasyOCRConfig(languages=[\"en\", \"ch_sim\"], gpu=True))\nresult = ocr.extract(image)\n# With PaddleOCR\nfrom omnidocs.tasks.ocr_extraction import PaddleOCR, PaddleOCRConfig\n\nocr = PaddleOCR(config=PaddleOCRConfig(lang=\"ch\", device=\"cpu\"))\nresult = ocr.extract(image)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BaseOCRExtractor","title":"BaseOCRExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for OCR extractors.</p> <p>All OCR extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyOCRExtractor(BaseOCRExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Initialize OCR engine\n            pass\n\n        def extract(self, image):\n            # Run OCR extraction\n            return OCROutput(...)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BaseOCRExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput containing detected text blocks with bounding boxes</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format is not supported</p> <code>RuntimeError</code> <p>If OCR engine is not initialized or extraction fails</p> Source code in <code>omnidocs/tasks/ocr_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR extraction on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n\n    Returns:\n        OCROutput containing detected text blocks with bounding boxes\n\n    Raises:\n        ValueError: If image format is not supported\n        RuntimeError: If OCR engine is not initialized or extraction fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.EasyOCR","title":"EasyOCR","text":"<pre><code>EasyOCR(config: EasyOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>EasyOCR text extractor.</p> <p>Single-backend model (PyTorch - CPU/GPU).</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\nocr = EasyOCR(config=EasyOCRConfig(languages=[\"en\"], gpu=True))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize EasyOCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>EasyOCRConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If easyocr is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def __init__(self, config: EasyOCRConfig):\n    \"\"\"\n    Initialize EasyOCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        ImportError: If easyocr is not installed\n    \"\"\"\n    self.config = config\n    self._reader = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.EasyOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    detail: int = 1,\n    paragraph: bool = False,\n    min_size: int = 10,\n    text_threshold: float = 0.7,\n    low_text: float = 0.4,\n    link_threshold: float = 0.4,\n    canvas_size: int = 2560,\n    mag_ratio: float = 1.0,\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>detail</code> <p>0 = simple output, 1 = detailed with boxes</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>paragraph</code> <p>Combine results into paragraphs</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>min_size</code> <p>Minimum text box size</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>text_threshold</code> <p>Text confidence threshold</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.7</code> </p> <code>low_text</code> <p>Low text bound</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>link_threshold</code> <p>Link threshold for text joining</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>canvas_size</code> <p>Max image dimension for processing</p> <p> TYPE: <code>int</code> DEFAULT: <code>2560</code> </p> <code>mag_ratio</code> <p>Magnification ratio</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks</p> RAISES DESCRIPTION <code>ValueError</code> <p>If detail is not 0 or 1</p> <code>RuntimeError</code> <p>If EasyOCR is not initialized</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    detail: int = 1,\n    paragraph: bool = False,\n    min_size: int = 10,\n    text_threshold: float = 0.7,\n    low_text: float = 0.4,\n    link_threshold: float = 0.4,\n    canvas_size: int = 2560,\n    mag_ratio: float = 1.0,\n) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n        detail: 0 = simple output, 1 = detailed with boxes\n        paragraph: Combine results into paragraphs\n        min_size: Minimum text box size\n        text_threshold: Text confidence threshold\n        low_text: Low text bound\n        link_threshold: Link threshold for text joining\n        canvas_size: Max image dimension for processing\n        mag_ratio: Magnification ratio\n\n    Returns:\n        OCROutput with detected text blocks\n\n    Raises:\n        ValueError: If detail is not 0 or 1\n        RuntimeError: If EasyOCR is not initialized\n    \"\"\"\n    if self._reader is None:\n        raise RuntimeError(\"EasyOCR not initialized. Call _load_model() first.\")\n\n    # Validate detail parameter\n    if detail not in (0, 1):\n        raise ValueError(f\"detail must be 0 or 1, got {detail}\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Convert to numpy array for EasyOCR\n    image_array = np.array(pil_image)\n\n    # Run EasyOCR\n    results = self._reader.readtext(\n        image_array,\n        detail=detail,\n        paragraph=paragraph,\n        min_size=min_size,\n        text_threshold=text_threshold,\n        low_text=low_text,\n        link_threshold=link_threshold,\n        canvas_size=canvas_size,\n        mag_ratio=mag_ratio,\n    )\n\n    # Parse results\n    text_blocks = []\n    full_text_parts = []\n\n    for result in results:\n        if detail == 0:\n            # Simple output: just text\n            text = result\n            confidence = 1.0\n            bbox = BoundingBox(x1=0, y1=0, x2=0, y2=0)\n            polygon = None\n        else:\n            # Detailed output: [polygon, text, confidence]\n            polygon_points, text, confidence = result\n\n            # EasyOCR returns 4 corner points: [[x1,y1], [x2,y1], [x2,y2], [x1,y2]]\n            # Convert to list of lists for storage\n            polygon = [list(p) for p in polygon_points]\n\n            # Convert to axis-aligned bounding box\n            bbox = BoundingBox.from_polygon(polygon)\n\n        if not text.strip():\n            continue\n\n        text_blocks.append(\n            TextBlock(\n                text=text,\n                bbox=bbox,\n                confidence=float(confidence),\n                granularity=(OCRGranularity.LINE if paragraph else OCRGranularity.WORD),\n                polygon=polygon,\n                language=\"+\".join(self.config.languages),\n            )\n        )\n\n        full_text_parts.append(text)\n\n    # Sort by position\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\" \".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.EasyOCR.extract_batch","title":"extract_batch","text":"<pre><code>extract_batch(\n    images: List[Union[Image, ndarray, str, Path]], **kwargs\n) -&gt; List[OCROutput]\n</code></pre> <p>Run OCR on multiple images.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of input images</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>**kwargs</code> <p>Arguments passed to extract()</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>List[OCROutput]</code> <p>List of OCROutput objects</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def extract_batch(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    **kwargs,\n) -&gt; List[OCROutput]:\n    \"\"\"\n    Run OCR on multiple images.\n\n    Args:\n        images: List of input images\n        **kwargs: Arguments passed to extract()\n\n    Returns:\n        List of OCROutput objects\n    \"\"\"\n    results = []\n    for img in images:\n        results.append(self.extract(img, **kwargs))\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.EasyOCRConfig","title":"EasyOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for EasyOCR extractor.</p> <p>This is a single-backend model (PyTorch - CPU/GPU).</p> Example <pre><code>config = EasyOCRConfig(languages=[\"en\", \"ch_sim\"], gpu=True)\nocr = EasyOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box coordinates in pixel space.</p> <p>Coordinates follow the convention: (x1, y1) is top-left, (x2, y2) is bottom-right. For rotated text, use the polygon field in TextBlock instead.</p> Example <pre><code>bbox = BoundingBox(x1=100, y1=50, x2=300, y2=80)\nprint(bbox.width, bbox.height)  # 200, 30\nprint(bbox.center)  # (200.0, 65.0)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x, y, width, height) format.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_xywh(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x, y, width, height) format.\"\"\"\n    return (self.x1, self.y1, self.width, self.height)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.from_polygon","title":"from_polygon  <code>classmethod</code>","text":"<pre><code>from_polygon(polygon: List[List[float]]) -&gt; BoundingBox\n</code></pre> <p>Create axis-aligned bounding box from polygon points.</p> PARAMETER DESCRIPTION <code>polygon</code> <p>List of [x, y] points (usually 4 for quadrilateral)</p> <p> TYPE: <code>List[List[float]]</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox that encloses all polygon points</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef from_polygon(cls, polygon: List[List[float]]) -&gt; \"BoundingBox\":\n    \"\"\"\n    Create axis-aligned bounding box from polygon points.\n\n    Args:\n        polygon: List of [x, y] points (usually 4 for quadrilateral)\n\n    Returns:\n        BoundingBox that encloses all polygon points\n    \"\"\"\n    if not polygon:\n        raise ValueError(\"Polygon cannot be empty\")\n\n    xs = [p[0] for p in polygon]\n    ys = [p[1] for p in polygon]\n    return cls(x1=min(xs), y1=min(ys), x2=max(xs), y2=max(ys))\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> <p>Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas. This provides consistent coordinates regardless of original image size.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas.\n    This provides consistent coordinates regardless of original image size.\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.to_absolute","title":"to_absolute","text":"<pre><code>to_absolute(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert from normalized (0-1024) to absolute pixel coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Target image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Target image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with absolute pixel coordinates</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_absolute(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert from normalized (0-1024) to absolute pixel coordinates.\n\n    Args:\n        image_width: Target image width in pixels\n        image_height: Target image height in pixels\n\n    Returns:\n        New BoundingBox with absolute pixel coordinates\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / NORMALIZED_SIZE * image_width,\n        y1=self.y1 / NORMALIZED_SIZE * image_height,\n        x2=self.x2 / NORMALIZED_SIZE * image_width,\n        y2=self.y2 / NORMALIZED_SIZE * image_height,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCRGranularity","title":"OCRGranularity","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>OCR detection granularity levels.</p> <p>Different OCR engines return results at different granularity levels. This enum standardizes the options across all extractors.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput","title":"OCROutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete OCR extraction results for a single image.</p> <p>Contains all detected text blocks with their bounding boxes, plus metadata about the extraction.</p> Example <pre><code>result = ocr.extract(image)\nprint(f\"Found {result.block_count} blocks\")\nprint(f\"Full text: {result.full_text}\")\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.block_count","title":"block_count  <code>property</code>","text":"<pre><code>block_count: int\n</code></pre> <p>Number of detected text blocks.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.word_count","title":"word_count  <code>property</code>","text":"<pre><code>word_count: int\n</code></pre> <p>Approximate word count from full text.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.average_confidence","title":"average_confidence  <code>property</code>","text":"<pre><code>average_confidence: float\n</code></pre> <p>Average confidence across all text blocks.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(\n    min_confidence: float,\n) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by minimum confidence.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def filter_by_confidence(self, min_confidence: float) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by minimum confidence.\"\"\"\n    return [b for b in self.text_blocks if b.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.filter_by_granularity","title":"filter_by_granularity","text":"<pre><code>filter_by_granularity(\n    granularity: OCRGranularity,\n) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by granularity level.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def filter_by_granularity(self, granularity: OCRGranularity) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by granularity level.\"\"\"\n    return [b for b in self.text_blocks if b.granularity == granularity]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"text_blocks\": [b.to_dict() for b in self.text_blocks],\n        \"full_text\": self.full_text,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"languages_detected\": self.languages_detected,\n        \"block_count\": self.block_count,\n        \"word_count\": self.word_count,\n        \"average_confidence\": self.average_confidence,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.sort_by_position","title":"sort_by_position","text":"<pre><code>sort_by_position(top_to_bottom: bool = True) -&gt; OCROutput\n</code></pre> <p>Return a new OCROutput with blocks sorted by position.</p> PARAMETER DESCRIPTION <code>top_to_bottom</code> <p>If True, sort by y-coordinate (reading order)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>New OCROutput with sorted text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def sort_by_position(self, top_to_bottom: bool = True) -&gt; \"OCROutput\":\n    \"\"\"\n    Return a new OCROutput with blocks sorted by position.\n\n    Args:\n        top_to_bottom: If True, sort by y-coordinate (reading order)\n\n    Returns:\n        New OCROutput with sorted text blocks\n    \"\"\"\n    sorted_blocks = sorted(\n        self.text_blocks,\n        key=lambda b: (b.bbox.y1, b.bbox.x1),\n        reverse=not top_to_bottom,\n    )\n    # Regenerate full_text in sorted order\n    full_text = \" \".join(b.text for b in sorted_blocks)\n\n    return OCROutput(\n        text_blocks=sorted_blocks,\n        full_text=full_text,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        model_name=self.model_name,\n        languages_detected=self.languages_detected,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.get_normalized_blocks","title":"get_normalized_blocks","text":"<pre><code>get_normalized_blocks() -&gt; List[Dict]\n</code></pre> <p>Get all text blocks with normalized (0-1024) coordinates.</p> RETURNS DESCRIPTION <code>List[Dict]</code> <p>List of dicts with normalized bbox coordinates and metadata.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def get_normalized_blocks(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all text blocks with normalized (0-1024) coordinates.\n\n    Returns:\n        List of dicts with normalized bbox coordinates and metadata.\n    \"\"\"\n    normalized = []\n    for block in self.text_blocks:\n        norm_bbox = block.bbox.to_normalized(self.image_width, self.image_height)\n        normalized.append(\n            {\n                \"text\": block.text,\n                \"bbox\": norm_bbox.to_list(),\n                \"confidence\": block.confidence,\n                \"granularity\": block.granularity.value,\n                \"language\": block.language,\n            }\n        )\n    return normalized\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.visualize","title":"visualize","text":"<pre><code>visualize(\n    image: Image,\n    output_path: Optional[Union[str, Path]] = None,\n    show_text: bool = True,\n    show_confidence: bool = False,\n    line_width: int = 2,\n    box_color: str = \"#2ECC71\",\n    text_color: str = \"#000000\",\n) -&gt; Image.Image\n</code></pre> <p>Visualize OCR results on the image.</p> <p>Draws bounding boxes around detected text with optional labels.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to draw on (will be copied, not modified)</p> <p> TYPE: <code>Image</code> </p> <code>output_path</code> <p>Optional path to save the visualization</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> <code>show_text</code> <p>Whether to show detected text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_confidence</code> <p>Whether to show confidence scores</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>line_width</code> <p>Width of bounding box lines</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>box_color</code> <p>Color for bounding boxes (hex)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'#2ECC71'</code> </p> <code>text_color</code> <p>Color for text labels (hex)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'#000000'</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image with visualizations drawn</p> Example <pre><code>result = ocr.extract(image)\nviz = result.visualize(image, output_path=\"ocr_viz.png\")\n</code></pre> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def visualize(\n    self,\n    image: \"Image.Image\",\n    output_path: Optional[Union[str, Path]] = None,\n    show_text: bool = True,\n    show_confidence: bool = False,\n    line_width: int = 2,\n    box_color: str = \"#2ECC71\",\n    text_color: str = \"#000000\",\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Visualize OCR results on the image.\n\n    Draws bounding boxes around detected text with optional labels.\n\n    Args:\n        image: PIL Image to draw on (will be copied, not modified)\n        output_path: Optional path to save the visualization\n        show_text: Whether to show detected text\n        show_confidence: Whether to show confidence scores\n        line_width: Width of bounding box lines\n        box_color: Color for bounding boxes (hex)\n        text_color: Color for text labels (hex)\n\n    Returns:\n        PIL Image with visualizations drawn\n\n    Example:\n        ```python\n        result = ocr.extract(image)\n        viz = result.visualize(image, output_path=\"ocr_viz.png\")\n        ```\n    \"\"\"\n    from PIL import ImageDraw, ImageFont\n\n    # Copy image to avoid modifying original\n    viz_image = image.copy().convert(\"RGB\")\n    draw = ImageDraw.Draw(viz_image)\n\n    # Try to get a font\n    try:\n        font = ImageFont.truetype(\"/System/Library/Fonts/Helvetica.ttc\", 12)\n    except Exception:\n        try:\n            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 12)\n        except Exception:\n            font = ImageFont.load_default()\n\n    for block in self.text_blocks:\n        coords = block.bbox.to_xyxy()\n\n        # Draw polygon if available, otherwise draw rectangle\n        if block.polygon:\n            flat_polygon = [coord for point in block.polygon for coord in point]\n            draw.polygon(flat_polygon, outline=box_color, width=line_width)\n        else:\n            draw.rectangle(coords, outline=box_color, width=line_width)\n\n        # Build label text\n        if show_text or show_confidence:\n            label_parts = []\n            if show_text:\n                # Truncate long text\n                text = block.text[:25] + \"...\" if len(block.text) &gt; 25 else block.text\n                label_parts.append(text)\n            if show_confidence:\n                label_parts.append(f\"{block.confidence:.2f}\")\n            label_text = \" | \".join(label_parts)\n\n            # Position label below the box\n            label_x = coords[0]\n            label_y = coords[3] + 2  # Below bottom edge\n\n            # Draw label with background\n            text_bbox = draw.textbbox((label_x, label_y), label_text, font=font)\n            padding = 2\n            draw.rectangle(\n                [\n                    text_bbox[0] - padding,\n                    text_bbox[1] - padding,\n                    text_bbox[2] + padding,\n                    text_bbox[3] + padding,\n                ],\n                fill=\"#FFFFFF\",\n                outline=box_color,\n            )\n            draw.text((label_x, label_y), label_text, fill=text_color, font=font)\n\n    # Save if path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        viz_image.save(output_path)\n\n    return viz_image\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; OCROutput\n</code></pre> <p>Load an OCROutput instance from a JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to JSON file</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput instance</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"OCROutput\":\n    \"\"\"\n    Load an OCROutput instance from a JSON file.\n\n    Args:\n        file_path: Path to JSON file\n\n    Returns:\n        OCROutput instance\n    \"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save OCROutput instance to a JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path where JSON file should be saved</p> <p> TYPE: <code>Union[str, Path]</code> </p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"\n    Save OCROutput instance to a JSON file.\n\n    Args:\n        file_path: Path where JSON file should be saved\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.TextBlock","title":"TextBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single detected text element with text, bounding box, and confidence.</p> <p>This is the fundamental unit of OCR output - can represent a character, word, line, or block depending on the OCR model and configuration.</p> Example <pre><code>block = TextBlock(\n        text=\"Hello\",\n        bbox=BoundingBox(x1=100, y1=50, x2=200, y2=80),\n        confidence=0.95,\n        granularity=OCRGranularity.WORD,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.TextBlock.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"text\": self.text,\n        \"bbox\": self.bbox.to_list(),\n        \"confidence\": self.confidence,\n        \"granularity\": self.granularity.value,\n        \"polygon\": self.polygon,\n        \"language\": self.language,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.TextBlock.get_normalized_bbox","title":"get_normalized_bbox","text":"<pre><code>get_normalized_bbox(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Get bounding box in normalized (0-1024) coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox with normalized coordinates</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def get_normalized_bbox(self, image_width: int, image_height: int) -&gt; BoundingBox:\n    \"\"\"\n    Get bounding box in normalized (0-1024) coordinates.\n\n    Args:\n        image_width: Original image width\n        image_height: Original image height\n\n    Returns:\n        BoundingBox with normalized coordinates\n    \"\"\"\n    return self.bbox.to_normalized(image_width, image_height)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.PaddleOCR","title":"PaddleOCR","text":"<pre><code>PaddleOCR(config: PaddleOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>PaddleOCR text extractor.</p> <p>Single-backend model (PaddlePaddle - CPU/GPU).</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import PaddleOCR, PaddleOCRConfig\n\nocr = PaddleOCR(config=PaddleOCRConfig(lang=\"en\", device=\"cpu\"))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize PaddleOCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>PaddleOCRConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If paddleocr or paddlepaddle is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/paddleocr.py</code> <pre><code>def __init__(self, config: PaddleOCRConfig):\n    \"\"\"\n    Initialize PaddleOCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        ImportError: If paddleocr or paddlepaddle is not installed\n    \"\"\"\n    self.config = config\n    self._ocr = None\n\n    # Normalize language code\n    self._lang = LANG_CODES.get(config.lang.lower(), config.lang)\n\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.PaddleOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/paddleocr.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with detected text blocks\n    \"\"\"\n    if self._ocr is None:\n        raise RuntimeError(\"PaddleOCR not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Convert to numpy array\n    image_array = np.array(pil_image)\n\n    # Run PaddleOCR v3.x - use predict() method\n    results = self._ocr.predict(image_array)\n\n    # Parse results\n    text_blocks = []\n\n    # PaddleOCR may return None or empty results\n    if results is None or len(results) == 0:\n        return OCROutput(\n            text_blocks=[],\n            full_text=\"\",\n            image_width=image_width,\n            image_height=image_height,\n            model_name=self.MODEL_NAME,\n            languages_detected=[self._lang],\n        )\n\n    # PaddleOCR v3.x returns list of dicts with 'rec_texts', 'rec_scores', 'dt_polys'\n    for result in results:\n        if result is None:\n            continue\n\n        rec_texts = result.get(\"rec_texts\", [])\n        rec_scores = result.get(\"rec_scores\", [])\n        dt_polys = result.get(\"dt_polys\", [])\n\n        for i, text in enumerate(rec_texts):\n            if not text.strip():\n                continue\n\n            confidence = rec_scores[i] if i &lt; len(rec_scores) else 1.0\n\n            # Get polygon and convert to list\n            polygon: Optional[List[List[float]]] = None\n            if i &lt; len(dt_polys) and dt_polys[i] is not None:\n                poly_array = dt_polys[i]\n                # Handle numpy array\n                if hasattr(poly_array, \"tolist\"):\n                    polygon = poly_array.tolist()\n                else:\n                    polygon = list(poly_array)\n\n            # Convert polygon to bbox\n            if polygon:\n                bbox = BoundingBox.from_polygon(polygon)\n            else:\n                bbox = BoundingBox(x1=0, y1=0, x2=0, y2=0)\n\n            text_blocks.append(\n                TextBlock(\n                    text=text,\n                    bbox=bbox,\n                    confidence=float(confidence),\n                    granularity=OCRGranularity.LINE,\n                    polygon=polygon,\n                    language=self._lang,\n                )\n            )\n\n    # Sort by position (top to bottom, left to right)\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    # Build full_text from sorted blocks to ensure reading order\n    full_text = \" \".join(block.text for block in text_blocks)\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=full_text,\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=[self._lang],\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.PaddleOCRConfig","title":"PaddleOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for PaddleOCR extractor.</p> <p>This is a single-backend model (PaddlePaddle - CPU/GPU).</p> Example <pre><code>config = PaddleOCRConfig(lang=\"ch\", device=\"gpu\")\nocr = PaddleOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.TesseractOCR","title":"TesseractOCR","text":"<pre><code>TesseractOCR(config: TesseractOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>Tesseract OCR extractor.</p> <p>Single-backend model (CPU only). Requires system Tesseract installation.</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\n\nocr = TesseractOCR(config=TesseractOCRConfig(languages=[\"eng\"]))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize Tesseract OCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>TesseractOCRConfig</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If Tesseract is not installed</p> <code>ImportError</code> <p>If pytesseract is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def __init__(self, config: TesseractOCRConfig):\n    \"\"\"\n    Initialize Tesseract OCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        RuntimeError: If Tesseract is not installed\n        ImportError: If pytesseract is not installed\n    \"\"\"\n    self.config = config\n    self._pytesseract = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.TesseractOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks at word level</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with detected text blocks at word level\n    \"\"\"\n    if self._pytesseract is None:\n        raise RuntimeError(\"Tesseract not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Build config string\n    config = f\"--oem {self.config.oem} --psm {self.config.psm}\"\n    if self.config.config_params:\n        for key, value in self.config.config_params.items():\n            config += f\" -c {key}={value}\"\n\n    # Language string\n    lang_str = \"+\".join(self.config.languages)\n\n    # Get detailed data (word-level boxes)\n    data = self._pytesseract.image_to_data(\n        pil_image,\n        lang=lang_str,\n        config=config,\n        output_type=self._pytesseract.Output.DICT,\n    )\n\n    # Parse results into TextBlocks\n    text_blocks = []\n    full_text_parts = []\n\n    n_boxes = len(data[\"text\"])\n    for i in range(n_boxes):\n        text = data[\"text\"][i].strip()\n        # Safely convert conf to float (handles string values from some Tesseract versions)\n        try:\n            conf = float(data[\"conf\"][i])\n        except (ValueError, TypeError):\n            conf = -1\n\n        # Skip empty text or low confidence (-1 means no confidence)\n        if not text or conf == -1:\n            continue\n\n        # Tesseract returns confidence as 0-100, normalize to 0-1\n        confidence = conf / 100.0\n\n        # Get bounding box\n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n\n        bbox = BoundingBox(\n            x1=float(x),\n            y1=float(y),\n            x2=float(x + w),\n            y2=float(y + h),\n        )\n\n        text_blocks.append(\n            TextBlock(\n                text=text,\n                bbox=bbox,\n                confidence=confidence,\n                granularity=OCRGranularity.WORD,\n                language=lang_str,\n            )\n        )\n\n        full_text_parts.append(text)\n\n    # Sort by position (top to bottom, left to right)\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\" \".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.TesseractOCR.extract_lines","title":"extract_lines","text":"<pre><code>extract_lines(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR and return line-level blocks.</p> <p>Groups words into lines based on Tesseract's line detection.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with line-level text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def extract_lines(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR and return line-level blocks.\n\n    Groups words into lines based on Tesseract's line detection.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with line-level text blocks\n    \"\"\"\n    if self._pytesseract is None:\n        raise RuntimeError(\"Tesseract not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Build config string (including config_params like extract method)\n    config = f\"--oem {self.config.oem} --psm {self.config.psm}\"\n    if self.config.config_params:\n        for key, value in self.config.config_params.items():\n            config += f\" -c {key}={value}\"\n\n    # Language string\n    lang_str = \"+\".join(self.config.languages)\n\n    # Get detailed data\n    data = self._pytesseract.image_to_data(\n        pil_image,\n        lang=lang_str,\n        config=config,\n        output_type=self._pytesseract.Output.DICT,\n    )\n\n    # Group words into lines\n    lines: Dict[tuple, Dict] = {}\n    n_boxes = len(data[\"text\"])\n\n    for i in range(n_boxes):\n        text = data[\"text\"][i].strip()\n        # Safely convert conf to float (handles string values from some Tesseract versions)\n        try:\n            conf = float(data[\"conf\"][i])\n        except (ValueError, TypeError):\n            conf = -1\n\n        if not text or conf == -1:\n            continue\n\n        # Tesseract provides block_num, par_num, line_num\n        line_key = (data[\"block_num\"][i], data[\"par_num\"][i], data[\"line_num\"][i])\n\n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n\n        if line_key not in lines:\n            lines[line_key] = {\n                \"words\": [],\n                \"confidences\": [],\n                \"x1\": x,\n                \"y1\": y,\n                \"x2\": x + w,\n                \"y2\": y + h,\n            }\n\n        lines[line_key][\"words\"].append(text)\n        lines[line_key][\"confidences\"].append(conf / 100.0)\n        lines[line_key][\"x1\"] = min(lines[line_key][\"x1\"], x)\n        lines[line_key][\"y1\"] = min(lines[line_key][\"y1\"], y)\n        lines[line_key][\"x2\"] = max(lines[line_key][\"x2\"], x + w)\n        lines[line_key][\"y2\"] = max(lines[line_key][\"y2\"], y + h)\n\n    # Convert to TextBlocks\n    text_blocks = []\n    full_text_parts = []\n\n    for line_key in sorted(lines.keys()):\n        line = lines[line_key]\n        line_text = \" \".join(line[\"words\"])\n        avg_conf = sum(line[\"confidences\"]) / len(line[\"confidences\"])\n\n        bbox = BoundingBox(\n            x1=float(line[\"x1\"]),\n            y1=float(line[\"y1\"]),\n            x2=float(line[\"x2\"]),\n            y2=float(line[\"y2\"]),\n        )\n\n        text_blocks.append(\n            TextBlock(\n                text=line_text,\n                bbox=bbox,\n                confidence=avg_conf,\n                granularity=OCRGranularity.LINE,\n                language=lang_str,\n            )\n        )\n\n        full_text_parts.append(line_text)\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\"\\n\".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.TesseractOCRConfig","title":"TesseractOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Tesseract OCR extractor.</p> <p>This is a single-backend model (CPU only, requires system Tesseract).</p> Example <pre><code>config = TesseractOCRConfig(languages=[\"eng\", \"fra\"], psm=3)\nocr = TesseractOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.base","title":"base","text":"<p>Base class for OCR extractors.</p> <p>Defines the abstract interface that all OCR extractors must implement.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor","title":"BaseOCRExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for OCR extractors.</p> <p>All OCR extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyOCRExtractor(BaseOCRExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Initialize OCR engine\n            pass\n\n        def extract(self, image):\n            # Run OCR extraction\n            return OCROutput(...)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput containing detected text blocks with bounding boxes</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format is not supported</p> <code>RuntimeError</code> <p>If OCR engine is not initialized or extraction fails</p> Source code in <code>omnidocs/tasks/ocr_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR extraction on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n\n    Returns:\n        OCROutput containing detected text blocks with bounding boxes\n\n    Raises:\n        ValueError: If image format is not supported\n        RuntimeError: If OCR engine is not initialized or extraction fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.easyocr","title":"easyocr","text":"<p>EasyOCR extractor.</p> <p>EasyOCR is a PyTorch-based OCR engine with excellent multi-language support. - GPU accelerated (optional) - Supports 80+ languages - Good for scene text and printed documents</p> Python Package <p>pip install easyocr</p> Model Download Location <p>By default, EasyOCR downloads models to ~/.EasyOCR/ Can be overridden with model_storage_directory parameter</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCRConfig","title":"EasyOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for EasyOCR extractor.</p> <p>This is a single-backend model (PyTorch - CPU/GPU).</p> Example <pre><code>config = EasyOCRConfig(languages=[\"en\", \"ch_sim\"], gpu=True)\nocr = EasyOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCR","title":"EasyOCR","text":"<pre><code>EasyOCR(config: EasyOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>EasyOCR text extractor.</p> <p>Single-backend model (PyTorch - CPU/GPU).</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\nocr = EasyOCR(config=EasyOCRConfig(languages=[\"en\"], gpu=True))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize EasyOCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>EasyOCRConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If easyocr is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def __init__(self, config: EasyOCRConfig):\n    \"\"\"\n    Initialize EasyOCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        ImportError: If easyocr is not installed\n    \"\"\"\n    self.config = config\n    self._reader = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    detail: int = 1,\n    paragraph: bool = False,\n    min_size: int = 10,\n    text_threshold: float = 0.7,\n    low_text: float = 0.4,\n    link_threshold: float = 0.4,\n    canvas_size: int = 2560,\n    mag_ratio: float = 1.0,\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>detail</code> <p>0 = simple output, 1 = detailed with boxes</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>paragraph</code> <p>Combine results into paragraphs</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>min_size</code> <p>Minimum text box size</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>text_threshold</code> <p>Text confidence threshold</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.7</code> </p> <code>low_text</code> <p>Low text bound</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>link_threshold</code> <p>Link threshold for text joining</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>canvas_size</code> <p>Max image dimension for processing</p> <p> TYPE: <code>int</code> DEFAULT: <code>2560</code> </p> <code>mag_ratio</code> <p>Magnification ratio</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks</p> RAISES DESCRIPTION <code>ValueError</code> <p>If detail is not 0 or 1</p> <code>RuntimeError</code> <p>If EasyOCR is not initialized</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    detail: int = 1,\n    paragraph: bool = False,\n    min_size: int = 10,\n    text_threshold: float = 0.7,\n    low_text: float = 0.4,\n    link_threshold: float = 0.4,\n    canvas_size: int = 2560,\n    mag_ratio: float = 1.0,\n) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n        detail: 0 = simple output, 1 = detailed with boxes\n        paragraph: Combine results into paragraphs\n        min_size: Minimum text box size\n        text_threshold: Text confidence threshold\n        low_text: Low text bound\n        link_threshold: Link threshold for text joining\n        canvas_size: Max image dimension for processing\n        mag_ratio: Magnification ratio\n\n    Returns:\n        OCROutput with detected text blocks\n\n    Raises:\n        ValueError: If detail is not 0 or 1\n        RuntimeError: If EasyOCR is not initialized\n    \"\"\"\n    if self._reader is None:\n        raise RuntimeError(\"EasyOCR not initialized. Call _load_model() first.\")\n\n    # Validate detail parameter\n    if detail not in (0, 1):\n        raise ValueError(f\"detail must be 0 or 1, got {detail}\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Convert to numpy array for EasyOCR\n    image_array = np.array(pil_image)\n\n    # Run EasyOCR\n    results = self._reader.readtext(\n        image_array,\n        detail=detail,\n        paragraph=paragraph,\n        min_size=min_size,\n        text_threshold=text_threshold,\n        low_text=low_text,\n        link_threshold=link_threshold,\n        canvas_size=canvas_size,\n        mag_ratio=mag_ratio,\n    )\n\n    # Parse results\n    text_blocks = []\n    full_text_parts = []\n\n    for result in results:\n        if detail == 0:\n            # Simple output: just text\n            text = result\n            confidence = 1.0\n            bbox = BoundingBox(x1=0, y1=0, x2=0, y2=0)\n            polygon = None\n        else:\n            # Detailed output: [polygon, text, confidence]\n            polygon_points, text, confidence = result\n\n            # EasyOCR returns 4 corner points: [[x1,y1], [x2,y1], [x2,y2], [x1,y2]]\n            # Convert to list of lists for storage\n            polygon = [list(p) for p in polygon_points]\n\n            # Convert to axis-aligned bounding box\n            bbox = BoundingBox.from_polygon(polygon)\n\n        if not text.strip():\n            continue\n\n        text_blocks.append(\n            TextBlock(\n                text=text,\n                bbox=bbox,\n                confidence=float(confidence),\n                granularity=(OCRGranularity.LINE if paragraph else OCRGranularity.WORD),\n                polygon=polygon,\n                language=\"+\".join(self.config.languages),\n            )\n        )\n\n        full_text_parts.append(text)\n\n    # Sort by position\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\" \".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCR.extract_batch","title":"extract_batch","text":"<pre><code>extract_batch(\n    images: List[Union[Image, ndarray, str, Path]], **kwargs\n) -&gt; List[OCROutput]\n</code></pre> <p>Run OCR on multiple images.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of input images</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>**kwargs</code> <p>Arguments passed to extract()</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>List[OCROutput]</code> <p>List of OCROutput objects</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def extract_batch(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    **kwargs,\n) -&gt; List[OCROutput]:\n    \"\"\"\n    Run OCR on multiple images.\n\n    Args:\n        images: List of input images\n        **kwargs: Arguments passed to extract()\n\n    Returns:\n        List of OCROutput objects\n    \"\"\"\n    results = []\n    for img in images:\n        results.append(self.extract(img, **kwargs))\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models","title":"models","text":"<p>Pydantic models for OCR extraction outputs.</p> <p>Defines standardized output types for OCR detection including text blocks with bounding boxes, confidence scores, and granularity levels.</p> <p>Key difference from Text Extraction: - OCR returns text WITH bounding boxes (word/line/character level) - Text Extraction returns formatted text (MD/HTML) WITHOUT bboxes</p> Coordinate Systems <ul> <li>Absolute (default): Coordinates in pixels relative to original image size</li> <li>Normalized (0-1024): Coordinates scaled to 0-1024 range (virtual 1024x1024 canvas)</li> </ul> <p>Use <code>bbox.to_normalized(width, height)</code> or <code>output.get_normalized_blocks()</code> to convert to normalized coordinates.</p> Example <pre><code>result = ocr.extract(image)  # Returns absolute pixel coordinates\nnormalized = result.get_normalized_blocks()  # Returns 0-1024 normalized coords\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCRGranularity","title":"OCRGranularity","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>OCR detection granularity levels.</p> <p>Different OCR engines return results at different granularity levels. This enum standardizes the options across all extractors.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box coordinates in pixel space.</p> <p>Coordinates follow the convention: (x1, y1) is top-left, (x2, y2) is bottom-right. For rotated text, use the polygon field in TextBlock instead.</p> Example <pre><code>bbox = BoundingBox(x1=100, y1=50, x2=300, y2=80)\nprint(bbox.width, bbox.height)  # 200, 30\nprint(bbox.center)  # (200.0, 65.0)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x, y, width, height) format.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_xywh(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x, y, width, height) format.\"\"\"\n    return (self.x1, self.y1, self.width, self.height)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.from_polygon","title":"from_polygon  <code>classmethod</code>","text":"<pre><code>from_polygon(polygon: List[List[float]]) -&gt; BoundingBox\n</code></pre> <p>Create axis-aligned bounding box from polygon points.</p> PARAMETER DESCRIPTION <code>polygon</code> <p>List of [x, y] points (usually 4 for quadrilateral)</p> <p> TYPE: <code>List[List[float]]</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox that encloses all polygon points</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef from_polygon(cls, polygon: List[List[float]]) -&gt; \"BoundingBox\":\n    \"\"\"\n    Create axis-aligned bounding box from polygon points.\n\n    Args:\n        polygon: List of [x, y] points (usually 4 for quadrilateral)\n\n    Returns:\n        BoundingBox that encloses all polygon points\n    \"\"\"\n    if not polygon:\n        raise ValueError(\"Polygon cannot be empty\")\n\n    xs = [p[0] for p in polygon]\n    ys = [p[1] for p in polygon]\n    return cls(x1=min(xs), y1=min(ys), x2=max(xs), y2=max(ys))\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> <p>Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas. This provides consistent coordinates regardless of original image size.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas.\n    This provides consistent coordinates regardless of original image size.\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_absolute","title":"to_absolute","text":"<pre><code>to_absolute(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert from normalized (0-1024) to absolute pixel coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Target image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Target image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with absolute pixel coordinates</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_absolute(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert from normalized (0-1024) to absolute pixel coordinates.\n\n    Args:\n        image_width: Target image width in pixels\n        image_height: Target image height in pixels\n\n    Returns:\n        New BoundingBox with absolute pixel coordinates\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / NORMALIZED_SIZE * image_width,\n        y1=self.y1 / NORMALIZED_SIZE * image_height,\n        x2=self.x2 / NORMALIZED_SIZE * image_width,\n        y2=self.y2 / NORMALIZED_SIZE * image_height,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.TextBlock","title":"TextBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single detected text element with text, bounding box, and confidence.</p> <p>This is the fundamental unit of OCR output - can represent a character, word, line, or block depending on the OCR model and configuration.</p> Example <pre><code>block = TextBlock(\n        text=\"Hello\",\n        bbox=BoundingBox(x1=100, y1=50, x2=200, y2=80),\n        confidence=0.95,\n        granularity=OCRGranularity.WORD,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.TextBlock.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"text\": self.text,\n        \"bbox\": self.bbox.to_list(),\n        \"confidence\": self.confidence,\n        \"granularity\": self.granularity.value,\n        \"polygon\": self.polygon,\n        \"language\": self.language,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.TextBlock.get_normalized_bbox","title":"get_normalized_bbox","text":"<pre><code>get_normalized_bbox(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Get bounding box in normalized (0-1024) coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox with normalized coordinates</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def get_normalized_bbox(self, image_width: int, image_height: int) -&gt; BoundingBox:\n    \"\"\"\n    Get bounding box in normalized (0-1024) coordinates.\n\n    Args:\n        image_width: Original image width\n        image_height: Original image height\n\n    Returns:\n        BoundingBox with normalized coordinates\n    \"\"\"\n    return self.bbox.to_normalized(image_width, image_height)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput","title":"OCROutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete OCR extraction results for a single image.</p> <p>Contains all detected text blocks with their bounding boxes, plus metadata about the extraction.</p> Example <pre><code>result = ocr.extract(image)\nprint(f\"Found {result.block_count} blocks\")\nprint(f\"Full text: {result.full_text}\")\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.block_count","title":"block_count  <code>property</code>","text":"<pre><code>block_count: int\n</code></pre> <p>Number of detected text blocks.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.word_count","title":"word_count  <code>property</code>","text":"<pre><code>word_count: int\n</code></pre> <p>Approximate word count from full text.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.average_confidence","title":"average_confidence  <code>property</code>","text":"<pre><code>average_confidence: float\n</code></pre> <p>Average confidence across all text blocks.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(\n    min_confidence: float,\n) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by minimum confidence.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def filter_by_confidence(self, min_confidence: float) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by minimum confidence.\"\"\"\n    return [b for b in self.text_blocks if b.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.filter_by_granularity","title":"filter_by_granularity","text":"<pre><code>filter_by_granularity(\n    granularity: OCRGranularity,\n) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by granularity level.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def filter_by_granularity(self, granularity: OCRGranularity) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by granularity level.\"\"\"\n    return [b for b in self.text_blocks if b.granularity == granularity]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"text_blocks\": [b.to_dict() for b in self.text_blocks],\n        \"full_text\": self.full_text,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"languages_detected\": self.languages_detected,\n        \"block_count\": self.block_count,\n        \"word_count\": self.word_count,\n        \"average_confidence\": self.average_confidence,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.sort_by_position","title":"sort_by_position","text":"<pre><code>sort_by_position(top_to_bottom: bool = True) -&gt; OCROutput\n</code></pre> <p>Return a new OCROutput with blocks sorted by position.</p> PARAMETER DESCRIPTION <code>top_to_bottom</code> <p>If True, sort by y-coordinate (reading order)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>New OCROutput with sorted text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def sort_by_position(self, top_to_bottom: bool = True) -&gt; \"OCROutput\":\n    \"\"\"\n    Return a new OCROutput with blocks sorted by position.\n\n    Args:\n        top_to_bottom: If True, sort by y-coordinate (reading order)\n\n    Returns:\n        New OCROutput with sorted text blocks\n    \"\"\"\n    sorted_blocks = sorted(\n        self.text_blocks,\n        key=lambda b: (b.bbox.y1, b.bbox.x1),\n        reverse=not top_to_bottom,\n    )\n    # Regenerate full_text in sorted order\n    full_text = \" \".join(b.text for b in sorted_blocks)\n\n    return OCROutput(\n        text_blocks=sorted_blocks,\n        full_text=full_text,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        model_name=self.model_name,\n        languages_detected=self.languages_detected,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.get_normalized_blocks","title":"get_normalized_blocks","text":"<pre><code>get_normalized_blocks() -&gt; List[Dict]\n</code></pre> <p>Get all text blocks with normalized (0-1024) coordinates.</p> RETURNS DESCRIPTION <code>List[Dict]</code> <p>List of dicts with normalized bbox coordinates and metadata.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def get_normalized_blocks(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all text blocks with normalized (0-1024) coordinates.\n\n    Returns:\n        List of dicts with normalized bbox coordinates and metadata.\n    \"\"\"\n    normalized = []\n    for block in self.text_blocks:\n        norm_bbox = block.bbox.to_normalized(self.image_width, self.image_height)\n        normalized.append(\n            {\n                \"text\": block.text,\n                \"bbox\": norm_bbox.to_list(),\n                \"confidence\": block.confidence,\n                \"granularity\": block.granularity.value,\n                \"language\": block.language,\n            }\n        )\n    return normalized\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.visualize","title":"visualize","text":"<pre><code>visualize(\n    image: Image,\n    output_path: Optional[Union[str, Path]] = None,\n    show_text: bool = True,\n    show_confidence: bool = False,\n    line_width: int = 2,\n    box_color: str = \"#2ECC71\",\n    text_color: str = \"#000000\",\n) -&gt; Image.Image\n</code></pre> <p>Visualize OCR results on the image.</p> <p>Draws bounding boxes around detected text with optional labels.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to draw on (will be copied, not modified)</p> <p> TYPE: <code>Image</code> </p> <code>output_path</code> <p>Optional path to save the visualization</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> <code>show_text</code> <p>Whether to show detected text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_confidence</code> <p>Whether to show confidence scores</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>line_width</code> <p>Width of bounding box lines</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>box_color</code> <p>Color for bounding boxes (hex)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'#2ECC71'</code> </p> <code>text_color</code> <p>Color for text labels (hex)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'#000000'</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image with visualizations drawn</p> Example <pre><code>result = ocr.extract(image)\nviz = result.visualize(image, output_path=\"ocr_viz.png\")\n</code></pre> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def visualize(\n    self,\n    image: \"Image.Image\",\n    output_path: Optional[Union[str, Path]] = None,\n    show_text: bool = True,\n    show_confidence: bool = False,\n    line_width: int = 2,\n    box_color: str = \"#2ECC71\",\n    text_color: str = \"#000000\",\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Visualize OCR results on the image.\n\n    Draws bounding boxes around detected text with optional labels.\n\n    Args:\n        image: PIL Image to draw on (will be copied, not modified)\n        output_path: Optional path to save the visualization\n        show_text: Whether to show detected text\n        show_confidence: Whether to show confidence scores\n        line_width: Width of bounding box lines\n        box_color: Color for bounding boxes (hex)\n        text_color: Color for text labels (hex)\n\n    Returns:\n        PIL Image with visualizations drawn\n\n    Example:\n        ```python\n        result = ocr.extract(image)\n        viz = result.visualize(image, output_path=\"ocr_viz.png\")\n        ```\n    \"\"\"\n    from PIL import ImageDraw, ImageFont\n\n    # Copy image to avoid modifying original\n    viz_image = image.copy().convert(\"RGB\")\n    draw = ImageDraw.Draw(viz_image)\n\n    # Try to get a font\n    try:\n        font = ImageFont.truetype(\"/System/Library/Fonts/Helvetica.ttc\", 12)\n    except Exception:\n        try:\n            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 12)\n        except Exception:\n            font = ImageFont.load_default()\n\n    for block in self.text_blocks:\n        coords = block.bbox.to_xyxy()\n\n        # Draw polygon if available, otherwise draw rectangle\n        if block.polygon:\n            flat_polygon = [coord for point in block.polygon for coord in point]\n            draw.polygon(flat_polygon, outline=box_color, width=line_width)\n        else:\n            draw.rectangle(coords, outline=box_color, width=line_width)\n\n        # Build label text\n        if show_text or show_confidence:\n            label_parts = []\n            if show_text:\n                # Truncate long text\n                text = block.text[:25] + \"...\" if len(block.text) &gt; 25 else block.text\n                label_parts.append(text)\n            if show_confidence:\n                label_parts.append(f\"{block.confidence:.2f}\")\n            label_text = \" | \".join(label_parts)\n\n            # Position label below the box\n            label_x = coords[0]\n            label_y = coords[3] + 2  # Below bottom edge\n\n            # Draw label with background\n            text_bbox = draw.textbbox((label_x, label_y), label_text, font=font)\n            padding = 2\n            draw.rectangle(\n                [\n                    text_bbox[0] - padding,\n                    text_bbox[1] - padding,\n                    text_bbox[2] + padding,\n                    text_bbox[3] + padding,\n                ],\n                fill=\"#FFFFFF\",\n                outline=box_color,\n            )\n            draw.text((label_x, label_y), label_text, fill=text_color, font=font)\n\n    # Save if path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        viz_image.save(output_path)\n\n    return viz_image\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; OCROutput\n</code></pre> <p>Load an OCROutput instance from a JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to JSON file</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput instance</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"OCROutput\":\n    \"\"\"\n    Load an OCROutput instance from a JSON file.\n\n    Args:\n        file_path: Path to JSON file\n\n    Returns:\n        OCROutput instance\n    \"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save OCROutput instance to a JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path where JSON file should be saved</p> <p> TYPE: <code>Union[str, Path]</code> </p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"\n    Save OCROutput instance to a JSON file.\n\n    Args:\n        file_path: Path where JSON file should be saved\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.paddleocr","title":"paddleocr","text":"<p>PaddleOCR extractor.</p> <p>PaddleOCR is an OCR toolkit developed by Baidu/PaddlePaddle. - Excellent for CJK languages (Chinese, Japanese, Korean) - GPU accelerated - Supports layout analysis + OCR</p> Python Package <p>pip install paddleocr paddlepaddle  # CPU version pip install paddleocr paddlepaddle-gpu  # GPU version</p> Model Download Location <p>By default, PaddleOCR downloads models to ~/.paddleocr/</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.paddleocr.PaddleOCRConfig","title":"PaddleOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for PaddleOCR extractor.</p> <p>This is a single-backend model (PaddlePaddle - CPU/GPU).</p> Example <pre><code>config = PaddleOCRConfig(lang=\"ch\", device=\"gpu\")\nocr = PaddleOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.paddleocr.PaddleOCR","title":"PaddleOCR","text":"<pre><code>PaddleOCR(config: PaddleOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>PaddleOCR text extractor.</p> <p>Single-backend model (PaddlePaddle - CPU/GPU).</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import PaddleOCR, PaddleOCRConfig\n\nocr = PaddleOCR(config=PaddleOCRConfig(lang=\"en\", device=\"cpu\"))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize PaddleOCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>PaddleOCRConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If paddleocr or paddlepaddle is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/paddleocr.py</code> <pre><code>def __init__(self, config: PaddleOCRConfig):\n    \"\"\"\n    Initialize PaddleOCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        ImportError: If paddleocr or paddlepaddle is not installed\n    \"\"\"\n    self.config = config\n    self._ocr = None\n\n    # Normalize language code\n    self._lang = LANG_CODES.get(config.lang.lower(), config.lang)\n\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.paddleocr.PaddleOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/paddleocr.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with detected text blocks\n    \"\"\"\n    if self._ocr is None:\n        raise RuntimeError(\"PaddleOCR not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Convert to numpy array\n    image_array = np.array(pil_image)\n\n    # Run PaddleOCR v3.x - use predict() method\n    results = self._ocr.predict(image_array)\n\n    # Parse results\n    text_blocks = []\n\n    # PaddleOCR may return None or empty results\n    if results is None or len(results) == 0:\n        return OCROutput(\n            text_blocks=[],\n            full_text=\"\",\n            image_width=image_width,\n            image_height=image_height,\n            model_name=self.MODEL_NAME,\n            languages_detected=[self._lang],\n        )\n\n    # PaddleOCR v3.x returns list of dicts with 'rec_texts', 'rec_scores', 'dt_polys'\n    for result in results:\n        if result is None:\n            continue\n\n        rec_texts = result.get(\"rec_texts\", [])\n        rec_scores = result.get(\"rec_scores\", [])\n        dt_polys = result.get(\"dt_polys\", [])\n\n        for i, text in enumerate(rec_texts):\n            if not text.strip():\n                continue\n\n            confidence = rec_scores[i] if i &lt; len(rec_scores) else 1.0\n\n            # Get polygon and convert to list\n            polygon: Optional[List[List[float]]] = None\n            if i &lt; len(dt_polys) and dt_polys[i] is not None:\n                poly_array = dt_polys[i]\n                # Handle numpy array\n                if hasattr(poly_array, \"tolist\"):\n                    polygon = poly_array.tolist()\n                else:\n                    polygon = list(poly_array)\n\n            # Convert polygon to bbox\n            if polygon:\n                bbox = BoundingBox.from_polygon(polygon)\n            else:\n                bbox = BoundingBox(x1=0, y1=0, x2=0, y2=0)\n\n            text_blocks.append(\n                TextBlock(\n                    text=text,\n                    bbox=bbox,\n                    confidence=float(confidence),\n                    granularity=OCRGranularity.LINE,\n                    polygon=polygon,\n                    language=self._lang,\n                )\n            )\n\n    # Sort by position (top to bottom, left to right)\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    # Build full_text from sorted blocks to ensure reading order\n    full_text = \" \".join(block.text for block in text_blocks)\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=full_text,\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=[self._lang],\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.tesseract","title":"tesseract","text":"<p>Tesseract OCR extractor.</p> <p>Tesseract is an open-source OCR engine maintained by Google. - CPU-based (no GPU required) - Requires system installation of Tesseract - Good for printed text, supports 100+ languages</p> System Requirements <p>macOS: brew install tesseract Ubuntu: sudo apt-get install tesseract-ocr Windows: Download from https://github.com/UB-Mannheim/tesseract/wiki</p> Python Package <p>pip install pytesseract</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCRConfig","title":"TesseractOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Tesseract OCR extractor.</p> <p>This is a single-backend model (CPU only, requires system Tesseract).</p> Example <pre><code>config = TesseractOCRConfig(languages=[\"eng\", \"fra\"], psm=3)\nocr = TesseractOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCR","title":"TesseractOCR","text":"<pre><code>TesseractOCR(config: TesseractOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>Tesseract OCR extractor.</p> <p>Single-backend model (CPU only). Requires system Tesseract installation.</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\n\nocr = TesseractOCR(config=TesseractOCRConfig(languages=[\"eng\"]))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize Tesseract OCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>TesseractOCRConfig</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If Tesseract is not installed</p> <code>ImportError</code> <p>If pytesseract is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def __init__(self, config: TesseractOCRConfig):\n    \"\"\"\n    Initialize Tesseract OCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        RuntimeError: If Tesseract is not installed\n        ImportError: If pytesseract is not installed\n    \"\"\"\n    self.config = config\n    self._pytesseract = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks at word level</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with detected text blocks at word level\n    \"\"\"\n    if self._pytesseract is None:\n        raise RuntimeError(\"Tesseract not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Build config string\n    config = f\"--oem {self.config.oem} --psm {self.config.psm}\"\n    if self.config.config_params:\n        for key, value in self.config.config_params.items():\n            config += f\" -c {key}={value}\"\n\n    # Language string\n    lang_str = \"+\".join(self.config.languages)\n\n    # Get detailed data (word-level boxes)\n    data = self._pytesseract.image_to_data(\n        pil_image,\n        lang=lang_str,\n        config=config,\n        output_type=self._pytesseract.Output.DICT,\n    )\n\n    # Parse results into TextBlocks\n    text_blocks = []\n    full_text_parts = []\n\n    n_boxes = len(data[\"text\"])\n    for i in range(n_boxes):\n        text = data[\"text\"][i].strip()\n        # Safely convert conf to float (handles string values from some Tesseract versions)\n        try:\n            conf = float(data[\"conf\"][i])\n        except (ValueError, TypeError):\n            conf = -1\n\n        # Skip empty text or low confidence (-1 means no confidence)\n        if not text or conf == -1:\n            continue\n\n        # Tesseract returns confidence as 0-100, normalize to 0-1\n        confidence = conf / 100.0\n\n        # Get bounding box\n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n\n        bbox = BoundingBox(\n            x1=float(x),\n            y1=float(y),\n            x2=float(x + w),\n            y2=float(y + h),\n        )\n\n        text_blocks.append(\n            TextBlock(\n                text=text,\n                bbox=bbox,\n                confidence=confidence,\n                granularity=OCRGranularity.WORD,\n                language=lang_str,\n            )\n        )\n\n        full_text_parts.append(text)\n\n    # Sort by position (top to bottom, left to right)\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\" \".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCR.extract_lines","title":"extract_lines","text":"<pre><code>extract_lines(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR and return line-level blocks.</p> <p>Groups words into lines based on Tesseract's line detection.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with line-level text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def extract_lines(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR and return line-level blocks.\n\n    Groups words into lines based on Tesseract's line detection.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with line-level text blocks\n    \"\"\"\n    if self._pytesseract is None:\n        raise RuntimeError(\"Tesseract not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Build config string (including config_params like extract method)\n    config = f\"--oem {self.config.oem} --psm {self.config.psm}\"\n    if self.config.config_params:\n        for key, value in self.config.config_params.items():\n            config += f\" -c {key}={value}\"\n\n    # Language string\n    lang_str = \"+\".join(self.config.languages)\n\n    # Get detailed data\n    data = self._pytesseract.image_to_data(\n        pil_image,\n        lang=lang_str,\n        config=config,\n        output_type=self._pytesseract.Output.DICT,\n    )\n\n    # Group words into lines\n    lines: Dict[tuple, Dict] = {}\n    n_boxes = len(data[\"text\"])\n\n    for i in range(n_boxes):\n        text = data[\"text\"][i].strip()\n        # Safely convert conf to float (handles string values from some Tesseract versions)\n        try:\n            conf = float(data[\"conf\"][i])\n        except (ValueError, TypeError):\n            conf = -1\n\n        if not text or conf == -1:\n            continue\n\n        # Tesseract provides block_num, par_num, line_num\n        line_key = (data[\"block_num\"][i], data[\"par_num\"][i], data[\"line_num\"][i])\n\n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n\n        if line_key not in lines:\n            lines[line_key] = {\n                \"words\": [],\n                \"confidences\": [],\n                \"x1\": x,\n                \"y1\": y,\n                \"x2\": x + w,\n                \"y2\": y + h,\n            }\n\n        lines[line_key][\"words\"].append(text)\n        lines[line_key][\"confidences\"].append(conf / 100.0)\n        lines[line_key][\"x1\"] = min(lines[line_key][\"x1\"], x)\n        lines[line_key][\"y1\"] = min(lines[line_key][\"y1\"], y)\n        lines[line_key][\"x2\"] = max(lines[line_key][\"x2\"], x + w)\n        lines[line_key][\"y2\"] = max(lines[line_key][\"y2\"], y + h)\n\n    # Convert to TextBlocks\n    text_blocks = []\n    full_text_parts = []\n\n    for line_key in sorted(lines.keys()):\n        line = lines[line_key]\n        line_text = \" \".join(line[\"words\"])\n        avg_conf = sum(line[\"confidences\"]) / len(line[\"confidences\"])\n\n        bbox = BoundingBox(\n            x1=float(line[\"x1\"]),\n            y1=float(line[\"y1\"]),\n            x2=float(line[\"x2\"]),\n            y2=float(line[\"y2\"]),\n        )\n\n        text_blocks.append(\n            TextBlock(\n                text=line_text,\n                bbox=bbox,\n                confidence=avg_conf,\n                granularity=OCRGranularity.LINE,\n                language=lang_str,\n            )\n        )\n\n        full_text_parts.append(line_text)\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\"\\n\".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction","title":"text_extraction","text":"<p>Text Extraction Module.</p> <p>Provides extractors for converting document images to structured text formats (HTML, Markdown, JSON). Uses Vision-Language Models for accurate text extraction with formatting preservation and optional layout detection.</p> Available Extractors <ul> <li>QwenTextExtractor: Qwen3-VL based extractor (multi-backend)</li> <li>DotsOCRTextExtractor: Dots OCR with layout-aware extraction (PyTorch/VLLM/API)</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.BaseTextExtractor","title":"BaseTextExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for text extractors.</p> <p>All text extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyTextExtractor(BaseTextExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Load model weights\n            pass\n\n        def extract(self, image, output_format=\"markdown\"):\n            # Run extraction\n            return TextOutput(...)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.BaseTextExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format or output_format is not supported</p> <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        ValueError: If image format or output_format is not supported\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.DotsOCRTextExtractor","title":"DotsOCRTextExtractor","text":"<pre><code>DotsOCRTextExtractor(backend: DotsOCRBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Dots OCR Vision-Language Model text extractor with layout detection.</p> <p>Extracts text from document images with layout information including: - 11 layout categories (Caption, Footnote, Formula, List-item, etc.) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown, LaTeX, HTML) - Reading order preservation</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\n\n# Extract with layout\nresult = extractor.extract(image, include_layout=True)\nprint(f\"Found {result.num_layout_elements} elements\")\nprint(result.content)\n</code></pre> <p>Initialize Dots OCR text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend - DotsOCRVLLMConfig: VLLM high-throughput backend - DotsOCRAPIConfig: API backend (online VLLM server)</p> <p> TYPE: <code>DotsOCRBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def __init__(self, backend: DotsOCRBackendConfig):\n    \"\"\"\n    Initialize Dots OCR text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend\n            - DotsOCRVLLMConfig: VLLM high-throughput backend\n            - DotsOCRAPIConfig: API backend (online VLLM server)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._model: Any = None\n    self._loaded = False\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.DotsOCRTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\n        \"markdown\", \"html\", \"json\"\n    ] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput\n</code></pre> <p>Extract text from image using Dots OCR.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\", \"html\", or \"json\")</p> <p> TYPE: <code>Literal['markdown', 'html', 'json']</code> DEFAULT: <code>'markdown'</code> </p> <code>include_layout</code> <p>Include layout bounding boxes in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>custom_prompt</code> <p>Override default extraction prompt</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_tokens</code> <p>Maximum tokens for generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>8192</code> </p> RETURNS DESCRIPTION <code>DotsOCRTextOutput</code> <p>DotsOCRTextOutput with extracted content and optional layout</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"markdown\", \"html\", \"json\"] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput:\n    \"\"\"\n    Extract text from image using Dots OCR.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\", \"html\", or \"json\")\n        include_layout: Include layout bounding boxes in output\n        custom_prompt: Override default extraction prompt\n        max_tokens: Maximum tokens for generation\n\n    Returns:\n        DotsOCRTextOutput with extracted content and optional layout\n\n    Raises:\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    img = self._prepare_image(image)\n\n    # Get prompt\n    prompt = custom_prompt or DOTS_OCR_PROMPT\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"DotsOCRPyTorchConfig\":\n        raw_output = self._infer_pytorch(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRVLLMConfig\":\n        raw_output = self._infer_vllm(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRAPIConfig\":\n        raw_output = self._infer_api(img, prompt, max_tokens)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse output\n    return self._parse_output(\n        raw_output,\n        img.size,\n        output_format,\n        include_layout,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.DotsOCRTextOutput","title":"DotsOCRTextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text extraction output from Dots OCR with layout information.</p> <p>Dots OCR provides structured output with: - Layout detection (11 categories) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown/LaTeX/HTML) - Reading order preservation</p> Layout Categories <p>Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title</p> Text Formatting <ul> <li>Text/Title/Section-header: Markdown</li> <li>Formula: LaTeX</li> <li>Table: HTML</li> <li>Picture: (text omitted)</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nresult = extractor.extract(image, include_layout=True)\nprint(result.content)  # Full text with formatting\nfor elem in result.layout:\n        print(f\"{elem.category}: {elem.bbox}\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.DotsOCRTextOutput.num_layout_elements","title":"num_layout_elements  <code>property</code>","text":"<pre><code>num_layout_elements: int\n</code></pre> <p>Number of detected layout elements.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.DotsOCRTextOutput.content_length","title":"content_length  <code>property</code>","text":"<pre><code>content_length: int\n</code></pre> <p>Length of extracted content in characters.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.LayoutElement","title":"LayoutElement","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single layout element from document layout detection.</p> <p>Represents a detected region in the document with its bounding box, category label, and extracted text content.</p> ATTRIBUTE DESCRIPTION <code>bbox</code> <p>Bounding box coordinates [x1, y1, x2, y2] (normalized to 0-1024)</p> <p> TYPE: <code>List[int]</code> </p> <code>category</code> <p>Layout category (e.g., \"Text\", \"Title\", \"Table\", \"Formula\")</p> <p> TYPE: <code>str</code> </p> <code>text</code> <p>Extracted text content (None for pictures)</p> <p> TYPE: <code>Optional[str]</code> </p> <code>confidence</code> <p>Detection confidence score (optional)</p> <p> TYPE: <code>Optional[float]</code> </p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.OutputFormat","title":"OutputFormat","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported text extraction output formats.</p> Each format has different characteristics <ul> <li>HTML: Structured with div elements, preserves layout semantics</li> <li>MARKDOWN: Portable, human-readable, good for documentation</li> <li>JSON: Structured data with layout information (Dots OCR)</li> </ul>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.TextOutput","title":"TextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text extraction output from a document image.</p> <p>Contains the extracted text content in the requested format, along with optional raw output and plain text versions.</p> Example <pre><code>result = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)  # Clean markdown\nprint(result.plain_text)  # Plain text without formatting\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.TextOutput.content_length","title":"content_length  <code>property</code>","text":"<pre><code>content_length: int\n</code></pre> <p>Length of the extracted content in characters.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.TextOutput.word_count","title":"word_count  <code>property</code>","text":"<pre><code>word_count: int\n</code></pre> <p>Approximate word count of the plain text.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.QwenTextExtractor","title":"QwenTextExtractor","text":"<pre><code>QwenTextExtractor(backend: QwenTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Qwen3-VL Vision-Language Model text extractor.</p> <p>Extracts text from document images and outputs as structured HTML or Markdown. Uses Qwen3-VL's built-in document parsing prompts.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Extract as Markdown\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n\n# Extract as HTML\nresult = extractor.extract(image, output_format=\"html\")\nprint(result.content)\n</code></pre> <p>Initialize Qwen text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenTextPyTorchConfig: PyTorch/HuggingFace backend - QwenTextVLLMConfig: VLLM high-throughput backend - QwenTextMLXConfig: MLX backend for Apple Silicon - QwenTextAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def __init__(self, backend: QwenTextBackendConfig):\n    \"\"\"\n    Initialize Qwen text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenTextPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenTextVLLMConfig: VLLM high-throughput backend\n            - QwenTextMLXConfig: MLX backend for Apple Silicon\n            - QwenTextAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.QwenTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML with div elements - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format or output_format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML with div elements\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format or output_format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}. Expected 'html' or 'markdown'.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Get prompt for output format\n    prompt = QWEN_PROMPTS[output_format]\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenTextAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    if output_format == \"html\":\n        cleaned_output = _clean_html_output(raw_output)\n    else:\n        cleaned_output = _clean_markdown_output(raw_output)\n\n    # Extract plain text\n    plain_text = _extract_plain_text(raw_output, output_format)\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=plain_text,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.base","title":"base","text":"<p>Base class for text extractors.</p> <p>Defines the abstract interface that all text extractors must implement.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.base.BaseTextExtractor","title":"BaseTextExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for text extractors.</p> <p>All text extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyTextExtractor(BaseTextExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Load model weights\n            pass\n\n        def extract(self, image, output_format=\"markdown\"):\n            # Run extraction\n            return TextOutput(...)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.base.BaseTextExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format or output_format is not supported</p> <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        ValueError: If image format or output_format is not supported\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr","title":"dotsocr","text":"<p>Dots OCR text extractor and backend configurations.</p> <p>Available backends: - PyTorch: DotsOCRPyTorchConfig (local GPU inference) - VLLM: DotsOCRVLLMConfig (offline batch inference) - API: DotsOCRAPIConfig (online VLLM server via OpenAI-compatible API)</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRAPIConfig","title":"DotsOCRAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Dots OCR.</p> <p>This config is for accessing a deployed VLLM server via OpenAI-compatible API. Typically used with modal_dotsocr_vllm_online.py deployment.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRAPIConfig\n\nconfig = DotsOCRAPIConfig(\n        model=\"dotsocr\",\n        api_base=\"https://your-modal-app.modal.run/v1\",\n        api_key=\"optional-key\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRTextExtractor","title":"DotsOCRTextExtractor","text":"<pre><code>DotsOCRTextExtractor(backend: DotsOCRBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Dots OCR Vision-Language Model text extractor with layout detection.</p> <p>Extracts text from document images with layout information including: - 11 layout categories (Caption, Footnote, Formula, List-item, etc.) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown, LaTeX, HTML) - Reading order preservation</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\n\n# Extract with layout\nresult = extractor.extract(image, include_layout=True)\nprint(f\"Found {result.num_layout_elements} elements\")\nprint(result.content)\n</code></pre> <p>Initialize Dots OCR text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend - DotsOCRVLLMConfig: VLLM high-throughput backend - DotsOCRAPIConfig: API backend (online VLLM server)</p> <p> TYPE: <code>DotsOCRBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def __init__(self, backend: DotsOCRBackendConfig):\n    \"\"\"\n    Initialize Dots OCR text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend\n            - DotsOCRVLLMConfig: VLLM high-throughput backend\n            - DotsOCRAPIConfig: API backend (online VLLM server)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._model: Any = None\n    self._loaded = False\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\n        \"markdown\", \"html\", \"json\"\n    ] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput\n</code></pre> <p>Extract text from image using Dots OCR.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\", \"html\", or \"json\")</p> <p> TYPE: <code>Literal['markdown', 'html', 'json']</code> DEFAULT: <code>'markdown'</code> </p> <code>include_layout</code> <p>Include layout bounding boxes in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>custom_prompt</code> <p>Override default extraction prompt</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_tokens</code> <p>Maximum tokens for generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>8192</code> </p> RETURNS DESCRIPTION <code>DotsOCRTextOutput</code> <p>DotsOCRTextOutput with extracted content and optional layout</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"markdown\", \"html\", \"json\"] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput:\n    \"\"\"\n    Extract text from image using Dots OCR.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\", \"html\", or \"json\")\n        include_layout: Include layout bounding boxes in output\n        custom_prompt: Override default extraction prompt\n        max_tokens: Maximum tokens for generation\n\n    Returns:\n        DotsOCRTextOutput with extracted content and optional layout\n\n    Raises:\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    img = self._prepare_image(image)\n\n    # Get prompt\n    prompt = custom_prompt or DOTS_OCR_PROMPT\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"DotsOCRPyTorchConfig\":\n        raw_output = self._infer_pytorch(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRVLLMConfig\":\n        raw_output = self._infer_vllm(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRAPIConfig\":\n        raw_output = self._infer_api(img, prompt, max_tokens)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse output\n    return self._parse_output(\n        raw_output,\n        img.size,\n        output_format,\n        include_layout,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRPyTorchConfig","title":"DotsOCRPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Dots OCR.</p> <p>Dots OCR provides layout-aware text extraction with 11 predefined layout categories (Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title).</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nconfig = DotsOCRPyTorchConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRVLLMConfig","title":"DotsOCRVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Dots OCR.</p> <p>VLLM provides high-throughput inference with optimizations like: - PagedAttention for efficient KV cache management - Continuous batching for higher throughput - Optimized CUDA kernels</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nconfig = DotsOCRVLLMConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.api","title":"api","text":"<p>API backend configuration for Dots OCR (VLLM online server).</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.api.DotsOCRAPIConfig","title":"DotsOCRAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Dots OCR.</p> <p>This config is for accessing a deployed VLLM server via OpenAI-compatible API. Typically used with modal_dotsocr_vllm_online.py deployment.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRAPIConfig\n\nconfig = DotsOCRAPIConfig(\n        model=\"dotsocr\",\n        api_base=\"https://your-modal-app.modal.run/v1\",\n        api_key=\"optional-key\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.extractor","title":"extractor","text":"<p>Dots OCR text extractor with layout-aware extraction.</p> <p>A Vision-Language Model optimized for document OCR with structured output containing layout information, bounding boxes, and multi-format text.</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\nresult = extractor.extract(image, include_layout=True)\nprint(result.content)\nfor elem in result.layout:\n        print(f\"{elem.category}: {elem.bbox}\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.extractor.DotsOCRTextExtractor","title":"DotsOCRTextExtractor","text":"<pre><code>DotsOCRTextExtractor(backend: DotsOCRBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Dots OCR Vision-Language Model text extractor with layout detection.</p> <p>Extracts text from document images with layout information including: - 11 layout categories (Caption, Footnote, Formula, List-item, etc.) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown, LaTeX, HTML) - Reading order preservation</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\n\n# Extract with layout\nresult = extractor.extract(image, include_layout=True)\nprint(f\"Found {result.num_layout_elements} elements\")\nprint(result.content)\n</code></pre> <p>Initialize Dots OCR text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend - DotsOCRVLLMConfig: VLLM high-throughput backend - DotsOCRAPIConfig: API backend (online VLLM server)</p> <p> TYPE: <code>DotsOCRBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def __init__(self, backend: DotsOCRBackendConfig):\n    \"\"\"\n    Initialize Dots OCR text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend\n            - DotsOCRVLLMConfig: VLLM high-throughput backend\n            - DotsOCRAPIConfig: API backend (online VLLM server)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._model: Any = None\n    self._loaded = False\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.extractor.DotsOCRTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\n        \"markdown\", \"html\", \"json\"\n    ] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput\n</code></pre> <p>Extract text from image using Dots OCR.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\", \"html\", or \"json\")</p> <p> TYPE: <code>Literal['markdown', 'html', 'json']</code> DEFAULT: <code>'markdown'</code> </p> <code>include_layout</code> <p>Include layout bounding boxes in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>custom_prompt</code> <p>Override default extraction prompt</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_tokens</code> <p>Maximum tokens for generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>8192</code> </p> RETURNS DESCRIPTION <code>DotsOCRTextOutput</code> <p>DotsOCRTextOutput with extracted content and optional layout</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"markdown\", \"html\", \"json\"] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput:\n    \"\"\"\n    Extract text from image using Dots OCR.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\", \"html\", or \"json\")\n        include_layout: Include layout bounding boxes in output\n        custom_prompt: Override default extraction prompt\n        max_tokens: Maximum tokens for generation\n\n    Returns:\n        DotsOCRTextOutput with extracted content and optional layout\n\n    Raises:\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    img = self._prepare_image(image)\n\n    # Get prompt\n    prompt = custom_prompt or DOTS_OCR_PROMPT\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"DotsOCRPyTorchConfig\":\n        raw_output = self._infer_pytorch(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRVLLMConfig\":\n        raw_output = self._infer_vllm(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRAPIConfig\":\n        raw_output = self._infer_api(img, prompt, max_tokens)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse output\n    return self._parse_output(\n        raw_output,\n        img.size,\n        output_format,\n        include_layout,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.pytorch","title":"pytorch","text":"<p>PyTorch backend configuration for Dots OCR.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.pytorch.DotsOCRPyTorchConfig","title":"DotsOCRPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Dots OCR.</p> <p>Dots OCR provides layout-aware text extraction with 11 predefined layout categories (Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title).</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nconfig = DotsOCRPyTorchConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.vllm","title":"vllm","text":"<p>VLLM backend configuration for Dots OCR.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.vllm.DotsOCRVLLMConfig","title":"DotsOCRVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Dots OCR.</p> <p>VLLM provides high-throughput inference with optimizations like: - PagedAttention for efficient KV cache management - Continuous batching for higher throughput - Optimized CUDA kernels</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nconfig = DotsOCRVLLMConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.models","title":"models","text":"<p>Pydantic models for text extraction outputs.</p> <p>Defines output types and format enums for text extraction.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.models.OutputFormat","title":"OutputFormat","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported text extraction output formats.</p> Each format has different characteristics <ul> <li>HTML: Structured with div elements, preserves layout semantics</li> <li>MARKDOWN: Portable, human-readable, good for documentation</li> <li>JSON: Structured data with layout information (Dots OCR)</li> </ul>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.models.TextOutput","title":"TextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text extraction output from a document image.</p> <p>Contains the extracted text content in the requested format, along with optional raw output and plain text versions.</p> Example <pre><code>result = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)  # Clean markdown\nprint(result.plain_text)  # Plain text without formatting\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.models.TextOutput.content_length","title":"content_length  <code>property</code>","text":"<pre><code>content_length: int\n</code></pre> <p>Length of the extracted content in characters.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.models.TextOutput.word_count","title":"word_count  <code>property</code>","text":"<pre><code>word_count: int\n</code></pre> <p>Approximate word count of the plain text.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.models.LayoutElement","title":"LayoutElement","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single layout element from document layout detection.</p> <p>Represents a detected region in the document with its bounding box, category label, and extracted text content.</p> ATTRIBUTE DESCRIPTION <code>bbox</code> <p>Bounding box coordinates [x1, y1, x2, y2] (normalized to 0-1024)</p> <p> TYPE: <code>List[int]</code> </p> <code>category</code> <p>Layout category (e.g., \"Text\", \"Title\", \"Table\", \"Formula\")</p> <p> TYPE: <code>str</code> </p> <code>text</code> <p>Extracted text content (None for pictures)</p> <p> TYPE: <code>Optional[str]</code> </p> <code>confidence</code> <p>Detection confidence score (optional)</p> <p> TYPE: <code>Optional[float]</code> </p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.models.DotsOCRTextOutput","title":"DotsOCRTextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text extraction output from Dots OCR with layout information.</p> <p>Dots OCR provides structured output with: - Layout detection (11 categories) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown/LaTeX/HTML) - Reading order preservation</p> Layout Categories <p>Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title</p> Text Formatting <ul> <li>Text/Title/Section-header: Markdown</li> <li>Formula: LaTeX</li> <li>Table: HTML</li> <li>Picture: (text omitted)</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nresult = extractor.extract(image, include_layout=True)\nprint(result.content)  # Full text with formatting\nfor elem in result.layout:\n        print(f\"{elem.category}: {elem.bbox}\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.models.DotsOCRTextOutput.num_layout_elements","title":"num_layout_elements  <code>property</code>","text":"<pre><code>num_layout_elements: int\n</code></pre> <p>Number of detected layout elements.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.models.DotsOCRTextOutput.content_length","title":"content_length  <code>property</code>","text":"<pre><code>content_length: int\n</code></pre> <p>Length of extracted content in characters.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen","title":"qwen","text":"<p>Qwen3-VL backend configurations and extractor for text extraction.</p> Available backends <ul> <li>QwenTextPyTorchConfig: PyTorch/HuggingFace backend</li> <li>QwenTextVLLMConfig: VLLM high-throughput backend</li> <li>QwenTextMLXConfig: MLX backend for Apple Silicon</li> <li>QwenTextAPIConfig: API backend (OpenRouter, etc.)</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nconfig = QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextAPIConfig","title":"QwenTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen text extraction.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenTextAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextExtractor","title":"QwenTextExtractor","text":"<pre><code>QwenTextExtractor(backend: QwenTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Qwen3-VL Vision-Language Model text extractor.</p> <p>Extracts text from document images and outputs as structured HTML or Markdown. Uses Qwen3-VL's built-in document parsing prompts.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Extract as Markdown\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n\n# Extract as HTML\nresult = extractor.extract(image, output_format=\"html\")\nprint(result.content)\n</code></pre> <p>Initialize Qwen text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenTextPyTorchConfig: PyTorch/HuggingFace backend - QwenTextVLLMConfig: VLLM high-throughput backend - QwenTextMLXConfig: MLX backend for Apple Silicon - QwenTextAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def __init__(self, backend: QwenTextBackendConfig):\n    \"\"\"\n    Initialize Qwen text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenTextPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenTextVLLMConfig: VLLM high-throughput backend\n            - QwenTextMLXConfig: MLX backend for Apple Silicon\n            - QwenTextAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML with div elements - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format or output_format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML with div elements\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format or output_format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}. Expected 'html' or 'markdown'.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Get prompt for output format\n    prompt = QWEN_PROMPTS[output_format]\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenTextAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    if output_format == \"html\":\n        cleaned_output = _clean_html_output(raw_output)\n    else:\n        cleaned_output = _clean_markdown_output(raw_output)\n\n    # Extract plain text\n    plain_text = _extract_plain_text(raw_output, output_format)\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=plain_text,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextMLXConfig","title":"QwenTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenTextMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextPyTorchConfig","title":"QwenTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextVLLMConfig","title":"QwenTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenTextVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.api","title":"api","text":"<p>API backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.api.QwenTextAPIConfig","title":"QwenTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen text extraction.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenTextAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.extractor","title":"extractor","text":"<p>Qwen3-VL text extractor.</p> <p>A Vision-Language Model for extracting text from document images as structured HTML or Markdown.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.extractor.QwenTextExtractor","title":"QwenTextExtractor","text":"<pre><code>QwenTextExtractor(backend: QwenTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Qwen3-VL Vision-Language Model text extractor.</p> <p>Extracts text from document images and outputs as structured HTML or Markdown. Uses Qwen3-VL's built-in document parsing prompts.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Extract as Markdown\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n\n# Extract as HTML\nresult = extractor.extract(image, output_format=\"html\")\nprint(result.content)\n</code></pre> <p>Initialize Qwen text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenTextPyTorchConfig: PyTorch/HuggingFace backend - QwenTextVLLMConfig: VLLM high-throughput backend - QwenTextMLXConfig: MLX backend for Apple Silicon - QwenTextAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def __init__(self, backend: QwenTextBackendConfig):\n    \"\"\"\n    Initialize Qwen text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenTextPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenTextVLLMConfig: VLLM high-throughput backend\n            - QwenTextMLXConfig: MLX backend for Apple Silicon\n            - QwenTextAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.extractor.QwenTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML with div elements - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format or output_format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML with div elements\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format or output_format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}. Expected 'html' or 'markdown'.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Get prompt for output format\n    prompt = QWEN_PROMPTS[output_format]\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenTextAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    if output_format == \"html\":\n        cleaned_output = _clean_html_output(raw_output)\n    else:\n        cleaned_output = _clean_markdown_output(raw_output)\n\n    # Extract plain text\n    plain_text = _extract_plain_text(raw_output, output_format)\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=plain_text,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.mlx","title":"mlx","text":"<p>MLX backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.mlx.QwenTextMLXConfig","title":"QwenTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenTextMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.pytorch","title":"pytorch","text":"<p>PyTorch/HuggingFace backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.pytorch.QwenTextPyTorchConfig","title":"QwenTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.vllm","title":"vllm","text":"<p>VLLM backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.vllm.QwenTextVLLMConfig","title":"QwenTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenTextVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/base/","title":"Base","text":"<p>Base class for layout extractors.</p> <p>Defines the abstract interface that all layout extractors must implement.</p>"},{"location":"reference/tasks/layout_extraction/base/#omnidocs.tasks.layout_extraction.base.BaseLayoutExtractor","title":"BaseLayoutExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for layout extractors.</p> <p>All layout extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyLayoutExtractor(BaseLayoutExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Load model weights\n            pass\n\n        def extract(self, image):\n            # Run extraction\n            return LayoutOutput(...)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/base/#omnidocs.tasks.layout_extraction.base.BaseLayoutExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput containing detected layout boxes with standardized labels</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format is not supported</p> <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/layout_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n\n    Returns:\n        LayoutOutput containing detected layout boxes with standardized labels\n\n    Raises:\n        ValueError: If image format is not supported\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/layout_extraction/doc_layout_yolo/","title":"Doc Layout YOLO","text":"<p>DocLayout-YOLO layout extractor.</p> <p>A YOLO-based model for document layout detection, optimized for academic papers and technical documents.</p> <p>Model: juliozhao/DocLayout-YOLO-DocStructBench</p>"},{"location":"reference/tasks/layout_extraction/doc_layout_yolo/#omnidocs.tasks.layout_extraction.doc_layout_yolo.DocLayoutYOLOConfig","title":"DocLayoutYOLOConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for DocLayout-YOLO layout extractor.</p> <p>This is a single-backend model (PyTorch only).</p> Example <pre><code>config = DocLayoutYOLOConfig(device=\"cuda\", confidence=0.3)\nextractor = DocLayoutYOLO(config=config)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/doc_layout_yolo/#omnidocs.tasks.layout_extraction.doc_layout_yolo.DocLayoutYOLO","title":"DocLayoutYOLO","text":"<pre><code>DocLayoutYOLO(config: DocLayoutYOLOConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>DocLayout-YOLO layout extractor.</p> <p>A YOLO-based model optimized for document layout detection. Detects: title, text, figure, table, formula, captions, etc.</p> <p>This is a single-backend model (PyTorch only).</p> Example <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\nextractor = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre> <p>Initialize DocLayout-YOLO extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object with device, model_path, etc.</p> <p> TYPE: <code>DocLayoutYOLOConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/doc_layout_yolo.py</code> <pre><code>def __init__(self, config: DocLayoutYOLOConfig):\n    \"\"\"\n    Initialize DocLayout-YOLO extractor.\n\n    Args:\n        config: Configuration object with device, model_path, etc.\n    \"\"\"\n    self.config = config\n    self._model = None\n    self._device = self._resolve_device(config.device)\n    self._model_path = self._resolve_model_path(config.model_path)\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/doc_layout_yolo/#omnidocs.tasks.layout_extraction.doc_layout_yolo.DocLayoutYOLO.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> Source code in <code>omnidocs/tasks/layout_extraction/doc_layout_yolo.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n    if self._model is None:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    img_width, img_height = pil_image.size\n\n    # Run inference\n    results = self._model.predict(\n        pil_image,\n        imgsz=self.config.img_size,\n        conf=self.config.confidence,\n        device=self._device,\n    )\n\n    result = results[0]\n\n    # Parse detections\n    layout_boxes = []\n\n    if hasattr(result, \"boxes\") and result.boxes is not None:\n        boxes = result.boxes\n\n        for i in range(len(boxes)):\n            # Get coordinates\n            bbox_coords = boxes.xyxy[i].cpu().numpy().tolist()\n\n            # Get class and confidence\n            class_id = int(boxes.cls[i].item())\n            confidence = float(boxes.conf[i].item())\n\n            # Get original label from class names\n            original_label = DOCLAYOUT_YOLO_CLASS_NAMES.get(class_id, f\"class_{class_id}\")\n\n            # Map to standardized label\n            standard_label = DOCLAYOUT_YOLO_MAPPING.to_standard(original_label)\n\n            layout_boxes.append(\n                LayoutBox(\n                    label=standard_label,\n                    bbox=BoundingBox.from_list(bbox_coords),\n                    confidence=confidence,\n                    class_id=class_id,\n                    original_label=original_label,\n                )\n            )\n\n    # Sort by y-coordinate (top to bottom reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=img_width,\n        image_height=img_height,\n        model_name=\"DocLayout-YOLO\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/","title":"Models","text":"<p>Pydantic models for layout extraction outputs.</p> <p>Defines standardized output types and label enums for layout detection.</p> Coordinate Systems <ul> <li>Absolute (default): Coordinates in pixels relative to original image size</li> <li>Normalized (0-1024): Coordinates scaled to 0-1024 range (virtual 1024x1024 canvas)</li> </ul> <p>Use <code>bbox.to_normalized(width, height)</code> or <code>output.get_normalized_bboxes()</code> to convert to normalized coordinates.</p> Example <pre><code>result = extractor.extract(image)  # Returns absolute pixel coordinates\nnormalized = result.get_normalized_bboxes()  # Returns 0-1024 normalized coords\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutLabel","title":"LayoutLabel","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Standardized layout labels used across all layout extractors.</p> <p>These provide a consistent vocabulary regardless of which model is used.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.CustomLabel","title":"CustomLabel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Type-safe custom layout label definition for VLM-based models.</p> <p>VLM models like Qwen3-VL support flexible custom labels beyond the standard LayoutLabel enum. Use this class to define custom labels with validation.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import CustomLabel\n\n# Simple custom label\ncode_block = CustomLabel(name=\"code_block\")\n\n# With metadata\nsidebar = CustomLabel(\n        name=\"sidebar\",\n        description=\"Secondary content panel\",\n        color=\"#9B59B6\",\n    )\n\n# Use with QwenLayoutDetector\nresult = detector.extract(image, custom_labels=[code_block, sidebar])\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LabelMapping","title":"LabelMapping","text":"<pre><code>LabelMapping(mapping: Dict[str, LayoutLabel])\n</code></pre> <p>Base class for model-specific label mappings.</p> <p>Each model maps its native labels to standardized LayoutLabel values.</p> <p>Initialize label mapping.</p> PARAMETER DESCRIPTION <code>mapping</code> <p>Dict mapping model-specific labels to LayoutLabel enum values</p> <p> TYPE: <code>Dict[str, LayoutLabel]</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def __init__(self, mapping: Dict[str, LayoutLabel]):\n    \"\"\"\n    Initialize label mapping.\n\n    Args:\n        mapping: Dict mapping model-specific labels to LayoutLabel enum values\n    \"\"\"\n    self._mapping = {k.lower(): v for k, v in mapping.items()}\n    self._reverse_mapping = {v: k for k, v in mapping.items()}\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LabelMapping.supported_labels","title":"supported_labels  <code>property</code>","text":"<pre><code>supported_labels: List[str]\n</code></pre> <p>Get list of supported model-specific labels.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LabelMapping.standard_labels","title":"standard_labels  <code>property</code>","text":"<pre><code>standard_labels: List[LayoutLabel]\n</code></pre> <p>Get list of standard labels this mapping produces.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LabelMapping.to_standard","title":"to_standard","text":"<pre><code>to_standard(model_label: str) -&gt; LayoutLabel\n</code></pre> <p>Convert model-specific label to standardized LayoutLabel.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_standard(self, model_label: str) -&gt; LayoutLabel:\n    \"\"\"Convert model-specific label to standardized LayoutLabel.\"\"\"\n    return self._mapping.get(model_label.lower(), LayoutLabel.UNKNOWN)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LabelMapping.from_standard","title":"from_standard","text":"<pre><code>from_standard(standard_label: LayoutLabel) -&gt; Optional[str]\n</code></pre> <p>Convert standardized LayoutLabel to model-specific label.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def from_standard(self, standard_label: LayoutLabel) -&gt; Optional[str]:\n    \"\"\"Convert standardized LayoutLabel to model-specific label.\"\"\"\n    return self._reverse_mapping.get(standard_label)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box coordinates in pixel space.</p> <p>Coordinates follow the convention: (x1, y1) is top-left, (x2, y2) is bottom-right.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x, y, width, height) format.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_xywh(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x, y, width, height) format.\"\"\"\n    return (self.x1, self.y1, self.width, self.height)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> <p>Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas. This provides consistent coordinates regardless of original image size.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Example <pre><code>bbox = BoundingBox(x1=100, y1=50, x2=500, y2=300)\nnormalized = bbox.to_normalized(1000, 800)\n# x: 100/1000*1024 = 102.4, y: 50/800*1024 = 64\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas.\n    This provides consistent coordinates regardless of original image size.\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n\n    Example:\n        ```python\n        bbox = BoundingBox(x1=100, y1=50, x2=500, y2=300)\n        normalized = bbox.to_normalized(1000, 800)\n        # x: 100/1000*1024 = 102.4, y: 50/800*1024 = 64\n        ```\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_absolute","title":"to_absolute","text":"<pre><code>to_absolute(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert from normalized (0-1024) to absolute pixel coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Target image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Target image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with absolute pixel coordinates</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_absolute(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert from normalized (0-1024) to absolute pixel coordinates.\n\n    Args:\n        image_width: Target image width in pixels\n        image_height: Target image height in pixels\n\n    Returns:\n        New BoundingBox with absolute pixel coordinates\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / NORMALIZED_SIZE * image_width,\n        y1=self.y1 / NORMALIZED_SIZE * image_height,\n        x2=self.x2 / NORMALIZED_SIZE * image_width,\n        y2=self.y2 / NORMALIZED_SIZE * image_height,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutBox","title":"LayoutBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single detected layout element with label, bounding box, and confidence.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutBox.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"label\": self.label.value,\n        \"bbox\": self.bbox.to_list(),\n        \"confidence\": self.confidence,\n        \"class_id\": self.class_id,\n        \"original_label\": self.original_label,\n    }\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutBox.get_normalized_bbox","title":"get_normalized_bbox","text":"<pre><code>get_normalized_bbox(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Get bounding box in normalized (0-1024) coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox with normalized coordinates</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def get_normalized_bbox(self, image_width: int, image_height: int) -&gt; BoundingBox:\n    \"\"\"\n    Get bounding box in normalized (0-1024) coordinates.\n\n    Args:\n        image_width: Original image width\n        image_height: Original image height\n\n    Returns:\n        BoundingBox with normalized coordinates\n    \"\"\"\n    return self.bbox.to_normalized(image_width, image_height)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput","title":"LayoutOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete layout extraction results for a single image.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput.element_count","title":"element_count  <code>property</code>","text":"<pre><code>element_count: int\n</code></pre> <p>Number of detected elements.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput.labels_found","title":"labels_found  <code>property</code>","text":"<pre><code>labels_found: List[str]\n</code></pre> <p>Unique labels found in detections.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput.filter_by_label","title":"filter_by_label","text":"<pre><code>filter_by_label(label: LayoutLabel) -&gt; List[LayoutBox]\n</code></pre> <p>Filter boxes by label.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def filter_by_label(self, label: LayoutLabel) -&gt; List[LayoutBox]:\n    \"\"\"Filter boxes by label.\"\"\"\n    return [box for box in self.bboxes if box.label == label]\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(\n    min_confidence: float,\n) -&gt; List[LayoutBox]\n</code></pre> <p>Filter boxes by minimum confidence.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def filter_by_confidence(self, min_confidence: float) -&gt; List[LayoutBox]:\n    \"\"\"Filter boxes by minimum confidence.\"\"\"\n    return [box for box in self.bboxes if box.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"bboxes\": [box.to_dict() for box in self.bboxes],\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"element_count\": self.element_count,\n        \"labels_found\": self.labels_found,\n    }\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput.sort_by_position","title":"sort_by_position","text":"<pre><code>sort_by_position(\n    top_to_bottom: bool = True,\n) -&gt; LayoutOutput\n</code></pre> <p>Return a new LayoutOutput with boxes sorted by position.</p> PARAMETER DESCRIPTION <code>top_to_bottom</code> <p>If True, sort by y-coordinate (reading order)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def sort_by_position(self, top_to_bottom: bool = True) -&gt; \"LayoutOutput\":\n    \"\"\"\n    Return a new LayoutOutput with boxes sorted by position.\n\n    Args:\n        top_to_bottom: If True, sort by y-coordinate (reading order)\n    \"\"\"\n    sorted_boxes = sorted(self.bboxes, key=lambda b: (b.bbox.y1, b.bbox.x1), reverse=not top_to_bottom)\n    return LayoutOutput(\n        bboxes=sorted_boxes,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        model_name=self.model_name,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput.get_normalized_bboxes","title":"get_normalized_bboxes","text":"<pre><code>get_normalized_bboxes() -&gt; List[Dict]\n</code></pre> <p>Get all bounding boxes in normalized (0-1024) coordinates.</p> RETURNS DESCRIPTION <code>List[Dict]</code> <p>List of dicts with normalized bbox coordinates and metadata.</p> Example <pre><code>result = extractor.extract(image)\nnormalized = result.get_normalized_bboxes()\nfor box in normalized:\n        print(f\"{box['label']}: {box['bbox']}\")  # coords in 0-1024 range\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def get_normalized_bboxes(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all bounding boxes in normalized (0-1024) coordinates.\n\n    Returns:\n        List of dicts with normalized bbox coordinates and metadata.\n\n    Example:\n        ```python\n        result = extractor.extract(image)\n        normalized = result.get_normalized_bboxes()\n        for box in normalized:\n                print(f\"{box['label']}: {box['bbox']}\")  # coords in 0-1024 range\n        ```\n    \"\"\"\n    normalized = []\n    for box in self.bboxes:\n        norm_bbox = box.bbox.to_normalized(self.image_width, self.image_height)\n        normalized.append(\n            {\n                \"label\": box.label.value,\n                \"bbox\": norm_bbox.to_list(),\n                \"confidence\": box.confidence,\n                \"class_id\": box.class_id,\n                \"original_label\": box.original_label,\n            }\n        )\n    return normalized\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput.visualize","title":"visualize","text":"<pre><code>visualize(\n    image: Image,\n    output_path: Optional[Union[str, Path]] = None,\n    show_labels: bool = True,\n    show_confidence: bool = True,\n    line_width: int = 3,\n    font_size: int = 12,\n) -&gt; Image.Image\n</code></pre> <p>Visualize layout detection results on the image.</p> <p>Draws bounding boxes with labels and confidence scores on the image. Each layout category has a distinct color for easy identification.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to draw on (will be copied, not modified)</p> <p> TYPE: <code>Image</code> </p> <code>output_path</code> <p>Optional path to save the visualization</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> <code>show_labels</code> <p>Whether to show label text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_confidence</code> <p>Whether to show confidence scores</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>line_width</code> <p>Width of bounding box lines</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>font_size</code> <p>Size of label text (note: uses default font)</p> <p> TYPE: <code>int</code> DEFAULT: <code>12</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image with visualizations drawn</p> Example <pre><code>result = extractor.extract(image)\nviz = result.visualize(image, output_path=\"layout_viz.png\")\nviz.show()  # Display in notebook/viewer\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def visualize(\n    self,\n    image: \"Image.Image\",\n    output_path: Optional[Union[str, Path]] = None,\n    show_labels: bool = True,\n    show_confidence: bool = True,\n    line_width: int = 3,\n    font_size: int = 12,\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Visualize layout detection results on the image.\n\n    Draws bounding boxes with labels and confidence scores on the image.\n    Each layout category has a distinct color for easy identification.\n\n    Args:\n        image: PIL Image to draw on (will be copied, not modified)\n        output_path: Optional path to save the visualization\n        show_labels: Whether to show label text\n        show_confidence: Whether to show confidence scores\n        line_width: Width of bounding box lines\n        font_size: Size of label text (note: uses default font)\n\n    Returns:\n        PIL Image with visualizations drawn\n\n    Example:\n        ```python\n        result = extractor.extract(image)\n        viz = result.visualize(image, output_path=\"layout_viz.png\")\n        viz.show()  # Display in notebook/viewer\n        ```\n    \"\"\"\n    from PIL import ImageDraw\n\n    # Copy image to avoid modifying original\n    viz_image = image.copy().convert(\"RGB\")\n    draw = ImageDraw.Draw(viz_image)\n\n    for box in self.bboxes:\n        # Get color for this label\n        color = LABEL_COLORS.get(box.label, \"#95A5A6\")\n\n        # Draw bounding box\n        coords = box.bbox.to_xyxy()\n        draw.rectangle(coords, outline=color, width=line_width)\n\n        # Build label text\n        if show_labels or show_confidence:\n            label_parts = []\n            if show_labels:\n                label_parts.append(box.label.value)\n            if show_confidence:\n                label_parts.append(f\"{box.confidence:.2f}\")\n            label_text = \" \".join(label_parts)\n\n            # Draw label background\n            text_bbox = draw.textbbox((coords[0], coords[1] - 20), label_text)\n            draw.rectangle(text_bbox, fill=color)\n\n            # Draw label text\n            draw.text(\n                (coords[0], coords[1] - 20),\n                label_text,\n                fill=\"white\",\n            )\n\n    # Save if path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        viz_image.save(output_path)\n\n    return viz_image\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; LayoutOutput\n</code></pre> <p>Load a LayoutOutput instance from a JSON file.</p> <p>Reads a JSON file and deserializes its contents into a LayoutOutput object. Uses Pydantic's model_validate_json for proper handling of nested objects.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to JSON file containing serialized LayoutOutput data.       Can be string or pathlib.Path object.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>Deserialized layout output instance from file.</p> <p> TYPE: <code>LayoutOutput</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>UnicodeDecodeError</code> <p>If file cannot be decoded as UTF-8.</p> <code>ValueError</code> <p>If file contents are not valid JSON.</p> <code>ValidationError</code> <p>If JSON data doesn't match LayoutOutput schema.</p> Example <p><pre><code>output = LayoutOutput.load_json('layout_results.json')\nprint(f\"Found {output.element_count} elements\")\n</code></pre> Found 5 elements</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"LayoutOutput\":\n    \"\"\"\n    Load a LayoutOutput instance from a JSON file.\n\n    Reads a JSON file and deserializes its contents into a LayoutOutput object.\n    Uses Pydantic's model_validate_json for proper handling of nested objects.\n\n    Args:\n        file_path: Path to JSON file containing serialized LayoutOutput data.\n                  Can be string or pathlib.Path object.\n\n    Returns:\n        LayoutOutput: Deserialized layout output instance from file.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n        UnicodeDecodeError: If file cannot be decoded as UTF-8.\n        ValueError: If file contents are not valid JSON.\n        ValidationError: If JSON data doesn't match LayoutOutput schema.\n\n    Example:\n        ```python\n        output = LayoutOutput.load_json('layout_results.json')\n        print(f\"Found {output.element_count} elements\")\n        ```\n        Found 5 elements\n    \"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save LayoutOutput instance to a JSON file.</p> <p>Serializes the LayoutOutput object to JSON and writes it to a file. Automatically creates parent directories if they don't exist. Uses UTF-8 encoding for compatibility and proper handling of special characters.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path where JSON file should be saved. Can be string or       pathlib.Path object. Parent directories will be created       if they don't exist.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> RAISES DESCRIPTION <code>OSError</code> <p>If file cannot be written due to permission or disk errors.</p> <code>TypeError</code> <p>If file_path is not a string or Path object.</p> Example <pre><code>output = LayoutOutput(bboxes=[], image_width=800, image_height=600)\noutput.save_json('results/layout_output.json')\n# File is created at results/layout_output.json\n# Parent 'results' directory is created if it didn't exist\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"\n    Save LayoutOutput instance to a JSON file.\n\n    Serializes the LayoutOutput object to JSON and writes it to a file.\n    Automatically creates parent directories if they don't exist. Uses UTF-8\n    encoding for compatibility and proper handling of special characters.\n\n    Args:\n        file_path: Path where JSON file should be saved. Can be string or\n                  pathlib.Path object. Parent directories will be created\n                  if they don't exist.\n\n    Returns:\n        None\n\n    Raises:\n        OSError: If file cannot be written due to permission or disk errors.\n        TypeError: If file_path is not a string or Path object.\n\n    Example:\n        ```python\n        output = LayoutOutput(bboxes=[], image_width=800, image_height=600)\n        output.save_json('results/layout_output.json')\n        # File is created at results/layout_output.json\n        # Parent 'results' directory is created if it didn't exist\n        ```\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/","title":"Overview","text":"<p>Layout Extraction Module.</p> <p>Provides extractors for detecting document layout elements such as titles, text blocks, figures, tables, formulas, and captions.</p> Available Extractors <ul> <li>DocLayoutYOLO: YOLO-based layout detector (fast, accurate)</li> <li>RTDETRLayoutExtractor: Transformer-based detector (more categories)</li> <li>QwenLayoutDetector: VLM-based detector with custom label support (multi-backend)</li> </ul> Example <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\nextractor = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n# VLM-based detection with custom labels\nfrom omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BaseLayoutExtractor","title":"BaseLayoutExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for layout extractors.</p> <p>All layout extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyLayoutExtractor(BaseLayoutExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Load model weights\n            pass\n\n        def extract(self, image):\n            # Run extraction\n            return LayoutOutput(...)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BaseLayoutExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput containing detected layout boxes with standardized labels</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format is not supported</p> <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/layout_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n\n    Returns:\n        LayoutOutput containing detected layout boxes with standardized labels\n\n    Raises:\n        ValueError: If image format is not supported\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.DocLayoutYOLO","title":"DocLayoutYOLO","text":"<pre><code>DocLayoutYOLO(config: DocLayoutYOLOConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>DocLayout-YOLO layout extractor.</p> <p>A YOLO-based model optimized for document layout detection. Detects: title, text, figure, table, formula, captions, etc.</p> <p>This is a single-backend model (PyTorch only).</p> Example <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\nextractor = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre> <p>Initialize DocLayout-YOLO extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object with device, model_path, etc.</p> <p> TYPE: <code>DocLayoutYOLOConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/doc_layout_yolo.py</code> <pre><code>def __init__(self, config: DocLayoutYOLOConfig):\n    \"\"\"\n    Initialize DocLayout-YOLO extractor.\n\n    Args:\n        config: Configuration object with device, model_path, etc.\n    \"\"\"\n    self.config = config\n    self._model = None\n    self._device = self._resolve_device(config.device)\n    self._model_path = self._resolve_model_path(config.model_path)\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.DocLayoutYOLO.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> Source code in <code>omnidocs/tasks/layout_extraction/doc_layout_yolo.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n    if self._model is None:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    img_width, img_height = pil_image.size\n\n    # Run inference\n    results = self._model.predict(\n        pil_image,\n        imgsz=self.config.img_size,\n        conf=self.config.confidence,\n        device=self._device,\n    )\n\n    result = results[0]\n\n    # Parse detections\n    layout_boxes = []\n\n    if hasattr(result, \"boxes\") and result.boxes is not None:\n        boxes = result.boxes\n\n        for i in range(len(boxes)):\n            # Get coordinates\n            bbox_coords = boxes.xyxy[i].cpu().numpy().tolist()\n\n            # Get class and confidence\n            class_id = int(boxes.cls[i].item())\n            confidence = float(boxes.conf[i].item())\n\n            # Get original label from class names\n            original_label = DOCLAYOUT_YOLO_CLASS_NAMES.get(class_id, f\"class_{class_id}\")\n\n            # Map to standardized label\n            standard_label = DOCLAYOUT_YOLO_MAPPING.to_standard(original_label)\n\n            layout_boxes.append(\n                LayoutBox(\n                    label=standard_label,\n                    bbox=BoundingBox.from_list(bbox_coords),\n                    confidence=confidence,\n                    class_id=class_id,\n                    original_label=original_label,\n                )\n            )\n\n    # Sort by y-coordinate (top to bottom reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=img_width,\n        image_height=img_height,\n        model_name=\"DocLayout-YOLO\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.DocLayoutYOLOConfig","title":"DocLayoutYOLOConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for DocLayout-YOLO layout extractor.</p> <p>This is a single-backend model (PyTorch only).</p> Example <pre><code>config = DocLayoutYOLOConfig(device=\"cuda\", confidence=0.3)\nextractor = DocLayoutYOLO(config=config)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box coordinates in pixel space.</p> <p>Coordinates follow the convention: (x1, y1) is top-left, (x2, y2) is bottom-right.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x, y, width, height) format.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_xywh(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x, y, width, height) format.\"\"\"\n    return (self.x1, self.y1, self.width, self.height)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> <p>Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas. This provides consistent coordinates regardless of original image size.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Example <pre><code>bbox = BoundingBox(x1=100, y1=50, x2=500, y2=300)\nnormalized = bbox.to_normalized(1000, 800)\n# x: 100/1000*1024 = 102.4, y: 50/800*1024 = 64\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas.\n    This provides consistent coordinates regardless of original image size.\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n\n    Example:\n        ```python\n        bbox = BoundingBox(x1=100, y1=50, x2=500, y2=300)\n        normalized = bbox.to_normalized(1000, 800)\n        # x: 100/1000*1024 = 102.4, y: 50/800*1024 = 64\n        ```\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox.to_absolute","title":"to_absolute","text":"<pre><code>to_absolute(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert from normalized (0-1024) to absolute pixel coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Target image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Target image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with absolute pixel coordinates</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_absolute(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert from normalized (0-1024) to absolute pixel coordinates.\n\n    Args:\n        image_width: Target image width in pixels\n        image_height: Target image height in pixels\n\n    Returns:\n        New BoundingBox with absolute pixel coordinates\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / NORMALIZED_SIZE * image_width,\n        y1=self.y1 / NORMALIZED_SIZE * image_height,\n        x2=self.x2 / NORMALIZED_SIZE * image_width,\n        y2=self.y2 / NORMALIZED_SIZE * image_height,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.CustomLabel","title":"CustomLabel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Type-safe custom layout label definition for VLM-based models.</p> <p>VLM models like Qwen3-VL support flexible custom labels beyond the standard LayoutLabel enum. Use this class to define custom labels with validation.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import CustomLabel\n\n# Simple custom label\ncode_block = CustomLabel(name=\"code_block\")\n\n# With metadata\nsidebar = CustomLabel(\n        name=\"sidebar\",\n        description=\"Secondary content panel\",\n        color=\"#9B59B6\",\n    )\n\n# Use with QwenLayoutDetector\nresult = detector.extract(image, custom_labels=[code_block, sidebar])\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LabelMapping","title":"LabelMapping","text":"<pre><code>LabelMapping(mapping: Dict[str, LayoutLabel])\n</code></pre> <p>Base class for model-specific label mappings.</p> <p>Each model maps its native labels to standardized LayoutLabel values.</p> <p>Initialize label mapping.</p> PARAMETER DESCRIPTION <code>mapping</code> <p>Dict mapping model-specific labels to LayoutLabel enum values</p> <p> TYPE: <code>Dict[str, LayoutLabel]</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def __init__(self, mapping: Dict[str, LayoutLabel]):\n    \"\"\"\n    Initialize label mapping.\n\n    Args:\n        mapping: Dict mapping model-specific labels to LayoutLabel enum values\n    \"\"\"\n    self._mapping = {k.lower(): v for k, v in mapping.items()}\n    self._reverse_mapping = {v: k for k, v in mapping.items()}\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LabelMapping.supported_labels","title":"supported_labels  <code>property</code>","text":"<pre><code>supported_labels: List[str]\n</code></pre> <p>Get list of supported model-specific labels.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LabelMapping.standard_labels","title":"standard_labels  <code>property</code>","text":"<pre><code>standard_labels: List[LayoutLabel]\n</code></pre> <p>Get list of standard labels this mapping produces.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LabelMapping.to_standard","title":"to_standard","text":"<pre><code>to_standard(model_label: str) -&gt; LayoutLabel\n</code></pre> <p>Convert model-specific label to standardized LayoutLabel.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_standard(self, model_label: str) -&gt; LayoutLabel:\n    \"\"\"Convert model-specific label to standardized LayoutLabel.\"\"\"\n    return self._mapping.get(model_label.lower(), LayoutLabel.UNKNOWN)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LabelMapping.from_standard","title":"from_standard","text":"<pre><code>from_standard(standard_label: LayoutLabel) -&gt; Optional[str]\n</code></pre> <p>Convert standardized LayoutLabel to model-specific label.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def from_standard(self, standard_label: LayoutLabel) -&gt; Optional[str]:\n    \"\"\"Convert standardized LayoutLabel to model-specific label.\"\"\"\n    return self._reverse_mapping.get(standard_label)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutBox","title":"LayoutBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single detected layout element with label, bounding box, and confidence.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutBox.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"label\": self.label.value,\n        \"bbox\": self.bbox.to_list(),\n        \"confidence\": self.confidence,\n        \"class_id\": self.class_id,\n        \"original_label\": self.original_label,\n    }\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutBox.get_normalized_bbox","title":"get_normalized_bbox","text":"<pre><code>get_normalized_bbox(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Get bounding box in normalized (0-1024) coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox with normalized coordinates</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def get_normalized_bbox(self, image_width: int, image_height: int) -&gt; BoundingBox:\n    \"\"\"\n    Get bounding box in normalized (0-1024) coordinates.\n\n    Args:\n        image_width: Original image width\n        image_height: Original image height\n\n    Returns:\n        BoundingBox with normalized coordinates\n    \"\"\"\n    return self.bbox.to_normalized(image_width, image_height)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutLabel","title":"LayoutLabel","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Standardized layout labels used across all layout extractors.</p> <p>These provide a consistent vocabulary regardless of which model is used.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput","title":"LayoutOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete layout extraction results for a single image.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.element_count","title":"element_count  <code>property</code>","text":"<pre><code>element_count: int\n</code></pre> <p>Number of detected elements.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.labels_found","title":"labels_found  <code>property</code>","text":"<pre><code>labels_found: List[str]\n</code></pre> <p>Unique labels found in detections.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.filter_by_label","title":"filter_by_label","text":"<pre><code>filter_by_label(label: LayoutLabel) -&gt; List[LayoutBox]\n</code></pre> <p>Filter boxes by label.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def filter_by_label(self, label: LayoutLabel) -&gt; List[LayoutBox]:\n    \"\"\"Filter boxes by label.\"\"\"\n    return [box for box in self.bboxes if box.label == label]\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(\n    min_confidence: float,\n) -&gt; List[LayoutBox]\n</code></pre> <p>Filter boxes by minimum confidence.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def filter_by_confidence(self, min_confidence: float) -&gt; List[LayoutBox]:\n    \"\"\"Filter boxes by minimum confidence.\"\"\"\n    return [box for box in self.bboxes if box.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"bboxes\": [box.to_dict() for box in self.bboxes],\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"element_count\": self.element_count,\n        \"labels_found\": self.labels_found,\n    }\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.sort_by_position","title":"sort_by_position","text":"<pre><code>sort_by_position(\n    top_to_bottom: bool = True,\n) -&gt; LayoutOutput\n</code></pre> <p>Return a new LayoutOutput with boxes sorted by position.</p> PARAMETER DESCRIPTION <code>top_to_bottom</code> <p>If True, sort by y-coordinate (reading order)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def sort_by_position(self, top_to_bottom: bool = True) -&gt; \"LayoutOutput\":\n    \"\"\"\n    Return a new LayoutOutput with boxes sorted by position.\n\n    Args:\n        top_to_bottom: If True, sort by y-coordinate (reading order)\n    \"\"\"\n    sorted_boxes = sorted(self.bboxes, key=lambda b: (b.bbox.y1, b.bbox.x1), reverse=not top_to_bottom)\n    return LayoutOutput(\n        bboxes=sorted_boxes,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        model_name=self.model_name,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.get_normalized_bboxes","title":"get_normalized_bboxes","text":"<pre><code>get_normalized_bboxes() -&gt; List[Dict]\n</code></pre> <p>Get all bounding boxes in normalized (0-1024) coordinates.</p> RETURNS DESCRIPTION <code>List[Dict]</code> <p>List of dicts with normalized bbox coordinates and metadata.</p> Example <pre><code>result = extractor.extract(image)\nnormalized = result.get_normalized_bboxes()\nfor box in normalized:\n        print(f\"{box['label']}: {box['bbox']}\")  # coords in 0-1024 range\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def get_normalized_bboxes(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all bounding boxes in normalized (0-1024) coordinates.\n\n    Returns:\n        List of dicts with normalized bbox coordinates and metadata.\n\n    Example:\n        ```python\n        result = extractor.extract(image)\n        normalized = result.get_normalized_bboxes()\n        for box in normalized:\n                print(f\"{box['label']}: {box['bbox']}\")  # coords in 0-1024 range\n        ```\n    \"\"\"\n    normalized = []\n    for box in self.bboxes:\n        norm_bbox = box.bbox.to_normalized(self.image_width, self.image_height)\n        normalized.append(\n            {\n                \"label\": box.label.value,\n                \"bbox\": norm_bbox.to_list(),\n                \"confidence\": box.confidence,\n                \"class_id\": box.class_id,\n                \"original_label\": box.original_label,\n            }\n        )\n    return normalized\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.visualize","title":"visualize","text":"<pre><code>visualize(\n    image: Image,\n    output_path: Optional[Union[str, Path]] = None,\n    show_labels: bool = True,\n    show_confidence: bool = True,\n    line_width: int = 3,\n    font_size: int = 12,\n) -&gt; Image.Image\n</code></pre> <p>Visualize layout detection results on the image.</p> <p>Draws bounding boxes with labels and confidence scores on the image. Each layout category has a distinct color for easy identification.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to draw on (will be copied, not modified)</p> <p> TYPE: <code>Image</code> </p> <code>output_path</code> <p>Optional path to save the visualization</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> <code>show_labels</code> <p>Whether to show label text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_confidence</code> <p>Whether to show confidence scores</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>line_width</code> <p>Width of bounding box lines</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>font_size</code> <p>Size of label text (note: uses default font)</p> <p> TYPE: <code>int</code> DEFAULT: <code>12</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image with visualizations drawn</p> Example <pre><code>result = extractor.extract(image)\nviz = result.visualize(image, output_path=\"layout_viz.png\")\nviz.show()  # Display in notebook/viewer\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def visualize(\n    self,\n    image: \"Image.Image\",\n    output_path: Optional[Union[str, Path]] = None,\n    show_labels: bool = True,\n    show_confidence: bool = True,\n    line_width: int = 3,\n    font_size: int = 12,\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Visualize layout detection results on the image.\n\n    Draws bounding boxes with labels and confidence scores on the image.\n    Each layout category has a distinct color for easy identification.\n\n    Args:\n        image: PIL Image to draw on (will be copied, not modified)\n        output_path: Optional path to save the visualization\n        show_labels: Whether to show label text\n        show_confidence: Whether to show confidence scores\n        line_width: Width of bounding box lines\n        font_size: Size of label text (note: uses default font)\n\n    Returns:\n        PIL Image with visualizations drawn\n\n    Example:\n        ```python\n        result = extractor.extract(image)\n        viz = result.visualize(image, output_path=\"layout_viz.png\")\n        viz.show()  # Display in notebook/viewer\n        ```\n    \"\"\"\n    from PIL import ImageDraw\n\n    # Copy image to avoid modifying original\n    viz_image = image.copy().convert(\"RGB\")\n    draw = ImageDraw.Draw(viz_image)\n\n    for box in self.bboxes:\n        # Get color for this label\n        color = LABEL_COLORS.get(box.label, \"#95A5A6\")\n\n        # Draw bounding box\n        coords = box.bbox.to_xyxy()\n        draw.rectangle(coords, outline=color, width=line_width)\n\n        # Build label text\n        if show_labels or show_confidence:\n            label_parts = []\n            if show_labels:\n                label_parts.append(box.label.value)\n            if show_confidence:\n                label_parts.append(f\"{box.confidence:.2f}\")\n            label_text = \" \".join(label_parts)\n\n            # Draw label background\n            text_bbox = draw.textbbox((coords[0], coords[1] - 20), label_text)\n            draw.rectangle(text_bbox, fill=color)\n\n            # Draw label text\n            draw.text(\n                (coords[0], coords[1] - 20),\n                label_text,\n                fill=\"white\",\n            )\n\n    # Save if path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        viz_image.save(output_path)\n\n    return viz_image\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; LayoutOutput\n</code></pre> <p>Load a LayoutOutput instance from a JSON file.</p> <p>Reads a JSON file and deserializes its contents into a LayoutOutput object. Uses Pydantic's model_validate_json for proper handling of nested objects.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to JSON file containing serialized LayoutOutput data.       Can be string or pathlib.Path object.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>Deserialized layout output instance from file.</p> <p> TYPE: <code>LayoutOutput</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>UnicodeDecodeError</code> <p>If file cannot be decoded as UTF-8.</p> <code>ValueError</code> <p>If file contents are not valid JSON.</p> <code>ValidationError</code> <p>If JSON data doesn't match LayoutOutput schema.</p> Example <p><pre><code>output = LayoutOutput.load_json('layout_results.json')\nprint(f\"Found {output.element_count} elements\")\n</code></pre> Found 5 elements</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"LayoutOutput\":\n    \"\"\"\n    Load a LayoutOutput instance from a JSON file.\n\n    Reads a JSON file and deserializes its contents into a LayoutOutput object.\n    Uses Pydantic's model_validate_json for proper handling of nested objects.\n\n    Args:\n        file_path: Path to JSON file containing serialized LayoutOutput data.\n                  Can be string or pathlib.Path object.\n\n    Returns:\n        LayoutOutput: Deserialized layout output instance from file.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n        UnicodeDecodeError: If file cannot be decoded as UTF-8.\n        ValueError: If file contents are not valid JSON.\n        ValidationError: If JSON data doesn't match LayoutOutput schema.\n\n    Example:\n        ```python\n        output = LayoutOutput.load_json('layout_results.json')\n        print(f\"Found {output.element_count} elements\")\n        ```\n        Found 5 elements\n    \"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save LayoutOutput instance to a JSON file.</p> <p>Serializes the LayoutOutput object to JSON and writes it to a file. Automatically creates parent directories if they don't exist. Uses UTF-8 encoding for compatibility and proper handling of special characters.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path where JSON file should be saved. Can be string or       pathlib.Path object. Parent directories will be created       if they don't exist.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> RAISES DESCRIPTION <code>OSError</code> <p>If file cannot be written due to permission or disk errors.</p> <code>TypeError</code> <p>If file_path is not a string or Path object.</p> Example <pre><code>output = LayoutOutput(bboxes=[], image_width=800, image_height=600)\noutput.save_json('results/layout_output.json')\n# File is created at results/layout_output.json\n# Parent 'results' directory is created if it didn't exist\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"\n    Save LayoutOutput instance to a JSON file.\n\n    Serializes the LayoutOutput object to JSON and writes it to a file.\n    Automatically creates parent directories if they don't exist. Uses UTF-8\n    encoding for compatibility and proper handling of special characters.\n\n    Args:\n        file_path: Path where JSON file should be saved. Can be string or\n                  pathlib.Path object. Parent directories will be created\n                  if they don't exist.\n\n    Returns:\n        None\n\n    Raises:\n        OSError: If file cannot be written due to permission or disk errors.\n        TypeError: If file_path is not a string or Path object.\n\n    Example:\n        ```python\n        output = LayoutOutput(bboxes=[], image_width=800, image_height=600)\n        output.save_json('results/layout_output.json')\n        # File is created at results/layout_output.json\n        # Parent 'results' directory is created if it didn't exist\n        ```\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.QwenLayoutDetector","title":"QwenLayoutDetector","text":"<pre><code>QwenLayoutDetector(backend: QwenLayoutBackendConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>Qwen3-VL Vision-Language Model layout detector.</p> <p>A flexible VLM-based layout detector that supports custom labels. Unlike fixed-label models (DocLayoutYOLO, RT-DETR), Qwen can detect any document elements specified at runtime.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\n# Initialize with PyTorch backend\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Basic extraction with default labels\nresult = detector.extract(image)\n\n# With custom labels (strings)\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n\n# With typed custom labels\nlabels = [\n        CustomLabel(name=\"code_block\", color=\"#E74C3C\"),\n        CustomLabel(name=\"sidebar\", description=\"Side panel content\"),\n    ]\nresult = detector.extract(image, custom_labels=labels)\n</code></pre> <p>Initialize Qwen layout detector.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend - QwenLayoutVLLMConfig: VLLM high-throughput backend - QwenLayoutMLXConfig: MLX backend for Apple Silicon - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenLayoutBackendConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def __init__(self, backend: QwenLayoutBackendConfig):\n    \"\"\"\n    Initialize Qwen layout detector.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenLayoutVLLMConfig: VLLM high-throughput backend\n            - QwenLayoutMLXConfig: MLX backend for Apple Silicon\n            - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.QwenLayoutDetector.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    custom_labels: Optional[\n        List[Union[str, CustomLabel]]\n    ] = None,\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout detection on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>custom_labels</code> <p>Optional custom labels to detect. Can be: - None: Use default labels (title, text, table, figure, etc.) - List[str]: Simple label names [\"code_block\", \"sidebar\"] - List[CustomLabel]: Typed labels with metadata</p> <p> TYPE: <code>Optional[List[Union[str, CustomLabel]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    custom_labels: Optional[List[Union[str, CustomLabel]]] = None,\n) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout detection on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        custom_labels: Optional custom labels to detect. Can be:\n            - None: Use default labels (title, text, table, figure, etc.)\n            - List[str]: Simple label names [\"code_block\", \"sidebar\"]\n            - List[CustomLabel]: Typed labels with metadata\n\n    Returns:\n        LayoutOutput with detected layout boxes\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Normalize labels\n    label_names = self._normalize_labels(custom_labels)\n\n    # Build prompt\n    prompt = self._build_detection_prompt(label_names)\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenLayoutPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenLayoutVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenLayoutMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenLayoutAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse detections\n    detections = self._parse_json_output(raw_output)\n\n    # Convert to LayoutOutput\n    layout_boxes = self._build_layout_boxes(detections, width, height)\n\n    # Sort by position (reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.RTDETRConfig","title":"RTDETRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for RT-DETR layout extractor.</p> <p>This is a single-backend model (PyTorch/Transformers only).</p> Example <pre><code>config = RTDETRConfig(device=\"cuda\", confidence=0.4)\nextractor = RTDETRLayoutExtractor(config=config)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.RTDETRLayoutExtractor","title":"RTDETRLayoutExtractor","text":"<pre><code>RTDETRLayoutExtractor(config: RTDETRConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>RT-DETR layout extractor using HuggingFace Transformers.</p> <p>A transformer-based real-time detection model for document layout. Detects: title, text, table, figure, list, formula, captions, headers, footers.</p> <p>This is a single-backend model (PyTorch/Transformers only).</p> Example <pre><code>from omnidocs.tasks.layout_extraction import RTDETRLayoutExtractor, RTDETRConfig\n\nextractor = RTDETRLayoutExtractor(config=RTDETRConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre> <p>Initialize RT-DETR layout extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object with device, model settings, etc.</p> <p> TYPE: <code>RTDETRConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/rtdetr.py</code> <pre><code>def __init__(self, config: RTDETRConfig):\n    \"\"\"\n    Initialize RT-DETR layout extractor.\n\n    Args:\n        config: Configuration object with device, model settings, etc.\n    \"\"\"\n    self.config = config\n    self._model = None\n    self._processor = None\n    self._device = self._resolve_device(config.device)\n    self._model_path = self._resolve_model_path(config.model_path)\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.RTDETRLayoutExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> Source code in <code>omnidocs/tasks/layout_extraction/rtdetr.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n    import torch\n\n    if self._model is None or self._processor is None:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    img_width, img_height = pil_image.size\n\n    # Preprocess\n    inputs = self._processor(\n        images=pil_image,\n        return_tensors=\"pt\",\n        size={\"height\": self.config.image_size, \"width\": self.config.image_size},\n    )\n\n    # Move to device\n    inputs = {k: v.to(self._device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n\n    # Run inference\n    with torch.no_grad():\n        outputs = self._model(**inputs)\n\n    # Post-process results\n    target_sizes = torch.tensor([[img_height, img_width]])\n    results = self._processor.post_process_object_detection(\n        outputs,\n        target_sizes=target_sizes,\n        threshold=self.config.confidence,\n    )[0]\n\n    # Parse detections\n    layout_boxes = []\n\n    for score, label_id, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n        confidence = float(score.item())\n        class_id = int(label_id.item())\n\n        # Get original label from model config\n        # Note: The model outputs 0-indexed class IDs, but id2label has background at index 0,\n        # so we add 1 to map correctly (e.g., model output 8 -&gt; id2label[9] = \"Table\")\n        original_label = self._model.config.id2label.get(class_id + 1, f\"class_{class_id}\")\n\n        # Map to standardized label\n        standard_label = RTDETR_MAPPING.to_standard(original_label)\n\n        # Box coordinates\n        box_coords = box.cpu().tolist()\n\n        layout_boxes.append(\n            LayoutBox(\n                label=standard_label,\n                bbox=BoundingBox.from_list(box_coords),\n                confidence=confidence,\n                class_id=class_id,\n                original_label=original_label,\n            )\n        )\n\n    # Sort by y-coordinate (top to bottom reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=img_width,\n        image_height=img_height,\n        model_name=\"RT-DETR (docling-layout)\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.base","title":"base","text":"<p>Base class for layout extractors.</p> <p>Defines the abstract interface that all layout extractors must implement.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.base.BaseLayoutExtractor","title":"BaseLayoutExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for layout extractors.</p> <p>All layout extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyLayoutExtractor(BaseLayoutExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Load model weights\n            pass\n\n        def extract(self, image):\n            # Run extraction\n            return LayoutOutput(...)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.base.BaseLayoutExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput containing detected layout boxes with standardized labels</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format is not supported</p> <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/layout_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n\n    Returns:\n        LayoutOutput containing detected layout boxes with standardized labels\n\n    Raises:\n        ValueError: If image format is not supported\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.doc_layout_yolo","title":"doc_layout_yolo","text":"<p>DocLayout-YOLO layout extractor.</p> <p>A YOLO-based model for document layout detection, optimized for academic papers and technical documents.</p> <p>Model: juliozhao/DocLayout-YOLO-DocStructBench</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.doc_layout_yolo.DocLayoutYOLOConfig","title":"DocLayoutYOLOConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for DocLayout-YOLO layout extractor.</p> <p>This is a single-backend model (PyTorch only).</p> Example <pre><code>config = DocLayoutYOLOConfig(device=\"cuda\", confidence=0.3)\nextractor = DocLayoutYOLO(config=config)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.doc_layout_yolo.DocLayoutYOLO","title":"DocLayoutYOLO","text":"<pre><code>DocLayoutYOLO(config: DocLayoutYOLOConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>DocLayout-YOLO layout extractor.</p> <p>A YOLO-based model optimized for document layout detection. Detects: title, text, figure, table, formula, captions, etc.</p> <p>This is a single-backend model (PyTorch only).</p> Example <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\nextractor = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre> <p>Initialize DocLayout-YOLO extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object with device, model_path, etc.</p> <p> TYPE: <code>DocLayoutYOLOConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/doc_layout_yolo.py</code> <pre><code>def __init__(self, config: DocLayoutYOLOConfig):\n    \"\"\"\n    Initialize DocLayout-YOLO extractor.\n\n    Args:\n        config: Configuration object with device, model_path, etc.\n    \"\"\"\n    self.config = config\n    self._model = None\n    self._device = self._resolve_device(config.device)\n    self._model_path = self._resolve_model_path(config.model_path)\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.doc_layout_yolo.DocLayoutYOLO.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> Source code in <code>omnidocs/tasks/layout_extraction/doc_layout_yolo.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n    if self._model is None:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    img_width, img_height = pil_image.size\n\n    # Run inference\n    results = self._model.predict(\n        pil_image,\n        imgsz=self.config.img_size,\n        conf=self.config.confidence,\n        device=self._device,\n    )\n\n    result = results[0]\n\n    # Parse detections\n    layout_boxes = []\n\n    if hasattr(result, \"boxes\") and result.boxes is not None:\n        boxes = result.boxes\n\n        for i in range(len(boxes)):\n            # Get coordinates\n            bbox_coords = boxes.xyxy[i].cpu().numpy().tolist()\n\n            # Get class and confidence\n            class_id = int(boxes.cls[i].item())\n            confidence = float(boxes.conf[i].item())\n\n            # Get original label from class names\n            original_label = DOCLAYOUT_YOLO_CLASS_NAMES.get(class_id, f\"class_{class_id}\")\n\n            # Map to standardized label\n            standard_label = DOCLAYOUT_YOLO_MAPPING.to_standard(original_label)\n\n            layout_boxes.append(\n                LayoutBox(\n                    label=standard_label,\n                    bbox=BoundingBox.from_list(bbox_coords),\n                    confidence=confidence,\n                    class_id=class_id,\n                    original_label=original_label,\n                )\n            )\n\n    # Sort by y-coordinate (top to bottom reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=img_width,\n        image_height=img_height,\n        model_name=\"DocLayout-YOLO\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models","title":"models","text":"<p>Pydantic models for layout extraction outputs.</p> <p>Defines standardized output types and label enums for layout detection.</p> Coordinate Systems <ul> <li>Absolute (default): Coordinates in pixels relative to original image size</li> <li>Normalized (0-1024): Coordinates scaled to 0-1024 range (virtual 1024x1024 canvas)</li> </ul> <p>Use <code>bbox.to_normalized(width, height)</code> or <code>output.get_normalized_bboxes()</code> to convert to normalized coordinates.</p> Example <pre><code>result = extractor.extract(image)  # Returns absolute pixel coordinates\nnormalized = result.get_normalized_bboxes()  # Returns 0-1024 normalized coords\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutLabel","title":"LayoutLabel","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Standardized layout labels used across all layout extractors.</p> <p>These provide a consistent vocabulary regardless of which model is used.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.CustomLabel","title":"CustomLabel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Type-safe custom layout label definition for VLM-based models.</p> <p>VLM models like Qwen3-VL support flexible custom labels beyond the standard LayoutLabel enum. Use this class to define custom labels with validation.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import CustomLabel\n\n# Simple custom label\ncode_block = CustomLabel(name=\"code_block\")\n\n# With metadata\nsidebar = CustomLabel(\n        name=\"sidebar\",\n        description=\"Secondary content panel\",\n        color=\"#9B59B6\",\n    )\n\n# Use with QwenLayoutDetector\nresult = detector.extract(image, custom_labels=[code_block, sidebar])\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LabelMapping","title":"LabelMapping","text":"<pre><code>LabelMapping(mapping: Dict[str, LayoutLabel])\n</code></pre> <p>Base class for model-specific label mappings.</p> <p>Each model maps its native labels to standardized LayoutLabel values.</p> <p>Initialize label mapping.</p> PARAMETER DESCRIPTION <code>mapping</code> <p>Dict mapping model-specific labels to LayoutLabel enum values</p> <p> TYPE: <code>Dict[str, LayoutLabel]</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def __init__(self, mapping: Dict[str, LayoutLabel]):\n    \"\"\"\n    Initialize label mapping.\n\n    Args:\n        mapping: Dict mapping model-specific labels to LayoutLabel enum values\n    \"\"\"\n    self._mapping = {k.lower(): v for k, v in mapping.items()}\n    self._reverse_mapping = {v: k for k, v in mapping.items()}\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LabelMapping.supported_labels","title":"supported_labels  <code>property</code>","text":"<pre><code>supported_labels: List[str]\n</code></pre> <p>Get list of supported model-specific labels.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LabelMapping.standard_labels","title":"standard_labels  <code>property</code>","text":"<pre><code>standard_labels: List[LayoutLabel]\n</code></pre> <p>Get list of standard labels this mapping produces.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LabelMapping.to_standard","title":"to_standard","text":"<pre><code>to_standard(model_label: str) -&gt; LayoutLabel\n</code></pre> <p>Convert model-specific label to standardized LayoutLabel.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_standard(self, model_label: str) -&gt; LayoutLabel:\n    \"\"\"Convert model-specific label to standardized LayoutLabel.\"\"\"\n    return self._mapping.get(model_label.lower(), LayoutLabel.UNKNOWN)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LabelMapping.from_standard","title":"from_standard","text":"<pre><code>from_standard(standard_label: LayoutLabel) -&gt; Optional[str]\n</code></pre> <p>Convert standardized LayoutLabel to model-specific label.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def from_standard(self, standard_label: LayoutLabel) -&gt; Optional[str]:\n    \"\"\"Convert standardized LayoutLabel to model-specific label.\"\"\"\n    return self._reverse_mapping.get(standard_label)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box coordinates in pixel space.</p> <p>Coordinates follow the convention: (x1, y1) is top-left, (x2, y2) is bottom-right.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x, y, width, height) format.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_xywh(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x, y, width, height) format.\"\"\"\n    return (self.x1, self.y1, self.width, self.height)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> <p>Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas. This provides consistent coordinates regardless of original image size.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Example <pre><code>bbox = BoundingBox(x1=100, y1=50, x2=500, y2=300)\nnormalized = bbox.to_normalized(1000, 800)\n# x: 100/1000*1024 = 102.4, y: 50/800*1024 = 64\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas.\n    This provides consistent coordinates regardless of original image size.\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n\n    Example:\n        ```python\n        bbox = BoundingBox(x1=100, y1=50, x2=500, y2=300)\n        normalized = bbox.to_normalized(1000, 800)\n        # x: 100/1000*1024 = 102.4, y: 50/800*1024 = 64\n        ```\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_absolute","title":"to_absolute","text":"<pre><code>to_absolute(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert from normalized (0-1024) to absolute pixel coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Target image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Target image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with absolute pixel coordinates</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_absolute(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert from normalized (0-1024) to absolute pixel coordinates.\n\n    Args:\n        image_width: Target image width in pixels\n        image_height: Target image height in pixels\n\n    Returns:\n        New BoundingBox with absolute pixel coordinates\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / NORMALIZED_SIZE * image_width,\n        y1=self.y1 / NORMALIZED_SIZE * image_height,\n        x2=self.x2 / NORMALIZED_SIZE * image_width,\n        y2=self.y2 / NORMALIZED_SIZE * image_height,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutBox","title":"LayoutBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single detected layout element with label, bounding box, and confidence.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutBox.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"label\": self.label.value,\n        \"bbox\": self.bbox.to_list(),\n        \"confidence\": self.confidence,\n        \"class_id\": self.class_id,\n        \"original_label\": self.original_label,\n    }\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutBox.get_normalized_bbox","title":"get_normalized_bbox","text":"<pre><code>get_normalized_bbox(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Get bounding box in normalized (0-1024) coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox with normalized coordinates</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def get_normalized_bbox(self, image_width: int, image_height: int) -&gt; BoundingBox:\n    \"\"\"\n    Get bounding box in normalized (0-1024) coordinates.\n\n    Args:\n        image_width: Original image width\n        image_height: Original image height\n\n    Returns:\n        BoundingBox with normalized coordinates\n    \"\"\"\n    return self.bbox.to_normalized(image_width, image_height)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput","title":"LayoutOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete layout extraction results for a single image.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.element_count","title":"element_count  <code>property</code>","text":"<pre><code>element_count: int\n</code></pre> <p>Number of detected elements.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.labels_found","title":"labels_found  <code>property</code>","text":"<pre><code>labels_found: List[str]\n</code></pre> <p>Unique labels found in detections.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.filter_by_label","title":"filter_by_label","text":"<pre><code>filter_by_label(label: LayoutLabel) -&gt; List[LayoutBox]\n</code></pre> <p>Filter boxes by label.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def filter_by_label(self, label: LayoutLabel) -&gt; List[LayoutBox]:\n    \"\"\"Filter boxes by label.\"\"\"\n    return [box for box in self.bboxes if box.label == label]\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(\n    min_confidence: float,\n) -&gt; List[LayoutBox]\n</code></pre> <p>Filter boxes by minimum confidence.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def filter_by_confidence(self, min_confidence: float) -&gt; List[LayoutBox]:\n    \"\"\"Filter boxes by minimum confidence.\"\"\"\n    return [box for box in self.bboxes if box.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"bboxes\": [box.to_dict() for box in self.bboxes],\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"element_count\": self.element_count,\n        \"labels_found\": self.labels_found,\n    }\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.sort_by_position","title":"sort_by_position","text":"<pre><code>sort_by_position(\n    top_to_bottom: bool = True,\n) -&gt; LayoutOutput\n</code></pre> <p>Return a new LayoutOutput with boxes sorted by position.</p> PARAMETER DESCRIPTION <code>top_to_bottom</code> <p>If True, sort by y-coordinate (reading order)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def sort_by_position(self, top_to_bottom: bool = True) -&gt; \"LayoutOutput\":\n    \"\"\"\n    Return a new LayoutOutput with boxes sorted by position.\n\n    Args:\n        top_to_bottom: If True, sort by y-coordinate (reading order)\n    \"\"\"\n    sorted_boxes = sorted(self.bboxes, key=lambda b: (b.bbox.y1, b.bbox.x1), reverse=not top_to_bottom)\n    return LayoutOutput(\n        bboxes=sorted_boxes,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        model_name=self.model_name,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.get_normalized_bboxes","title":"get_normalized_bboxes","text":"<pre><code>get_normalized_bboxes() -&gt; List[Dict]\n</code></pre> <p>Get all bounding boxes in normalized (0-1024) coordinates.</p> RETURNS DESCRIPTION <code>List[Dict]</code> <p>List of dicts with normalized bbox coordinates and metadata.</p> Example <pre><code>result = extractor.extract(image)\nnormalized = result.get_normalized_bboxes()\nfor box in normalized:\n        print(f\"{box['label']}: {box['bbox']}\")  # coords in 0-1024 range\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def get_normalized_bboxes(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all bounding boxes in normalized (0-1024) coordinates.\n\n    Returns:\n        List of dicts with normalized bbox coordinates and metadata.\n\n    Example:\n        ```python\n        result = extractor.extract(image)\n        normalized = result.get_normalized_bboxes()\n        for box in normalized:\n                print(f\"{box['label']}: {box['bbox']}\")  # coords in 0-1024 range\n        ```\n    \"\"\"\n    normalized = []\n    for box in self.bboxes:\n        norm_bbox = box.bbox.to_normalized(self.image_width, self.image_height)\n        normalized.append(\n            {\n                \"label\": box.label.value,\n                \"bbox\": norm_bbox.to_list(),\n                \"confidence\": box.confidence,\n                \"class_id\": box.class_id,\n                \"original_label\": box.original_label,\n            }\n        )\n    return normalized\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.visualize","title":"visualize","text":"<pre><code>visualize(\n    image: Image,\n    output_path: Optional[Union[str, Path]] = None,\n    show_labels: bool = True,\n    show_confidence: bool = True,\n    line_width: int = 3,\n    font_size: int = 12,\n) -&gt; Image.Image\n</code></pre> <p>Visualize layout detection results on the image.</p> <p>Draws bounding boxes with labels and confidence scores on the image. Each layout category has a distinct color for easy identification.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to draw on (will be copied, not modified)</p> <p> TYPE: <code>Image</code> </p> <code>output_path</code> <p>Optional path to save the visualization</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> <code>show_labels</code> <p>Whether to show label text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_confidence</code> <p>Whether to show confidence scores</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>line_width</code> <p>Width of bounding box lines</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>font_size</code> <p>Size of label text (note: uses default font)</p> <p> TYPE: <code>int</code> DEFAULT: <code>12</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image with visualizations drawn</p> Example <pre><code>result = extractor.extract(image)\nviz = result.visualize(image, output_path=\"layout_viz.png\")\nviz.show()  # Display in notebook/viewer\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def visualize(\n    self,\n    image: \"Image.Image\",\n    output_path: Optional[Union[str, Path]] = None,\n    show_labels: bool = True,\n    show_confidence: bool = True,\n    line_width: int = 3,\n    font_size: int = 12,\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Visualize layout detection results on the image.\n\n    Draws bounding boxes with labels and confidence scores on the image.\n    Each layout category has a distinct color for easy identification.\n\n    Args:\n        image: PIL Image to draw on (will be copied, not modified)\n        output_path: Optional path to save the visualization\n        show_labels: Whether to show label text\n        show_confidence: Whether to show confidence scores\n        line_width: Width of bounding box lines\n        font_size: Size of label text (note: uses default font)\n\n    Returns:\n        PIL Image with visualizations drawn\n\n    Example:\n        ```python\n        result = extractor.extract(image)\n        viz = result.visualize(image, output_path=\"layout_viz.png\")\n        viz.show()  # Display in notebook/viewer\n        ```\n    \"\"\"\n    from PIL import ImageDraw\n\n    # Copy image to avoid modifying original\n    viz_image = image.copy().convert(\"RGB\")\n    draw = ImageDraw.Draw(viz_image)\n\n    for box in self.bboxes:\n        # Get color for this label\n        color = LABEL_COLORS.get(box.label, \"#95A5A6\")\n\n        # Draw bounding box\n        coords = box.bbox.to_xyxy()\n        draw.rectangle(coords, outline=color, width=line_width)\n\n        # Build label text\n        if show_labels or show_confidence:\n            label_parts = []\n            if show_labels:\n                label_parts.append(box.label.value)\n            if show_confidence:\n                label_parts.append(f\"{box.confidence:.2f}\")\n            label_text = \" \".join(label_parts)\n\n            # Draw label background\n            text_bbox = draw.textbbox((coords[0], coords[1] - 20), label_text)\n            draw.rectangle(text_bbox, fill=color)\n\n            # Draw label text\n            draw.text(\n                (coords[0], coords[1] - 20),\n                label_text,\n                fill=\"white\",\n            )\n\n    # Save if path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        viz_image.save(output_path)\n\n    return viz_image\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; LayoutOutput\n</code></pre> <p>Load a LayoutOutput instance from a JSON file.</p> <p>Reads a JSON file and deserializes its contents into a LayoutOutput object. Uses Pydantic's model_validate_json for proper handling of nested objects.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to JSON file containing serialized LayoutOutput data.       Can be string or pathlib.Path object.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>Deserialized layout output instance from file.</p> <p> TYPE: <code>LayoutOutput</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>UnicodeDecodeError</code> <p>If file cannot be decoded as UTF-8.</p> <code>ValueError</code> <p>If file contents are not valid JSON.</p> <code>ValidationError</code> <p>If JSON data doesn't match LayoutOutput schema.</p> Example <p><pre><code>output = LayoutOutput.load_json('layout_results.json')\nprint(f\"Found {output.element_count} elements\")\n</code></pre> Found 5 elements</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"LayoutOutput\":\n    \"\"\"\n    Load a LayoutOutput instance from a JSON file.\n\n    Reads a JSON file and deserializes its contents into a LayoutOutput object.\n    Uses Pydantic's model_validate_json for proper handling of nested objects.\n\n    Args:\n        file_path: Path to JSON file containing serialized LayoutOutput data.\n                  Can be string or pathlib.Path object.\n\n    Returns:\n        LayoutOutput: Deserialized layout output instance from file.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n        UnicodeDecodeError: If file cannot be decoded as UTF-8.\n        ValueError: If file contents are not valid JSON.\n        ValidationError: If JSON data doesn't match LayoutOutput schema.\n\n    Example:\n        ```python\n        output = LayoutOutput.load_json('layout_results.json')\n        print(f\"Found {output.element_count} elements\")\n        ```\n        Found 5 elements\n    \"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save LayoutOutput instance to a JSON file.</p> <p>Serializes the LayoutOutput object to JSON and writes it to a file. Automatically creates parent directories if they don't exist. Uses UTF-8 encoding for compatibility and proper handling of special characters.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path where JSON file should be saved. Can be string or       pathlib.Path object. Parent directories will be created       if they don't exist.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> RAISES DESCRIPTION <code>OSError</code> <p>If file cannot be written due to permission or disk errors.</p> <code>TypeError</code> <p>If file_path is not a string or Path object.</p> Example <pre><code>output = LayoutOutput(bboxes=[], image_width=800, image_height=600)\noutput.save_json('results/layout_output.json')\n# File is created at results/layout_output.json\n# Parent 'results' directory is created if it didn't exist\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"\n    Save LayoutOutput instance to a JSON file.\n\n    Serializes the LayoutOutput object to JSON and writes it to a file.\n    Automatically creates parent directories if they don't exist. Uses UTF-8\n    encoding for compatibility and proper handling of special characters.\n\n    Args:\n        file_path: Path where JSON file should be saved. Can be string or\n                  pathlib.Path object. Parent directories will be created\n                  if they don't exist.\n\n    Returns:\n        None\n\n    Raises:\n        OSError: If file cannot be written due to permission or disk errors.\n        TypeError: If file_path is not a string or Path object.\n\n    Example:\n        ```python\n        output = LayoutOutput(bboxes=[], image_width=800, image_height=600)\n        output.save_json('results/layout_output.json')\n        # File is created at results/layout_output.json\n        # Parent 'results' directory is created if it didn't exist\n        ```\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen","title":"qwen","text":"<p>Qwen3-VL backend configurations and detector for layout detection.</p> Available backends <ul> <li>QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend</li> <li>QwenLayoutVLLMConfig: VLLM high-throughput backend</li> <li>QwenLayoutMLXConfig: MLX backend for Apple Silicon</li> <li>QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</li> </ul> Example <pre><code>from omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\nconfig = QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutAPIConfig","title":"QwenLayoutAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen layout detection.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenLayoutAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutDetector","title":"QwenLayoutDetector","text":"<pre><code>QwenLayoutDetector(backend: QwenLayoutBackendConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>Qwen3-VL Vision-Language Model layout detector.</p> <p>A flexible VLM-based layout detector that supports custom labels. Unlike fixed-label models (DocLayoutYOLO, RT-DETR), Qwen can detect any document elements specified at runtime.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\n# Initialize with PyTorch backend\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Basic extraction with default labels\nresult = detector.extract(image)\n\n# With custom labels (strings)\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n\n# With typed custom labels\nlabels = [\n        CustomLabel(name=\"code_block\", color=\"#E74C3C\"),\n        CustomLabel(name=\"sidebar\", description=\"Side panel content\"),\n    ]\nresult = detector.extract(image, custom_labels=labels)\n</code></pre> <p>Initialize Qwen layout detector.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend - QwenLayoutVLLMConfig: VLLM high-throughput backend - QwenLayoutMLXConfig: MLX backend for Apple Silicon - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenLayoutBackendConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def __init__(self, backend: QwenLayoutBackendConfig):\n    \"\"\"\n    Initialize Qwen layout detector.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenLayoutVLLMConfig: VLLM high-throughput backend\n            - QwenLayoutMLXConfig: MLX backend for Apple Silicon\n            - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutDetector.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    custom_labels: Optional[\n        List[Union[str, CustomLabel]]\n    ] = None,\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout detection on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>custom_labels</code> <p>Optional custom labels to detect. Can be: - None: Use default labels (title, text, table, figure, etc.) - List[str]: Simple label names [\"code_block\", \"sidebar\"] - List[CustomLabel]: Typed labels with metadata</p> <p> TYPE: <code>Optional[List[Union[str, CustomLabel]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    custom_labels: Optional[List[Union[str, CustomLabel]]] = None,\n) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout detection on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        custom_labels: Optional custom labels to detect. Can be:\n            - None: Use default labels (title, text, table, figure, etc.)\n            - List[str]: Simple label names [\"code_block\", \"sidebar\"]\n            - List[CustomLabel]: Typed labels with metadata\n\n    Returns:\n        LayoutOutput with detected layout boxes\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Normalize labels\n    label_names = self._normalize_labels(custom_labels)\n\n    # Build prompt\n    prompt = self._build_detection_prompt(label_names)\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenLayoutPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenLayoutVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenLayoutMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenLayoutAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse detections\n    detections = self._parse_json_output(raw_output)\n\n    # Convert to LayoutOutput\n    layout_boxes = self._build_layout_boxes(detections, width, height)\n\n    # Sort by position (reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutMLXConfig","title":"QwenLayoutMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen layout detection.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenLayoutMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutPyTorchConfig","title":"QwenLayoutPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen layout detection.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutVLLMConfig","title":"QwenLayoutVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen layout detection.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.api","title":"api","text":"<p>API backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.api.QwenLayoutAPIConfig","title":"QwenLayoutAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen layout detection.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenLayoutAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.detector","title":"detector","text":"<p>Qwen3-VL layout detector.</p> <p>A Vision-Language Model for flexible layout detection with custom label support. Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = detector.extract(image)\n\n# With custom labels\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.detector.QwenLayoutDetector","title":"QwenLayoutDetector","text":"<pre><code>QwenLayoutDetector(backend: QwenLayoutBackendConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>Qwen3-VL Vision-Language Model layout detector.</p> <p>A flexible VLM-based layout detector that supports custom labels. Unlike fixed-label models (DocLayoutYOLO, RT-DETR), Qwen can detect any document elements specified at runtime.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\n# Initialize with PyTorch backend\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Basic extraction with default labels\nresult = detector.extract(image)\n\n# With custom labels (strings)\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n\n# With typed custom labels\nlabels = [\n        CustomLabel(name=\"code_block\", color=\"#E74C3C\"),\n        CustomLabel(name=\"sidebar\", description=\"Side panel content\"),\n    ]\nresult = detector.extract(image, custom_labels=labels)\n</code></pre> <p>Initialize Qwen layout detector.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend - QwenLayoutVLLMConfig: VLLM high-throughput backend - QwenLayoutMLXConfig: MLX backend for Apple Silicon - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenLayoutBackendConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def __init__(self, backend: QwenLayoutBackendConfig):\n    \"\"\"\n    Initialize Qwen layout detector.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenLayoutVLLMConfig: VLLM high-throughput backend\n            - QwenLayoutMLXConfig: MLX backend for Apple Silicon\n            - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.detector.QwenLayoutDetector.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    custom_labels: Optional[\n        List[Union[str, CustomLabel]]\n    ] = None,\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout detection on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>custom_labels</code> <p>Optional custom labels to detect. Can be: - None: Use default labels (title, text, table, figure, etc.) - List[str]: Simple label names [\"code_block\", \"sidebar\"] - List[CustomLabel]: Typed labels with metadata</p> <p> TYPE: <code>Optional[List[Union[str, CustomLabel]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    custom_labels: Optional[List[Union[str, CustomLabel]]] = None,\n) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout detection on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        custom_labels: Optional custom labels to detect. Can be:\n            - None: Use default labels (title, text, table, figure, etc.)\n            - List[str]: Simple label names [\"code_block\", \"sidebar\"]\n            - List[CustomLabel]: Typed labels with metadata\n\n    Returns:\n        LayoutOutput with detected layout boxes\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Normalize labels\n    label_names = self._normalize_labels(custom_labels)\n\n    # Build prompt\n    prompt = self._build_detection_prompt(label_names)\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenLayoutPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenLayoutVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenLayoutMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenLayoutAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse detections\n    detections = self._parse_json_output(raw_output)\n\n    # Convert to LayoutOutput\n    layout_boxes = self._build_layout_boxes(detections, width, height)\n\n    # Sort by position (reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.mlx","title":"mlx","text":"<p>MLX backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.mlx.QwenLayoutMLXConfig","title":"QwenLayoutMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen layout detection.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenLayoutMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.pytorch","title":"pytorch","text":"<p>PyTorch/HuggingFace backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.pytorch.QwenLayoutPyTorchConfig","title":"QwenLayoutPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen layout detection.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.vllm","title":"vllm","text":"<p>VLLM backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.vllm.QwenLayoutVLLMConfig","title":"QwenLayoutVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen layout detection.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.rtdetr","title":"rtdetr","text":"<p>RT-DETR layout extractor.</p> <p>A transformer-based real-time detection model for document layout detection. Uses HuggingFace Transformers implementation.</p> <p>Model: HuggingPanda/docling-layout</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.rtdetr.RTDETRConfig","title":"RTDETRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for RT-DETR layout extractor.</p> <p>This is a single-backend model (PyTorch/Transformers only).</p> Example <pre><code>config = RTDETRConfig(device=\"cuda\", confidence=0.4)\nextractor = RTDETRLayoutExtractor(config=config)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.rtdetr.RTDETRLayoutExtractor","title":"RTDETRLayoutExtractor","text":"<pre><code>RTDETRLayoutExtractor(config: RTDETRConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>RT-DETR layout extractor using HuggingFace Transformers.</p> <p>A transformer-based real-time detection model for document layout. Detects: title, text, table, figure, list, formula, captions, headers, footers.</p> <p>This is a single-backend model (PyTorch/Transformers only).</p> Example <pre><code>from omnidocs.tasks.layout_extraction import RTDETRLayoutExtractor, RTDETRConfig\n\nextractor = RTDETRLayoutExtractor(config=RTDETRConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre> <p>Initialize RT-DETR layout extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object with device, model settings, etc.</p> <p> TYPE: <code>RTDETRConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/rtdetr.py</code> <pre><code>def __init__(self, config: RTDETRConfig):\n    \"\"\"\n    Initialize RT-DETR layout extractor.\n\n    Args:\n        config: Configuration object with device, model settings, etc.\n    \"\"\"\n    self.config = config\n    self._model = None\n    self._processor = None\n    self._device = self._resolve_device(config.device)\n    self._model_path = self._resolve_model_path(config.model_path)\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.rtdetr.RTDETRLayoutExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> Source code in <code>omnidocs/tasks/layout_extraction/rtdetr.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n    import torch\n\n    if self._model is None or self._processor is None:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    img_width, img_height = pil_image.size\n\n    # Preprocess\n    inputs = self._processor(\n        images=pil_image,\n        return_tensors=\"pt\",\n        size={\"height\": self.config.image_size, \"width\": self.config.image_size},\n    )\n\n    # Move to device\n    inputs = {k: v.to(self._device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n\n    # Run inference\n    with torch.no_grad():\n        outputs = self._model(**inputs)\n\n    # Post-process results\n    target_sizes = torch.tensor([[img_height, img_width]])\n    results = self._processor.post_process_object_detection(\n        outputs,\n        target_sizes=target_sizes,\n        threshold=self.config.confidence,\n    )[0]\n\n    # Parse detections\n    layout_boxes = []\n\n    for score, label_id, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n        confidence = float(score.item())\n        class_id = int(label_id.item())\n\n        # Get original label from model config\n        # Note: The model outputs 0-indexed class IDs, but id2label has background at index 0,\n        # so we add 1 to map correctly (e.g., model output 8 -&gt; id2label[9] = \"Table\")\n        original_label = self._model.config.id2label.get(class_id + 1, f\"class_{class_id}\")\n\n        # Map to standardized label\n        standard_label = RTDETR_MAPPING.to_standard(original_label)\n\n        # Box coordinates\n        box_coords = box.cpu().tolist()\n\n        layout_boxes.append(\n            LayoutBox(\n                label=standard_label,\n                bbox=BoundingBox.from_list(box_coords),\n                confidence=confidence,\n                class_id=class_id,\n                original_label=original_label,\n            )\n        )\n\n    # Sort by y-coordinate (top to bottom reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=img_width,\n        image_height=img_height,\n        model_name=\"RT-DETR (docling-layout)\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/rtdetr/","title":"Rtdetr","text":"<p>RT-DETR layout extractor.</p> <p>A transformer-based real-time detection model for document layout detection. Uses HuggingFace Transformers implementation.</p> <p>Model: HuggingPanda/docling-layout</p>"},{"location":"reference/tasks/layout_extraction/rtdetr/#omnidocs.tasks.layout_extraction.rtdetr.RTDETRConfig","title":"RTDETRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for RT-DETR layout extractor.</p> <p>This is a single-backend model (PyTorch/Transformers only).</p> Example <pre><code>config = RTDETRConfig(device=\"cuda\", confidence=0.4)\nextractor = RTDETRLayoutExtractor(config=config)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/rtdetr/#omnidocs.tasks.layout_extraction.rtdetr.RTDETRLayoutExtractor","title":"RTDETRLayoutExtractor","text":"<pre><code>RTDETRLayoutExtractor(config: RTDETRConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>RT-DETR layout extractor using HuggingFace Transformers.</p> <p>A transformer-based real-time detection model for document layout. Detects: title, text, table, figure, list, formula, captions, headers, footers.</p> <p>This is a single-backend model (PyTorch/Transformers only).</p> Example <pre><code>from omnidocs.tasks.layout_extraction import RTDETRLayoutExtractor, RTDETRConfig\n\nextractor = RTDETRLayoutExtractor(config=RTDETRConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre> <p>Initialize RT-DETR layout extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object with device, model settings, etc.</p> <p> TYPE: <code>RTDETRConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/rtdetr.py</code> <pre><code>def __init__(self, config: RTDETRConfig):\n    \"\"\"\n    Initialize RT-DETR layout extractor.\n\n    Args:\n        config: Configuration object with device, model settings, etc.\n    \"\"\"\n    self.config = config\n    self._model = None\n    self._processor = None\n    self._device = self._resolve_device(config.device)\n    self._model_path = self._resolve_model_path(config.model_path)\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/rtdetr/#omnidocs.tasks.layout_extraction.rtdetr.RTDETRLayoutExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> Source code in <code>omnidocs/tasks/layout_extraction/rtdetr.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n    import torch\n\n    if self._model is None or self._processor is None:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    img_width, img_height = pil_image.size\n\n    # Preprocess\n    inputs = self._processor(\n        images=pil_image,\n        return_tensors=\"pt\",\n        size={\"height\": self.config.image_size, \"width\": self.config.image_size},\n    )\n\n    # Move to device\n    inputs = {k: v.to(self._device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n\n    # Run inference\n    with torch.no_grad():\n        outputs = self._model(**inputs)\n\n    # Post-process results\n    target_sizes = torch.tensor([[img_height, img_width]])\n    results = self._processor.post_process_object_detection(\n        outputs,\n        target_sizes=target_sizes,\n        threshold=self.config.confidence,\n    )[0]\n\n    # Parse detections\n    layout_boxes = []\n\n    for score, label_id, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n        confidence = float(score.item())\n        class_id = int(label_id.item())\n\n        # Get original label from model config\n        # Note: The model outputs 0-indexed class IDs, but id2label has background at index 0,\n        # so we add 1 to map correctly (e.g., model output 8 -&gt; id2label[9] = \"Table\")\n        original_label = self._model.config.id2label.get(class_id + 1, f\"class_{class_id}\")\n\n        # Map to standardized label\n        standard_label = RTDETR_MAPPING.to_standard(original_label)\n\n        # Box coordinates\n        box_coords = box.cpu().tolist()\n\n        layout_boxes.append(\n            LayoutBox(\n                label=standard_label,\n                bbox=BoundingBox.from_list(box_coords),\n                confidence=confidence,\n                class_id=class_id,\n                original_label=original_label,\n            )\n        )\n\n    # Sort by y-coordinate (top to bottom reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=img_width,\n        image_height=img_height,\n        model_name=\"RT-DETR (docling-layout)\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/api/","title":"API","text":"<p>API backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/qwen/api/#omnidocs.tasks.layout_extraction.qwen.api.QwenLayoutAPIConfig","title":"QwenLayoutAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen layout detection.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenLayoutAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/detector/","title":"Detector","text":"<p>Qwen3-VL layout detector.</p> <p>A Vision-Language Model for flexible layout detection with custom label support. Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = detector.extract(image)\n\n# With custom labels\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/detector/#omnidocs.tasks.layout_extraction.qwen.detector.QwenLayoutDetector","title":"QwenLayoutDetector","text":"<pre><code>QwenLayoutDetector(backend: QwenLayoutBackendConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>Qwen3-VL Vision-Language Model layout detector.</p> <p>A flexible VLM-based layout detector that supports custom labels. Unlike fixed-label models (DocLayoutYOLO, RT-DETR), Qwen can detect any document elements specified at runtime.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\n# Initialize with PyTorch backend\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Basic extraction with default labels\nresult = detector.extract(image)\n\n# With custom labels (strings)\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n\n# With typed custom labels\nlabels = [\n        CustomLabel(name=\"code_block\", color=\"#E74C3C\"),\n        CustomLabel(name=\"sidebar\", description=\"Side panel content\"),\n    ]\nresult = detector.extract(image, custom_labels=labels)\n</code></pre> <p>Initialize Qwen layout detector.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend - QwenLayoutVLLMConfig: VLLM high-throughput backend - QwenLayoutMLXConfig: MLX backend for Apple Silicon - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenLayoutBackendConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def __init__(self, backend: QwenLayoutBackendConfig):\n    \"\"\"\n    Initialize Qwen layout detector.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenLayoutVLLMConfig: VLLM high-throughput backend\n            - QwenLayoutMLXConfig: MLX backend for Apple Silicon\n            - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/detector/#omnidocs.tasks.layout_extraction.qwen.detector.QwenLayoutDetector.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    custom_labels: Optional[\n        List[Union[str, CustomLabel]]\n    ] = None,\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout detection on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>custom_labels</code> <p>Optional custom labels to detect. Can be: - None: Use default labels (title, text, table, figure, etc.) - List[str]: Simple label names [\"code_block\", \"sidebar\"] - List[CustomLabel]: Typed labels with metadata</p> <p> TYPE: <code>Optional[List[Union[str, CustomLabel]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    custom_labels: Optional[List[Union[str, CustomLabel]]] = None,\n) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout detection on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        custom_labels: Optional custom labels to detect. Can be:\n            - None: Use default labels (title, text, table, figure, etc.)\n            - List[str]: Simple label names [\"code_block\", \"sidebar\"]\n            - List[CustomLabel]: Typed labels with metadata\n\n    Returns:\n        LayoutOutput with detected layout boxes\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Normalize labels\n    label_names = self._normalize_labels(custom_labels)\n\n    # Build prompt\n    prompt = self._build_detection_prompt(label_names)\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenLayoutPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenLayoutVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenLayoutMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenLayoutAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse detections\n    detections = self._parse_json_output(raw_output)\n\n    # Convert to LayoutOutput\n    layout_boxes = self._build_layout_boxes(detections, width, height)\n\n    # Sort by position (reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/mlx/","title":"MLX","text":"<p>MLX backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/qwen/mlx/#omnidocs.tasks.layout_extraction.qwen.mlx.QwenLayoutMLXConfig","title":"QwenLayoutMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen layout detection.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenLayoutMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/","title":"Overview","text":"<p>Qwen3-VL backend configurations and detector for layout detection.</p> Available backends <ul> <li>QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend</li> <li>QwenLayoutVLLMConfig: VLLM high-throughput backend</li> <li>QwenLayoutMLXConfig: MLX backend for Apple Silicon</li> <li>QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</li> </ul> Example <pre><code>from omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\nconfig = QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutAPIConfig","title":"QwenLayoutAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen layout detection.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenLayoutAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutDetector","title":"QwenLayoutDetector","text":"<pre><code>QwenLayoutDetector(backend: QwenLayoutBackendConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>Qwen3-VL Vision-Language Model layout detector.</p> <p>A flexible VLM-based layout detector that supports custom labels. Unlike fixed-label models (DocLayoutYOLO, RT-DETR), Qwen can detect any document elements specified at runtime.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\n# Initialize with PyTorch backend\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Basic extraction with default labels\nresult = detector.extract(image)\n\n# With custom labels (strings)\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n\n# With typed custom labels\nlabels = [\n        CustomLabel(name=\"code_block\", color=\"#E74C3C\"),\n        CustomLabel(name=\"sidebar\", description=\"Side panel content\"),\n    ]\nresult = detector.extract(image, custom_labels=labels)\n</code></pre> <p>Initialize Qwen layout detector.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend - QwenLayoutVLLMConfig: VLLM high-throughput backend - QwenLayoutMLXConfig: MLX backend for Apple Silicon - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenLayoutBackendConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def __init__(self, backend: QwenLayoutBackendConfig):\n    \"\"\"\n    Initialize Qwen layout detector.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenLayoutVLLMConfig: VLLM high-throughput backend\n            - QwenLayoutMLXConfig: MLX backend for Apple Silicon\n            - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutDetector.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    custom_labels: Optional[\n        List[Union[str, CustomLabel]]\n    ] = None,\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout detection on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>custom_labels</code> <p>Optional custom labels to detect. Can be: - None: Use default labels (title, text, table, figure, etc.) - List[str]: Simple label names [\"code_block\", \"sidebar\"] - List[CustomLabel]: Typed labels with metadata</p> <p> TYPE: <code>Optional[List[Union[str, CustomLabel]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    custom_labels: Optional[List[Union[str, CustomLabel]]] = None,\n) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout detection on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        custom_labels: Optional custom labels to detect. Can be:\n            - None: Use default labels (title, text, table, figure, etc.)\n            - List[str]: Simple label names [\"code_block\", \"sidebar\"]\n            - List[CustomLabel]: Typed labels with metadata\n\n    Returns:\n        LayoutOutput with detected layout boxes\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Normalize labels\n    label_names = self._normalize_labels(custom_labels)\n\n    # Build prompt\n    prompt = self._build_detection_prompt(label_names)\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenLayoutPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenLayoutVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenLayoutMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenLayoutAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse detections\n    detections = self._parse_json_output(raw_output)\n\n    # Convert to LayoutOutput\n    layout_boxes = self._build_layout_boxes(detections, width, height)\n\n    # Sort by position (reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutMLXConfig","title":"QwenLayoutMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen layout detection.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenLayoutMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutPyTorchConfig","title":"QwenLayoutPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen layout detection.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutVLLMConfig","title":"QwenLayoutVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen layout detection.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.api","title":"api","text":"<p>API backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.api.QwenLayoutAPIConfig","title":"QwenLayoutAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen layout detection.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenLayoutAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.detector","title":"detector","text":"<p>Qwen3-VL layout detector.</p> <p>A Vision-Language Model for flexible layout detection with custom label support. Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = detector.extract(image)\n\n# With custom labels\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.detector.QwenLayoutDetector","title":"QwenLayoutDetector","text":"<pre><code>QwenLayoutDetector(backend: QwenLayoutBackendConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>Qwen3-VL Vision-Language Model layout detector.</p> <p>A flexible VLM-based layout detector that supports custom labels. Unlike fixed-label models (DocLayoutYOLO, RT-DETR), Qwen can detect any document elements specified at runtime.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\n# Initialize with PyTorch backend\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Basic extraction with default labels\nresult = detector.extract(image)\n\n# With custom labels (strings)\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n\n# With typed custom labels\nlabels = [\n        CustomLabel(name=\"code_block\", color=\"#E74C3C\"),\n        CustomLabel(name=\"sidebar\", description=\"Side panel content\"),\n    ]\nresult = detector.extract(image, custom_labels=labels)\n</code></pre> <p>Initialize Qwen layout detector.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend - QwenLayoutVLLMConfig: VLLM high-throughput backend - QwenLayoutMLXConfig: MLX backend for Apple Silicon - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenLayoutBackendConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def __init__(self, backend: QwenLayoutBackendConfig):\n    \"\"\"\n    Initialize Qwen layout detector.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenLayoutVLLMConfig: VLLM high-throughput backend\n            - QwenLayoutMLXConfig: MLX backend for Apple Silicon\n            - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.detector.QwenLayoutDetector.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    custom_labels: Optional[\n        List[Union[str, CustomLabel]]\n    ] = None,\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout detection on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>custom_labels</code> <p>Optional custom labels to detect. Can be: - None: Use default labels (title, text, table, figure, etc.) - List[str]: Simple label names [\"code_block\", \"sidebar\"] - List[CustomLabel]: Typed labels with metadata</p> <p> TYPE: <code>Optional[List[Union[str, CustomLabel]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    custom_labels: Optional[List[Union[str, CustomLabel]]] = None,\n) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout detection on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        custom_labels: Optional custom labels to detect. Can be:\n            - None: Use default labels (title, text, table, figure, etc.)\n            - List[str]: Simple label names [\"code_block\", \"sidebar\"]\n            - List[CustomLabel]: Typed labels with metadata\n\n    Returns:\n        LayoutOutput with detected layout boxes\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Normalize labels\n    label_names = self._normalize_labels(custom_labels)\n\n    # Build prompt\n    prompt = self._build_detection_prompt(label_names)\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenLayoutPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenLayoutVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenLayoutMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenLayoutAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse detections\n    detections = self._parse_json_output(raw_output)\n\n    # Convert to LayoutOutput\n    layout_boxes = self._build_layout_boxes(detections, width, height)\n\n    # Sort by position (reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.mlx","title":"mlx","text":"<p>MLX backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.mlx.QwenLayoutMLXConfig","title":"QwenLayoutMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen layout detection.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenLayoutMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.pytorch","title":"pytorch","text":"<p>PyTorch/HuggingFace backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.pytorch.QwenLayoutPyTorchConfig","title":"QwenLayoutPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen layout detection.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.vllm","title":"vllm","text":"<p>VLLM backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.vllm.QwenLayoutVLLMConfig","title":"QwenLayoutVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen layout detection.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/pytorch/","title":"PyTorch","text":"<p>PyTorch/HuggingFace backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/qwen/pytorch/#omnidocs.tasks.layout_extraction.qwen.pytorch.QwenLayoutPyTorchConfig","title":"QwenLayoutPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen layout detection.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/vllm/","title":"VLLM","text":"<p>VLLM backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/qwen/vllm/#omnidocs.tasks.layout_extraction.qwen.vllm.QwenLayoutVLLMConfig","title":"QwenLayoutVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen layout detection.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/base/","title":"Base","text":"<p>Base class for OCR extractors.</p> <p>Defines the abstract interface that all OCR extractors must implement.</p>"},{"location":"reference/tasks/ocr_extraction/base/#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor","title":"BaseOCRExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for OCR extractors.</p> <p>All OCR extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyOCRExtractor(BaseOCRExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Initialize OCR engine\n            pass\n\n        def extract(self, image):\n            # Run OCR extraction\n            return OCROutput(...)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/base/#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput containing detected text blocks with bounding boxes</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format is not supported</p> <code>RuntimeError</code> <p>If OCR engine is not initialized or extraction fails</p> Source code in <code>omnidocs/tasks/ocr_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR extraction on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n\n    Returns:\n        OCROutput containing detected text blocks with bounding boxes\n\n    Raises:\n        ValueError: If image format is not supported\n        RuntimeError: If OCR engine is not initialized or extraction fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/easyocr/","title":"EasyOCR","text":"<p>EasyOCR extractor.</p> <p>EasyOCR is a PyTorch-based OCR engine with excellent multi-language support. - GPU accelerated (optional) - Supports 80+ languages - Good for scene text and printed documents</p> Python Package <p>pip install easyocr</p> Model Download Location <p>By default, EasyOCR downloads models to ~/.EasyOCR/ Can be overridden with model_storage_directory parameter</p>"},{"location":"reference/tasks/ocr_extraction/easyocr/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCRConfig","title":"EasyOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for EasyOCR extractor.</p> <p>This is a single-backend model (PyTorch - CPU/GPU).</p> Example <pre><code>config = EasyOCRConfig(languages=[\"en\", \"ch_sim\"], gpu=True)\nocr = EasyOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/easyocr/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCR","title":"EasyOCR","text":"<pre><code>EasyOCR(config: EasyOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>EasyOCR text extractor.</p> <p>Single-backend model (PyTorch - CPU/GPU).</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\nocr = EasyOCR(config=EasyOCRConfig(languages=[\"en\"], gpu=True))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize EasyOCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>EasyOCRConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If easyocr is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def __init__(self, config: EasyOCRConfig):\n    \"\"\"\n    Initialize EasyOCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        ImportError: If easyocr is not installed\n    \"\"\"\n    self.config = config\n    self._reader = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/easyocr/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    detail: int = 1,\n    paragraph: bool = False,\n    min_size: int = 10,\n    text_threshold: float = 0.7,\n    low_text: float = 0.4,\n    link_threshold: float = 0.4,\n    canvas_size: int = 2560,\n    mag_ratio: float = 1.0,\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>detail</code> <p>0 = simple output, 1 = detailed with boxes</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>paragraph</code> <p>Combine results into paragraphs</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>min_size</code> <p>Minimum text box size</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>text_threshold</code> <p>Text confidence threshold</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.7</code> </p> <code>low_text</code> <p>Low text bound</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>link_threshold</code> <p>Link threshold for text joining</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>canvas_size</code> <p>Max image dimension for processing</p> <p> TYPE: <code>int</code> DEFAULT: <code>2560</code> </p> <code>mag_ratio</code> <p>Magnification ratio</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks</p> RAISES DESCRIPTION <code>ValueError</code> <p>If detail is not 0 or 1</p> <code>RuntimeError</code> <p>If EasyOCR is not initialized</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    detail: int = 1,\n    paragraph: bool = False,\n    min_size: int = 10,\n    text_threshold: float = 0.7,\n    low_text: float = 0.4,\n    link_threshold: float = 0.4,\n    canvas_size: int = 2560,\n    mag_ratio: float = 1.0,\n) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n        detail: 0 = simple output, 1 = detailed with boxes\n        paragraph: Combine results into paragraphs\n        min_size: Minimum text box size\n        text_threshold: Text confidence threshold\n        low_text: Low text bound\n        link_threshold: Link threshold for text joining\n        canvas_size: Max image dimension for processing\n        mag_ratio: Magnification ratio\n\n    Returns:\n        OCROutput with detected text blocks\n\n    Raises:\n        ValueError: If detail is not 0 or 1\n        RuntimeError: If EasyOCR is not initialized\n    \"\"\"\n    if self._reader is None:\n        raise RuntimeError(\"EasyOCR not initialized. Call _load_model() first.\")\n\n    # Validate detail parameter\n    if detail not in (0, 1):\n        raise ValueError(f\"detail must be 0 or 1, got {detail}\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Convert to numpy array for EasyOCR\n    image_array = np.array(pil_image)\n\n    # Run EasyOCR\n    results = self._reader.readtext(\n        image_array,\n        detail=detail,\n        paragraph=paragraph,\n        min_size=min_size,\n        text_threshold=text_threshold,\n        low_text=low_text,\n        link_threshold=link_threshold,\n        canvas_size=canvas_size,\n        mag_ratio=mag_ratio,\n    )\n\n    # Parse results\n    text_blocks = []\n    full_text_parts = []\n\n    for result in results:\n        if detail == 0:\n            # Simple output: just text\n            text = result\n            confidence = 1.0\n            bbox = BoundingBox(x1=0, y1=0, x2=0, y2=0)\n            polygon = None\n        else:\n            # Detailed output: [polygon, text, confidence]\n            polygon_points, text, confidence = result\n\n            # EasyOCR returns 4 corner points: [[x1,y1], [x2,y1], [x2,y2], [x1,y2]]\n            # Convert to list of lists for storage\n            polygon = [list(p) for p in polygon_points]\n\n            # Convert to axis-aligned bounding box\n            bbox = BoundingBox.from_polygon(polygon)\n\n        if not text.strip():\n            continue\n\n        text_blocks.append(\n            TextBlock(\n                text=text,\n                bbox=bbox,\n                confidence=float(confidence),\n                granularity=(OCRGranularity.LINE if paragraph else OCRGranularity.WORD),\n                polygon=polygon,\n                language=\"+\".join(self.config.languages),\n            )\n        )\n\n        full_text_parts.append(text)\n\n    # Sort by position\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\" \".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/easyocr/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCR.extract_batch","title":"extract_batch","text":"<pre><code>extract_batch(\n    images: List[Union[Image, ndarray, str, Path]], **kwargs\n) -&gt; List[OCROutput]\n</code></pre> <p>Run OCR on multiple images.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of input images</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>**kwargs</code> <p>Arguments passed to extract()</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>List[OCROutput]</code> <p>List of OCROutput objects</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def extract_batch(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    **kwargs,\n) -&gt; List[OCROutput]:\n    \"\"\"\n    Run OCR on multiple images.\n\n    Args:\n        images: List of input images\n        **kwargs: Arguments passed to extract()\n\n    Returns:\n        List of OCROutput objects\n    \"\"\"\n    results = []\n    for img in images:\n        results.append(self.extract(img, **kwargs))\n    return results\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/","title":"Models","text":"<p>Pydantic models for OCR extraction outputs.</p> <p>Defines standardized output types for OCR detection including text blocks with bounding boxes, confidence scores, and granularity levels.</p> <p>Key difference from Text Extraction: - OCR returns text WITH bounding boxes (word/line/character level) - Text Extraction returns formatted text (MD/HTML) WITHOUT bboxes</p> Coordinate Systems <ul> <li>Absolute (default): Coordinates in pixels relative to original image size</li> <li>Normalized (0-1024): Coordinates scaled to 0-1024 range (virtual 1024x1024 canvas)</li> </ul> <p>Use <code>bbox.to_normalized(width, height)</code> or <code>output.get_normalized_blocks()</code> to convert to normalized coordinates.</p> Example <pre><code>result = ocr.extract(image)  # Returns absolute pixel coordinates\nnormalized = result.get_normalized_blocks()  # Returns 0-1024 normalized coords\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCRGranularity","title":"OCRGranularity","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>OCR detection granularity levels.</p> <p>Different OCR engines return results at different granularity levels. This enum standardizes the options across all extractors.</p>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box coordinates in pixel space.</p> <p>Coordinates follow the convention: (x1, y1) is top-left, (x2, y2) is bottom-right. For rotated text, use the polygon field in TextBlock instead.</p> Example <pre><code>bbox = BoundingBox(x1=100, y1=50, x2=300, y2=80)\nprint(bbox.width, bbox.height)  # 200, 30\nprint(bbox.center)  # (200.0, 65.0)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x, y, width, height) format.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_xywh(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x, y, width, height) format.\"\"\"\n    return (self.x1, self.y1, self.width, self.height)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.from_polygon","title":"from_polygon  <code>classmethod</code>","text":"<pre><code>from_polygon(polygon: List[List[float]]) -&gt; BoundingBox\n</code></pre> <p>Create axis-aligned bounding box from polygon points.</p> PARAMETER DESCRIPTION <code>polygon</code> <p>List of [x, y] points (usually 4 for quadrilateral)</p> <p> TYPE: <code>List[List[float]]</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox that encloses all polygon points</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef from_polygon(cls, polygon: List[List[float]]) -&gt; \"BoundingBox\":\n    \"\"\"\n    Create axis-aligned bounding box from polygon points.\n\n    Args:\n        polygon: List of [x, y] points (usually 4 for quadrilateral)\n\n    Returns:\n        BoundingBox that encloses all polygon points\n    \"\"\"\n    if not polygon:\n        raise ValueError(\"Polygon cannot be empty\")\n\n    xs = [p[0] for p in polygon]\n    ys = [p[1] for p in polygon]\n    return cls(x1=min(xs), y1=min(ys), x2=max(xs), y2=max(ys))\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> <p>Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas. This provides consistent coordinates regardless of original image size.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas.\n    This provides consistent coordinates regardless of original image size.\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_absolute","title":"to_absolute","text":"<pre><code>to_absolute(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert from normalized (0-1024) to absolute pixel coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Target image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Target image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with absolute pixel coordinates</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_absolute(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert from normalized (0-1024) to absolute pixel coordinates.\n\n    Args:\n        image_width: Target image width in pixels\n        image_height: Target image height in pixels\n\n    Returns:\n        New BoundingBox with absolute pixel coordinates\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / NORMALIZED_SIZE * image_width,\n        y1=self.y1 / NORMALIZED_SIZE * image_height,\n        x2=self.x2 / NORMALIZED_SIZE * image_width,\n        y2=self.y2 / NORMALIZED_SIZE * image_height,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.TextBlock","title":"TextBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single detected text element with text, bounding box, and confidence.</p> <p>This is the fundamental unit of OCR output - can represent a character, word, line, or block depending on the OCR model and configuration.</p> Example <pre><code>block = TextBlock(\n        text=\"Hello\",\n        bbox=BoundingBox(x1=100, y1=50, x2=200, y2=80),\n        confidence=0.95,\n        granularity=OCRGranularity.WORD,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.TextBlock.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"text\": self.text,\n        \"bbox\": self.bbox.to_list(),\n        \"confidence\": self.confidence,\n        \"granularity\": self.granularity.value,\n        \"polygon\": self.polygon,\n        \"language\": self.language,\n    }\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.TextBlock.get_normalized_bbox","title":"get_normalized_bbox","text":"<pre><code>get_normalized_bbox(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Get bounding box in normalized (0-1024) coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox with normalized coordinates</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def get_normalized_bbox(self, image_width: int, image_height: int) -&gt; BoundingBox:\n    \"\"\"\n    Get bounding box in normalized (0-1024) coordinates.\n\n    Args:\n        image_width: Original image width\n        image_height: Original image height\n\n    Returns:\n        BoundingBox with normalized coordinates\n    \"\"\"\n    return self.bbox.to_normalized(image_width, image_height)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput","title":"OCROutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete OCR extraction results for a single image.</p> <p>Contains all detected text blocks with their bounding boxes, plus metadata about the extraction.</p> Example <pre><code>result = ocr.extract(image)\nprint(f\"Found {result.block_count} blocks\")\nprint(f\"Full text: {result.full_text}\")\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.block_count","title":"block_count  <code>property</code>","text":"<pre><code>block_count: int\n</code></pre> <p>Number of detected text blocks.</p>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.word_count","title":"word_count  <code>property</code>","text":"<pre><code>word_count: int\n</code></pre> <p>Approximate word count from full text.</p>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.average_confidence","title":"average_confidence  <code>property</code>","text":"<pre><code>average_confidence: float\n</code></pre> <p>Average confidence across all text blocks.</p>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(\n    min_confidence: float,\n) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by minimum confidence.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def filter_by_confidence(self, min_confidence: float) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by minimum confidence.\"\"\"\n    return [b for b in self.text_blocks if b.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.filter_by_granularity","title":"filter_by_granularity","text":"<pre><code>filter_by_granularity(\n    granularity: OCRGranularity,\n) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by granularity level.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def filter_by_granularity(self, granularity: OCRGranularity) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by granularity level.\"\"\"\n    return [b for b in self.text_blocks if b.granularity == granularity]\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"text_blocks\": [b.to_dict() for b in self.text_blocks],\n        \"full_text\": self.full_text,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"languages_detected\": self.languages_detected,\n        \"block_count\": self.block_count,\n        \"word_count\": self.word_count,\n        \"average_confidence\": self.average_confidence,\n    }\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.sort_by_position","title":"sort_by_position","text":"<pre><code>sort_by_position(top_to_bottom: bool = True) -&gt; OCROutput\n</code></pre> <p>Return a new OCROutput with blocks sorted by position.</p> PARAMETER DESCRIPTION <code>top_to_bottom</code> <p>If True, sort by y-coordinate (reading order)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>New OCROutput with sorted text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def sort_by_position(self, top_to_bottom: bool = True) -&gt; \"OCROutput\":\n    \"\"\"\n    Return a new OCROutput with blocks sorted by position.\n\n    Args:\n        top_to_bottom: If True, sort by y-coordinate (reading order)\n\n    Returns:\n        New OCROutput with sorted text blocks\n    \"\"\"\n    sorted_blocks = sorted(\n        self.text_blocks,\n        key=lambda b: (b.bbox.y1, b.bbox.x1),\n        reverse=not top_to_bottom,\n    )\n    # Regenerate full_text in sorted order\n    full_text = \" \".join(b.text for b in sorted_blocks)\n\n    return OCROutput(\n        text_blocks=sorted_blocks,\n        full_text=full_text,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        model_name=self.model_name,\n        languages_detected=self.languages_detected,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.get_normalized_blocks","title":"get_normalized_blocks","text":"<pre><code>get_normalized_blocks() -&gt; List[Dict]\n</code></pre> <p>Get all text blocks with normalized (0-1024) coordinates.</p> RETURNS DESCRIPTION <code>List[Dict]</code> <p>List of dicts with normalized bbox coordinates and metadata.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def get_normalized_blocks(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all text blocks with normalized (0-1024) coordinates.\n\n    Returns:\n        List of dicts with normalized bbox coordinates and metadata.\n    \"\"\"\n    normalized = []\n    for block in self.text_blocks:\n        norm_bbox = block.bbox.to_normalized(self.image_width, self.image_height)\n        normalized.append(\n            {\n                \"text\": block.text,\n                \"bbox\": norm_bbox.to_list(),\n                \"confidence\": block.confidence,\n                \"granularity\": block.granularity.value,\n                \"language\": block.language,\n            }\n        )\n    return normalized\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.visualize","title":"visualize","text":"<pre><code>visualize(\n    image: Image,\n    output_path: Optional[Union[str, Path]] = None,\n    show_text: bool = True,\n    show_confidence: bool = False,\n    line_width: int = 2,\n    box_color: str = \"#2ECC71\",\n    text_color: str = \"#000000\",\n) -&gt; Image.Image\n</code></pre> <p>Visualize OCR results on the image.</p> <p>Draws bounding boxes around detected text with optional labels.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to draw on (will be copied, not modified)</p> <p> TYPE: <code>Image</code> </p> <code>output_path</code> <p>Optional path to save the visualization</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> <code>show_text</code> <p>Whether to show detected text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_confidence</code> <p>Whether to show confidence scores</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>line_width</code> <p>Width of bounding box lines</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>box_color</code> <p>Color for bounding boxes (hex)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'#2ECC71'</code> </p> <code>text_color</code> <p>Color for text labels (hex)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'#000000'</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image with visualizations drawn</p> Example <pre><code>result = ocr.extract(image)\nviz = result.visualize(image, output_path=\"ocr_viz.png\")\n</code></pre> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def visualize(\n    self,\n    image: \"Image.Image\",\n    output_path: Optional[Union[str, Path]] = None,\n    show_text: bool = True,\n    show_confidence: bool = False,\n    line_width: int = 2,\n    box_color: str = \"#2ECC71\",\n    text_color: str = \"#000000\",\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Visualize OCR results on the image.\n\n    Draws bounding boxes around detected text with optional labels.\n\n    Args:\n        image: PIL Image to draw on (will be copied, not modified)\n        output_path: Optional path to save the visualization\n        show_text: Whether to show detected text\n        show_confidence: Whether to show confidence scores\n        line_width: Width of bounding box lines\n        box_color: Color for bounding boxes (hex)\n        text_color: Color for text labels (hex)\n\n    Returns:\n        PIL Image with visualizations drawn\n\n    Example:\n        ```python\n        result = ocr.extract(image)\n        viz = result.visualize(image, output_path=\"ocr_viz.png\")\n        ```\n    \"\"\"\n    from PIL import ImageDraw, ImageFont\n\n    # Copy image to avoid modifying original\n    viz_image = image.copy().convert(\"RGB\")\n    draw = ImageDraw.Draw(viz_image)\n\n    # Try to get a font\n    try:\n        font = ImageFont.truetype(\"/System/Library/Fonts/Helvetica.ttc\", 12)\n    except Exception:\n        try:\n            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 12)\n        except Exception:\n            font = ImageFont.load_default()\n\n    for block in self.text_blocks:\n        coords = block.bbox.to_xyxy()\n\n        # Draw polygon if available, otherwise draw rectangle\n        if block.polygon:\n            flat_polygon = [coord for point in block.polygon for coord in point]\n            draw.polygon(flat_polygon, outline=box_color, width=line_width)\n        else:\n            draw.rectangle(coords, outline=box_color, width=line_width)\n\n        # Build label text\n        if show_text or show_confidence:\n            label_parts = []\n            if show_text:\n                # Truncate long text\n                text = block.text[:25] + \"...\" if len(block.text) &gt; 25 else block.text\n                label_parts.append(text)\n            if show_confidence:\n                label_parts.append(f\"{block.confidence:.2f}\")\n            label_text = \" | \".join(label_parts)\n\n            # Position label below the box\n            label_x = coords[0]\n            label_y = coords[3] + 2  # Below bottom edge\n\n            # Draw label with background\n            text_bbox = draw.textbbox((label_x, label_y), label_text, font=font)\n            padding = 2\n            draw.rectangle(\n                [\n                    text_bbox[0] - padding,\n                    text_bbox[1] - padding,\n                    text_bbox[2] + padding,\n                    text_bbox[3] + padding,\n                ],\n                fill=\"#FFFFFF\",\n                outline=box_color,\n            )\n            draw.text((label_x, label_y), label_text, fill=text_color, font=font)\n\n    # Save if path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        viz_image.save(output_path)\n\n    return viz_image\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; OCROutput\n</code></pre> <p>Load an OCROutput instance from a JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to JSON file</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput instance</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"OCROutput\":\n    \"\"\"\n    Load an OCROutput instance from a JSON file.\n\n    Args:\n        file_path: Path to JSON file\n\n    Returns:\n        OCROutput instance\n    \"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save OCROutput instance to a JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path where JSON file should be saved</p> <p> TYPE: <code>Union[str, Path]</code> </p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"\n    Save OCROutput instance to a JSON file.\n\n    Args:\n        file_path: Path where JSON file should be saved\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/","title":"Overview","text":"<p>OCR Extraction Module.</p> <p>Provides extractors for detecting text with bounding boxes from document images. Returns text content along with spatial coordinates (unlike Text Extraction which returns formatted Markdown/HTML without coordinates).</p> Available Extractors <ul> <li>TesseractOCR: Open-source OCR (CPU, requires system Tesseract)</li> <li>EasyOCR: PyTorch-based OCR (CPU/GPU, 80+ languages)</li> <li>PaddleOCR: PaddlePaddle-based OCR (CPU/GPU, excellent CJK support)</li> </ul> Key Difference from Text Extraction <ul> <li>OCR Extraction: Text + Bounding Boxes (spatial location)</li> <li>Text Extraction: Markdown/HTML (formatted document export)</li> </ul> Example <pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\n\nocr = TesseractOCR(config=TesseractOCRConfig(languages=[\"eng\"]))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()} (conf: {block.confidence:.2f})\")\n# With EasyOCR\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\nocr = EasyOCR(config=EasyOCRConfig(languages=[\"en\", \"ch_sim\"], gpu=True))\nresult = ocr.extract(image)\n# With PaddleOCR\nfrom omnidocs.tasks.ocr_extraction import PaddleOCR, PaddleOCRConfig\n\nocr = PaddleOCR(config=PaddleOCRConfig(lang=\"ch\", device=\"cpu\"))\nresult = ocr.extract(image)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BaseOCRExtractor","title":"BaseOCRExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for OCR extractors.</p> <p>All OCR extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyOCRExtractor(BaseOCRExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Initialize OCR engine\n            pass\n\n        def extract(self, image):\n            # Run OCR extraction\n            return OCROutput(...)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BaseOCRExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput containing detected text blocks with bounding boxes</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format is not supported</p> <code>RuntimeError</code> <p>If OCR engine is not initialized or extraction fails</p> Source code in <code>omnidocs/tasks/ocr_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR extraction on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n\n    Returns:\n        OCROutput containing detected text blocks with bounding boxes\n\n    Raises:\n        ValueError: If image format is not supported\n        RuntimeError: If OCR engine is not initialized or extraction fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.EasyOCR","title":"EasyOCR","text":"<pre><code>EasyOCR(config: EasyOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>EasyOCR text extractor.</p> <p>Single-backend model (PyTorch - CPU/GPU).</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\nocr = EasyOCR(config=EasyOCRConfig(languages=[\"en\"], gpu=True))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize EasyOCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>EasyOCRConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If easyocr is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def __init__(self, config: EasyOCRConfig):\n    \"\"\"\n    Initialize EasyOCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        ImportError: If easyocr is not installed\n    \"\"\"\n    self.config = config\n    self._reader = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.EasyOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    detail: int = 1,\n    paragraph: bool = False,\n    min_size: int = 10,\n    text_threshold: float = 0.7,\n    low_text: float = 0.4,\n    link_threshold: float = 0.4,\n    canvas_size: int = 2560,\n    mag_ratio: float = 1.0,\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>detail</code> <p>0 = simple output, 1 = detailed with boxes</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>paragraph</code> <p>Combine results into paragraphs</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>min_size</code> <p>Minimum text box size</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>text_threshold</code> <p>Text confidence threshold</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.7</code> </p> <code>low_text</code> <p>Low text bound</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>link_threshold</code> <p>Link threshold for text joining</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>canvas_size</code> <p>Max image dimension for processing</p> <p> TYPE: <code>int</code> DEFAULT: <code>2560</code> </p> <code>mag_ratio</code> <p>Magnification ratio</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks</p> RAISES DESCRIPTION <code>ValueError</code> <p>If detail is not 0 or 1</p> <code>RuntimeError</code> <p>If EasyOCR is not initialized</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    detail: int = 1,\n    paragraph: bool = False,\n    min_size: int = 10,\n    text_threshold: float = 0.7,\n    low_text: float = 0.4,\n    link_threshold: float = 0.4,\n    canvas_size: int = 2560,\n    mag_ratio: float = 1.0,\n) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n        detail: 0 = simple output, 1 = detailed with boxes\n        paragraph: Combine results into paragraphs\n        min_size: Minimum text box size\n        text_threshold: Text confidence threshold\n        low_text: Low text bound\n        link_threshold: Link threshold for text joining\n        canvas_size: Max image dimension for processing\n        mag_ratio: Magnification ratio\n\n    Returns:\n        OCROutput with detected text blocks\n\n    Raises:\n        ValueError: If detail is not 0 or 1\n        RuntimeError: If EasyOCR is not initialized\n    \"\"\"\n    if self._reader is None:\n        raise RuntimeError(\"EasyOCR not initialized. Call _load_model() first.\")\n\n    # Validate detail parameter\n    if detail not in (0, 1):\n        raise ValueError(f\"detail must be 0 or 1, got {detail}\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Convert to numpy array for EasyOCR\n    image_array = np.array(pil_image)\n\n    # Run EasyOCR\n    results = self._reader.readtext(\n        image_array,\n        detail=detail,\n        paragraph=paragraph,\n        min_size=min_size,\n        text_threshold=text_threshold,\n        low_text=low_text,\n        link_threshold=link_threshold,\n        canvas_size=canvas_size,\n        mag_ratio=mag_ratio,\n    )\n\n    # Parse results\n    text_blocks = []\n    full_text_parts = []\n\n    for result in results:\n        if detail == 0:\n            # Simple output: just text\n            text = result\n            confidence = 1.0\n            bbox = BoundingBox(x1=0, y1=0, x2=0, y2=0)\n            polygon = None\n        else:\n            # Detailed output: [polygon, text, confidence]\n            polygon_points, text, confidence = result\n\n            # EasyOCR returns 4 corner points: [[x1,y1], [x2,y1], [x2,y2], [x1,y2]]\n            # Convert to list of lists for storage\n            polygon = [list(p) for p in polygon_points]\n\n            # Convert to axis-aligned bounding box\n            bbox = BoundingBox.from_polygon(polygon)\n\n        if not text.strip():\n            continue\n\n        text_blocks.append(\n            TextBlock(\n                text=text,\n                bbox=bbox,\n                confidence=float(confidence),\n                granularity=(OCRGranularity.LINE if paragraph else OCRGranularity.WORD),\n                polygon=polygon,\n                language=\"+\".join(self.config.languages),\n            )\n        )\n\n        full_text_parts.append(text)\n\n    # Sort by position\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\" \".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.EasyOCR.extract_batch","title":"extract_batch","text":"<pre><code>extract_batch(\n    images: List[Union[Image, ndarray, str, Path]], **kwargs\n) -&gt; List[OCROutput]\n</code></pre> <p>Run OCR on multiple images.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of input images</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>**kwargs</code> <p>Arguments passed to extract()</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>List[OCROutput]</code> <p>List of OCROutput objects</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def extract_batch(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    **kwargs,\n) -&gt; List[OCROutput]:\n    \"\"\"\n    Run OCR on multiple images.\n\n    Args:\n        images: List of input images\n        **kwargs: Arguments passed to extract()\n\n    Returns:\n        List of OCROutput objects\n    \"\"\"\n    results = []\n    for img in images:\n        results.append(self.extract(img, **kwargs))\n    return results\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.EasyOCRConfig","title":"EasyOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for EasyOCR extractor.</p> <p>This is a single-backend model (PyTorch - CPU/GPU).</p> Example <pre><code>config = EasyOCRConfig(languages=[\"en\", \"ch_sim\"], gpu=True)\nocr = EasyOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box coordinates in pixel space.</p> <p>Coordinates follow the convention: (x1, y1) is top-left, (x2, y2) is bottom-right. For rotated text, use the polygon field in TextBlock instead.</p> Example <pre><code>bbox = BoundingBox(x1=100, y1=50, x2=300, y2=80)\nprint(bbox.width, bbox.height)  # 200, 30\nprint(bbox.center)  # (200.0, 65.0)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x, y, width, height) format.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_xywh(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x, y, width, height) format.\"\"\"\n    return (self.x1, self.y1, self.width, self.height)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.from_polygon","title":"from_polygon  <code>classmethod</code>","text":"<pre><code>from_polygon(polygon: List[List[float]]) -&gt; BoundingBox\n</code></pre> <p>Create axis-aligned bounding box from polygon points.</p> PARAMETER DESCRIPTION <code>polygon</code> <p>List of [x, y] points (usually 4 for quadrilateral)</p> <p> TYPE: <code>List[List[float]]</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox that encloses all polygon points</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef from_polygon(cls, polygon: List[List[float]]) -&gt; \"BoundingBox\":\n    \"\"\"\n    Create axis-aligned bounding box from polygon points.\n\n    Args:\n        polygon: List of [x, y] points (usually 4 for quadrilateral)\n\n    Returns:\n        BoundingBox that encloses all polygon points\n    \"\"\"\n    if not polygon:\n        raise ValueError(\"Polygon cannot be empty\")\n\n    xs = [p[0] for p in polygon]\n    ys = [p[1] for p in polygon]\n    return cls(x1=min(xs), y1=min(ys), x2=max(xs), y2=max(ys))\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> <p>Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas. This provides consistent coordinates regardless of original image size.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas.\n    This provides consistent coordinates regardless of original image size.\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.to_absolute","title":"to_absolute","text":"<pre><code>to_absolute(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert from normalized (0-1024) to absolute pixel coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Target image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Target image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with absolute pixel coordinates</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_absolute(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert from normalized (0-1024) to absolute pixel coordinates.\n\n    Args:\n        image_width: Target image width in pixels\n        image_height: Target image height in pixels\n\n    Returns:\n        New BoundingBox with absolute pixel coordinates\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / NORMALIZED_SIZE * image_width,\n        y1=self.y1 / NORMALIZED_SIZE * image_height,\n        x2=self.x2 / NORMALIZED_SIZE * image_width,\n        y2=self.y2 / NORMALIZED_SIZE * image_height,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCRGranularity","title":"OCRGranularity","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>OCR detection granularity levels.</p> <p>Different OCR engines return results at different granularity levels. This enum standardizes the options across all extractors.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput","title":"OCROutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete OCR extraction results for a single image.</p> <p>Contains all detected text blocks with their bounding boxes, plus metadata about the extraction.</p> Example <pre><code>result = ocr.extract(image)\nprint(f\"Found {result.block_count} blocks\")\nprint(f\"Full text: {result.full_text}\")\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.block_count","title":"block_count  <code>property</code>","text":"<pre><code>block_count: int\n</code></pre> <p>Number of detected text blocks.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.word_count","title":"word_count  <code>property</code>","text":"<pre><code>word_count: int\n</code></pre> <p>Approximate word count from full text.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.average_confidence","title":"average_confidence  <code>property</code>","text":"<pre><code>average_confidence: float\n</code></pre> <p>Average confidence across all text blocks.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(\n    min_confidence: float,\n) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by minimum confidence.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def filter_by_confidence(self, min_confidence: float) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by minimum confidence.\"\"\"\n    return [b for b in self.text_blocks if b.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.filter_by_granularity","title":"filter_by_granularity","text":"<pre><code>filter_by_granularity(\n    granularity: OCRGranularity,\n) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by granularity level.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def filter_by_granularity(self, granularity: OCRGranularity) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by granularity level.\"\"\"\n    return [b for b in self.text_blocks if b.granularity == granularity]\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"text_blocks\": [b.to_dict() for b in self.text_blocks],\n        \"full_text\": self.full_text,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"languages_detected\": self.languages_detected,\n        \"block_count\": self.block_count,\n        \"word_count\": self.word_count,\n        \"average_confidence\": self.average_confidence,\n    }\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.sort_by_position","title":"sort_by_position","text":"<pre><code>sort_by_position(top_to_bottom: bool = True) -&gt; OCROutput\n</code></pre> <p>Return a new OCROutput with blocks sorted by position.</p> PARAMETER DESCRIPTION <code>top_to_bottom</code> <p>If True, sort by y-coordinate (reading order)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>New OCROutput with sorted text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def sort_by_position(self, top_to_bottom: bool = True) -&gt; \"OCROutput\":\n    \"\"\"\n    Return a new OCROutput with blocks sorted by position.\n\n    Args:\n        top_to_bottom: If True, sort by y-coordinate (reading order)\n\n    Returns:\n        New OCROutput with sorted text blocks\n    \"\"\"\n    sorted_blocks = sorted(\n        self.text_blocks,\n        key=lambda b: (b.bbox.y1, b.bbox.x1),\n        reverse=not top_to_bottom,\n    )\n    # Regenerate full_text in sorted order\n    full_text = \" \".join(b.text for b in sorted_blocks)\n\n    return OCROutput(\n        text_blocks=sorted_blocks,\n        full_text=full_text,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        model_name=self.model_name,\n        languages_detected=self.languages_detected,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.get_normalized_blocks","title":"get_normalized_blocks","text":"<pre><code>get_normalized_blocks() -&gt; List[Dict]\n</code></pre> <p>Get all text blocks with normalized (0-1024) coordinates.</p> RETURNS DESCRIPTION <code>List[Dict]</code> <p>List of dicts with normalized bbox coordinates and metadata.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def get_normalized_blocks(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all text blocks with normalized (0-1024) coordinates.\n\n    Returns:\n        List of dicts with normalized bbox coordinates and metadata.\n    \"\"\"\n    normalized = []\n    for block in self.text_blocks:\n        norm_bbox = block.bbox.to_normalized(self.image_width, self.image_height)\n        normalized.append(\n            {\n                \"text\": block.text,\n                \"bbox\": norm_bbox.to_list(),\n                \"confidence\": block.confidence,\n                \"granularity\": block.granularity.value,\n                \"language\": block.language,\n            }\n        )\n    return normalized\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.visualize","title":"visualize","text":"<pre><code>visualize(\n    image: Image,\n    output_path: Optional[Union[str, Path]] = None,\n    show_text: bool = True,\n    show_confidence: bool = False,\n    line_width: int = 2,\n    box_color: str = \"#2ECC71\",\n    text_color: str = \"#000000\",\n) -&gt; Image.Image\n</code></pre> <p>Visualize OCR results on the image.</p> <p>Draws bounding boxes around detected text with optional labels.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to draw on (will be copied, not modified)</p> <p> TYPE: <code>Image</code> </p> <code>output_path</code> <p>Optional path to save the visualization</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> <code>show_text</code> <p>Whether to show detected text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_confidence</code> <p>Whether to show confidence scores</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>line_width</code> <p>Width of bounding box lines</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>box_color</code> <p>Color for bounding boxes (hex)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'#2ECC71'</code> </p> <code>text_color</code> <p>Color for text labels (hex)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'#000000'</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image with visualizations drawn</p> Example <pre><code>result = ocr.extract(image)\nviz = result.visualize(image, output_path=\"ocr_viz.png\")\n</code></pre> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def visualize(\n    self,\n    image: \"Image.Image\",\n    output_path: Optional[Union[str, Path]] = None,\n    show_text: bool = True,\n    show_confidence: bool = False,\n    line_width: int = 2,\n    box_color: str = \"#2ECC71\",\n    text_color: str = \"#000000\",\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Visualize OCR results on the image.\n\n    Draws bounding boxes around detected text with optional labels.\n\n    Args:\n        image: PIL Image to draw on (will be copied, not modified)\n        output_path: Optional path to save the visualization\n        show_text: Whether to show detected text\n        show_confidence: Whether to show confidence scores\n        line_width: Width of bounding box lines\n        box_color: Color for bounding boxes (hex)\n        text_color: Color for text labels (hex)\n\n    Returns:\n        PIL Image with visualizations drawn\n\n    Example:\n        ```python\n        result = ocr.extract(image)\n        viz = result.visualize(image, output_path=\"ocr_viz.png\")\n        ```\n    \"\"\"\n    from PIL import ImageDraw, ImageFont\n\n    # Copy image to avoid modifying original\n    viz_image = image.copy().convert(\"RGB\")\n    draw = ImageDraw.Draw(viz_image)\n\n    # Try to get a font\n    try:\n        font = ImageFont.truetype(\"/System/Library/Fonts/Helvetica.ttc\", 12)\n    except Exception:\n        try:\n            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 12)\n        except Exception:\n            font = ImageFont.load_default()\n\n    for block in self.text_blocks:\n        coords = block.bbox.to_xyxy()\n\n        # Draw polygon if available, otherwise draw rectangle\n        if block.polygon:\n            flat_polygon = [coord for point in block.polygon for coord in point]\n            draw.polygon(flat_polygon, outline=box_color, width=line_width)\n        else:\n            draw.rectangle(coords, outline=box_color, width=line_width)\n\n        # Build label text\n        if show_text or show_confidence:\n            label_parts = []\n            if show_text:\n                # Truncate long text\n                text = block.text[:25] + \"...\" if len(block.text) &gt; 25 else block.text\n                label_parts.append(text)\n            if show_confidence:\n                label_parts.append(f\"{block.confidence:.2f}\")\n            label_text = \" | \".join(label_parts)\n\n            # Position label below the box\n            label_x = coords[0]\n            label_y = coords[3] + 2  # Below bottom edge\n\n            # Draw label with background\n            text_bbox = draw.textbbox((label_x, label_y), label_text, font=font)\n            padding = 2\n            draw.rectangle(\n                [\n                    text_bbox[0] - padding,\n                    text_bbox[1] - padding,\n                    text_bbox[2] + padding,\n                    text_bbox[3] + padding,\n                ],\n                fill=\"#FFFFFF\",\n                outline=box_color,\n            )\n            draw.text((label_x, label_y), label_text, fill=text_color, font=font)\n\n    # Save if path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        viz_image.save(output_path)\n\n    return viz_image\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; OCROutput\n</code></pre> <p>Load an OCROutput instance from a JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to JSON file</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput instance</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"OCROutput\":\n    \"\"\"\n    Load an OCROutput instance from a JSON file.\n\n    Args:\n        file_path: Path to JSON file\n\n    Returns:\n        OCROutput instance\n    \"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save OCROutput instance to a JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path where JSON file should be saved</p> <p> TYPE: <code>Union[str, Path]</code> </p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"\n    Save OCROutput instance to a JSON file.\n\n    Args:\n        file_path: Path where JSON file should be saved\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.TextBlock","title":"TextBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single detected text element with text, bounding box, and confidence.</p> <p>This is the fundamental unit of OCR output - can represent a character, word, line, or block depending on the OCR model and configuration.</p> Example <pre><code>block = TextBlock(\n        text=\"Hello\",\n        bbox=BoundingBox(x1=100, y1=50, x2=200, y2=80),\n        confidence=0.95,\n        granularity=OCRGranularity.WORD,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.TextBlock.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"text\": self.text,\n        \"bbox\": self.bbox.to_list(),\n        \"confidence\": self.confidence,\n        \"granularity\": self.granularity.value,\n        \"polygon\": self.polygon,\n        \"language\": self.language,\n    }\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.TextBlock.get_normalized_bbox","title":"get_normalized_bbox","text":"<pre><code>get_normalized_bbox(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Get bounding box in normalized (0-1024) coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox with normalized coordinates</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def get_normalized_bbox(self, image_width: int, image_height: int) -&gt; BoundingBox:\n    \"\"\"\n    Get bounding box in normalized (0-1024) coordinates.\n\n    Args:\n        image_width: Original image width\n        image_height: Original image height\n\n    Returns:\n        BoundingBox with normalized coordinates\n    \"\"\"\n    return self.bbox.to_normalized(image_width, image_height)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.PaddleOCR","title":"PaddleOCR","text":"<pre><code>PaddleOCR(config: PaddleOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>PaddleOCR text extractor.</p> <p>Single-backend model (PaddlePaddle - CPU/GPU).</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import PaddleOCR, PaddleOCRConfig\n\nocr = PaddleOCR(config=PaddleOCRConfig(lang=\"en\", device=\"cpu\"))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize PaddleOCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>PaddleOCRConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If paddleocr or paddlepaddle is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/paddleocr.py</code> <pre><code>def __init__(self, config: PaddleOCRConfig):\n    \"\"\"\n    Initialize PaddleOCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        ImportError: If paddleocr or paddlepaddle is not installed\n    \"\"\"\n    self.config = config\n    self._ocr = None\n\n    # Normalize language code\n    self._lang = LANG_CODES.get(config.lang.lower(), config.lang)\n\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.PaddleOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/paddleocr.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with detected text blocks\n    \"\"\"\n    if self._ocr is None:\n        raise RuntimeError(\"PaddleOCR not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Convert to numpy array\n    image_array = np.array(pil_image)\n\n    # Run PaddleOCR v3.x - use predict() method\n    results = self._ocr.predict(image_array)\n\n    # Parse results\n    text_blocks = []\n\n    # PaddleOCR may return None or empty results\n    if results is None or len(results) == 0:\n        return OCROutput(\n            text_blocks=[],\n            full_text=\"\",\n            image_width=image_width,\n            image_height=image_height,\n            model_name=self.MODEL_NAME,\n            languages_detected=[self._lang],\n        )\n\n    # PaddleOCR v3.x returns list of dicts with 'rec_texts', 'rec_scores', 'dt_polys'\n    for result in results:\n        if result is None:\n            continue\n\n        rec_texts = result.get(\"rec_texts\", [])\n        rec_scores = result.get(\"rec_scores\", [])\n        dt_polys = result.get(\"dt_polys\", [])\n\n        for i, text in enumerate(rec_texts):\n            if not text.strip():\n                continue\n\n            confidence = rec_scores[i] if i &lt; len(rec_scores) else 1.0\n\n            # Get polygon and convert to list\n            polygon: Optional[List[List[float]]] = None\n            if i &lt; len(dt_polys) and dt_polys[i] is not None:\n                poly_array = dt_polys[i]\n                # Handle numpy array\n                if hasattr(poly_array, \"tolist\"):\n                    polygon = poly_array.tolist()\n                else:\n                    polygon = list(poly_array)\n\n            # Convert polygon to bbox\n            if polygon:\n                bbox = BoundingBox.from_polygon(polygon)\n            else:\n                bbox = BoundingBox(x1=0, y1=0, x2=0, y2=0)\n\n            text_blocks.append(\n                TextBlock(\n                    text=text,\n                    bbox=bbox,\n                    confidence=float(confidence),\n                    granularity=OCRGranularity.LINE,\n                    polygon=polygon,\n                    language=self._lang,\n                )\n            )\n\n    # Sort by position (top to bottom, left to right)\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    # Build full_text from sorted blocks to ensure reading order\n    full_text = \" \".join(block.text for block in text_blocks)\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=full_text,\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=[self._lang],\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.PaddleOCRConfig","title":"PaddleOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for PaddleOCR extractor.</p> <p>This is a single-backend model (PaddlePaddle - CPU/GPU).</p> Example <pre><code>config = PaddleOCRConfig(lang=\"ch\", device=\"gpu\")\nocr = PaddleOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.TesseractOCR","title":"TesseractOCR","text":"<pre><code>TesseractOCR(config: TesseractOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>Tesseract OCR extractor.</p> <p>Single-backend model (CPU only). Requires system Tesseract installation.</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\n\nocr = TesseractOCR(config=TesseractOCRConfig(languages=[\"eng\"]))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize Tesseract OCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>TesseractOCRConfig</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If Tesseract is not installed</p> <code>ImportError</code> <p>If pytesseract is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def __init__(self, config: TesseractOCRConfig):\n    \"\"\"\n    Initialize Tesseract OCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        RuntimeError: If Tesseract is not installed\n        ImportError: If pytesseract is not installed\n    \"\"\"\n    self.config = config\n    self._pytesseract = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.TesseractOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks at word level</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with detected text blocks at word level\n    \"\"\"\n    if self._pytesseract is None:\n        raise RuntimeError(\"Tesseract not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Build config string\n    config = f\"--oem {self.config.oem} --psm {self.config.psm}\"\n    if self.config.config_params:\n        for key, value in self.config.config_params.items():\n            config += f\" -c {key}={value}\"\n\n    # Language string\n    lang_str = \"+\".join(self.config.languages)\n\n    # Get detailed data (word-level boxes)\n    data = self._pytesseract.image_to_data(\n        pil_image,\n        lang=lang_str,\n        config=config,\n        output_type=self._pytesseract.Output.DICT,\n    )\n\n    # Parse results into TextBlocks\n    text_blocks = []\n    full_text_parts = []\n\n    n_boxes = len(data[\"text\"])\n    for i in range(n_boxes):\n        text = data[\"text\"][i].strip()\n        # Safely convert conf to float (handles string values from some Tesseract versions)\n        try:\n            conf = float(data[\"conf\"][i])\n        except (ValueError, TypeError):\n            conf = -1\n\n        # Skip empty text or low confidence (-1 means no confidence)\n        if not text or conf == -1:\n            continue\n\n        # Tesseract returns confidence as 0-100, normalize to 0-1\n        confidence = conf / 100.0\n\n        # Get bounding box\n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n\n        bbox = BoundingBox(\n            x1=float(x),\n            y1=float(y),\n            x2=float(x + w),\n            y2=float(y + h),\n        )\n\n        text_blocks.append(\n            TextBlock(\n                text=text,\n                bbox=bbox,\n                confidence=confidence,\n                granularity=OCRGranularity.WORD,\n                language=lang_str,\n            )\n        )\n\n        full_text_parts.append(text)\n\n    # Sort by position (top to bottom, left to right)\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\" \".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.TesseractOCR.extract_lines","title":"extract_lines","text":"<pre><code>extract_lines(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR and return line-level blocks.</p> <p>Groups words into lines based on Tesseract's line detection.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with line-level text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def extract_lines(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR and return line-level blocks.\n\n    Groups words into lines based on Tesseract's line detection.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with line-level text blocks\n    \"\"\"\n    if self._pytesseract is None:\n        raise RuntimeError(\"Tesseract not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Build config string (including config_params like extract method)\n    config = f\"--oem {self.config.oem} --psm {self.config.psm}\"\n    if self.config.config_params:\n        for key, value in self.config.config_params.items():\n            config += f\" -c {key}={value}\"\n\n    # Language string\n    lang_str = \"+\".join(self.config.languages)\n\n    # Get detailed data\n    data = self._pytesseract.image_to_data(\n        pil_image,\n        lang=lang_str,\n        config=config,\n        output_type=self._pytesseract.Output.DICT,\n    )\n\n    # Group words into lines\n    lines: Dict[tuple, Dict] = {}\n    n_boxes = len(data[\"text\"])\n\n    for i in range(n_boxes):\n        text = data[\"text\"][i].strip()\n        # Safely convert conf to float (handles string values from some Tesseract versions)\n        try:\n            conf = float(data[\"conf\"][i])\n        except (ValueError, TypeError):\n            conf = -1\n\n        if not text or conf == -1:\n            continue\n\n        # Tesseract provides block_num, par_num, line_num\n        line_key = (data[\"block_num\"][i], data[\"par_num\"][i], data[\"line_num\"][i])\n\n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n\n        if line_key not in lines:\n            lines[line_key] = {\n                \"words\": [],\n                \"confidences\": [],\n                \"x1\": x,\n                \"y1\": y,\n                \"x2\": x + w,\n                \"y2\": y + h,\n            }\n\n        lines[line_key][\"words\"].append(text)\n        lines[line_key][\"confidences\"].append(conf / 100.0)\n        lines[line_key][\"x1\"] = min(lines[line_key][\"x1\"], x)\n        lines[line_key][\"y1\"] = min(lines[line_key][\"y1\"], y)\n        lines[line_key][\"x2\"] = max(lines[line_key][\"x2\"], x + w)\n        lines[line_key][\"y2\"] = max(lines[line_key][\"y2\"], y + h)\n\n    # Convert to TextBlocks\n    text_blocks = []\n    full_text_parts = []\n\n    for line_key in sorted(lines.keys()):\n        line = lines[line_key]\n        line_text = \" \".join(line[\"words\"])\n        avg_conf = sum(line[\"confidences\"]) / len(line[\"confidences\"])\n\n        bbox = BoundingBox(\n            x1=float(line[\"x1\"]),\n            y1=float(line[\"y1\"]),\n            x2=float(line[\"x2\"]),\n            y2=float(line[\"y2\"]),\n        )\n\n        text_blocks.append(\n            TextBlock(\n                text=line_text,\n                bbox=bbox,\n                confidence=avg_conf,\n                granularity=OCRGranularity.LINE,\n                language=lang_str,\n            )\n        )\n\n        full_text_parts.append(line_text)\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\"\\n\".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.TesseractOCRConfig","title":"TesseractOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Tesseract OCR extractor.</p> <p>This is a single-backend model (CPU only, requires system Tesseract).</p> Example <pre><code>config = TesseractOCRConfig(languages=[\"eng\", \"fra\"], psm=3)\nocr = TesseractOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.base","title":"base","text":"<p>Base class for OCR extractors.</p> <p>Defines the abstract interface that all OCR extractors must implement.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor","title":"BaseOCRExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for OCR extractors.</p> <p>All OCR extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyOCRExtractor(BaseOCRExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Initialize OCR engine\n            pass\n\n        def extract(self, image):\n            # Run OCR extraction\n            return OCROutput(...)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput containing detected text blocks with bounding boxes</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format is not supported</p> <code>RuntimeError</code> <p>If OCR engine is not initialized or extraction fails</p> Source code in <code>omnidocs/tasks/ocr_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR extraction on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n\n    Returns:\n        OCROutput containing detected text blocks with bounding boxes\n\n    Raises:\n        ValueError: If image format is not supported\n        RuntimeError: If OCR engine is not initialized or extraction fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.easyocr","title":"easyocr","text":"<p>EasyOCR extractor.</p> <p>EasyOCR is a PyTorch-based OCR engine with excellent multi-language support. - GPU accelerated (optional) - Supports 80+ languages - Good for scene text and printed documents</p> Python Package <p>pip install easyocr</p> Model Download Location <p>By default, EasyOCR downloads models to ~/.EasyOCR/ Can be overridden with model_storage_directory parameter</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCRConfig","title":"EasyOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for EasyOCR extractor.</p> <p>This is a single-backend model (PyTorch - CPU/GPU).</p> Example <pre><code>config = EasyOCRConfig(languages=[\"en\", \"ch_sim\"], gpu=True)\nocr = EasyOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCR","title":"EasyOCR","text":"<pre><code>EasyOCR(config: EasyOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>EasyOCR text extractor.</p> <p>Single-backend model (PyTorch - CPU/GPU).</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\nocr = EasyOCR(config=EasyOCRConfig(languages=[\"en\"], gpu=True))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize EasyOCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>EasyOCRConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If easyocr is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def __init__(self, config: EasyOCRConfig):\n    \"\"\"\n    Initialize EasyOCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        ImportError: If easyocr is not installed\n    \"\"\"\n    self.config = config\n    self._reader = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    detail: int = 1,\n    paragraph: bool = False,\n    min_size: int = 10,\n    text_threshold: float = 0.7,\n    low_text: float = 0.4,\n    link_threshold: float = 0.4,\n    canvas_size: int = 2560,\n    mag_ratio: float = 1.0,\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>detail</code> <p>0 = simple output, 1 = detailed with boxes</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>paragraph</code> <p>Combine results into paragraphs</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>min_size</code> <p>Minimum text box size</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>text_threshold</code> <p>Text confidence threshold</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.7</code> </p> <code>low_text</code> <p>Low text bound</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>link_threshold</code> <p>Link threshold for text joining</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>canvas_size</code> <p>Max image dimension for processing</p> <p> TYPE: <code>int</code> DEFAULT: <code>2560</code> </p> <code>mag_ratio</code> <p>Magnification ratio</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks</p> RAISES DESCRIPTION <code>ValueError</code> <p>If detail is not 0 or 1</p> <code>RuntimeError</code> <p>If EasyOCR is not initialized</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    detail: int = 1,\n    paragraph: bool = False,\n    min_size: int = 10,\n    text_threshold: float = 0.7,\n    low_text: float = 0.4,\n    link_threshold: float = 0.4,\n    canvas_size: int = 2560,\n    mag_ratio: float = 1.0,\n) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n        detail: 0 = simple output, 1 = detailed with boxes\n        paragraph: Combine results into paragraphs\n        min_size: Minimum text box size\n        text_threshold: Text confidence threshold\n        low_text: Low text bound\n        link_threshold: Link threshold for text joining\n        canvas_size: Max image dimension for processing\n        mag_ratio: Magnification ratio\n\n    Returns:\n        OCROutput with detected text blocks\n\n    Raises:\n        ValueError: If detail is not 0 or 1\n        RuntimeError: If EasyOCR is not initialized\n    \"\"\"\n    if self._reader is None:\n        raise RuntimeError(\"EasyOCR not initialized. Call _load_model() first.\")\n\n    # Validate detail parameter\n    if detail not in (0, 1):\n        raise ValueError(f\"detail must be 0 or 1, got {detail}\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Convert to numpy array for EasyOCR\n    image_array = np.array(pil_image)\n\n    # Run EasyOCR\n    results = self._reader.readtext(\n        image_array,\n        detail=detail,\n        paragraph=paragraph,\n        min_size=min_size,\n        text_threshold=text_threshold,\n        low_text=low_text,\n        link_threshold=link_threshold,\n        canvas_size=canvas_size,\n        mag_ratio=mag_ratio,\n    )\n\n    # Parse results\n    text_blocks = []\n    full_text_parts = []\n\n    for result in results:\n        if detail == 0:\n            # Simple output: just text\n            text = result\n            confidence = 1.0\n            bbox = BoundingBox(x1=0, y1=0, x2=0, y2=0)\n            polygon = None\n        else:\n            # Detailed output: [polygon, text, confidence]\n            polygon_points, text, confidence = result\n\n            # EasyOCR returns 4 corner points: [[x1,y1], [x2,y1], [x2,y2], [x1,y2]]\n            # Convert to list of lists for storage\n            polygon = [list(p) for p in polygon_points]\n\n            # Convert to axis-aligned bounding box\n            bbox = BoundingBox.from_polygon(polygon)\n\n        if not text.strip():\n            continue\n\n        text_blocks.append(\n            TextBlock(\n                text=text,\n                bbox=bbox,\n                confidence=float(confidence),\n                granularity=(OCRGranularity.LINE if paragraph else OCRGranularity.WORD),\n                polygon=polygon,\n                language=\"+\".join(self.config.languages),\n            )\n        )\n\n        full_text_parts.append(text)\n\n    # Sort by position\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\" \".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCR.extract_batch","title":"extract_batch","text":"<pre><code>extract_batch(\n    images: List[Union[Image, ndarray, str, Path]], **kwargs\n) -&gt; List[OCROutput]\n</code></pre> <p>Run OCR on multiple images.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of input images</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>**kwargs</code> <p>Arguments passed to extract()</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>List[OCROutput]</code> <p>List of OCROutput objects</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def extract_batch(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    **kwargs,\n) -&gt; List[OCROutput]:\n    \"\"\"\n    Run OCR on multiple images.\n\n    Args:\n        images: List of input images\n        **kwargs: Arguments passed to extract()\n\n    Returns:\n        List of OCROutput objects\n    \"\"\"\n    results = []\n    for img in images:\n        results.append(self.extract(img, **kwargs))\n    return results\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models","title":"models","text":"<p>Pydantic models for OCR extraction outputs.</p> <p>Defines standardized output types for OCR detection including text blocks with bounding boxes, confidence scores, and granularity levels.</p> <p>Key difference from Text Extraction: - OCR returns text WITH bounding boxes (word/line/character level) - Text Extraction returns formatted text (MD/HTML) WITHOUT bboxes</p> Coordinate Systems <ul> <li>Absolute (default): Coordinates in pixels relative to original image size</li> <li>Normalized (0-1024): Coordinates scaled to 0-1024 range (virtual 1024x1024 canvas)</li> </ul> <p>Use <code>bbox.to_normalized(width, height)</code> or <code>output.get_normalized_blocks()</code> to convert to normalized coordinates.</p> Example <pre><code>result = ocr.extract(image)  # Returns absolute pixel coordinates\nnormalized = result.get_normalized_blocks()  # Returns 0-1024 normalized coords\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCRGranularity","title":"OCRGranularity","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>OCR detection granularity levels.</p> <p>Different OCR engines return results at different granularity levels. This enum standardizes the options across all extractors.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box coordinates in pixel space.</p> <p>Coordinates follow the convention: (x1, y1) is top-left, (x2, y2) is bottom-right. For rotated text, use the polygon field in TextBlock instead.</p> Example <pre><code>bbox = BoundingBox(x1=100, y1=50, x2=300, y2=80)\nprint(bbox.width, bbox.height)  # 200, 30\nprint(bbox.center)  # (200.0, 65.0)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x, y, width, height) format.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_xywh(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x, y, width, height) format.\"\"\"\n    return (self.x1, self.y1, self.width, self.height)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.from_polygon","title":"from_polygon  <code>classmethod</code>","text":"<pre><code>from_polygon(polygon: List[List[float]]) -&gt; BoundingBox\n</code></pre> <p>Create axis-aligned bounding box from polygon points.</p> PARAMETER DESCRIPTION <code>polygon</code> <p>List of [x, y] points (usually 4 for quadrilateral)</p> <p> TYPE: <code>List[List[float]]</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox that encloses all polygon points</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef from_polygon(cls, polygon: List[List[float]]) -&gt; \"BoundingBox\":\n    \"\"\"\n    Create axis-aligned bounding box from polygon points.\n\n    Args:\n        polygon: List of [x, y] points (usually 4 for quadrilateral)\n\n    Returns:\n        BoundingBox that encloses all polygon points\n    \"\"\"\n    if not polygon:\n        raise ValueError(\"Polygon cannot be empty\")\n\n    xs = [p[0] for p in polygon]\n    ys = [p[1] for p in polygon]\n    return cls(x1=min(xs), y1=min(ys), x2=max(xs), y2=max(ys))\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> <p>Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas. This provides consistent coordinates regardless of original image size.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas.\n    This provides consistent coordinates regardless of original image size.\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_absolute","title":"to_absolute","text":"<pre><code>to_absolute(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert from normalized (0-1024) to absolute pixel coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Target image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Target image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with absolute pixel coordinates</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_absolute(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert from normalized (0-1024) to absolute pixel coordinates.\n\n    Args:\n        image_width: Target image width in pixels\n        image_height: Target image height in pixels\n\n    Returns:\n        New BoundingBox with absolute pixel coordinates\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / NORMALIZED_SIZE * image_width,\n        y1=self.y1 / NORMALIZED_SIZE * image_height,\n        x2=self.x2 / NORMALIZED_SIZE * image_width,\n        y2=self.y2 / NORMALIZED_SIZE * image_height,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.TextBlock","title":"TextBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single detected text element with text, bounding box, and confidence.</p> <p>This is the fundamental unit of OCR output - can represent a character, word, line, or block depending on the OCR model and configuration.</p> Example <pre><code>block = TextBlock(\n        text=\"Hello\",\n        bbox=BoundingBox(x1=100, y1=50, x2=200, y2=80),\n        confidence=0.95,\n        granularity=OCRGranularity.WORD,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.TextBlock.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"text\": self.text,\n        \"bbox\": self.bbox.to_list(),\n        \"confidence\": self.confidence,\n        \"granularity\": self.granularity.value,\n        \"polygon\": self.polygon,\n        \"language\": self.language,\n    }\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.TextBlock.get_normalized_bbox","title":"get_normalized_bbox","text":"<pre><code>get_normalized_bbox(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Get bounding box in normalized (0-1024) coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox with normalized coordinates</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def get_normalized_bbox(self, image_width: int, image_height: int) -&gt; BoundingBox:\n    \"\"\"\n    Get bounding box in normalized (0-1024) coordinates.\n\n    Args:\n        image_width: Original image width\n        image_height: Original image height\n\n    Returns:\n        BoundingBox with normalized coordinates\n    \"\"\"\n    return self.bbox.to_normalized(image_width, image_height)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput","title":"OCROutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete OCR extraction results for a single image.</p> <p>Contains all detected text blocks with their bounding boxes, plus metadata about the extraction.</p> Example <pre><code>result = ocr.extract(image)\nprint(f\"Found {result.block_count} blocks\")\nprint(f\"Full text: {result.full_text}\")\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.block_count","title":"block_count  <code>property</code>","text":"<pre><code>block_count: int\n</code></pre> <p>Number of detected text blocks.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.word_count","title":"word_count  <code>property</code>","text":"<pre><code>word_count: int\n</code></pre> <p>Approximate word count from full text.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.average_confidence","title":"average_confidence  <code>property</code>","text":"<pre><code>average_confidence: float\n</code></pre> <p>Average confidence across all text blocks.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(\n    min_confidence: float,\n) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by minimum confidence.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def filter_by_confidence(self, min_confidence: float) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by minimum confidence.\"\"\"\n    return [b for b in self.text_blocks if b.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.filter_by_granularity","title":"filter_by_granularity","text":"<pre><code>filter_by_granularity(\n    granularity: OCRGranularity,\n) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by granularity level.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def filter_by_granularity(self, granularity: OCRGranularity) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by granularity level.\"\"\"\n    return [b for b in self.text_blocks if b.granularity == granularity]\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"text_blocks\": [b.to_dict() for b in self.text_blocks],\n        \"full_text\": self.full_text,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"languages_detected\": self.languages_detected,\n        \"block_count\": self.block_count,\n        \"word_count\": self.word_count,\n        \"average_confidence\": self.average_confidence,\n    }\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.sort_by_position","title":"sort_by_position","text":"<pre><code>sort_by_position(top_to_bottom: bool = True) -&gt; OCROutput\n</code></pre> <p>Return a new OCROutput with blocks sorted by position.</p> PARAMETER DESCRIPTION <code>top_to_bottom</code> <p>If True, sort by y-coordinate (reading order)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>New OCROutput with sorted text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def sort_by_position(self, top_to_bottom: bool = True) -&gt; \"OCROutput\":\n    \"\"\"\n    Return a new OCROutput with blocks sorted by position.\n\n    Args:\n        top_to_bottom: If True, sort by y-coordinate (reading order)\n\n    Returns:\n        New OCROutput with sorted text blocks\n    \"\"\"\n    sorted_blocks = sorted(\n        self.text_blocks,\n        key=lambda b: (b.bbox.y1, b.bbox.x1),\n        reverse=not top_to_bottom,\n    )\n    # Regenerate full_text in sorted order\n    full_text = \" \".join(b.text for b in sorted_blocks)\n\n    return OCROutput(\n        text_blocks=sorted_blocks,\n        full_text=full_text,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        model_name=self.model_name,\n        languages_detected=self.languages_detected,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.get_normalized_blocks","title":"get_normalized_blocks","text":"<pre><code>get_normalized_blocks() -&gt; List[Dict]\n</code></pre> <p>Get all text blocks with normalized (0-1024) coordinates.</p> RETURNS DESCRIPTION <code>List[Dict]</code> <p>List of dicts with normalized bbox coordinates and metadata.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def get_normalized_blocks(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all text blocks with normalized (0-1024) coordinates.\n\n    Returns:\n        List of dicts with normalized bbox coordinates and metadata.\n    \"\"\"\n    normalized = []\n    for block in self.text_blocks:\n        norm_bbox = block.bbox.to_normalized(self.image_width, self.image_height)\n        normalized.append(\n            {\n                \"text\": block.text,\n                \"bbox\": norm_bbox.to_list(),\n                \"confidence\": block.confidence,\n                \"granularity\": block.granularity.value,\n                \"language\": block.language,\n            }\n        )\n    return normalized\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.visualize","title":"visualize","text":"<pre><code>visualize(\n    image: Image,\n    output_path: Optional[Union[str, Path]] = None,\n    show_text: bool = True,\n    show_confidence: bool = False,\n    line_width: int = 2,\n    box_color: str = \"#2ECC71\",\n    text_color: str = \"#000000\",\n) -&gt; Image.Image\n</code></pre> <p>Visualize OCR results on the image.</p> <p>Draws bounding boxes around detected text with optional labels.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to draw on (will be copied, not modified)</p> <p> TYPE: <code>Image</code> </p> <code>output_path</code> <p>Optional path to save the visualization</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> <code>show_text</code> <p>Whether to show detected text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_confidence</code> <p>Whether to show confidence scores</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>line_width</code> <p>Width of bounding box lines</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>box_color</code> <p>Color for bounding boxes (hex)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'#2ECC71'</code> </p> <code>text_color</code> <p>Color for text labels (hex)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'#000000'</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image with visualizations drawn</p> Example <pre><code>result = ocr.extract(image)\nviz = result.visualize(image, output_path=\"ocr_viz.png\")\n</code></pre> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def visualize(\n    self,\n    image: \"Image.Image\",\n    output_path: Optional[Union[str, Path]] = None,\n    show_text: bool = True,\n    show_confidence: bool = False,\n    line_width: int = 2,\n    box_color: str = \"#2ECC71\",\n    text_color: str = \"#000000\",\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Visualize OCR results on the image.\n\n    Draws bounding boxes around detected text with optional labels.\n\n    Args:\n        image: PIL Image to draw on (will be copied, not modified)\n        output_path: Optional path to save the visualization\n        show_text: Whether to show detected text\n        show_confidence: Whether to show confidence scores\n        line_width: Width of bounding box lines\n        box_color: Color for bounding boxes (hex)\n        text_color: Color for text labels (hex)\n\n    Returns:\n        PIL Image with visualizations drawn\n\n    Example:\n        ```python\n        result = ocr.extract(image)\n        viz = result.visualize(image, output_path=\"ocr_viz.png\")\n        ```\n    \"\"\"\n    from PIL import ImageDraw, ImageFont\n\n    # Copy image to avoid modifying original\n    viz_image = image.copy().convert(\"RGB\")\n    draw = ImageDraw.Draw(viz_image)\n\n    # Try to get a font\n    try:\n        font = ImageFont.truetype(\"/System/Library/Fonts/Helvetica.ttc\", 12)\n    except Exception:\n        try:\n            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 12)\n        except Exception:\n            font = ImageFont.load_default()\n\n    for block in self.text_blocks:\n        coords = block.bbox.to_xyxy()\n\n        # Draw polygon if available, otherwise draw rectangle\n        if block.polygon:\n            flat_polygon = [coord for point in block.polygon for coord in point]\n            draw.polygon(flat_polygon, outline=box_color, width=line_width)\n        else:\n            draw.rectangle(coords, outline=box_color, width=line_width)\n\n        # Build label text\n        if show_text or show_confidence:\n            label_parts = []\n            if show_text:\n                # Truncate long text\n                text = block.text[:25] + \"...\" if len(block.text) &gt; 25 else block.text\n                label_parts.append(text)\n            if show_confidence:\n                label_parts.append(f\"{block.confidence:.2f}\")\n            label_text = \" | \".join(label_parts)\n\n            # Position label below the box\n            label_x = coords[0]\n            label_y = coords[3] + 2  # Below bottom edge\n\n            # Draw label with background\n            text_bbox = draw.textbbox((label_x, label_y), label_text, font=font)\n            padding = 2\n            draw.rectangle(\n                [\n                    text_bbox[0] - padding,\n                    text_bbox[1] - padding,\n                    text_bbox[2] + padding,\n                    text_bbox[3] + padding,\n                ],\n                fill=\"#FFFFFF\",\n                outline=box_color,\n            )\n            draw.text((label_x, label_y), label_text, fill=text_color, font=font)\n\n    # Save if path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        viz_image.save(output_path)\n\n    return viz_image\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; OCROutput\n</code></pre> <p>Load an OCROutput instance from a JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to JSON file</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput instance</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"OCROutput\":\n    \"\"\"\n    Load an OCROutput instance from a JSON file.\n\n    Args:\n        file_path: Path to JSON file\n\n    Returns:\n        OCROutput instance\n    \"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save OCROutput instance to a JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path where JSON file should be saved</p> <p> TYPE: <code>Union[str, Path]</code> </p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"\n    Save OCROutput instance to a JSON file.\n\n    Args:\n        file_path: Path where JSON file should be saved\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.paddleocr","title":"paddleocr","text":"<p>PaddleOCR extractor.</p> <p>PaddleOCR is an OCR toolkit developed by Baidu/PaddlePaddle. - Excellent for CJK languages (Chinese, Japanese, Korean) - GPU accelerated - Supports layout analysis + OCR</p> Python Package <p>pip install paddleocr paddlepaddle  # CPU version pip install paddleocr paddlepaddle-gpu  # GPU version</p> Model Download Location <p>By default, PaddleOCR downloads models to ~/.paddleocr/</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.paddleocr.PaddleOCRConfig","title":"PaddleOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for PaddleOCR extractor.</p> <p>This is a single-backend model (PaddlePaddle - CPU/GPU).</p> Example <pre><code>config = PaddleOCRConfig(lang=\"ch\", device=\"gpu\")\nocr = PaddleOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.paddleocr.PaddleOCR","title":"PaddleOCR","text":"<pre><code>PaddleOCR(config: PaddleOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>PaddleOCR text extractor.</p> <p>Single-backend model (PaddlePaddle - CPU/GPU).</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import PaddleOCR, PaddleOCRConfig\n\nocr = PaddleOCR(config=PaddleOCRConfig(lang=\"en\", device=\"cpu\"))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize PaddleOCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>PaddleOCRConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If paddleocr or paddlepaddle is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/paddleocr.py</code> <pre><code>def __init__(self, config: PaddleOCRConfig):\n    \"\"\"\n    Initialize PaddleOCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        ImportError: If paddleocr or paddlepaddle is not installed\n    \"\"\"\n    self.config = config\n    self._ocr = None\n\n    # Normalize language code\n    self._lang = LANG_CODES.get(config.lang.lower(), config.lang)\n\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.paddleocr.PaddleOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/paddleocr.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with detected text blocks\n    \"\"\"\n    if self._ocr is None:\n        raise RuntimeError(\"PaddleOCR not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Convert to numpy array\n    image_array = np.array(pil_image)\n\n    # Run PaddleOCR v3.x - use predict() method\n    results = self._ocr.predict(image_array)\n\n    # Parse results\n    text_blocks = []\n\n    # PaddleOCR may return None or empty results\n    if results is None or len(results) == 0:\n        return OCROutput(\n            text_blocks=[],\n            full_text=\"\",\n            image_width=image_width,\n            image_height=image_height,\n            model_name=self.MODEL_NAME,\n            languages_detected=[self._lang],\n        )\n\n    # PaddleOCR v3.x returns list of dicts with 'rec_texts', 'rec_scores', 'dt_polys'\n    for result in results:\n        if result is None:\n            continue\n\n        rec_texts = result.get(\"rec_texts\", [])\n        rec_scores = result.get(\"rec_scores\", [])\n        dt_polys = result.get(\"dt_polys\", [])\n\n        for i, text in enumerate(rec_texts):\n            if not text.strip():\n                continue\n\n            confidence = rec_scores[i] if i &lt; len(rec_scores) else 1.0\n\n            # Get polygon and convert to list\n            polygon: Optional[List[List[float]]] = None\n            if i &lt; len(dt_polys) and dt_polys[i] is not None:\n                poly_array = dt_polys[i]\n                # Handle numpy array\n                if hasattr(poly_array, \"tolist\"):\n                    polygon = poly_array.tolist()\n                else:\n                    polygon = list(poly_array)\n\n            # Convert polygon to bbox\n            if polygon:\n                bbox = BoundingBox.from_polygon(polygon)\n            else:\n                bbox = BoundingBox(x1=0, y1=0, x2=0, y2=0)\n\n            text_blocks.append(\n                TextBlock(\n                    text=text,\n                    bbox=bbox,\n                    confidence=float(confidence),\n                    granularity=OCRGranularity.LINE,\n                    polygon=polygon,\n                    language=self._lang,\n                )\n            )\n\n    # Sort by position (top to bottom, left to right)\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    # Build full_text from sorted blocks to ensure reading order\n    full_text = \" \".join(block.text for block in text_blocks)\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=full_text,\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=[self._lang],\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.tesseract","title":"tesseract","text":"<p>Tesseract OCR extractor.</p> <p>Tesseract is an open-source OCR engine maintained by Google. - CPU-based (no GPU required) - Requires system installation of Tesseract - Good for printed text, supports 100+ languages</p> System Requirements <p>macOS: brew install tesseract Ubuntu: sudo apt-get install tesseract-ocr Windows: Download from https://github.com/UB-Mannheim/tesseract/wiki</p> Python Package <p>pip install pytesseract</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCRConfig","title":"TesseractOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Tesseract OCR extractor.</p> <p>This is a single-backend model (CPU only, requires system Tesseract).</p> Example <pre><code>config = TesseractOCRConfig(languages=[\"eng\", \"fra\"], psm=3)\nocr = TesseractOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCR","title":"TesseractOCR","text":"<pre><code>TesseractOCR(config: TesseractOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>Tesseract OCR extractor.</p> <p>Single-backend model (CPU only). Requires system Tesseract installation.</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\n\nocr = TesseractOCR(config=TesseractOCRConfig(languages=[\"eng\"]))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize Tesseract OCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>TesseractOCRConfig</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If Tesseract is not installed</p> <code>ImportError</code> <p>If pytesseract is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def __init__(self, config: TesseractOCRConfig):\n    \"\"\"\n    Initialize Tesseract OCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        RuntimeError: If Tesseract is not installed\n        ImportError: If pytesseract is not installed\n    \"\"\"\n    self.config = config\n    self._pytesseract = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks at word level</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with detected text blocks at word level\n    \"\"\"\n    if self._pytesseract is None:\n        raise RuntimeError(\"Tesseract not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Build config string\n    config = f\"--oem {self.config.oem} --psm {self.config.psm}\"\n    if self.config.config_params:\n        for key, value in self.config.config_params.items():\n            config += f\" -c {key}={value}\"\n\n    # Language string\n    lang_str = \"+\".join(self.config.languages)\n\n    # Get detailed data (word-level boxes)\n    data = self._pytesseract.image_to_data(\n        pil_image,\n        lang=lang_str,\n        config=config,\n        output_type=self._pytesseract.Output.DICT,\n    )\n\n    # Parse results into TextBlocks\n    text_blocks = []\n    full_text_parts = []\n\n    n_boxes = len(data[\"text\"])\n    for i in range(n_boxes):\n        text = data[\"text\"][i].strip()\n        # Safely convert conf to float (handles string values from some Tesseract versions)\n        try:\n            conf = float(data[\"conf\"][i])\n        except (ValueError, TypeError):\n            conf = -1\n\n        # Skip empty text or low confidence (-1 means no confidence)\n        if not text or conf == -1:\n            continue\n\n        # Tesseract returns confidence as 0-100, normalize to 0-1\n        confidence = conf / 100.0\n\n        # Get bounding box\n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n\n        bbox = BoundingBox(\n            x1=float(x),\n            y1=float(y),\n            x2=float(x + w),\n            y2=float(y + h),\n        )\n\n        text_blocks.append(\n            TextBlock(\n                text=text,\n                bbox=bbox,\n                confidence=confidence,\n                granularity=OCRGranularity.WORD,\n                language=lang_str,\n            )\n        )\n\n        full_text_parts.append(text)\n\n    # Sort by position (top to bottom, left to right)\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\" \".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCR.extract_lines","title":"extract_lines","text":"<pre><code>extract_lines(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR and return line-level blocks.</p> <p>Groups words into lines based on Tesseract's line detection.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with line-level text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def extract_lines(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR and return line-level blocks.\n\n    Groups words into lines based on Tesseract's line detection.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with line-level text blocks\n    \"\"\"\n    if self._pytesseract is None:\n        raise RuntimeError(\"Tesseract not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Build config string (including config_params like extract method)\n    config = f\"--oem {self.config.oem} --psm {self.config.psm}\"\n    if self.config.config_params:\n        for key, value in self.config.config_params.items():\n            config += f\" -c {key}={value}\"\n\n    # Language string\n    lang_str = \"+\".join(self.config.languages)\n\n    # Get detailed data\n    data = self._pytesseract.image_to_data(\n        pil_image,\n        lang=lang_str,\n        config=config,\n        output_type=self._pytesseract.Output.DICT,\n    )\n\n    # Group words into lines\n    lines: Dict[tuple, Dict] = {}\n    n_boxes = len(data[\"text\"])\n\n    for i in range(n_boxes):\n        text = data[\"text\"][i].strip()\n        # Safely convert conf to float (handles string values from some Tesseract versions)\n        try:\n            conf = float(data[\"conf\"][i])\n        except (ValueError, TypeError):\n            conf = -1\n\n        if not text or conf == -1:\n            continue\n\n        # Tesseract provides block_num, par_num, line_num\n        line_key = (data[\"block_num\"][i], data[\"par_num\"][i], data[\"line_num\"][i])\n\n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n\n        if line_key not in lines:\n            lines[line_key] = {\n                \"words\": [],\n                \"confidences\": [],\n                \"x1\": x,\n                \"y1\": y,\n                \"x2\": x + w,\n                \"y2\": y + h,\n            }\n\n        lines[line_key][\"words\"].append(text)\n        lines[line_key][\"confidences\"].append(conf / 100.0)\n        lines[line_key][\"x1\"] = min(lines[line_key][\"x1\"], x)\n        lines[line_key][\"y1\"] = min(lines[line_key][\"y1\"], y)\n        lines[line_key][\"x2\"] = max(lines[line_key][\"x2\"], x + w)\n        lines[line_key][\"y2\"] = max(lines[line_key][\"y2\"], y + h)\n\n    # Convert to TextBlocks\n    text_blocks = []\n    full_text_parts = []\n\n    for line_key in sorted(lines.keys()):\n        line = lines[line_key]\n        line_text = \" \".join(line[\"words\"])\n        avg_conf = sum(line[\"confidences\"]) / len(line[\"confidences\"])\n\n        bbox = BoundingBox(\n            x1=float(line[\"x1\"]),\n            y1=float(line[\"y1\"]),\n            x2=float(line[\"x2\"]),\n            y2=float(line[\"y2\"]),\n        )\n\n        text_blocks.append(\n            TextBlock(\n                text=line_text,\n                bbox=bbox,\n                confidence=avg_conf,\n                granularity=OCRGranularity.LINE,\n                language=lang_str,\n            )\n        )\n\n        full_text_parts.append(line_text)\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\"\\n\".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/paddleocr/","title":"PaddleOCR","text":"<p>PaddleOCR extractor.</p> <p>PaddleOCR is an OCR toolkit developed by Baidu/PaddlePaddle. - Excellent for CJK languages (Chinese, Japanese, Korean) - GPU accelerated - Supports layout analysis + OCR</p> Python Package <p>pip install paddleocr paddlepaddle  # CPU version pip install paddleocr paddlepaddle-gpu  # GPU version</p> Model Download Location <p>By default, PaddleOCR downloads models to ~/.paddleocr/</p>"},{"location":"reference/tasks/ocr_extraction/paddleocr/#omnidocs.tasks.ocr_extraction.paddleocr.PaddleOCRConfig","title":"PaddleOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for PaddleOCR extractor.</p> <p>This is a single-backend model (PaddlePaddle - CPU/GPU).</p> Example <pre><code>config = PaddleOCRConfig(lang=\"ch\", device=\"gpu\")\nocr = PaddleOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/paddleocr/#omnidocs.tasks.ocr_extraction.paddleocr.PaddleOCR","title":"PaddleOCR","text":"<pre><code>PaddleOCR(config: PaddleOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>PaddleOCR text extractor.</p> <p>Single-backend model (PaddlePaddle - CPU/GPU).</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import PaddleOCR, PaddleOCRConfig\n\nocr = PaddleOCR(config=PaddleOCRConfig(lang=\"en\", device=\"cpu\"))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize PaddleOCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>PaddleOCRConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If paddleocr or paddlepaddle is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/paddleocr.py</code> <pre><code>def __init__(self, config: PaddleOCRConfig):\n    \"\"\"\n    Initialize PaddleOCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        ImportError: If paddleocr or paddlepaddle is not installed\n    \"\"\"\n    self.config = config\n    self._ocr = None\n\n    # Normalize language code\n    self._lang = LANG_CODES.get(config.lang.lower(), config.lang)\n\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/paddleocr/#omnidocs.tasks.ocr_extraction.paddleocr.PaddleOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/paddleocr.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with detected text blocks\n    \"\"\"\n    if self._ocr is None:\n        raise RuntimeError(\"PaddleOCR not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Convert to numpy array\n    image_array = np.array(pil_image)\n\n    # Run PaddleOCR v3.x - use predict() method\n    results = self._ocr.predict(image_array)\n\n    # Parse results\n    text_blocks = []\n\n    # PaddleOCR may return None or empty results\n    if results is None or len(results) == 0:\n        return OCROutput(\n            text_blocks=[],\n            full_text=\"\",\n            image_width=image_width,\n            image_height=image_height,\n            model_name=self.MODEL_NAME,\n            languages_detected=[self._lang],\n        )\n\n    # PaddleOCR v3.x returns list of dicts with 'rec_texts', 'rec_scores', 'dt_polys'\n    for result in results:\n        if result is None:\n            continue\n\n        rec_texts = result.get(\"rec_texts\", [])\n        rec_scores = result.get(\"rec_scores\", [])\n        dt_polys = result.get(\"dt_polys\", [])\n\n        for i, text in enumerate(rec_texts):\n            if not text.strip():\n                continue\n\n            confidence = rec_scores[i] if i &lt; len(rec_scores) else 1.0\n\n            # Get polygon and convert to list\n            polygon: Optional[List[List[float]]] = None\n            if i &lt; len(dt_polys) and dt_polys[i] is not None:\n                poly_array = dt_polys[i]\n                # Handle numpy array\n                if hasattr(poly_array, \"tolist\"):\n                    polygon = poly_array.tolist()\n                else:\n                    polygon = list(poly_array)\n\n            # Convert polygon to bbox\n            if polygon:\n                bbox = BoundingBox.from_polygon(polygon)\n            else:\n                bbox = BoundingBox(x1=0, y1=0, x2=0, y2=0)\n\n            text_blocks.append(\n                TextBlock(\n                    text=text,\n                    bbox=bbox,\n                    confidence=float(confidence),\n                    granularity=OCRGranularity.LINE,\n                    polygon=polygon,\n                    language=self._lang,\n                )\n            )\n\n    # Sort by position (top to bottom, left to right)\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    # Build full_text from sorted blocks to ensure reading order\n    full_text = \" \".join(block.text for block in text_blocks)\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=full_text,\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=[self._lang],\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/tesseract/","title":"Tesseract","text":"<p>Tesseract OCR extractor.</p> <p>Tesseract is an open-source OCR engine maintained by Google. - CPU-based (no GPU required) - Requires system installation of Tesseract - Good for printed text, supports 100+ languages</p> System Requirements <p>macOS: brew install tesseract Ubuntu: sudo apt-get install tesseract-ocr Windows: Download from https://github.com/UB-Mannheim/tesseract/wiki</p> Python Package <p>pip install pytesseract</p>"},{"location":"reference/tasks/ocr_extraction/tesseract/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCRConfig","title":"TesseractOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Tesseract OCR extractor.</p> <p>This is a single-backend model (CPU only, requires system Tesseract).</p> Example <pre><code>config = TesseractOCRConfig(languages=[\"eng\", \"fra\"], psm=3)\nocr = TesseractOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/tesseract/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCR","title":"TesseractOCR","text":"<pre><code>TesseractOCR(config: TesseractOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>Tesseract OCR extractor.</p> <p>Single-backend model (CPU only). Requires system Tesseract installation.</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\n\nocr = TesseractOCR(config=TesseractOCRConfig(languages=[\"eng\"]))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize Tesseract OCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>TesseractOCRConfig</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If Tesseract is not installed</p> <code>ImportError</code> <p>If pytesseract is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def __init__(self, config: TesseractOCRConfig):\n    \"\"\"\n    Initialize Tesseract OCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        RuntimeError: If Tesseract is not installed\n        ImportError: If pytesseract is not installed\n    \"\"\"\n    self.config = config\n    self._pytesseract = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/tesseract/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks at word level</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with detected text blocks at word level\n    \"\"\"\n    if self._pytesseract is None:\n        raise RuntimeError(\"Tesseract not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Build config string\n    config = f\"--oem {self.config.oem} --psm {self.config.psm}\"\n    if self.config.config_params:\n        for key, value in self.config.config_params.items():\n            config += f\" -c {key}={value}\"\n\n    # Language string\n    lang_str = \"+\".join(self.config.languages)\n\n    # Get detailed data (word-level boxes)\n    data = self._pytesseract.image_to_data(\n        pil_image,\n        lang=lang_str,\n        config=config,\n        output_type=self._pytesseract.Output.DICT,\n    )\n\n    # Parse results into TextBlocks\n    text_blocks = []\n    full_text_parts = []\n\n    n_boxes = len(data[\"text\"])\n    for i in range(n_boxes):\n        text = data[\"text\"][i].strip()\n        # Safely convert conf to float (handles string values from some Tesseract versions)\n        try:\n            conf = float(data[\"conf\"][i])\n        except (ValueError, TypeError):\n            conf = -1\n\n        # Skip empty text or low confidence (-1 means no confidence)\n        if not text or conf == -1:\n            continue\n\n        # Tesseract returns confidence as 0-100, normalize to 0-1\n        confidence = conf / 100.0\n\n        # Get bounding box\n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n\n        bbox = BoundingBox(\n            x1=float(x),\n            y1=float(y),\n            x2=float(x + w),\n            y2=float(y + h),\n        )\n\n        text_blocks.append(\n            TextBlock(\n                text=text,\n                bbox=bbox,\n                confidence=confidence,\n                granularity=OCRGranularity.WORD,\n                language=lang_str,\n            )\n        )\n\n        full_text_parts.append(text)\n\n    # Sort by position (top to bottom, left to right)\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\" \".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/tesseract/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCR.extract_lines","title":"extract_lines","text":"<pre><code>extract_lines(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR and return line-level blocks.</p> <p>Groups words into lines based on Tesseract's line detection.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with line-level text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def extract_lines(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR and return line-level blocks.\n\n    Groups words into lines based on Tesseract's line detection.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with line-level text blocks\n    \"\"\"\n    if self._pytesseract is None:\n        raise RuntimeError(\"Tesseract not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Build config string (including config_params like extract method)\n    config = f\"--oem {self.config.oem} --psm {self.config.psm}\"\n    if self.config.config_params:\n        for key, value in self.config.config_params.items():\n            config += f\" -c {key}={value}\"\n\n    # Language string\n    lang_str = \"+\".join(self.config.languages)\n\n    # Get detailed data\n    data = self._pytesseract.image_to_data(\n        pil_image,\n        lang=lang_str,\n        config=config,\n        output_type=self._pytesseract.Output.DICT,\n    )\n\n    # Group words into lines\n    lines: Dict[tuple, Dict] = {}\n    n_boxes = len(data[\"text\"])\n\n    for i in range(n_boxes):\n        text = data[\"text\"][i].strip()\n        # Safely convert conf to float (handles string values from some Tesseract versions)\n        try:\n            conf = float(data[\"conf\"][i])\n        except (ValueError, TypeError):\n            conf = -1\n\n        if not text or conf == -1:\n            continue\n\n        # Tesseract provides block_num, par_num, line_num\n        line_key = (data[\"block_num\"][i], data[\"par_num\"][i], data[\"line_num\"][i])\n\n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n\n        if line_key not in lines:\n            lines[line_key] = {\n                \"words\": [],\n                \"confidences\": [],\n                \"x1\": x,\n                \"y1\": y,\n                \"x2\": x + w,\n                \"y2\": y + h,\n            }\n\n        lines[line_key][\"words\"].append(text)\n        lines[line_key][\"confidences\"].append(conf / 100.0)\n        lines[line_key][\"x1\"] = min(lines[line_key][\"x1\"], x)\n        lines[line_key][\"y1\"] = min(lines[line_key][\"y1\"], y)\n        lines[line_key][\"x2\"] = max(lines[line_key][\"x2\"], x + w)\n        lines[line_key][\"y2\"] = max(lines[line_key][\"y2\"], y + h)\n\n    # Convert to TextBlocks\n    text_blocks = []\n    full_text_parts = []\n\n    for line_key in sorted(lines.keys()):\n        line = lines[line_key]\n        line_text = \" \".join(line[\"words\"])\n        avg_conf = sum(line[\"confidences\"]) / len(line[\"confidences\"])\n\n        bbox = BoundingBox(\n            x1=float(line[\"x1\"]),\n            y1=float(line[\"y1\"]),\n            x2=float(line[\"x2\"]),\n            y2=float(line[\"y2\"]),\n        )\n\n        text_blocks.append(\n            TextBlock(\n                text=line_text,\n                bbox=bbox,\n                confidence=avg_conf,\n                granularity=OCRGranularity.LINE,\n                language=lang_str,\n            )\n        )\n\n        full_text_parts.append(line_text)\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\"\\n\".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/base/","title":"Base","text":"<p>Base class for text extractors.</p> <p>Defines the abstract interface that all text extractors must implement.</p>"},{"location":"reference/tasks/text_extraction/base/#omnidocs.tasks.text_extraction.base.BaseTextExtractor","title":"BaseTextExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for text extractors.</p> <p>All text extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyTextExtractor(BaseTextExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Load model weights\n            pass\n\n        def extract(self, image, output_format=\"markdown\"):\n            # Run extraction\n            return TextOutput(...)\n</code></pre>"},{"location":"reference/tasks/text_extraction/base/#omnidocs.tasks.text_extraction.base.BaseTextExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format or output_format is not supported</p> <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        ValueError: If image format or output_format is not supported\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/text_extraction/models/","title":"Models","text":"<p>Pydantic models for text extraction outputs.</p> <p>Defines output types and format enums for text extraction.</p>"},{"location":"reference/tasks/text_extraction/models/#omnidocs.tasks.text_extraction.models.OutputFormat","title":"OutputFormat","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported text extraction output formats.</p> Each format has different characteristics <ul> <li>HTML: Structured with div elements, preserves layout semantics</li> <li>MARKDOWN: Portable, human-readable, good for documentation</li> <li>JSON: Structured data with layout information (Dots OCR)</li> </ul>"},{"location":"reference/tasks/text_extraction/models/#omnidocs.tasks.text_extraction.models.TextOutput","title":"TextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text extraction output from a document image.</p> <p>Contains the extracted text content in the requested format, along with optional raw output and plain text versions.</p> Example <pre><code>result = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)  # Clean markdown\nprint(result.plain_text)  # Plain text without formatting\n</code></pre>"},{"location":"reference/tasks/text_extraction/models/#omnidocs.tasks.text_extraction.models.TextOutput.content_length","title":"content_length  <code>property</code>","text":"<pre><code>content_length: int\n</code></pre> <p>Length of the extracted content in characters.</p>"},{"location":"reference/tasks/text_extraction/models/#omnidocs.tasks.text_extraction.models.TextOutput.word_count","title":"word_count  <code>property</code>","text":"<pre><code>word_count: int\n</code></pre> <p>Approximate word count of the plain text.</p>"},{"location":"reference/tasks/text_extraction/models/#omnidocs.tasks.text_extraction.models.LayoutElement","title":"LayoutElement","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single layout element from document layout detection.</p> <p>Represents a detected region in the document with its bounding box, category label, and extracted text content.</p> ATTRIBUTE DESCRIPTION <code>bbox</code> <p>Bounding box coordinates [x1, y1, x2, y2] (normalized to 0-1024)</p> <p> TYPE: <code>List[int]</code> </p> <code>category</code> <p>Layout category (e.g., \"Text\", \"Title\", \"Table\", \"Formula\")</p> <p> TYPE: <code>str</code> </p> <code>text</code> <p>Extracted text content (None for pictures)</p> <p> TYPE: <code>Optional[str]</code> </p> <code>confidence</code> <p>Detection confidence score (optional)</p> <p> TYPE: <code>Optional[float]</code> </p>"},{"location":"reference/tasks/text_extraction/models/#omnidocs.tasks.text_extraction.models.DotsOCRTextOutput","title":"DotsOCRTextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text extraction output from Dots OCR with layout information.</p> <p>Dots OCR provides structured output with: - Layout detection (11 categories) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown/LaTeX/HTML) - Reading order preservation</p> Layout Categories <p>Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title</p> Text Formatting <ul> <li>Text/Title/Section-header: Markdown</li> <li>Formula: LaTeX</li> <li>Table: HTML</li> <li>Picture: (text omitted)</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nresult = extractor.extract(image, include_layout=True)\nprint(result.content)  # Full text with formatting\nfor elem in result.layout:\n        print(f\"{elem.category}: {elem.bbox}\")\n</code></pre>"},{"location":"reference/tasks/text_extraction/models/#omnidocs.tasks.text_extraction.models.DotsOCRTextOutput.num_layout_elements","title":"num_layout_elements  <code>property</code>","text":"<pre><code>num_layout_elements: int\n</code></pre> <p>Number of detected layout elements.</p>"},{"location":"reference/tasks/text_extraction/models/#omnidocs.tasks.text_extraction.models.DotsOCRTextOutput.content_length","title":"content_length  <code>property</code>","text":"<pre><code>content_length: int\n</code></pre> <p>Length of extracted content in characters.</p>"},{"location":"reference/tasks/text_extraction/overview/","title":"Overview","text":"<p>Text Extraction Module.</p> <p>Provides extractors for converting document images to structured text formats (HTML, Markdown, JSON). Uses Vision-Language Models for accurate text extraction with formatting preservation and optional layout detection.</p> Available Extractors <ul> <li>QwenTextExtractor: Qwen3-VL based extractor (multi-backend)</li> <li>DotsOCRTextExtractor: Dots OCR with layout-aware extraction (PyTorch/VLLM/API)</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.BaseTextExtractor","title":"BaseTextExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for text extractors.</p> <p>All text extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyTextExtractor(BaseTextExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Load model weights\n            pass\n\n        def extract(self, image, output_format=\"markdown\"):\n            # Run extraction\n            return TextOutput(...)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.BaseTextExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format or output_format is not supported</p> <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        ValueError: If image format or output_format is not supported\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.DotsOCRTextExtractor","title":"DotsOCRTextExtractor","text":"<pre><code>DotsOCRTextExtractor(backend: DotsOCRBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Dots OCR Vision-Language Model text extractor with layout detection.</p> <p>Extracts text from document images with layout information including: - 11 layout categories (Caption, Footnote, Formula, List-item, etc.) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown, LaTeX, HTML) - Reading order preservation</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\n\n# Extract with layout\nresult = extractor.extract(image, include_layout=True)\nprint(f\"Found {result.num_layout_elements} elements\")\nprint(result.content)\n</code></pre> <p>Initialize Dots OCR text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend - DotsOCRVLLMConfig: VLLM high-throughput backend - DotsOCRAPIConfig: API backend (online VLLM server)</p> <p> TYPE: <code>DotsOCRBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def __init__(self, backend: DotsOCRBackendConfig):\n    \"\"\"\n    Initialize Dots OCR text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend\n            - DotsOCRVLLMConfig: VLLM high-throughput backend\n            - DotsOCRAPIConfig: API backend (online VLLM server)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._model: Any = None\n    self._loaded = False\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.DotsOCRTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\n        \"markdown\", \"html\", \"json\"\n    ] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput\n</code></pre> <p>Extract text from image using Dots OCR.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\", \"html\", or \"json\")</p> <p> TYPE: <code>Literal['markdown', 'html', 'json']</code> DEFAULT: <code>'markdown'</code> </p> <code>include_layout</code> <p>Include layout bounding boxes in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>custom_prompt</code> <p>Override default extraction prompt</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_tokens</code> <p>Maximum tokens for generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>8192</code> </p> RETURNS DESCRIPTION <code>DotsOCRTextOutput</code> <p>DotsOCRTextOutput with extracted content and optional layout</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"markdown\", \"html\", \"json\"] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput:\n    \"\"\"\n    Extract text from image using Dots OCR.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\", \"html\", or \"json\")\n        include_layout: Include layout bounding boxes in output\n        custom_prompt: Override default extraction prompt\n        max_tokens: Maximum tokens for generation\n\n    Returns:\n        DotsOCRTextOutput with extracted content and optional layout\n\n    Raises:\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    img = self._prepare_image(image)\n\n    # Get prompt\n    prompt = custom_prompt or DOTS_OCR_PROMPT\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"DotsOCRPyTorchConfig\":\n        raw_output = self._infer_pytorch(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRVLLMConfig\":\n        raw_output = self._infer_vllm(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRAPIConfig\":\n        raw_output = self._infer_api(img, prompt, max_tokens)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse output\n    return self._parse_output(\n        raw_output,\n        img.size,\n        output_format,\n        include_layout,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.DotsOCRTextOutput","title":"DotsOCRTextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text extraction output from Dots OCR with layout information.</p> <p>Dots OCR provides structured output with: - Layout detection (11 categories) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown/LaTeX/HTML) - Reading order preservation</p> Layout Categories <p>Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title</p> Text Formatting <ul> <li>Text/Title/Section-header: Markdown</li> <li>Formula: LaTeX</li> <li>Table: HTML</li> <li>Picture: (text omitted)</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nresult = extractor.extract(image, include_layout=True)\nprint(result.content)  # Full text with formatting\nfor elem in result.layout:\n        print(f\"{elem.category}: {elem.bbox}\")\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.DotsOCRTextOutput.num_layout_elements","title":"num_layout_elements  <code>property</code>","text":"<pre><code>num_layout_elements: int\n</code></pre> <p>Number of detected layout elements.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.DotsOCRTextOutput.content_length","title":"content_length  <code>property</code>","text":"<pre><code>content_length: int\n</code></pre> <p>Length of extracted content in characters.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.LayoutElement","title":"LayoutElement","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single layout element from document layout detection.</p> <p>Represents a detected region in the document with its bounding box, category label, and extracted text content.</p> ATTRIBUTE DESCRIPTION <code>bbox</code> <p>Bounding box coordinates [x1, y1, x2, y2] (normalized to 0-1024)</p> <p> TYPE: <code>List[int]</code> </p> <code>category</code> <p>Layout category (e.g., \"Text\", \"Title\", \"Table\", \"Formula\")</p> <p> TYPE: <code>str</code> </p> <code>text</code> <p>Extracted text content (None for pictures)</p> <p> TYPE: <code>Optional[str]</code> </p> <code>confidence</code> <p>Detection confidence score (optional)</p> <p> TYPE: <code>Optional[float]</code> </p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.OutputFormat","title":"OutputFormat","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported text extraction output formats.</p> Each format has different characteristics <ul> <li>HTML: Structured with div elements, preserves layout semantics</li> <li>MARKDOWN: Portable, human-readable, good for documentation</li> <li>JSON: Structured data with layout information (Dots OCR)</li> </ul>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.TextOutput","title":"TextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text extraction output from a document image.</p> <p>Contains the extracted text content in the requested format, along with optional raw output and plain text versions.</p> Example <pre><code>result = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)  # Clean markdown\nprint(result.plain_text)  # Plain text without formatting\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.TextOutput.content_length","title":"content_length  <code>property</code>","text":"<pre><code>content_length: int\n</code></pre> <p>Length of the extracted content in characters.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.TextOutput.word_count","title":"word_count  <code>property</code>","text":"<pre><code>word_count: int\n</code></pre> <p>Approximate word count of the plain text.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.QwenTextExtractor","title":"QwenTextExtractor","text":"<pre><code>QwenTextExtractor(backend: QwenTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Qwen3-VL Vision-Language Model text extractor.</p> <p>Extracts text from document images and outputs as structured HTML or Markdown. Uses Qwen3-VL's built-in document parsing prompts.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Extract as Markdown\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n\n# Extract as HTML\nresult = extractor.extract(image, output_format=\"html\")\nprint(result.content)\n</code></pre> <p>Initialize Qwen text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenTextPyTorchConfig: PyTorch/HuggingFace backend - QwenTextVLLMConfig: VLLM high-throughput backend - QwenTextMLXConfig: MLX backend for Apple Silicon - QwenTextAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def __init__(self, backend: QwenTextBackendConfig):\n    \"\"\"\n    Initialize Qwen text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenTextPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenTextVLLMConfig: VLLM high-throughput backend\n            - QwenTextMLXConfig: MLX backend for Apple Silicon\n            - QwenTextAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.QwenTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML with div elements - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format or output_format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML with div elements\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format or output_format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}. Expected 'html' or 'markdown'.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Get prompt for output format\n    prompt = QWEN_PROMPTS[output_format]\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenTextAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    if output_format == \"html\":\n        cleaned_output = _clean_html_output(raw_output)\n    else:\n        cleaned_output = _clean_markdown_output(raw_output)\n\n    # Extract plain text\n    plain_text = _extract_plain_text(raw_output, output_format)\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=plain_text,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.base","title":"base","text":"<p>Base class for text extractors.</p> <p>Defines the abstract interface that all text extractors must implement.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.base.BaseTextExtractor","title":"BaseTextExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for text extractors.</p> <p>All text extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyTextExtractor(BaseTextExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Load model weights\n            pass\n\n        def extract(self, image, output_format=\"markdown\"):\n            # Run extraction\n            return TextOutput(...)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.base.BaseTextExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format or output_format is not supported</p> <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        ValueError: If image format or output_format is not supported\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr","title":"dotsocr","text":"<p>Dots OCR text extractor and backend configurations.</p> <p>Available backends: - PyTorch: DotsOCRPyTorchConfig (local GPU inference) - VLLM: DotsOCRVLLMConfig (offline batch inference) - API: DotsOCRAPIConfig (online VLLM server via OpenAI-compatible API)</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRAPIConfig","title":"DotsOCRAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Dots OCR.</p> <p>This config is for accessing a deployed VLLM server via OpenAI-compatible API. Typically used with modal_dotsocr_vllm_online.py deployment.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRAPIConfig\n\nconfig = DotsOCRAPIConfig(\n        model=\"dotsocr\",\n        api_base=\"https://your-modal-app.modal.run/v1\",\n        api_key=\"optional-key\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRTextExtractor","title":"DotsOCRTextExtractor","text":"<pre><code>DotsOCRTextExtractor(backend: DotsOCRBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Dots OCR Vision-Language Model text extractor with layout detection.</p> <p>Extracts text from document images with layout information including: - 11 layout categories (Caption, Footnote, Formula, List-item, etc.) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown, LaTeX, HTML) - Reading order preservation</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\n\n# Extract with layout\nresult = extractor.extract(image, include_layout=True)\nprint(f\"Found {result.num_layout_elements} elements\")\nprint(result.content)\n</code></pre> <p>Initialize Dots OCR text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend - DotsOCRVLLMConfig: VLLM high-throughput backend - DotsOCRAPIConfig: API backend (online VLLM server)</p> <p> TYPE: <code>DotsOCRBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def __init__(self, backend: DotsOCRBackendConfig):\n    \"\"\"\n    Initialize Dots OCR text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend\n            - DotsOCRVLLMConfig: VLLM high-throughput backend\n            - DotsOCRAPIConfig: API backend (online VLLM server)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._model: Any = None\n    self._loaded = False\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\n        \"markdown\", \"html\", \"json\"\n    ] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput\n</code></pre> <p>Extract text from image using Dots OCR.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\", \"html\", or \"json\")</p> <p> TYPE: <code>Literal['markdown', 'html', 'json']</code> DEFAULT: <code>'markdown'</code> </p> <code>include_layout</code> <p>Include layout bounding boxes in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>custom_prompt</code> <p>Override default extraction prompt</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_tokens</code> <p>Maximum tokens for generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>8192</code> </p> RETURNS DESCRIPTION <code>DotsOCRTextOutput</code> <p>DotsOCRTextOutput with extracted content and optional layout</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"markdown\", \"html\", \"json\"] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput:\n    \"\"\"\n    Extract text from image using Dots OCR.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\", \"html\", or \"json\")\n        include_layout: Include layout bounding boxes in output\n        custom_prompt: Override default extraction prompt\n        max_tokens: Maximum tokens for generation\n\n    Returns:\n        DotsOCRTextOutput with extracted content and optional layout\n\n    Raises:\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    img = self._prepare_image(image)\n\n    # Get prompt\n    prompt = custom_prompt or DOTS_OCR_PROMPT\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"DotsOCRPyTorchConfig\":\n        raw_output = self._infer_pytorch(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRVLLMConfig\":\n        raw_output = self._infer_vllm(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRAPIConfig\":\n        raw_output = self._infer_api(img, prompt, max_tokens)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse output\n    return self._parse_output(\n        raw_output,\n        img.size,\n        output_format,\n        include_layout,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRPyTorchConfig","title":"DotsOCRPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Dots OCR.</p> <p>Dots OCR provides layout-aware text extraction with 11 predefined layout categories (Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title).</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nconfig = DotsOCRPyTorchConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRVLLMConfig","title":"DotsOCRVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Dots OCR.</p> <p>VLLM provides high-throughput inference with optimizations like: - PagedAttention for efficient KV cache management - Continuous batching for higher throughput - Optimized CUDA kernels</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nconfig = DotsOCRVLLMConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.api","title":"api","text":"<p>API backend configuration for Dots OCR (VLLM online server).</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.api.DotsOCRAPIConfig","title":"DotsOCRAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Dots OCR.</p> <p>This config is for accessing a deployed VLLM server via OpenAI-compatible API. Typically used with modal_dotsocr_vllm_online.py deployment.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRAPIConfig\n\nconfig = DotsOCRAPIConfig(\n        model=\"dotsocr\",\n        api_base=\"https://your-modal-app.modal.run/v1\",\n        api_key=\"optional-key\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.extractor","title":"extractor","text":"<p>Dots OCR text extractor with layout-aware extraction.</p> <p>A Vision-Language Model optimized for document OCR with structured output containing layout information, bounding boxes, and multi-format text.</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\nresult = extractor.extract(image, include_layout=True)\nprint(result.content)\nfor elem in result.layout:\n        print(f\"{elem.category}: {elem.bbox}\")\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.extractor.DotsOCRTextExtractor","title":"DotsOCRTextExtractor","text":"<pre><code>DotsOCRTextExtractor(backend: DotsOCRBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Dots OCR Vision-Language Model text extractor with layout detection.</p> <p>Extracts text from document images with layout information including: - 11 layout categories (Caption, Footnote, Formula, List-item, etc.) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown, LaTeX, HTML) - Reading order preservation</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\n\n# Extract with layout\nresult = extractor.extract(image, include_layout=True)\nprint(f\"Found {result.num_layout_elements} elements\")\nprint(result.content)\n</code></pre> <p>Initialize Dots OCR text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend - DotsOCRVLLMConfig: VLLM high-throughput backend - DotsOCRAPIConfig: API backend (online VLLM server)</p> <p> TYPE: <code>DotsOCRBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def __init__(self, backend: DotsOCRBackendConfig):\n    \"\"\"\n    Initialize Dots OCR text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend\n            - DotsOCRVLLMConfig: VLLM high-throughput backend\n            - DotsOCRAPIConfig: API backend (online VLLM server)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._model: Any = None\n    self._loaded = False\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.extractor.DotsOCRTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\n        \"markdown\", \"html\", \"json\"\n    ] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput\n</code></pre> <p>Extract text from image using Dots OCR.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\", \"html\", or \"json\")</p> <p> TYPE: <code>Literal['markdown', 'html', 'json']</code> DEFAULT: <code>'markdown'</code> </p> <code>include_layout</code> <p>Include layout bounding boxes in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>custom_prompt</code> <p>Override default extraction prompt</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_tokens</code> <p>Maximum tokens for generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>8192</code> </p> RETURNS DESCRIPTION <code>DotsOCRTextOutput</code> <p>DotsOCRTextOutput with extracted content and optional layout</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"markdown\", \"html\", \"json\"] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput:\n    \"\"\"\n    Extract text from image using Dots OCR.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\", \"html\", or \"json\")\n        include_layout: Include layout bounding boxes in output\n        custom_prompt: Override default extraction prompt\n        max_tokens: Maximum tokens for generation\n\n    Returns:\n        DotsOCRTextOutput with extracted content and optional layout\n\n    Raises:\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    img = self._prepare_image(image)\n\n    # Get prompt\n    prompt = custom_prompt or DOTS_OCR_PROMPT\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"DotsOCRPyTorchConfig\":\n        raw_output = self._infer_pytorch(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRVLLMConfig\":\n        raw_output = self._infer_vllm(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRAPIConfig\":\n        raw_output = self._infer_api(img, prompt, max_tokens)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse output\n    return self._parse_output(\n        raw_output,\n        img.size,\n        output_format,\n        include_layout,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.pytorch","title":"pytorch","text":"<p>PyTorch backend configuration for Dots OCR.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.pytorch.DotsOCRPyTorchConfig","title":"DotsOCRPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Dots OCR.</p> <p>Dots OCR provides layout-aware text extraction with 11 predefined layout categories (Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title).</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nconfig = DotsOCRPyTorchConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.vllm","title":"vllm","text":"<p>VLLM backend configuration for Dots OCR.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.vllm.DotsOCRVLLMConfig","title":"DotsOCRVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Dots OCR.</p> <p>VLLM provides high-throughput inference with optimizations like: - PagedAttention for efficient KV cache management - Continuous batching for higher throughput - Optimized CUDA kernels</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nconfig = DotsOCRVLLMConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.models","title":"models","text":"<p>Pydantic models for text extraction outputs.</p> <p>Defines output types and format enums for text extraction.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.models.OutputFormat","title":"OutputFormat","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported text extraction output formats.</p> Each format has different characteristics <ul> <li>HTML: Structured with div elements, preserves layout semantics</li> <li>MARKDOWN: Portable, human-readable, good for documentation</li> <li>JSON: Structured data with layout information (Dots OCR)</li> </ul>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.models.TextOutput","title":"TextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text extraction output from a document image.</p> <p>Contains the extracted text content in the requested format, along with optional raw output and plain text versions.</p> Example <pre><code>result = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)  # Clean markdown\nprint(result.plain_text)  # Plain text without formatting\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.models.TextOutput.content_length","title":"content_length  <code>property</code>","text":"<pre><code>content_length: int\n</code></pre> <p>Length of the extracted content in characters.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.models.TextOutput.word_count","title":"word_count  <code>property</code>","text":"<pre><code>word_count: int\n</code></pre> <p>Approximate word count of the plain text.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.models.LayoutElement","title":"LayoutElement","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single layout element from document layout detection.</p> <p>Represents a detected region in the document with its bounding box, category label, and extracted text content.</p> ATTRIBUTE DESCRIPTION <code>bbox</code> <p>Bounding box coordinates [x1, y1, x2, y2] (normalized to 0-1024)</p> <p> TYPE: <code>List[int]</code> </p> <code>category</code> <p>Layout category (e.g., \"Text\", \"Title\", \"Table\", \"Formula\")</p> <p> TYPE: <code>str</code> </p> <code>text</code> <p>Extracted text content (None for pictures)</p> <p> TYPE: <code>Optional[str]</code> </p> <code>confidence</code> <p>Detection confidence score (optional)</p> <p> TYPE: <code>Optional[float]</code> </p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.models.DotsOCRTextOutput","title":"DotsOCRTextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text extraction output from Dots OCR with layout information.</p> <p>Dots OCR provides structured output with: - Layout detection (11 categories) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown/LaTeX/HTML) - Reading order preservation</p> Layout Categories <p>Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title</p> Text Formatting <ul> <li>Text/Title/Section-header: Markdown</li> <li>Formula: LaTeX</li> <li>Table: HTML</li> <li>Picture: (text omitted)</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nresult = extractor.extract(image, include_layout=True)\nprint(result.content)  # Full text with formatting\nfor elem in result.layout:\n        print(f\"{elem.category}: {elem.bbox}\")\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.models.DotsOCRTextOutput.num_layout_elements","title":"num_layout_elements  <code>property</code>","text":"<pre><code>num_layout_elements: int\n</code></pre> <p>Number of detected layout elements.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.models.DotsOCRTextOutput.content_length","title":"content_length  <code>property</code>","text":"<pre><code>content_length: int\n</code></pre> <p>Length of extracted content in characters.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen","title":"qwen","text":"<p>Qwen3-VL backend configurations and extractor for text extraction.</p> Available backends <ul> <li>QwenTextPyTorchConfig: PyTorch/HuggingFace backend</li> <li>QwenTextVLLMConfig: VLLM high-throughput backend</li> <li>QwenTextMLXConfig: MLX backend for Apple Silicon</li> <li>QwenTextAPIConfig: API backend (OpenRouter, etc.)</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nconfig = QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextAPIConfig","title":"QwenTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen text extraction.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenTextAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextExtractor","title":"QwenTextExtractor","text":"<pre><code>QwenTextExtractor(backend: QwenTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Qwen3-VL Vision-Language Model text extractor.</p> <p>Extracts text from document images and outputs as structured HTML or Markdown. Uses Qwen3-VL's built-in document parsing prompts.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Extract as Markdown\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n\n# Extract as HTML\nresult = extractor.extract(image, output_format=\"html\")\nprint(result.content)\n</code></pre> <p>Initialize Qwen text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenTextPyTorchConfig: PyTorch/HuggingFace backend - QwenTextVLLMConfig: VLLM high-throughput backend - QwenTextMLXConfig: MLX backend for Apple Silicon - QwenTextAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def __init__(self, backend: QwenTextBackendConfig):\n    \"\"\"\n    Initialize Qwen text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenTextPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenTextVLLMConfig: VLLM high-throughput backend\n            - QwenTextMLXConfig: MLX backend for Apple Silicon\n            - QwenTextAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML with div elements - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format or output_format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML with div elements\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format or output_format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}. Expected 'html' or 'markdown'.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Get prompt for output format\n    prompt = QWEN_PROMPTS[output_format]\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenTextAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    if output_format == \"html\":\n        cleaned_output = _clean_html_output(raw_output)\n    else:\n        cleaned_output = _clean_markdown_output(raw_output)\n\n    # Extract plain text\n    plain_text = _extract_plain_text(raw_output, output_format)\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=plain_text,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextMLXConfig","title":"QwenTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenTextMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextPyTorchConfig","title":"QwenTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextVLLMConfig","title":"QwenTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenTextVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.api","title":"api","text":"<p>API backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.api.QwenTextAPIConfig","title":"QwenTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen text extraction.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenTextAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.extractor","title":"extractor","text":"<p>Qwen3-VL text extractor.</p> <p>A Vision-Language Model for extracting text from document images as structured HTML or Markdown.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.extractor.QwenTextExtractor","title":"QwenTextExtractor","text":"<pre><code>QwenTextExtractor(backend: QwenTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Qwen3-VL Vision-Language Model text extractor.</p> <p>Extracts text from document images and outputs as structured HTML or Markdown. Uses Qwen3-VL's built-in document parsing prompts.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Extract as Markdown\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n\n# Extract as HTML\nresult = extractor.extract(image, output_format=\"html\")\nprint(result.content)\n</code></pre> <p>Initialize Qwen text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenTextPyTorchConfig: PyTorch/HuggingFace backend - QwenTextVLLMConfig: VLLM high-throughput backend - QwenTextMLXConfig: MLX backend for Apple Silicon - QwenTextAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def __init__(self, backend: QwenTextBackendConfig):\n    \"\"\"\n    Initialize Qwen text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenTextPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenTextVLLMConfig: VLLM high-throughput backend\n            - QwenTextMLXConfig: MLX backend for Apple Silicon\n            - QwenTextAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.extractor.QwenTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML with div elements - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format or output_format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML with div elements\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format or output_format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}. Expected 'html' or 'markdown'.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Get prompt for output format\n    prompt = QWEN_PROMPTS[output_format]\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenTextAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    if output_format == \"html\":\n        cleaned_output = _clean_html_output(raw_output)\n    else:\n        cleaned_output = _clean_markdown_output(raw_output)\n\n    # Extract plain text\n    plain_text = _extract_plain_text(raw_output, output_format)\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=plain_text,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.mlx","title":"mlx","text":"<p>MLX backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.mlx.QwenTextMLXConfig","title":"QwenTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenTextMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.pytorch","title":"pytorch","text":"<p>PyTorch/HuggingFace backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.pytorch.QwenTextPyTorchConfig","title":"QwenTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.vllm","title":"vllm","text":"<p>VLLM backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.vllm.QwenTextVLLMConfig","title":"QwenTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenTextVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/api/","title":"API","text":"<p>API backend configuration for Dots OCR (VLLM online server).</p>"},{"location":"reference/tasks/text_extraction/dots_ocr/api/#omnidocs.tasks.text_extraction.dotsocr.api.DotsOCRAPIConfig","title":"DotsOCRAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Dots OCR.</p> <p>This config is for accessing a deployed VLLM server via OpenAI-compatible API. Typically used with modal_dotsocr_vllm_online.py deployment.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRAPIConfig\n\nconfig = DotsOCRAPIConfig(\n        model=\"dotsocr\",\n        api_base=\"https://your-modal-app.modal.run/v1\",\n        api_key=\"optional-key\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/extractor/","title":"Extractor","text":"<p>Dots OCR text extractor with layout-aware extraction.</p> <p>A Vision-Language Model optimized for document OCR with structured output containing layout information, bounding boxes, and multi-format text.</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\nresult = extractor.extract(image, include_layout=True)\nprint(result.content)\nfor elem in result.layout:\n        print(f\"{elem.category}: {elem.bbox}\")\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/extractor/#omnidocs.tasks.text_extraction.dotsocr.extractor.DotsOCRTextExtractor","title":"DotsOCRTextExtractor","text":"<pre><code>DotsOCRTextExtractor(backend: DotsOCRBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Dots OCR Vision-Language Model text extractor with layout detection.</p> <p>Extracts text from document images with layout information including: - 11 layout categories (Caption, Footnote, Formula, List-item, etc.) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown, LaTeX, HTML) - Reading order preservation</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\n\n# Extract with layout\nresult = extractor.extract(image, include_layout=True)\nprint(f\"Found {result.num_layout_elements} elements\")\nprint(result.content)\n</code></pre> <p>Initialize Dots OCR text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend - DotsOCRVLLMConfig: VLLM high-throughput backend - DotsOCRAPIConfig: API backend (online VLLM server)</p> <p> TYPE: <code>DotsOCRBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def __init__(self, backend: DotsOCRBackendConfig):\n    \"\"\"\n    Initialize Dots OCR text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend\n            - DotsOCRVLLMConfig: VLLM high-throughput backend\n            - DotsOCRAPIConfig: API backend (online VLLM server)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._model: Any = None\n    self._loaded = False\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/extractor/#omnidocs.tasks.text_extraction.dotsocr.extractor.DotsOCRTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\n        \"markdown\", \"html\", \"json\"\n    ] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput\n</code></pre> <p>Extract text from image using Dots OCR.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\", \"html\", or \"json\")</p> <p> TYPE: <code>Literal['markdown', 'html', 'json']</code> DEFAULT: <code>'markdown'</code> </p> <code>include_layout</code> <p>Include layout bounding boxes in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>custom_prompt</code> <p>Override default extraction prompt</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_tokens</code> <p>Maximum tokens for generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>8192</code> </p> RETURNS DESCRIPTION <code>DotsOCRTextOutput</code> <p>DotsOCRTextOutput with extracted content and optional layout</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"markdown\", \"html\", \"json\"] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput:\n    \"\"\"\n    Extract text from image using Dots OCR.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\", \"html\", or \"json\")\n        include_layout: Include layout bounding boxes in output\n        custom_prompt: Override default extraction prompt\n        max_tokens: Maximum tokens for generation\n\n    Returns:\n        DotsOCRTextOutput with extracted content and optional layout\n\n    Raises:\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    img = self._prepare_image(image)\n\n    # Get prompt\n    prompt = custom_prompt or DOTS_OCR_PROMPT\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"DotsOCRPyTorchConfig\":\n        raw_output = self._infer_pytorch(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRVLLMConfig\":\n        raw_output = self._infer_vllm(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRAPIConfig\":\n        raw_output = self._infer_api(img, prompt, max_tokens)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse output\n    return self._parse_output(\n        raw_output,\n        img.size,\n        output_format,\n        include_layout,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/","title":"Overview","text":"<p>Dots OCR text extractor and backend configurations.</p> <p>Available backends: - PyTorch: DotsOCRPyTorchConfig (local GPU inference) - VLLM: DotsOCRVLLMConfig (offline batch inference) - API: DotsOCRAPIConfig (online VLLM server via OpenAI-compatible API)</p>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRAPIConfig","title":"DotsOCRAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Dots OCR.</p> <p>This config is for accessing a deployed VLLM server via OpenAI-compatible API. Typically used with modal_dotsocr_vllm_online.py deployment.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRAPIConfig\n\nconfig = DotsOCRAPIConfig(\n        model=\"dotsocr\",\n        api_base=\"https://your-modal-app.modal.run/v1\",\n        api_key=\"optional-key\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRTextExtractor","title":"DotsOCRTextExtractor","text":"<pre><code>DotsOCRTextExtractor(backend: DotsOCRBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Dots OCR Vision-Language Model text extractor with layout detection.</p> <p>Extracts text from document images with layout information including: - 11 layout categories (Caption, Footnote, Formula, List-item, etc.) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown, LaTeX, HTML) - Reading order preservation</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\n\n# Extract with layout\nresult = extractor.extract(image, include_layout=True)\nprint(f\"Found {result.num_layout_elements} elements\")\nprint(result.content)\n</code></pre> <p>Initialize Dots OCR text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend - DotsOCRVLLMConfig: VLLM high-throughput backend - DotsOCRAPIConfig: API backend (online VLLM server)</p> <p> TYPE: <code>DotsOCRBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def __init__(self, backend: DotsOCRBackendConfig):\n    \"\"\"\n    Initialize Dots OCR text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend\n            - DotsOCRVLLMConfig: VLLM high-throughput backend\n            - DotsOCRAPIConfig: API backend (online VLLM server)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._model: Any = None\n    self._loaded = False\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\n        \"markdown\", \"html\", \"json\"\n    ] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput\n</code></pre> <p>Extract text from image using Dots OCR.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\", \"html\", or \"json\")</p> <p> TYPE: <code>Literal['markdown', 'html', 'json']</code> DEFAULT: <code>'markdown'</code> </p> <code>include_layout</code> <p>Include layout bounding boxes in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>custom_prompt</code> <p>Override default extraction prompt</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_tokens</code> <p>Maximum tokens for generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>8192</code> </p> RETURNS DESCRIPTION <code>DotsOCRTextOutput</code> <p>DotsOCRTextOutput with extracted content and optional layout</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"markdown\", \"html\", \"json\"] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput:\n    \"\"\"\n    Extract text from image using Dots OCR.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\", \"html\", or \"json\")\n        include_layout: Include layout bounding boxes in output\n        custom_prompt: Override default extraction prompt\n        max_tokens: Maximum tokens for generation\n\n    Returns:\n        DotsOCRTextOutput with extracted content and optional layout\n\n    Raises:\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    img = self._prepare_image(image)\n\n    # Get prompt\n    prompt = custom_prompt or DOTS_OCR_PROMPT\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"DotsOCRPyTorchConfig\":\n        raw_output = self._infer_pytorch(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRVLLMConfig\":\n        raw_output = self._infer_vllm(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRAPIConfig\":\n        raw_output = self._infer_api(img, prompt, max_tokens)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse output\n    return self._parse_output(\n        raw_output,\n        img.size,\n        output_format,\n        include_layout,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRPyTorchConfig","title":"DotsOCRPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Dots OCR.</p> <p>Dots OCR provides layout-aware text extraction with 11 predefined layout categories (Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title).</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nconfig = DotsOCRPyTorchConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRVLLMConfig","title":"DotsOCRVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Dots OCR.</p> <p>VLLM provides high-throughput inference with optimizations like: - PagedAttention for efficient KV cache management - Continuous batching for higher throughput - Optimized CUDA kernels</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nconfig = DotsOCRVLLMConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.api","title":"api","text":"<p>API backend configuration for Dots OCR (VLLM online server).</p>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.api.DotsOCRAPIConfig","title":"DotsOCRAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Dots OCR.</p> <p>This config is for accessing a deployed VLLM server via OpenAI-compatible API. Typically used with modal_dotsocr_vllm_online.py deployment.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRAPIConfig\n\nconfig = DotsOCRAPIConfig(\n        model=\"dotsocr\",\n        api_base=\"https://your-modal-app.modal.run/v1\",\n        api_key=\"optional-key\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.extractor","title":"extractor","text":"<p>Dots OCR text extractor with layout-aware extraction.</p> <p>A Vision-Language Model optimized for document OCR with structured output containing layout information, bounding boxes, and multi-format text.</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\nresult = extractor.extract(image, include_layout=True)\nprint(result.content)\nfor elem in result.layout:\n        print(f\"{elem.category}: {elem.bbox}\")\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.extractor.DotsOCRTextExtractor","title":"DotsOCRTextExtractor","text":"<pre><code>DotsOCRTextExtractor(backend: DotsOCRBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Dots OCR Vision-Language Model text extractor with layout detection.</p> <p>Extracts text from document images with layout information including: - 11 layout categories (Caption, Footnote, Formula, List-item, etc.) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown, LaTeX, HTML) - Reading order preservation</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\n\n# Extract with layout\nresult = extractor.extract(image, include_layout=True)\nprint(f\"Found {result.num_layout_elements} elements\")\nprint(result.content)\n</code></pre> <p>Initialize Dots OCR text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend - DotsOCRVLLMConfig: VLLM high-throughput backend - DotsOCRAPIConfig: API backend (online VLLM server)</p> <p> TYPE: <code>DotsOCRBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def __init__(self, backend: DotsOCRBackendConfig):\n    \"\"\"\n    Initialize Dots OCR text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend\n            - DotsOCRVLLMConfig: VLLM high-throughput backend\n            - DotsOCRAPIConfig: API backend (online VLLM server)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._model: Any = None\n    self._loaded = False\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.extractor.DotsOCRTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\n        \"markdown\", \"html\", \"json\"\n    ] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput\n</code></pre> <p>Extract text from image using Dots OCR.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\", \"html\", or \"json\")</p> <p> TYPE: <code>Literal['markdown', 'html', 'json']</code> DEFAULT: <code>'markdown'</code> </p> <code>include_layout</code> <p>Include layout bounding boxes in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>custom_prompt</code> <p>Override default extraction prompt</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_tokens</code> <p>Maximum tokens for generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>8192</code> </p> RETURNS DESCRIPTION <code>DotsOCRTextOutput</code> <p>DotsOCRTextOutput with extracted content and optional layout</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"markdown\", \"html\", \"json\"] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput:\n    \"\"\"\n    Extract text from image using Dots OCR.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\", \"html\", or \"json\")\n        include_layout: Include layout bounding boxes in output\n        custom_prompt: Override default extraction prompt\n        max_tokens: Maximum tokens for generation\n\n    Returns:\n        DotsOCRTextOutput with extracted content and optional layout\n\n    Raises:\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    img = self._prepare_image(image)\n\n    # Get prompt\n    prompt = custom_prompt or DOTS_OCR_PROMPT\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"DotsOCRPyTorchConfig\":\n        raw_output = self._infer_pytorch(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRVLLMConfig\":\n        raw_output = self._infer_vllm(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRAPIConfig\":\n        raw_output = self._infer_api(img, prompt, max_tokens)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse output\n    return self._parse_output(\n        raw_output,\n        img.size,\n        output_format,\n        include_layout,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.pytorch","title":"pytorch","text":"<p>PyTorch backend configuration for Dots OCR.</p>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.pytorch.DotsOCRPyTorchConfig","title":"DotsOCRPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Dots OCR.</p> <p>Dots OCR provides layout-aware text extraction with 11 predefined layout categories (Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title).</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nconfig = DotsOCRPyTorchConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.vllm","title":"vllm","text":"<p>VLLM backend configuration for Dots OCR.</p>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.vllm.DotsOCRVLLMConfig","title":"DotsOCRVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Dots OCR.</p> <p>VLLM provides high-throughput inference with optimizations like: - PagedAttention for efficient KV cache management - Continuous batching for higher throughput - Optimized CUDA kernels</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nconfig = DotsOCRVLLMConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/pytorch/","title":"PyTorch","text":"<p>PyTorch backend configuration for Dots OCR.</p>"},{"location":"reference/tasks/text_extraction/dots_ocr/pytorch/#omnidocs.tasks.text_extraction.dotsocr.pytorch.DotsOCRPyTorchConfig","title":"DotsOCRPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Dots OCR.</p> <p>Dots OCR provides layout-aware text extraction with 11 predefined layout categories (Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title).</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nconfig = DotsOCRPyTorchConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/vllm/","title":"VLLM","text":"<p>VLLM backend configuration for Dots OCR.</p>"},{"location":"reference/tasks/text_extraction/dots_ocr/vllm/#omnidocs.tasks.text_extraction.dotsocr.vllm.DotsOCRVLLMConfig","title":"DotsOCRVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Dots OCR.</p> <p>VLLM provides high-throughput inference with optimizations like: - PagedAttention for efficient KV cache management - Continuous batching for higher throughput - Optimized CUDA kernels</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nconfig = DotsOCRVLLMConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/api/","title":"API","text":"<p>API backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/qwen/api/#omnidocs.tasks.text_extraction.qwen.api.QwenTextAPIConfig","title":"QwenTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen text extraction.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenTextAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/extractor/","title":"Extractor","text":"<p>Qwen3-VL text extractor.</p> <p>A Vision-Language Model for extracting text from document images as structured HTML or Markdown.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/extractor/#omnidocs.tasks.text_extraction.qwen.extractor.QwenTextExtractor","title":"QwenTextExtractor","text":"<pre><code>QwenTextExtractor(backend: QwenTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Qwen3-VL Vision-Language Model text extractor.</p> <p>Extracts text from document images and outputs as structured HTML or Markdown. Uses Qwen3-VL's built-in document parsing prompts.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Extract as Markdown\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n\n# Extract as HTML\nresult = extractor.extract(image, output_format=\"html\")\nprint(result.content)\n</code></pre> <p>Initialize Qwen text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenTextPyTorchConfig: PyTorch/HuggingFace backend - QwenTextVLLMConfig: VLLM high-throughput backend - QwenTextMLXConfig: MLX backend for Apple Silicon - QwenTextAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def __init__(self, backend: QwenTextBackendConfig):\n    \"\"\"\n    Initialize Qwen text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenTextPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenTextVLLMConfig: VLLM high-throughput backend\n            - QwenTextMLXConfig: MLX backend for Apple Silicon\n            - QwenTextAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/extractor/#omnidocs.tasks.text_extraction.qwen.extractor.QwenTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML with div elements - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format or output_format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML with div elements\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format or output_format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}. Expected 'html' or 'markdown'.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Get prompt for output format\n    prompt = QWEN_PROMPTS[output_format]\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenTextAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    if output_format == \"html\":\n        cleaned_output = _clean_html_output(raw_output)\n    else:\n        cleaned_output = _clean_markdown_output(raw_output)\n\n    # Extract plain text\n    plain_text = _extract_plain_text(raw_output, output_format)\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=plain_text,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/mlx/","title":"MLX","text":"<p>MLX backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/qwen/mlx/#omnidocs.tasks.text_extraction.qwen.mlx.QwenTextMLXConfig","title":"QwenTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenTextMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/","title":"Overview","text":"<p>Qwen3-VL backend configurations and extractor for text extraction.</p> Available backends <ul> <li>QwenTextPyTorchConfig: PyTorch/HuggingFace backend</li> <li>QwenTextVLLMConfig: VLLM high-throughput backend</li> <li>QwenTextMLXConfig: MLX backend for Apple Silicon</li> <li>QwenTextAPIConfig: API backend (OpenRouter, etc.)</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nconfig = QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextAPIConfig","title":"QwenTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen text extraction.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenTextAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextExtractor","title":"QwenTextExtractor","text":"<pre><code>QwenTextExtractor(backend: QwenTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Qwen3-VL Vision-Language Model text extractor.</p> <p>Extracts text from document images and outputs as structured HTML or Markdown. Uses Qwen3-VL's built-in document parsing prompts.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Extract as Markdown\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n\n# Extract as HTML\nresult = extractor.extract(image, output_format=\"html\")\nprint(result.content)\n</code></pre> <p>Initialize Qwen text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenTextPyTorchConfig: PyTorch/HuggingFace backend - QwenTextVLLMConfig: VLLM high-throughput backend - QwenTextMLXConfig: MLX backend for Apple Silicon - QwenTextAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def __init__(self, backend: QwenTextBackendConfig):\n    \"\"\"\n    Initialize Qwen text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenTextPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenTextVLLMConfig: VLLM high-throughput backend\n            - QwenTextMLXConfig: MLX backend for Apple Silicon\n            - QwenTextAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML with div elements - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format or output_format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML with div elements\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format or output_format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}. Expected 'html' or 'markdown'.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Get prompt for output format\n    prompt = QWEN_PROMPTS[output_format]\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenTextAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    if output_format == \"html\":\n        cleaned_output = _clean_html_output(raw_output)\n    else:\n        cleaned_output = _clean_markdown_output(raw_output)\n\n    # Extract plain text\n    plain_text = _extract_plain_text(raw_output, output_format)\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=plain_text,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextMLXConfig","title":"QwenTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenTextMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextPyTorchConfig","title":"QwenTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextVLLMConfig","title":"QwenTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenTextVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.api","title":"api","text":"<p>API backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.api.QwenTextAPIConfig","title":"QwenTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen text extraction.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenTextAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.extractor","title":"extractor","text":"<p>Qwen3-VL text extractor.</p> <p>A Vision-Language Model for extracting text from document images as structured HTML or Markdown.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.extractor.QwenTextExtractor","title":"QwenTextExtractor","text":"<pre><code>QwenTextExtractor(backend: QwenTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Qwen3-VL Vision-Language Model text extractor.</p> <p>Extracts text from document images and outputs as structured HTML or Markdown. Uses Qwen3-VL's built-in document parsing prompts.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Extract as Markdown\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n\n# Extract as HTML\nresult = extractor.extract(image, output_format=\"html\")\nprint(result.content)\n</code></pre> <p>Initialize Qwen text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenTextPyTorchConfig: PyTorch/HuggingFace backend - QwenTextVLLMConfig: VLLM high-throughput backend - QwenTextMLXConfig: MLX backend for Apple Silicon - QwenTextAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def __init__(self, backend: QwenTextBackendConfig):\n    \"\"\"\n    Initialize Qwen text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenTextPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenTextVLLMConfig: VLLM high-throughput backend\n            - QwenTextMLXConfig: MLX backend for Apple Silicon\n            - QwenTextAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.extractor.QwenTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML with div elements - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format or output_format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML with div elements\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format or output_format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}. Expected 'html' or 'markdown'.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Get prompt for output format\n    prompt = QWEN_PROMPTS[output_format]\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenTextAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    if output_format == \"html\":\n        cleaned_output = _clean_html_output(raw_output)\n    else:\n        cleaned_output = _clean_markdown_output(raw_output)\n\n    # Extract plain text\n    plain_text = _extract_plain_text(raw_output, output_format)\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=plain_text,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.mlx","title":"mlx","text":"<p>MLX backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.mlx.QwenTextMLXConfig","title":"QwenTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenTextMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.pytorch","title":"pytorch","text":"<p>PyTorch/HuggingFace backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.pytorch.QwenTextPyTorchConfig","title":"QwenTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.vllm","title":"vllm","text":"<p>VLLM backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.vllm.QwenTextVLLMConfig","title":"QwenTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenTextVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/pytorch/","title":"PyTorch","text":"<p>PyTorch/HuggingFace backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/qwen/pytorch/#omnidocs.tasks.text_extraction.qwen.pytorch.QwenTextPyTorchConfig","title":"QwenTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/vllm/","title":"VLLM","text":"<p>VLLM backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/qwen/vllm/#omnidocs.tasks.text_extraction.qwen.vllm.QwenTextVLLMConfig","title":"QwenTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenTextVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"}]}