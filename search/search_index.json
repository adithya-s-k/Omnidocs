{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"OmniDocs: Your Complete Toolkit for Intelligent Document Understanding","text":"<p>OmniDocs is a unified Python framework for intelligent document understanding. Extract text, tables, layout, and more from PDFs and images using state-of-the-art models\u2014all with a single, powerful API.</p>"},{"location":"index.html#key-features","title":"\u2728 Key Features","text":"<ul> <li>Unified API: One simple interface for every document AI task.</li> <li>Layout Detection: YOLO, Surya, PaddleOCR, and more.</li> <li>OCR Extraction: PaddleOCR, Tesseract, EasyOCR, Surya OCR.</li> <li>Text Extraction:   PyPDF2, PyMuPDF, pdfplumber, docling_parse, pdftext, Surya</li> <li>Math Expression Extraction:</li> <li>Donut: NAVER CLOVA Donut model for math/LaTeX extraction.</li> <li>Nougat: Facebook's Nougat model for LaTeX from academic documents.</li> <li>Surya Math: Surya-based mathematical expression extraction.</li> <li>UniMERNet: Universal Mathematical Expression Recognition Network.</li> <li>Table Extraction: Camelot, Tabula, PDFPlumber, Table Transformer, TableFormer, Surya Table</li> <li>Reading Order &amp; Structure: Advanced document parsing that just works.</li> <li>Multilingual: Supports 90+ languages out of the box.</li> <li>Extensible: Easily plug in your own models and build custom workflows.</li> </ul>"},{"location":"index.html#installation","title":"\u26a1 Installation","text":"<p>Install OmniDocs from PyPI with a single command:</p> <p>```bash pip install omnidocs ````</p> <p>For full setup (GPU, <code>conda</code>, <code>poetry</code>, etc.), check the Installation Guide.</p>"},{"location":"index.html#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>See the Quick Start Guide for a comprehensive introduction to using OmniDocs.</p>"},{"location":"index.html#the-arsenal-all-supported-backends","title":"\ud83d\udcda The Arsenal: All Supported Backends","text":"Layout Analysis Models (Click to Expand)  - DocLayout YOLO - PPStructure (Paddle OCR) - RT DETR (Docling) - Florence-2-DocLayNet - Surya Layout   Text Extraction Libraries (Click to Expand)  - PyPDF2 - PyMuPDF - pdfplumber - docling_parse - pdftext - surya_text   OCR Models (Click to Expand)  - Paddle OCR - Tesseract - EasyOCR - Surya OCR   Math Expression Extraction Models (Click to Expand)  - Donut - Nougat - Surya Math - UniMERNet   Table Extraction Models (Click to Expand)  - PPStructure (Paddle OCR) - Camelot - Tabula - PDFPlumber - Table Transformer - TableFormer - Surya Table"},{"location":"index.html#learn-more","title":"\ud83d\uddfa\ufe0f Learn More","text":"<p>Tutorials: Hands-on notebooks and guides for every task.  API Reference: The full dictionary of all public methods and classes.</p>"},{"location":"index.html#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Contributions are welcome! If you want to help make OmniDocs even better, see our CONTRIBUTING.md guide.</p>"},{"location":"index.html#license","title":"\ud83d\udee1\ufe0f License","text":"<p>The OmniDocs framework is MIT licensed. The underlying models and libraries may have their own licenses\u2014please verify before use in production.</p>"},{"location":"index.html#support-the-project","title":"\ud83c\udf1f Support the Project","text":"<p>If you find OmniDocs helpful, please \u2b50 the repo on GitHub!</p>"},{"location":"index.html#join-the-community","title":"\ud83d\udde8\ufe0f Join the Community","text":"<p>Issues: Report bugs or suggest features here Email: adithyaskolavi@gmail.com or laxmansrivastacc@gmail.com</p>"},{"location":"api_reference/index.html","title":"\ud83d\udcda API Reference","text":"<p>Welcome to the comprehensive OmniDocs API Reference! This documentation provides detailed information about all classes, functions, and modules in the OmniDocs ecosystem.</p>"},{"location":"api_reference/index.html#quick-navigation","title":"\ud83d\ude80 Quick Navigation","text":""},{"location":"api_reference/index.html#core-components","title":"\ud83e\udde9 Core Components","text":"<ul> <li>Core Classes - Base classes and fundamental components</li> <li>Utils - Utility functions and helpers</li> </ul>"},{"location":"api_reference/index.html#tasks-extractors","title":"\ud83d\udccb Tasks &amp; Extractors","text":"<ul> <li>Layout Analysis - Document structure detection</li> <li>Text Extraction - Text parsing from documents</li> <li>Table Extraction - Tabular data extraction</li> <li>OCR - Optical Character Recognition</li> <li>Math Expression - Mathematical formula extraction</li> </ul>"},{"location":"api_reference/index.html#getting-started-with-the-api","title":"\ud83c\udfaf Getting Started with the API","text":""},{"location":"api_reference/index.html#basic-usage-pattern","title":"Basic Usage Pattern","text":"<p>All OmniDocs extractors follow a consistent interface:</p> <pre><code># 1. Import the extractor\nfrom omnidocs.tasks.{task}.extractors.{extractor} import {ExtractorClass}\n\n# 2. Initialize with configuration\nextractor = ExtractorClass(\n    # Common parameters\n    device='cpu',           # or 'cuda' for GPU\n    show_log=True,         # Enable logging\n    languages=['en'],      # Supported languages\n    # Extractor-specific parameters...\n)\n\n# 3. Extract from document\nresult = extractor.extract(\"path/to/document.pdf\")\n\n# 4. Access results\nprint(result.full_text)    # For text-based results\nprint(result.tables)       # For table results\nprint(result.texts)        # For OCR results\n</code></pre>"},{"location":"api_reference/index.html#common-parameters","title":"Common Parameters","text":"<p>Most extractors support these common parameters:</p> Parameter Type Default Description <code>device</code> <code>str</code> <code>'cpu'</code> Device to run on ('cpu' or 'cuda') <code>show_log</code> <code>bool</code> <code>False</code> Enable detailed logging <code>languages</code> <code>List[str]</code> <code>['en']</code> Languages to support"},{"location":"api_reference/index.html#result-objects","title":"Result Objects","text":"<p>All extractors return structured result objects:</p>"},{"location":"api_reference/index.html#ocroutput","title":"OCROutput","text":"<pre><code>class OCROutput:\n    texts: List[OCRText]           # Individual text regions\n    full_text: str                 # Combined text\n    source_img_size: Tuple[int, int]  # Original image dimensions\n    processing_time: Optional[float]   # Extraction time\n    metadata: Dict[str, Any]       # Additional information\n</code></pre>"},{"location":"api_reference/index.html#tableoutput","title":"TableOutput","text":"<pre><code>class TableOutput:\n    tables: List[Table]            # Extracted tables\n    source_file: str              # Source document path\n    processing_time: Optional[float]  # Extraction time\n    metadata: Dict[str, Any]      # Additional information\n</code></pre>"},{"location":"api_reference/index.html#textoutput","title":"TextOutput","text":"<pre><code>class TextOutput:\n    text_blocks: List[TextBlock]   # Text blocks with positions\n    full_text: str                # Combined text\n    source_file: str              # Source document path\n    processing_time: Optional[float]  # Extraction time\n    metadata: Dict[str, Any]      # Additional information\n</code></pre>"},{"location":"api_reference/index.html#advanced-usage","title":"\ud83d\udd27 Advanced Usage","text":""},{"location":"api_reference/index.html#batch-processing","title":"Batch Processing","text":"<p>Process multiple documents efficiently:</p> <pre><code>from pathlib import Path\nfrom omnidocs.tasks.ocr_extraction.extractors.easy_ocr import EasyOCRExtractor\n\nextractor = EasyOCRExtractor()\ndocuments = Path(\"documents/\").glob(\"*.pdf\")\n\nresults = []\nfor doc in documents:\n    try:\n        result = extractor.extract(str(doc))\n        results.append({\n            'file': doc.name,\n            'text': result.full_text,\n            'confidence': sum(t.confidence for t in result.texts) / len(result.texts)\n        })\n    except Exception as e:\n        print(f\"Error processing {doc}: {e}\")\n</code></pre>"},{"location":"api_reference/index.html#custom-configuration","title":"Custom Configuration","text":"<p>Configure extractors for specific use cases:</p> <pre><code># High-accuracy OCR setup\nocr_extractor = EasyOCRExtractor(\n    languages=['en', 'fr', 'de'],\n    device='cuda',\n    show_log=True\n)\n\n# Fast table extraction setup\ntable_extractor = CamelotExtractor(\n    flavor='stream',        # Faster than 'lattice'\n    edge_tol=500,          # Edge tolerance\n    row_tol=2              # Row tolerance\n)\n</code></pre>"},{"location":"api_reference/index.html#error-handling","title":"Error Handling","text":"<p>Robust error handling patterns:</p> <pre><code>from omnidocs.tasks.table_extraction.extractors.camelot import CamelotExtractor\n\nextractor = CamelotExtractor()\n\ntry:\n    result = extractor.extract(\"document.pdf\")\n    if result.tables:\n        print(f\"Successfully extracted {len(result.tables)} tables\")\n    else:\n        print(\"No tables found in document\")\nexcept FileNotFoundError:\n    print(\"Document file not found\")\nexcept Exception as e:\n    print(f\"Extraction failed: {e}\")\n</code></pre>"},{"location":"api_reference/index.html#visualization","title":"\ud83c\udfa8 Visualization","text":"<p>Most extractors support result visualization:</p> <pre><code># Visualize OCR results\nocr_result = ocr_extractor.extract(\"image.png\")\nocr_extractor.visualize(\n    result=ocr_result,\n    image_path=\"image.png\",\n    output_path=\"ocr_visualization.png\",\n    show_text=True,\n    show_confidence=True\n)\n\n# Visualize table extraction\ntable_result = table_extractor.extract(\"document.pdf\")\ntable_extractor.visualize(\n    result=table_result,\n    image_path=\"document.pdf\",\n    output_path=\"table_visualization.png\"\n)\n</code></pre>"},{"location":"api_reference/index.html#performance-optimization","title":"\ud83d\udcca Performance Optimization","text":""},{"location":"api_reference/index.html#gpu-acceleration","title":"GPU Acceleration","text":"<p>Enable GPU support for faster processing:</p> <pre><code># Check GPU availability\nimport torch\nif torch.cuda.is_available():\n    device = 'cuda'\n    print(f\"Using GPU: {torch.cuda.get_device_name()}\")\nelse:\n    device = 'cpu'\n    print(\"Using CPU\")\n\n# Initialize with GPU\nextractor = EasyOCRExtractor(device=device)\n</code></pre>"},{"location":"api_reference/index.html#memory-management","title":"Memory Management","text":"<p>For large-scale processing:</p> <pre><code>import gc\nfrom omnidocs.tasks.ocr_extraction.extractors.easy_ocr import EasyOCRExtractor\n\nextractor = EasyOCRExtractor()\n\nfor i, document in enumerate(large_document_list):\n    result = extractor.extract(document)\n    # Process result...\n\n    # Clean up memory every 100 documents\n    if i % 100 == 0:\n        gc.collect()\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n</code></pre>"},{"location":"api_reference/index.html#debugging","title":"\ud83d\udd0d Debugging","text":""},{"location":"api_reference/index.html#enable-detailed-logging","title":"Enable Detailed Logging","text":"<pre><code>import logging\nfrom omnidocs.utils.logging import get_logger\n\n# Set up logging\nlogging.basicConfig(level=logging.DEBUG)\nlogger = get_logger(__name__)\n\n# Initialize extractor with logging\nextractor = EasyOCRExtractor(show_log=True)\n</code></pre>"},{"location":"api_reference/index.html#inspect-results","title":"Inspect Results","text":"<pre><code>result = extractor.extract(\"document.pdf\")\n\n# Inspect result structure\nprint(f\"Result type: {type(result)}\")\nprint(f\"Available attributes: {dir(result)}\")\n\n# For OCR results\nif hasattr(result, 'texts'):\n    print(f\"Number of text regions: {len(result.texts)}\")\n    for i, text in enumerate(result.texts[:3]):  # First 3\n        print(f\"Text {i}: {text.text[:50]}...\")\n        print(f\"Confidence: {text.confidence:.3f}\")\n        print(f\"Bbox: {text.bbox}\")\n\n# For table results\nif hasattr(result, 'tables'):\n    print(f\"Number of tables: {len(result.tables)}\")\n    for i, table in enumerate(result.tables):\n        print(f\"Table {i} shape: {table.df.shape}\")\n</code></pre>"},{"location":"api_reference/index.html#examples-by-use-case","title":"\ud83d\udcda Examples by Use Case","text":""},{"location":"api_reference/index.html#document-digitization","title":"Document Digitization","text":"<pre><code>from omnidocs.tasks.ocr_extraction.extractors.easy_ocr import EasyOCRExtractor\n\nextractor = EasyOCRExtractor(languages=['en'])\nresult = extractor.extract(\"scanned_document.png\")\nwith open(\"digitized.txt\", \"w\") as f:\n    f.write(result.full_text)\n</code></pre>"},{"location":"api_reference/index.html#financial-report-processing","title":"Financial Report Processing","text":"<pre><code>from omnidocs.tasks.table_extraction.extractors.camelot import CamelotExtractor\n\nextractor = CamelotExtractor()\nresult = extractor.extract(\"financial_report.pdf\")\nfor i, table in enumerate(result.tables):\n    table.df.to_csv(f\"financial_table_{i}.csv\", index=False)\n</code></pre>"},{"location":"api_reference/index.html#academic-paper-analysis","title":"Academic Paper Analysis","text":"<pre><code>from omnidocs.tasks.math_expression_extraction.extractors.nougat import NougatExtractor\n\nextractor = NougatExtractor()\nresult = extractor.extract(\"research_paper.pdf\")\nprint(\"Extracted LaTeX formulas:\")\nprint(result.full_text)\n</code></pre>"},{"location":"api_reference/index.html#common-issues-solutions","title":"\ud83d\udea8 Common Issues &amp; Solutions","text":""},{"location":"api_reference/index.html#import-errors","title":"Import Errors","text":"<pre><code># Check if dependencies are installed\ntry:\n    from omnidocs.tasks.ocr_extraction.extractors.easy_ocr import EasyOCRExtractor\n    print(\"\u2705 EasyOCR available\")\nexcept ImportError as e:\n    print(f\"\u274c EasyOCR not available: {e}\")\n    print(\"Install with: pip install easyocr\")\n</code></pre>"},{"location":"api_reference/index.html#memory-issues","title":"Memory Issues","text":"<pre><code># For large documents, process page by page\nfrom omnidocs.tasks.table_extraction.extractors.camelot import CamelotExtractor\n\nextractor = CamelotExtractor()\n# Process specific pages instead of all pages\nresult = extractor.extract(\"large_document.pdf\", pages=\"1-5\")\n</code></pre>"},{"location":"api_reference/index.html#language-support","title":"Language Support","text":"<pre><code># Check supported languages\nextractor = EasyOCRExtractor()\nsupported = extractor.get_supported_languages()\nprint(f\"Supported languages: {supported}\")\n</code></pre>"},{"location":"api_reference/index.html#related-resources","title":"\ud83d\udd17 Related Resources","text":"<ul> <li>Getting Started Guide - Quick introduction</li> <li>Task Tutorials - Detailed task-specific guides</li> <li>GitHub Repository - Source code and issues</li> <li>Contributing Guide - How to contribute</li> </ul> <p>This API reference is automatically generated from the source code. For the most up-to-date information, please refer to the docstrings in the source code.</p>"},{"location":"api_reference/core.html","title":"\ud83e\udde9 Core Classes","text":"<p>This section documents the core base classes and fundamental components that power all OmniDocs extractors.</p>"},{"location":"api_reference/core.html#base-extractor-classes","title":"Base Extractor Classes","text":""},{"location":"api_reference/core.html#baseocrextractor","title":"BaseOCRExtractor","text":"<p>The foundation for all OCR (Optical Character Recognition) extractors.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor","title":"omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor","text":"<pre><code>BaseOCRExtractor(device: Optional[str] = None, show_log: bool = False, languages: Optional[List[str]] = None, engine_name: Optional[str] = None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for OCR text extraction models.</p> <p>Initialize the OCR extractor.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run model on ('cuda' or 'cpu')</p> <code>None</code> <code>show_log</code> <code>bool</code> <p>Whether to show detailed logs</p> <code>False</code> <code>languages</code> <code>Optional[List[str]]</code> <p>List of language codes to support (e.g., ['en', 'zh'])</p> <code>None</code> <code>engine_name</code> <code>Optional[str]</code> <p>Name of the OCR engine for language mapping</p> <code>None</code>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; OCROutput\n</code></pre> <p>Extract text from input image.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path, Image]</code> <p>Path to input image or image data</p> required <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>OCROutput</code> <p>OCROutput containing extracted text</p>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.extract_all","title":"extract_all","text":"<pre><code>extract_all(input_paths: List[Union[str, Path, Image]], **kwargs) -&gt; List[OCROutput]\n</code></pre> <p>Extract text from multiple images.</p> <p>Parameters:</p> Name Type Description Default <code>input_paths</code> <code>List[Union[str, Path, Image]]</code> <p>List of image paths or image data</p> required <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[OCROutput]</code> <p>List of OCROutput objects</p>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.extract_with_layout","title":"extract_with_layout","text":"<pre><code>extract_with_layout(input_path: Union[str, Path, Image], layout_regions: Optional[List[Dict]] = None, **kwargs) -&gt; OCROutput\n</code></pre> <p>Extract text with optional layout information.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path, Image]</code> <p>Path to input image or image data</p> required <code>layout_regions</code> <code>Optional[List[Dict]]</code> <p>Optional list of layout regions to focus OCR on</p> <code>None</code> <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>OCROutput</code> <p>OCROutput containing extracted text</p>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.preprocess_input","title":"preprocess_input","text":"<pre><code>preprocess_input(input_path: Union[str, Path, Image, ndarray]) -&gt; List[Image.Image]\n</code></pre> <p>Convert input to list of PIL Images.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path, Image, ndarray]</code> <p>Input image path or image data</p> required <p>Returns:</p> Type Description <code>List[Image]</code> <p>List of PIL Images</p>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: Any, img_size: Tuple[int, int]) -&gt; OCROutput\n</code></pre> <p>Convert raw OCR output to standardized OCROutput format.</p> <p>Parameters:</p> Name Type Description Default <code>raw_output</code> <code>Any</code> <p>Raw output from OCR engine</p> required <code>img_size</code> <code>Tuple[int, int]</code> <p>Original image size (width, height)</p> required <p>Returns:</p> Type Description <code>OCROutput</code> <p>Standardized OCROutput object</p>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.visualize","title":"visualize","text":"<pre><code>visualize(ocr_result: OCROutput, image_path: Union[str, Path, Image], output_path: str = 'visualized.png', box_color: str = 'red', box_width: int = 2, show_text: bool = False, text_color: str = 'blue', font_size: int = 12) -&gt; None\n</code></pre> <p>Visualize OCR results by drawing bounding boxes on the original image.</p> <p>This method allows users to easily see which extractor is working better by visualizing the detected text regions with bounding boxes.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.get_supported_languages","title":"get_supported_languages","text":"<pre><code>get_supported_languages() -&gt; List[str]\n</code></pre> <p>Get list of supported language codes.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.set_languages","title":"set_languages","text":"<pre><code>set_languages(languages: List[str]) -&gt; None\n</code></pre> <p>Update supported languages for OCR extraction.</p>"},{"location":"api_reference/core.html#key-features","title":"Key Features","text":"<ul> <li>Unified Interface: Consistent API across all OCR engines</li> <li>Language Support: Multi-language text recognition</li> <li>Batch Processing: Process multiple documents efficiently</li> <li>Visualization: Built-in result visualization</li> <li>Device Management: CPU/GPU support</li> </ul>"},{"location":"api_reference/core.html#usage-example","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.ocr_extraction.extractors.easy_ocr import EasyOCRExtractor\n\n# Initialize extractor\nextractor = EasyOCRExtractor(\n    languages=['en', 'fr'],\n    device='cuda',\n    show_log=True\n)\n\n# Extract text\nresult = extractor.extract(\"document.png\")\nprint(f\"Extracted: {result.full_text}\")\n\n# Visualize results\nextractor.visualize(\n    result=result,\n    image_path=\"document.png\",\n    output_path=\"visualization.png\"\n)\n</code></pre>"},{"location":"api_reference/core.html#basetableextractor","title":"BaseTableExtractor","text":"<p>The foundation for all table extraction implementations.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.table_extraction.base.BaseTableExtractor","title":"omnidocs.tasks.table_extraction.base.BaseTableExtractor","text":"<pre><code>BaseTableExtractor(device: Optional[str] = None, show_log: bool = False, engine_name: Optional[str] = None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for table extraction models.</p> <p>Initialize the table extractor.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run model on ('cuda' or 'cpu')</p> <code>None</code> <code>show_log</code> <code>bool</code> <p>Whether to show detailed logs</p> <code>False</code> <code>engine_name</code> <code>Optional[str]</code> <p>Name of the table extraction engine</p> <code>None</code>"},{"location":"api_reference/core.html#omnidocs.tasks.table_extraction.base.BaseTableExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; TableOutput\n</code></pre> <p>Extract tables from input image.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path, Image]</code> <p>Path to input image or image data</p> required <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>TableOutput</code> <p>TableOutput containing extracted tables</p>"},{"location":"api_reference/core.html#omnidocs.tasks.table_extraction.base.BaseTableExtractor.extract_all","title":"extract_all","text":"<pre><code>extract_all(input_paths: List[Union[str, Path, Image]], **kwargs) -&gt; List[TableOutput]\n</code></pre> <p>Extract tables from multiple images.</p> <p>Parameters:</p> Name Type Description Default <code>input_paths</code> <code>List[Union[str, Path, Image]]</code> <p>List of image paths or image data</p> required <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[TableOutput]</code> <p>List of TableOutput objects</p>"},{"location":"api_reference/core.html#omnidocs.tasks.table_extraction.base.BaseTableExtractor.extract_with_layout","title":"extract_with_layout","text":"<pre><code>extract_with_layout(input_path: Union[str, Path, Image], layout_regions: Optional[List[Dict]] = None, **kwargs) -&gt; TableOutput\n</code></pre> <p>Extract tables with optional layout information.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path, Image]</code> <p>Path to input image or image data</p> required <code>layout_regions</code> <code>Optional[List[Dict]]</code> <p>Optional list of layout regions containing tables</p> <code>None</code> <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>TableOutput</code> <p>TableOutput containing extracted tables</p>"},{"location":"api_reference/core.html#omnidocs.tasks.table_extraction.base.BaseTableExtractor.preprocess_input","title":"preprocess_input","text":"<pre><code>preprocess_input(input_path: Union[str, Path, Image, ndarray]) -&gt; List[Image.Image]\n</code></pre> <p>Convert input to list of PIL Images.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path, Image, ndarray]</code> <p>Input image path or image data</p> required <p>Returns:</p> Type Description <code>List[Image]</code> <p>List of PIL Images</p>"},{"location":"api_reference/core.html#omnidocs.tasks.table_extraction.base.BaseTableExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: Any, img_size: Tuple[int, int]) -&gt; TableOutput\n</code></pre> <p>Convert raw table extraction output to standardized TableOutput format.</p> <p>Parameters:</p> Name Type Description Default <code>raw_output</code> <code>Any</code> <p>Raw output from table extraction engine</p> required <code>img_size</code> <code>Tuple[int, int]</code> <p>Original image size (width, height)</p> required <p>Returns:</p> Type Description <code>TableOutput</code> <p>Standardized TableOutput object</p>"},{"location":"api_reference/core.html#omnidocs.tasks.table_extraction.base.BaseTableExtractor.visualize","title":"visualize","text":"<pre><code>visualize(table_result: TableOutput, image_path: Union[str, Path, Image], output_path: str = 'visualized_tables.png', table_color: str = 'red', cell_color: str = 'blue', box_width: int = 2, show_text: bool = False, text_color: str = 'green', font_size: int = 12, show_table_ids: bool = True) -&gt; None\n</code></pre> <p>Visualize table extraction results by drawing bounding boxes on the original image.</p> <p>This method allows users to easily see which extractor is working better by visualizing the detected tables and cells with bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>table_result</code> <code>TableOutput</code> <p>TableOutput containing extracted tables</p> required <code>image_path</code> <code>Union[str, Path, Image]</code> <p>Path to original image or PIL Image object</p> required <code>output_path</code> <code>str</code> <p>Path to save the annotated image</p> <code>'visualized_tables.png'</code> <code>table_color</code> <code>str</code> <p>Color for table bounding boxes</p> <code>'red'</code> <code>cell_color</code> <code>str</code> <p>Color for cell bounding boxes</p> <code>'blue'</code> <code>box_width</code> <code>int</code> <p>Width of bounding box lines</p> <code>2</code> <code>show_text</code> <code>bool</code> <p>Whether to overlay cell text</p> <code>False</code> <code>text_color</code> <code>str</code> <p>Color for text overlay</p> <code>'green'</code> <code>font_size</code> <code>int</code> <p>Font size for text overlay</p> <code>12</code> <code>show_table_ids</code> <code>bool</code> <p>Whether to show table IDs</p> <code>True</code>"},{"location":"api_reference/core.html#key-features_1","title":"Key Features","text":"<ul> <li>Multiple Formats: Support for PDF and image inputs</li> <li>Structured Output: Returns pandas DataFrames</li> <li>Coordinate Transformation: Handles PDF to image coordinate mapping</li> <li>Batch Processing: Process multiple documents</li> <li>Visualization: Table detection visualization</li> </ul>"},{"location":"api_reference/core.html#usage-example_1","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.table_extraction.extractors.camelot import CamelotExtractor\n\n# Initialize extractor\nextractor = CamelotExtractor(\n    flavor='lattice',\n    pages='all'\n)\n\n# Extract tables\nresult = extractor.extract(\"report.pdf\")\n\n# Access tables as DataFrames\nfor i, table in enumerate(result.tables):\n    print(f\"Table {i} shape: {table.df.shape}\")\n    table.df.to_csv(f\"table_{i}.csv\", index=False)\n</code></pre>"},{"location":"api_reference/core.html#basetextextractor","title":"BaseTextExtractor","text":"<p>The foundation for text extraction from documents.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.text_extraction.base.BaseTextExtractor","title":"omnidocs.tasks.text_extraction.base.BaseTextExtractor","text":"<pre><code>BaseTextExtractor(device: Optional[str] = None, show_log: bool = False, engine_name: Optional[str] = None, extract_images: bool = False)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for text extraction models.</p> <p>Initialize the text extractor.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run model on ('cuda' or 'cpu')</p> <code>None</code> <code>show_log</code> <code>bool</code> <p>Whether to show detailed logs</p> <code>False</code> <code>engine_name</code> <code>Optional[str]</code> <p>Name of the text extraction engine</p> <code>None</code> <code>extract_images</code> <code>bool</code> <p>Whether to extract images alongside text</p> <code>False</code>"},{"location":"api_reference/core.html#omnidocs.tasks.text_extraction.base.BaseTextExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(input_path: Union[str, Path], **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text from input document.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input document</p> required <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>TextOutput containing extracted text</p>"},{"location":"api_reference/core.html#omnidocs.tasks.text_extraction.base.BaseTextExtractor.extract_all","title":"extract_all","text":"<pre><code>extract_all(input_paths: List[Union[str, Path]], **kwargs) -&gt; List[TextOutput]\n</code></pre> <p>Extract text from multiple documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_paths</code> <code>List[Union[str, Path]]</code> <p>List of document paths</p> required <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[TextOutput]</code> <p>List of TextOutput objects</p>"},{"location":"api_reference/core.html#omnidocs.tasks.text_extraction.base.BaseTextExtractor.preprocess_input","title":"preprocess_input","text":"<pre><code>preprocess_input(input_path: Union[str, Path]) -&gt; Any\n</code></pre> <p>Preprocess input document for text extraction.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input document</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Preprocessed document object</p>"},{"location":"api_reference/core.html#omnidocs.tasks.text_extraction.base.BaseTextExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: Any, source_info: Optional[Dict] = None) -&gt; TextOutput\n</code></pre> <p>Convert raw text extraction output to standardized TextOutput format.</p> <p>Parameters:</p> Name Type Description Default <code>raw_output</code> <code>Any</code> <p>Raw output from text extraction engine</p> required <code>source_info</code> <code>Optional[Dict]</code> <p>Optional source document information</p> <code>None</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>Standardized TextOutput object</p>"},{"location":"api_reference/core.html#key-features_2","title":"Key Features","text":"<ul> <li>Multi-format Support: PDF, DOCX, HTML, and more</li> <li>Layout Preservation: Maintains document structure</li> <li>Metadata Extraction: Document properties and formatting</li> <li>Batch Processing: Handle multiple documents</li> </ul>"},{"location":"api_reference/core.html#usage-example_2","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.text_extraction.extractors.pymupdf import PyMuPDFExtractor\n\n# Initialize extractor\nextractor = PyMuPDFExtractor()\n\n# Extract text with layout\nresult = extractor.extract(\"document.pdf\")\n\n# Access structured text\nprint(f\"Full text: {result.full_text}\")\nfor block in result.text_blocks:\n    print(f\"Block: {block.text[:50]}...\")\n    print(f\"Position: {block.bbox}\")\n</code></pre>"},{"location":"api_reference/core.html#data-models","title":"Data Models","text":""},{"location":"api_reference/core.html#ocrtext","title":"OCRText","text":"<p>Represents a single text region detected by OCR.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.OCRText","title":"omnidocs.tasks.ocr_extraction.base.OCRText","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for individual OCR text detection.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>str</code> <p>Extracted text content</p> <code>confidence</code> <code>Optional[float]</code> <p>Confidence score for the text detection</p> <code>bbox</code> <code>Optional[List[float]]</code> <p>Bounding box coordinates [x1, y1, x2, y2]</p> <code>polygon</code> <code>Optional[List[List[float]]]</code> <p>Optional polygon coordinates for irregular text regions</p> <code>language</code> <code>Optional[str]</code> <p>Detected language code (e.g., 'en', 'zh', 'fr')</p> <code>reading_order</code> <code>Optional[int]</code> <p>Optional reading order index for text sequencing</p>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.OCRText.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p>"},{"location":"api_reference/core.html#attributes","title":"Attributes","text":"<ul> <li><code>text</code> (str): The recognized text content</li> <li><code>confidence</code> (float): Recognition confidence score (0.0-1.0)</li> <li><code>bbox</code> (List[float]): Bounding box coordinates [x1, y1, x2, y2]</li> <li><code>polygon</code> (List[List[float]]): Precise polygon coordinates</li> <li><code>language</code> (Optional[str]): Detected language code</li> <li><code>reading_order</code> (int): Reading order index</li> </ul>"},{"location":"api_reference/core.html#example","title":"Example","text":"<pre><code># Access OCR text regions\nfor text_region in ocr_result.texts:\n    print(f\"Text: {text_region.text}\")\n    print(f\"Confidence: {text_region.confidence:.3f}\")\n    print(f\"Bbox: {text_region.bbox}\")\n    print(f\"Language: {text_region.language}\")\n</code></pre>"},{"location":"api_reference/core.html#ocroutput","title":"OCROutput","text":"<p>Complete OCR extraction result.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.OCROutput","title":"omnidocs.tasks.ocr_extraction.base.OCROutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for OCR extraction results.</p> <p>Attributes:</p> Name Type Description <code>texts</code> <code>List[OCRText]</code> <p>List of detected text objects</p> <code>full_text</code> <code>str</code> <p>Combined text from all detections</p> <code>source_img_size</code> <code>Optional[Tuple[int, int]]</code> <p>Original image dimensions (width, height)</p> <code>processing_time</code> <code>Optional[float]</code> <p>Time taken for OCR processing</p> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata from the OCR engine</p>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.OCROutput.get_sorted_by_reading_order","title":"get_sorted_by_reading_order","text":"<pre><code>get_sorted_by_reading_order() -&gt; List[OCRText]\n</code></pre> <p>Get texts sorted by reading order (top-to-bottom, left-to-right if no reading_order).</p>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.OCROutput.get_text_by_confidence","title":"get_text_by_confidence","text":"<pre><code>get_text_by_confidence(min_confidence: float = 0.5) -&gt; List[OCRText]\n</code></pre> <p>Filter texts by minimum confidence threshold.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.OCROutput.save_json","title":"save_json","text":"<pre><code>save_json(output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save output to JSON file.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.OCROutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p>"},{"location":"api_reference/core.html#key-methods","title":"Key Methods","text":"<ul> <li><code>get_text_by_confidence(min_confidence)</code>: Filter by confidence threshold</li> <li><code>get_sorted_by_reading_order()</code>: Sort by reading order</li> <li><code>save_json(output_path)</code>: Save results to JSON</li> <li><code>to_dict()</code>: Convert to dictionary</li> </ul>"},{"location":"api_reference/core.html#example_1","title":"Example","text":"<pre><code>result = extractor.extract(\"image.png\")\n\n# Filter high-confidence text\nhigh_conf_texts = result.get_text_by_confidence(0.8)\nprint(f\"High confidence regions: {len(high_conf_texts)}\")\n\n# Save results\nresult.save_json(\"ocr_results.json\")\n</code></pre>"},{"location":"api_reference/core.html#table","title":"Table","text":"<p>Represents an extracted table with structure and data.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.table_extraction.base.Table","title":"omnidocs.tasks.table_extraction.base.Table","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for extracted table.</p> <p>Attributes:</p> Name Type Description <code>cells</code> <code>List[TableCell]</code> <p>List of table cells</p> <code>num_rows</code> <code>int</code> <p>Number of rows in the table</p> <code>num_cols</code> <code>int</code> <p>Number of columns in the table</p> <code>bbox</code> <code>Optional[List[float]]</code> <p>Bounding box of the entire table [x1, y1, x2, y2]</p> <code>confidence</code> <code>Optional[float]</code> <p>Overall table detection confidence</p> <code>table_id</code> <code>Optional[str]</code> <p>Optional table identifier</p> <code>caption</code> <code>Optional[str]</code> <p>Optional table caption</p> <code>structure_confidence</code> <code>Optional[float]</code> <p>Confidence score for table structure detection</p>"},{"location":"api_reference/core.html#omnidocs.tasks.table_extraction.base.Table.to_csv","title":"to_csv","text":"<pre><code>to_csv() -&gt; str\n</code></pre> <p>Convert table to CSV format.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.table_extraction.base.Table.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.table_extraction.base.Table.to_html","title":"to_html","text":"<pre><code>to_html() -&gt; str\n</code></pre> <p>Convert table to HTML format.</p>"},{"location":"api_reference/core.html#key-properties","title":"Key Properties","text":"<ul> <li><code>df</code> (pandas.DataFrame): Table data as DataFrame</li> <li><code>bbox</code> (List[float]): Table bounding box</li> <li><code>confidence</code> (float): Extraction confidence</li> <li><code>page_number</code> (int): Source page number</li> </ul>"},{"location":"api_reference/core.html#key-methods_1","title":"Key Methods","text":"<ul> <li><code>to_csv()</code>: Export as CSV string</li> <li><code>to_html()</code>: Export as HTML string</li> <li><code>to_dict()</code>: Convert to dictionary</li> </ul>"},{"location":"api_reference/core.html#example_2","title":"Example","text":"<pre><code>for table in table_result.tables:\n    # Access as DataFrame\n    df = table.df\n    print(f\"Table shape: {df.shape}\")\n\n    # Export formats\n    csv_content = table.to_csv()\n    html_content = table.to_html()\n\n    # Save to file\n    df.to_excel(f\"table_page_{table.page_number}.xlsx\")\n</code></pre>"},{"location":"api_reference/core.html#tableoutput","title":"TableOutput","text":"<p>Complete table extraction result.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.table_extraction.base.TableOutput","title":"omnidocs.tasks.table_extraction.base.TableOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for table extraction results.</p> <p>Attributes:</p> Name Type Description <code>tables</code> <code>List[Table]</code> <p>List of extracted tables</p> <code>source_img_size</code> <code>Optional[Tuple[int, int]]</code> <p>Original image dimensions (width, height)</p> <code>processing_time</code> <code>Optional[float]</code> <p>Time taken for table extraction</p> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata from the extraction engine</p>"},{"location":"api_reference/core.html#omnidocs.tasks.table_extraction.base.TableOutput.get_tables_by_confidence","title":"get_tables_by_confidence","text":"<pre><code>get_tables_by_confidence(min_confidence: float = 0.5) -&gt; List[Table]\n</code></pre> <p>Filter tables by minimum confidence threshold.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.table_extraction.base.TableOutput.save_json","title":"save_json","text":"<pre><code>save_json(output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save output to JSON file.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.table_extraction.base.TableOutput.save_tables_as_csv","title":"save_tables_as_csv","text":"<pre><code>save_tables_as_csv(output_dir: Union[str, Path]) -&gt; List[Path]\n</code></pre> <p>Save all tables as separate CSV files.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.table_extraction.base.TableOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p>"},{"location":"api_reference/core.html#key-methods_2","title":"Key Methods","text":"<ul> <li><code>get_tables_by_confidence(min_confidence)</code>: Filter by confidence</li> <li><code>save_tables_as_csv(output_dir)</code>: Save all tables as CSV files</li> <li><code>save_json(output_path)</code>: Save metadata to JSON</li> </ul>"},{"location":"api_reference/core.html#example_3","title":"Example","text":"<pre><code>result = extractor.extract(\"document.pdf\")\n\n# Filter high-confidence tables\ngood_tables = result.get_tables_by_confidence(0.7)\n\n# Save all tables\ncsv_files = result.save_tables_as_csv(\"output_tables/\")\nprint(f\"Saved {len(csv_files)} CSV files\")\n</code></pre>"},{"location":"api_reference/core.html#mapper-classes","title":"Mapper Classes","text":""},{"location":"api_reference/core.html#baseocrmapper","title":"BaseOCRMapper","text":"<p>Handles language code mapping and normalization for OCR engines.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.BaseOCRMapper","title":"omnidocs.tasks.ocr_extraction.base.BaseOCRMapper","text":"<pre><code>BaseOCRMapper(engine_name: str)\n</code></pre> <p>Base class for mapping OCR engine-specific outputs to standardized format.</p> <p>Initialize mapper for specific OCR engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine_name</code> <code>str</code> <p>Name of the OCR engine (e.g., 'tesseract', 'paddle', 'easyocr')</p> required"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.BaseOCRMapper.detect_text_language","title":"detect_text_language","text":"<pre><code>detect_text_language(text: str) -&gt; Optional[str]\n</code></pre> <p>Detect language of extracted text.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.BaseOCRMapper.from_standard_language","title":"from_standard_language","text":"<pre><code>from_standard_language(standard_language: str) -&gt; str\n</code></pre> <p>Convert standard ISO 639-1 language code to engine-specific format.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.BaseOCRMapper.get_supported_languages","title":"get_supported_languages","text":"<pre><code>get_supported_languages() -&gt; List[str]\n</code></pre> <p>Get list of supported languages for this engine.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.BaseOCRMapper.normalize_bbox","title":"normalize_bbox","text":"<pre><code>normalize_bbox(bbox: List[float], img_width: int, img_height: int) -&gt; List[float]\n</code></pre> <p>Normalize bounding box coordinates to absolute pixel values.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.ocr_extraction.base.BaseOCRMapper.to_standard_language","title":"to_standard_language","text":"<pre><code>to_standard_language(engine_language: str) -&gt; str\n</code></pre> <p>Convert engine-specific language code to standard ISO 639-1.</p>"},{"location":"api_reference/core.html#key-methods_3","title":"Key Methods","text":"<ul> <li><code>to_standard_language(engine_language)</code>: Convert to standard language code</li> <li><code>from_standard_language(standard_language)</code>: Convert from standard language code</li> <li><code>get_supported_languages()</code>: List supported languages</li> <li><code>normalize_bbox(bbox, img_width, img_height)</code>: Normalize bounding box coordinates</li> </ul>"},{"location":"api_reference/core.html#basetablemapper","title":"BaseTableMapper","text":"<p>Handles coordinate transformation and table structure mapping.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.table_extraction.base.BaseTableMapper","title":"omnidocs.tasks.table_extraction.base.BaseTableMapper","text":"<pre><code>BaseTableMapper(engine_name: str)\n</code></pre> <p>Base class for mapping table extraction engine-specific outputs to standardized format.</p> <p>Initialize mapper for specific table extraction engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine_name</code> <code>str</code> <p>Name of the table extraction engine</p> required"},{"location":"api_reference/core.html#omnidocs.tasks.table_extraction.base.BaseTableMapper.detect_header_rows","title":"detect_header_rows","text":"<pre><code>detect_header_rows(cells: List[TableCell]) -&gt; List[TableCell]\n</code></pre> <p>Detect and mark header cells based on position and formatting.</p>"},{"location":"api_reference/core.html#omnidocs.tasks.table_extraction.base.BaseTableMapper.normalize_bbox","title":"normalize_bbox","text":"<pre><code>normalize_bbox(bbox: List[float], img_width: int, img_height: int) -&gt; List[float]\n</code></pre> <p>Normalize bounding box coordinates to absolute pixel values.</p>"},{"location":"api_reference/core.html#key-methods_4","title":"Key Methods","text":"<ul> <li><code>normalize_bbox(bbox, img_width, img_height)</code>: Normalize coordinates</li> <li><code>detect_header_rows(cells)</code>: Identify header rows</li> </ul>"},{"location":"api_reference/core.html#abstract-base-classes","title":"Abstract Base Classes","text":"<p>All extractors inherit from these abstract base classes, ensuring consistent interfaces:</p> <pre><code>from abc import ABC, abstractmethod\n\nclass BaseExtractor(ABC):\n    \"\"\"Abstract base class for all extractors.\"\"\"\n\n    @abstractmethod\n    def extract(self, input_path: Union[str, Path]) -&gt; Any:\n        \"\"\"Extract data from input document.\"\"\"\n        pass\n\n    @abstractmethod\n    def preprocess_input(self, input_path: Union[str, Path]) -&gt; Any:\n        \"\"\"Preprocess input for extraction.\"\"\"\n        pass\n\n    @abstractmethod\n    def postprocess_output(self, raw_output: Any) -&gt; Any:\n        \"\"\"Convert raw output to standardized format.\"\"\"\n        pass\n</code></pre>"},{"location":"api_reference/core.html#common-patterns","title":"Common Patterns","text":""},{"location":"api_reference/core.html#initialization-pattern","title":"Initialization Pattern","text":"<p>All extractors follow this initialization pattern:</p> <pre><code>class SomeExtractor(BaseExtractor):\n    def __init__(\n        self,\n        device: Optional[str] = None,\n        show_log: bool = False,\n        languages: Optional[List[str]] = None,\n        **kwargs\n    ):\n        super().__init__(device, show_log, languages)\n        # Extractor-specific initialization\n        self._load_model()\n</code></pre>"},{"location":"api_reference/core.html#processing-pipeline","title":"Processing Pipeline","text":"<p>Standard processing flow:</p> <ol> <li>Input Validation: Check file existence and format</li> <li>Preprocessing: Convert to required format (PIL Image, etc.)</li> <li>Model Inference: Run the actual extraction</li> <li>Postprocessing: Convert to standardized output format</li> <li>Result Packaging: Create result object with metadata</li> </ol>"},{"location":"api_reference/core.html#error-handling","title":"Error Handling","text":"<p>Consistent error handling across extractors:</p> <pre><code>try:\n    result = extractor.extract(\"document.pdf\")\nexcept FileNotFoundError:\n    print(\"Document not found\")\nexcept ImportError:\n    print(\"Required dependencies not installed\")\nexcept Exception as e:\n    print(f\"Extraction failed: {e}\")\n</code></pre>"},{"location":"api_reference/core.html#performance-considerations","title":"Performance Considerations","text":""},{"location":"api_reference/core.html#memory-management","title":"Memory Management","text":"<ul> <li>Use generators for batch processing large datasets</li> <li>Clear GPU memory between large operations</li> <li>Implement proper cleanup in <code>__del__</code> methods</li> </ul>"},{"location":"api_reference/core.html#gpu-utilization","title":"GPU Utilization","text":"<ul> <li>Check GPU availability before initialization</li> <li>Batch operations when possible</li> <li>Use appropriate tensor data types</li> </ul>"},{"location":"api_reference/core.html#caching","title":"Caching","text":"<ul> <li>Cache model loading where appropriate</li> <li>Implement result caching for repeated operations</li> <li>Use memory-mapped files for large datasets</li> </ul>"},{"location":"api_reference/core.html#extension-points","title":"Extension Points","text":""},{"location":"api_reference/core.html#custom-extractors","title":"Custom Extractors","text":"<p>Create custom extractors by inheriting from base classes:</p> <pre><code>from omnidocs.tasks.ocr_extraction.base import BaseOCRExtractor\n\nclass CustomOCRExtractor(BaseOCRExtractor):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        # Custom initialization\n\n    def _load_model(self):\n        # Load your custom model\n        pass\n\n    def postprocess_output(self, raw_output, img_size):\n        # Convert to OCROutput format\n        pass\n</code></pre>"},{"location":"api_reference/core.html#custom-mappers","title":"Custom Mappers","text":"<p>Implement custom language or coordinate mappers:</p> <pre><code>from omnidocs.tasks.ocr_extraction.base import BaseOCRMapper\n\nclass CustomMapper(BaseOCRMapper):\n    def __init__(self):\n        super().__init__('custom_engine')\n        self._setup_custom_mapping()\n\n    def _setup_custom_mapping(self):\n        # Define your language mappings\n        pass\n</code></pre> <p>This core architecture ensures consistency, extensibility, and maintainability across all OmniDocs extractors.</p>"},{"location":"api_reference/overview.html","title":"API Reference Overview","text":"<p>Welcome to the OmniDocs API Reference! This section provides a high-level overview of the library's architecture and how its various components fit together. For detailed documentation on specific classes, functions, and modules, please refer to the dedicated sections.</p>"},{"location":"api_reference/overview.html#core-concepts","title":"Core Concepts","text":"<p>OmniDocs is built around a modular and extensible design, centered on the following core concepts:</p> <ul> <li>Extractors: These are the primary classes responsible for performing specific document processing tasks (e.g., OCR, table extraction, text extraction). All extractors inherit from a common <code>BaseExtractor</code> class, ensuring a consistent API.</li> <li>Data Models: Standardized Pydantic models are used to represent the output of each extraction task (e.g., <code>OCROutput</code>, <code>TableOutput</code>, <code>TextOutput</code>). This ensures consistency and ease of use across different extractors.</li> <li>Mappers: Helper classes that handle task-specific logic, such as language code mapping for OCR engines or coordinate transformations for layout analysis.</li> <li>Utilities: A collection of helper functions for common tasks like logging, document handling (opening PDFs/images), image processing, and file validation.</li> </ul>"},{"location":"api_reference/overview.html#architecture","title":"Architecture","text":"<p>The library is structured into the following main packages:</p> <ul> <li><code>omnidocs.tasks</code>: Contains sub-packages for each document AI task (e.g., <code>layout_analysis</code>, <code>ocr_extraction</code>, <code>table_extraction</code>, <code>text_extraction</code>, <code>math_expression_extraction</code>). Each task sub-package further contains its specific <code>extractors</code> and <code>base</code> classes.</li> <li><code>omnidocs.utils</code>: Provides general-purpose utility functions and helpers used across the library.</li> <li><code>omnidocs.models</code>: (If applicable) Contains definitions for deep learning models or model-related utilities.</li> <li><code>omnidocs.workflows</code>: (If applicable) Contains higher-level pipelines that combine multiple extractors to achieve complex document processing workflows.</li> </ul>"},{"location":"api_reference/overview.html#how-to-use-this-reference","title":"How to Use This Reference","text":"<p>This API reference is organized to help you quickly find the information you need:</p> <ul> <li>Python API: Start here for a general introduction to the API, common usage patterns, and result object structures.</li> <li>Core Classes: Dive into the foundational base classes and data models that define the common interfaces and outputs across OmniDocs.</li> <li>Tasks: Explore the specific extractors available for each document AI task. Each task section provides detailed documentation for its extractors, including initialization parameters, <code>extract</code> method signatures, and usage examples.</li> <li>Utilities: Find documentation for various helper functions related to logging, file handling, image processing, and more.</li> </ul>"},{"location":"api_reference/overview.html#getting-started","title":"Getting Started","text":"<p>If you're new to OmniDocs, we recommend starting with the Quick Start Guide for hands-on examples and a smooth onboarding experience.</p>"},{"location":"api_reference/overview.html#contributing","title":"Contributing","text":"<p>We welcome contributions to OmniDocs! Please refer to our Contributing Guide for details on how to get involved.</p>"},{"location":"api_reference/python_api.html","title":"\ud83d\udc0d Python API Reference","text":"<p>Welcome to the OmniDocs Python API Reference! This page provides live, auto-generated documentation for every major module, extractor, and utility in the OmniDocs ecosystem. Use this as your single source of truth for all classes, functions, and configuration options.</p>"},{"location":"api_reference/python_api.html#core-package","title":"\ud83d\udce6 Core Package","text":"<p>The main OmniDocs package provides the top-level API, configuration, and shared utilities.</p>"},{"location":"api_reference/python_api.html#omnidocs","title":"omnidocs","text":""},{"location":"api_reference/python_api.html#tasks-extractors","title":"\ud83e\udde9 Tasks &amp; Extractors","text":"<p>OmniDocs organizes all document AI into modular tasks. Each task has its own extractors, which you can import and use directly. Click any section below to expand the full API for that task.</p>"},{"location":"api_reference/python_api.html#layout-analysis","title":"\ud83d\udcd0 Layout Analysis","text":"<p>Detect and analyze document structure, regions, and reading order.</p>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis","title":"omnidocs.tasks.layout_analysis","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.FlorenceLayoutDetector","title":"FlorenceLayoutDetector","text":"<pre><code>FlorenceLayoutDetector(device: Optional[str] = None, show_log: bool = False, trust_remote_code: bool = True, **kwargs)\n</code></pre> <p>               Bases: <code>BaseLayoutDetector</code></p> <p>Florence-based layout detection implementation.</p> <p>Initialize Florence Layout Detector.</p> Source code in <code>omnidocs/tasks/layout_analysis/extractors/florence.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    trust_remote_code: bool = True,\n    **kwargs\n):\n    \"\"\"Initialize Florence Layout Detector.\"\"\"\n    super().__init__(show_log=show_log)\n\n    # Initialize label mapper\n    self._label_mapper = FlorenceLayoutMapper()\n\n    logger.info(\"Initializing FlorenceLayoutDetector\")\n\n    if device:\n        self.device = device\n    logger.info(f\"Using device: {self.device}\")\n\n    try:\n        from transformers import AutoProcessor, AutoModelForCausalLM\n    except ImportError as ex:\n        logger.error(\"Failed to import transformers\")\n        raise ImportError(\n            \"transformers is not available. Please install it with: pip install transformers\"\n        ) from ex\n\n    # Initialize the model and processor\n    try:\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.MODEL_REPO,\n            trust_remote_code=trust_remote_code,\n            **kwargs\n        )\n        self.processor = AutoProcessor.from_pretrained(\n            self.MODEL_REPO,\n            trust_remote_code=trust_remote_code\n        )\n        self.model.to(self.device)\n        logger.success(\"Model initialized successfully\")\n    except Exception as e:\n        logger.error(\"Failed to initialize model\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.FlorenceLayoutDetector.detect","title":"detect","text":"<pre><code>detect(input_path: Union[str, Path], max_new_tokens: int = 1024, do_sample: bool = False, num_beams: int = 3, **kwargs) -&gt; Tuple[Image.Image, LayoutOutput]\n</code></pre> <p>Run layout detection with standardized labels.</p> Source code in <code>omnidocs/tasks/layout_analysis/extractors/florence.py</code> <pre><code>@log_execution_time\ndef detect(\n    self,\n    input_path: Union[str, Path],\n    max_new_tokens: int = 1024,\n    do_sample: bool = False,\n    num_beams: int = 3,\n    **kwargs\n) -&gt; Tuple[Image.Image, LayoutOutput]:\n    \"\"\"Run layout detection with standardized labels.\"\"\"\n    try:\n        # Load and preprocess input\n        image = Image.open(input_path).convert(\"RGB\")\n\n        # Prepare inputs\n        prompt = \"&lt;OD&gt;\"\n        inputs = self.processor(\n            text=prompt,\n            images=image,\n            return_tensors=\"pt\"\n        ).to(self.device)\n\n        # Generate predictions\n        generated_ids = self.model.generate(\n            input_ids=inputs[\"input_ids\"],\n            pixel_values=inputs[\"pixel_values\"],\n            max_new_tokens=max_new_tokens,\n            do_sample=do_sample,\n            num_beams=num_beams,\n            **kwargs\n        )\n\n        # Decode and post-process\n        generated_text = self.processor.batch_decode(\n            generated_ids,\n            skip_special_tokens=False\n        )[0]\n\n        parsed_result = self.processor.post_process_generation(\n            generated_text,\n            task=\"&lt;OD&gt;\",\n            image_size=(image.width, image.height)\n        )\n\n        # Convert to standard format\n        layout_boxes = []\n        for bbox, label in zip(\n            parsed_result[\"&lt;OD&gt;\"][\"bboxes\"],\n            parsed_result[\"&lt;OD&gt;\"][\"labels\"]\n        ):\n            mapped_label = self.map_label(label.lower())\n            if mapped_label:\n                layout_boxes.append(\n                    LayoutBox(\n                        label=mapped_label,\n                        bbox=[float(coord) for coord in bbox],\n                        confidence=None  # Florence model doesn't provide confidence scores\n                    )\n                )\n\n        # Create annotated image\n        annotated_img = image.copy()\n        draw = ImageDraw.Draw(annotated_img)\n\n        # Draw boxes and labels\n        for box in layout_boxes:\n            color = self.color_map.get(box.label, 'gray')\n            coords = box.bbox\n            draw.rectangle(coords, outline=color, width=3)\n            draw.text((coords[0], coords[1]-20), box.label, fill=color)\n\n        return annotated_img, LayoutOutput(bboxes=layout_boxes)\n\n    except Exception as e:\n        logger.error(\"Error during prediction\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.FlorenceLayoutDetector.visualize","title":"visualize","text":"<pre><code>visualize(detection_result: Tuple[Image, LayoutOutput], output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save annotated image and layout data to files.</p> <p>Parameters:</p> Name Type Description Default <code>detection_result</code> <code>Tuple[Image, LayoutOutput]</code> <p>Tuple containing (PIL Image, LayoutOutput)</p> required <code>output_path</code> <code>Union[str, Path]</code> <p>Path to save visualization</p> required Source code in <code>omnidocs/tasks/layout_analysis/extractors/florence.py</code> <pre><code>def visualize(\n    self,\n    detection_result: Tuple[Image.Image, LayoutOutput],\n    output_path: Union[str, Path],\n) -&gt; None:\n    \"\"\"\n    Save annotated image and layout data to files.\n\n    Args:\n        detection_result: Tuple containing (PIL Image, LayoutOutput)\n        output_path: Path to save visualization\n    \"\"\"\n    super().visualize(detection_result, output_path)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.PaddleLayoutDetector","title":"PaddleLayoutDetector","text":"<pre><code>PaddleLayoutDetector(device: Optional[str] = None, show_log: bool = False, **kwargs)\n</code></pre> <p>               Bases: <code>BaseLayoutDetector</code></p> <p>PaddleOCR-based layout detection implementation.</p> <p>Initialize PaddleOCR Layout Detector.</p> Source code in <code>omnidocs/tasks/layout_analysis/extractors/paddle.py</code> <pre><code>def __init__(\n    self, \n    device: Optional[str] = None,\n    show_log: bool = False,\n    **kwargs\n):\n    \"\"\"Initialize PaddleOCR Layout Detector.\"\"\"\n    super().__init__()\n\n    # Initialize label mapper\n    self._label_mapper = PaddleLayoutMapper()\n\n    # Log initialization\n    logger.info(\"Initializing PaddleLayoutDetector\")\n\n    # Set device if specified\n    if device:\n        self.device = device\n    logger.info(f\"Using device: {self.device}\")\n\n    try:\n        from paddleocr import PPStructure\n    except ImportError as ex:\n        logger.error(\"Failed to import paddleocr\")\n        raise ImportError(\n            \"paddleocr is not available. Please install it with: pip install paddleocr\"\n        ) from ex\n\n\n    # Initialize the model\n    try:\n        self.model = PPStructure(\n            table=True,\n            ocr=True,\n            show_log=show_log,\n            **kwargs\n        )\n        logger.success(\"Model initialized successfully\")\n    except Exception as e:\n        logger.error(\"Failed to initialize model\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.PaddleLayoutDetector.detect","title":"detect","text":"<pre><code>detect(input_path: Union[str, Path], **kwargs) -&gt; Tuple[Image.Image, LayoutOutput]\n</code></pre> <p>Run layout detection with standardized labels.</p> Source code in <code>omnidocs/tasks/layout_analysis/extractors/paddle.py</code> <pre><code>@log_execution_time\ndef detect(\n    self, \n    input_path: Union[str, Path], \n    **kwargs\n) -&gt; Tuple[Image.Image, LayoutOutput]:\n    \"\"\"Run layout detection with standardized labels.\"\"\"\n    try:\n        # Load and preprocess input\n        images = self.preprocess_input(input_path)\n\n        results = []\n        for img in images:\n            # Get detection results\n            det_result = self.model(img)\n\n            # Convert to PIL Image if needed\n            if isinstance(img, np.ndarray):\n                img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n\n            # Create annotated image\n            annotated_img = img.copy()\n            draw = ImageDraw.Draw(annotated_img)\n\n            # Convert detection results to LayoutBox objects with standardized labels\n            layout_boxes = []\n\n            for block in det_result:\n                # Extract coordinates and type\n                x1, y1, x2, y2 = block['bbox']\n                model_label = block['type']\n                mapped_label = self.map_label(model_label)\n\n                if mapped_label:  # Only include boxes with valid mapped labels\n                    layout_boxes.append(\n                        LayoutBox(\n                            label=mapped_label,\n                            bbox=[float(x1), float(y1), float(x2), float(y2)],\n                            confidence=block.get('confidence', None)\n                        )\n                    )\n\n                    # Draw with standardized colors\n                    color = self.color_map.get(mapped_label, 'gray')\n                    draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n                    draw.text((x1, y1-20), mapped_label, fill=color)\n\n            results.append((\n                annotated_img,\n                LayoutOutput(bboxes=layout_boxes)\n            ))\n\n        return results[0] if results else (None, LayoutOutput(bboxes=[]))\n\n    except Exception as e:\n        logger.error(\"Error during prediction\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.PaddleLayoutDetector.visualize","title":"visualize","text":"<pre><code>visualize(detection_result: Tuple[Image, LayoutOutput], output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save annotated image and layout data to files.</p> <p>Parameters:</p> Name Type Description Default <code>detection_result</code> <code>Tuple[Image, LayoutOutput]</code> <p>Tuple containing (PIL Image, LayoutOutput)</p> required <code>output_path</code> <code>Union[str, Path]</code> <p>Path to save visualization</p> required Source code in <code>omnidocs/tasks/layout_analysis/extractors/paddle.py</code> <pre><code>def visualize(\n    self,\n    detection_result: Tuple[Image.Image, LayoutOutput],\n    output_path: Union[str, Path],\n) -&gt; None:\n    \"\"\"\n    Save annotated image and layout data to files.\n\n    Args:\n        detection_result: Tuple containing (PIL Image, LayoutOutput)\n        output_path: Path to save visualization\n    \"\"\"\n    super().visualize(detection_result, output_path)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.RTDETRLayoutDetector","title":"RTDETRLayoutDetector","text":"<pre><code>RTDETRLayoutDetector(device: Optional[str] = None, show_log: bool = False, model_path: Optional[Union[str, Path]] = None, num_threads: Optional[int] = 4, use_cpu_only: bool = True)\n</code></pre> <p>               Bases: <code>BaseLayoutDetector</code></p> <p>RT-DETR-based layout detection implementation.</p> <p>Initialize RT-DETR Layout Detector with careful device handling.</p> Source code in <code>omnidocs/tasks/layout_analysis/extractors/rtdetr.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    model_path: Optional[Union[str, Path]] = None,\n    num_threads: Optional[int] = 4,\n    use_cpu_only: bool = True\n):\n    \"\"\"Initialize RT-DETR Layout Detector with careful device handling.\"\"\"\n    super().__init__(show_log=show_log)\n\n    self._label_mapper = RTDETRLayoutMapper()\n\n    if self.show_log:\n        logger.info(\"Initializing RTDETRLayoutDetector\")\n\n    # Set default paths\n    if model_path is None:\n        model_path = _MODELS_DIR / \"rtdetr_layout\" / self.MODEL_REPO.replace(\"/\", \"_\")\n\n    self.model_path = Path(model_path)\n    self.num_threads = num_threads\n\n    # Careful device handling\n    if use_cpu_only:\n        self.device = \"cpu\"\n        if self.show_log:\n            logger.info(\"Forced CPU usage due to use_cpu_only flag\")\n    elif device:\n        self.device = device\n        if self.show_log:\n            logger.info(f\"Using specified device: {device}\")\n    else:\n        # Check CUDA availability with error handling\n        try:\n            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            if self.show_log:\n                logger.info(f\"Automatically selected device: {self.device}\")\n        except Exception as e:\n            self.device = \"cpu\"\n            if self.show_log:\n                logger.warning(f\"Error checking CUDA availability: {e}. Defaulting to CPU\")\n\n    self.num_threads = num_threads or int(os.environ.get(\"OMP_NUM_THREADS\", 4))\n\n    # Set thread count for CPU operations\n    if self.device == \"cpu\":\n        torch.set_num_threads(self.num_threads)\n        if self.show_log:\n            logger.info(f\"Set CPU threads to {self.num_threads}\")\n\n    # Model parameters\n    self.image_size = 640\n    self.confidence_threshold = 0.6\n\n    # Check dependencies\n    self._check_dependencies()\n\n    # Download model if needed\n    if not self._model_exists():\n        if self.show_log:\n            logger.info(f\"Model not found at {self.model_path}, will download from HuggingFace\")\n        self._download_model()\n\n    # Load model\n    try:\n        self._load_model()\n        if self.show_log:\n            logger.success(\"Model initialized successfully\")\n    except Exception as e:\n        if self.show_log:\n            logger.error(\"Failed to initialize model\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.RTDETRLayoutDetector.detect","title":"detect","text":"<pre><code>detect(input_path: Union[str, Path], confidence_threshold: Optional[float] = None, **kwargs) -&gt; Tuple[Image.Image, LayoutOutput]\n</code></pre> <p>Run layout detection using RT-DETR Transformers model.</p> Source code in <code>omnidocs/tasks/layout_analysis/extractors/rtdetr.py</code> <pre><code>@log_execution_time\ndef detect(\n    self,\n    input_path: Union[str, Path],\n    confidence_threshold: Optional[float] = None,\n    **kwargs\n) -&gt; Tuple[Image.Image, LayoutOutput]:\n    \"\"\"Run layout detection using RT-DETR Transformers model.\"\"\"\n    if self.model is None:\n        raise RuntimeError(\"Model not loaded. Initialization failed.\")\n\n    try:\n        # Load and preprocess image\n        if isinstance(input_path, (str, Path)):\n            image = Image.open(input_path).convert(\"RGB\")\n        elif isinstance(input_path, Image.Image):\n            image = input_path.convert(\"RGB\")\n        elif isinstance(input_path, np.ndarray):\n            image = Image.fromarray(input_path).convert(\"RGB\")\n        else:\n            raise ValueError(\"Unsupported input type\")\n\n        # Preprocess the image using the image processor\n        resize = {\"height\": self.image_size, \"width\": self.image_size}\n        inputs = self.image_processor(\n            images=image,\n            return_tensors=\"pt\",\n            size=resize,\n        )\n\n        # Move inputs to the correct device\n        if self.device == \"cuda\":\n            inputs = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n\n        # Run inference\n        try:\n            with torch.no_grad():\n                outputs = self.model(**inputs)\n        except Exception as e:\n            raise RuntimeError(f\"Error during model inference: {e}\") from e \n\n        # Post-process results\n        threshold = confidence_threshold or self.confidence_threshold\n        results = self.image_processor.post_process_object_detection(\n            outputs,\n            target_sizes=torch.tensor([image.size[::-1]]),\n            threshold=threshold\n        )\n\n        # Process predictions\n        layout_boxes = []\n\n        for result in results:\n            for score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n                score_val = float(score.item())\n                label_idx = int(label_id.item())\n\n                # Get label from model config (add 1 because model config is 0-indexed)\n                model_label = self.model.config.id2label.get(label_idx + 1)\n                if not model_label:\n                    continue\n\n                # Map to standardized label\n                mapped_label = self.map_label(model_label)\n                if not mapped_label:\n                    continue\n\n                # Convert box coordinates (already in image space)\n                box = [round(i, 2) for i in box.tolist()]\n                left, top, right, bottom = box\n\n                layout_boxes.append(\n                    LayoutBox(\n                        label=mapped_label,\n                        bbox=[left, top, right, bottom],\n                        confidence=score_val\n                    )\n                )\n\n        # Create annotated image\n        annotated_img = image.copy()\n        draw = ImageDraw.Draw(annotated_img)\n\n        # Draw boxes with standardized colors\n        for box in layout_boxes:\n            color = self.color_map.get(box.label, 'gray')\n            coords = box.bbox\n            draw.rectangle(coords, outline=color, width=3)\n            draw.text((coords[0], coords[1]-20), box.label, fill=color)\n\n        return annotated_img, LayoutOutput(bboxes=layout_boxes)\n\n    except Exception as e:\n        if self.show_log:\n            logger.error(\"Error during prediction\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.SuryaLayoutDetector","title":"SuryaLayoutDetector","text":"<pre><code>SuryaLayoutDetector(device: Optional[str] = None, show_log: bool = False, **kwargs)\n</code></pre> <p>               Bases: <code>BaseLayoutDetector</code></p> <p>Surya-based layout detection implementation.</p> <p>Initialize Surya Layout Detector.</p> Source code in <code>omnidocs/tasks/layout_analysis/extractors/surya.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    **kwargs\n):\n    \"\"\"Initialize Surya Layout Detector.\"\"\"\n    super().__init__(show_log=show_log)\n\n    # Initialize label mapper\n    self._label_mapper = SuryaLayoutMapper()\n\n    if self.show_log:\n        logger.info(\"Initializing SuryaLayoutDetector\")\n\n    # Set device if specified, otherwise use default from parent\n    if device:\n        self.device = device\n\n    if self.show_log:\n        logger.info(f\"Using device: {self.device}\")\n\n    try:\n        # Import required libraries - use new API\n        import surya\n        if self.show_log:\n            logger.info(f\"Found surya package at: {surya.__file__}\")\n    except ImportError as ex:\n        if self.show_log:\n            logger.error(\"Failed to import surya\")\n        raise ImportError(\n            \"surya is not available. Please install it with: pip install surya-ocr\"\n        ) from ex\n\n    try:\n        # Initialize detection and layout models using new API\n        from surya.layout import LayoutPredictor\n\n        self.layout_predictor = LayoutPredictor()\n\n        if self.show_log:\n            logger.success(\"Models initialized successfully\")\n\n    except Exception as e:\n        if self.show_log:\n            logger.error(\"Failed to initialize models\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.SuryaLayoutDetector.detect","title":"detect","text":"<pre><code>detect(input_path: Union[str, Path], **kwargs) -&gt; Tuple[Image.Image, LayoutOutput]\n</code></pre> <p>Run layout detection with standardized labels.</p> Source code in <code>omnidocs/tasks/layout_analysis/extractors/surya.py</code> <pre><code>@log_execution_time\ndef detect(\n    self,\n    input_path: Union[str, Path],\n    **kwargs\n) -&gt; Tuple[Image.Image, LayoutOutput]:\n    \"\"\"Run layout detection with standardized labels.\"\"\"\n    try:\n        # Load and preprocess input\n        if isinstance(input_path, (str, Path)):\n            image = Image.open(input_path).convert(\"RGB\")\n        elif isinstance(input_path, Image.Image):\n            image = input_path.convert(\"RGB\")\n        elif isinstance(input_path, np.ndarray):\n            image = Image.fromarray(input_path).convert(\"RGB\")\n        else:\n            raise ValueError(\"Unsupported input type\")\n\n        # Run layout detection using new API\n        layout_predictions = self.layout_predictor([image])\n\n        # Process the layout prediction (take first since we only processed one image)\n        layout_pred = layout_predictions[0]\n\n        # Convert to standardized format\n        layout_boxes = []\n        for box in layout_pred.bboxes:\n            mapped_label = self.map_label(box.label)\n            if mapped_label:\n                layout_boxes.append(\n                    LayoutBox(\n                        label=mapped_label,\n                        bbox=box.bbox,  # Already in [x1, y1, x2, y2] format\n                        confidence=box.confidence\n                    )\n                )\n\n        # Create annotated image\n        annotated_img = image.copy()\n        draw = ImageDraw.Draw(annotated_img)\n\n        # Draw boxes with standardized colors\n        for box in layout_boxes:\n            color = self.color_map.get(box.label, 'gray')\n            coords = box.bbox\n            draw.rectangle(coords, outline=color, width=3)\n            draw.text((coords[0], coords[1]-20), box.label, fill=color)\n\n        # Create LayoutOutput with image size\n        layout_output = LayoutOutput(\n            bboxes=layout_boxes,\n            image_size=image.size\n        )\n\n        return annotated_img, layout_output\n\n    except Exception as e:\n        if self.show_log:\n            logger.error(\"Error during prediction\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.SuryaLayoutDetector.visualize","title":"visualize","text":"<pre><code>visualize(detection_result: Tuple[Image, LayoutOutput], output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save annotated image and layout data to files.</p> <p>Parameters:</p> Name Type Description Default <code>detection_result</code> <code>Tuple[Image, LayoutOutput]</code> <p>Tuple containing (PIL Image, LayoutOutput)</p> required <code>output_path</code> <code>Union[str, Path]</code> <p>Path to save visualization</p> required Source code in <code>omnidocs/tasks/layout_analysis/extractors/surya.py</code> <pre><code>def visualize(\n    self,\n    detection_result: Tuple[Image.Image, LayoutOutput],\n    output_path: Union[str, Path],\n) -&gt; None:\n    \"\"\"\n    Save annotated image and layout data to files.\n\n    Args:\n        detection_result: Tuple containing (PIL Image, LayoutOutput)\n        output_path: Path to save visualization\n    \"\"\"\n    super().visualize(detection_result, output_path)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.YOLOLayoutDetector","title":"YOLOLayoutDetector","text":"<pre><code>YOLOLayoutDetector(device: Optional[str] = None, show_log: bool = False, model_path: Optional[Union[str, Path]] = None)\n</code></pre> <p>               Bases: <code>BaseLayoutDetector</code></p> <p>YOLO-based layout detection implementation.</p> <p>Initialize YOLO Layout Detector.</p> Source code in <code>omnidocs/tasks/layout_analysis/extractors/doc_layout_yolo.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    model_path: Optional[Union[str, Path]] = None\n):\n    \"\"\"Initialize YOLO Layout Detector.\"\"\"\n    super().__init__(show_log=show_log)\n\n    self._label_mapper = YOLOLayoutMapper()\n    if self.show_log:\n        logger.info(f\"Initializing YOLOLayoutDetector\")\n\n    if device:\n        self.device = device\n    if self.show_log:\n        logger.info(f\"Using device: {self.device}\")\n\n    # Set default paths\n    if model_path is None:\n        model_path = _MODELS_DIR / \"yolo_layout\" / self.MODEL_REPO.replace(\"/\", \"_\")\n\n    self.model_path = Path(model_path)\n    if self.show_log:\n        logger.info(f\"Model directory: {self.model_path}\")\n\n    self.conf_threshold = 0.2\n    self.img_size = 1024\n\n    # Check dependencies\n    self._check_dependencies()\n\n    # Download model if needed\n    if not self._model_exists():\n        if self.show_log:\n            logger.info(f\"Model not found at {self.model_path}, will download from HuggingFace\")\n        self._download_model()\n\n    # Load model\n    try:\n        self._load_model()\n        if self.show_log:\n            logger.success(\"Model initialized successfully\")\n    except Exception as e:\n        if self.show_log:\n            logger.error(\"Failed to initialize model\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.YOLOLayoutDetector.detect","title":"detect","text":"<pre><code>detect(input_path: Union[str, Path], conf_threshold: float = None, img_size: int = None, **kwargs) -&gt; Tuple[Image.Image, LayoutOutput]\n</code></pre> <p>Run layout detection with standardized labels.</p> Source code in <code>omnidocs/tasks/layout_analysis/extractors/doc_layout_yolo.py</code> <pre><code>@log_execution_time\ndef detect(\n    self,\n    input_path: Union[str, Path],\n    conf_threshold: float = None,\n    img_size: int = None,\n    **kwargs,\n) -&gt; Tuple[Image.Image, LayoutOutput]:\n    \"\"\"Run layout detection with standardized labels.\"\"\"\n    if self.model is None:\n        raise RuntimeError(\"Model not loaded. Initialization failed.\")\n\n    conf = conf_threshold if conf_threshold else self.conf_threshold\n    imgsz = img_size if img_size else self.img_size\n\n    try:\n        images = self.preprocess_input(input_path)\n\n        results = []\n        for img in images:\n            # Get detection results\n            det_result = self.model.predict(\n                img, imgsz=imgsz, conf=conf, device=self.device, **kwargs\n            )\n\n            # Convert detection results to LayoutBox objects\n            layout_boxes = []\n            for box in det_result[0].boxes:\n                model_label = det_result[0].names[int(box.cls[0])]\n                mapped_label = self.map_label(model_label)\n\n                if mapped_label:\n                    layout_boxes.append(\n                        LayoutBox(\n                            label=mapped_label,\n                            bbox=box.xyxy[0].tolist(),\n                            confidence=float(box.conf[0]) if box.conf is not None else None\n                        )\n                    )\n\n            # Get the annotated image (will be a numpy array)\n            annotated_img_array = det_result[0].plot(labels=False)  # Disable YOLO's default labels\n\n            # Convert numpy array to PIL Image\n            annotated_img = Image.fromarray(cv2.cvtColor(annotated_img_array, cv2.COLOR_BGR2RGB))\n\n            # Draw standardized labels on the image\n            draw = ImageDraw.Draw(annotated_img)\n            for box in layout_boxes:\n                color = self.color_map.get(box.label, 'gray')\n                coords = box.bbox\n                draw.rectangle(coords, outline=color, width=3)\n                draw.text((coords[0], coords[1]-20), box.label, fill=color)\n\n            results.append((\n                annotated_img,\n                LayoutOutput(bboxes=layout_boxes)\n            ))\n\n        return results[0] if results else (None, LayoutOutput(bboxes=[]))\n\n    except Exception as e:\n        if self.show_log:\n            logger.error(\"Error during prediction\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.YOLOLayoutDetector.visualize","title":"visualize","text":"<pre><code>visualize(detection_result: Tuple[Image, LayoutOutput], output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save the annotated image to file.</p> <p>Parameters:</p> Name Type Description Default <code>detection_result</code> <code>Tuple[Image, LayoutOutput]</code> <p>Tuple containing (PIL Image, LayoutOutput)</p> required <code>output_path</code> <code>Union[str, Path]</code> <p>Path to save visualization</p> required Source code in <code>omnidocs/tasks/layout_analysis/extractors/doc_layout_yolo.py</code> <pre><code>def visualize(\n    self,\n    detection_result: Tuple[Image.Image, LayoutOutput],\n    output_path: Union[str, Path],\n) -&gt; None:\n    \"\"\"\n    Save the annotated image to file.\n\n    Args:\n        detection_result: Tuple containing (PIL Image, LayoutOutput)\n        output_path: Path to save visualization\n    \"\"\"\n    annotated_image, _ = detection_result\n\n    # Convert numpy array to PIL Image if necessary\n    if isinstance(annotated_image, np.ndarray):\n        annotated_image = Image.fromarray(annotated_image)\n\n    if annotated_image is not None:\n        annotated_image.save(str(output_path))\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo","title":"omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo.YOLOLayoutDetector","title":"YOLOLayoutDetector","text":"<pre><code>YOLOLayoutDetector(device: Optional[str] = None, show_log: bool = False, model_path: Optional[Union[str, Path]] = None)\n</code></pre> <p>               Bases: <code>BaseLayoutDetector</code></p> <p>YOLO-based layout detection implementation.</p> <p>Initialize YOLO Layout Detector.</p> Source code in <code>omnidocs/tasks/layout_analysis/extractors/doc_layout_yolo.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    model_path: Optional[Union[str, Path]] = None\n):\n    \"\"\"Initialize YOLO Layout Detector.\"\"\"\n    super().__init__(show_log=show_log)\n\n    self._label_mapper = YOLOLayoutMapper()\n    if self.show_log:\n        logger.info(f\"Initializing YOLOLayoutDetector\")\n\n    if device:\n        self.device = device\n    if self.show_log:\n        logger.info(f\"Using device: {self.device}\")\n\n    # Set default paths\n    if model_path is None:\n        model_path = _MODELS_DIR / \"yolo_layout\" / self.MODEL_REPO.replace(\"/\", \"_\")\n\n    self.model_path = Path(model_path)\n    if self.show_log:\n        logger.info(f\"Model directory: {self.model_path}\")\n\n    self.conf_threshold = 0.2\n    self.img_size = 1024\n\n    # Check dependencies\n    self._check_dependencies()\n\n    # Download model if needed\n    if not self._model_exists():\n        if self.show_log:\n            logger.info(f\"Model not found at {self.model_path}, will download from HuggingFace\")\n        self._download_model()\n\n    # Load model\n    try:\n        self._load_model()\n        if self.show_log:\n            logger.success(\"Model initialized successfully\")\n    except Exception as e:\n        if self.show_log:\n            logger.error(\"Failed to initialize model\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo.YOLOLayoutDetector.detect","title":"detect","text":"<pre><code>detect(input_path: Union[str, Path], conf_threshold: float = None, img_size: int = None, **kwargs) -&gt; Tuple[Image.Image, LayoutOutput]\n</code></pre> <p>Run layout detection with standardized labels.</p> Source code in <code>omnidocs/tasks/layout_analysis/extractors/doc_layout_yolo.py</code> <pre><code>@log_execution_time\ndef detect(\n    self,\n    input_path: Union[str, Path],\n    conf_threshold: float = None,\n    img_size: int = None,\n    **kwargs,\n) -&gt; Tuple[Image.Image, LayoutOutput]:\n    \"\"\"Run layout detection with standardized labels.\"\"\"\n    if self.model is None:\n        raise RuntimeError(\"Model not loaded. Initialization failed.\")\n\n    conf = conf_threshold if conf_threshold else self.conf_threshold\n    imgsz = img_size if img_size else self.img_size\n\n    try:\n        images = self.preprocess_input(input_path)\n\n        results = []\n        for img in images:\n            # Get detection results\n            det_result = self.model.predict(\n                img, imgsz=imgsz, conf=conf, device=self.device, **kwargs\n            )\n\n            # Convert detection results to LayoutBox objects\n            layout_boxes = []\n            for box in det_result[0].boxes:\n                model_label = det_result[0].names[int(box.cls[0])]\n                mapped_label = self.map_label(model_label)\n\n                if mapped_label:\n                    layout_boxes.append(\n                        LayoutBox(\n                            label=mapped_label,\n                            bbox=box.xyxy[0].tolist(),\n                            confidence=float(box.conf[0]) if box.conf is not None else None\n                        )\n                    )\n\n            # Get the annotated image (will be a numpy array)\n            annotated_img_array = det_result[0].plot(labels=False)  # Disable YOLO's default labels\n\n            # Convert numpy array to PIL Image\n            annotated_img = Image.fromarray(cv2.cvtColor(annotated_img_array, cv2.COLOR_BGR2RGB))\n\n            # Draw standardized labels on the image\n            draw = ImageDraw.Draw(annotated_img)\n            for box in layout_boxes:\n                color = self.color_map.get(box.label, 'gray')\n                coords = box.bbox\n                draw.rectangle(coords, outline=color, width=3)\n                draw.text((coords[0], coords[1]-20), box.label, fill=color)\n\n            results.append((\n                annotated_img,\n                LayoutOutput(bboxes=layout_boxes)\n            ))\n\n        return results[0] if results else (None, LayoutOutput(bboxes=[]))\n\n    except Exception as e:\n        if self.show_log:\n            logger.error(\"Error during prediction\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo.YOLOLayoutDetector.visualize","title":"visualize","text":"<pre><code>visualize(detection_result: Tuple[Image, LayoutOutput], output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save the annotated image to file.</p> <p>Parameters:</p> Name Type Description Default <code>detection_result</code> <code>Tuple[Image, LayoutOutput]</code> <p>Tuple containing (PIL Image, LayoutOutput)</p> required <code>output_path</code> <code>Union[str, Path]</code> <p>Path to save visualization</p> required Source code in <code>omnidocs/tasks/layout_analysis/extractors/doc_layout_yolo.py</code> <pre><code>def visualize(\n    self,\n    detection_result: Tuple[Image.Image, LayoutOutput],\n    output_path: Union[str, Path],\n) -&gt; None:\n    \"\"\"\n    Save the annotated image to file.\n\n    Args:\n        detection_result: Tuple containing (PIL Image, LayoutOutput)\n        output_path: Path to save visualization\n    \"\"\"\n    annotated_image, _ = detection_result\n\n    # Convert numpy array to PIL Image if necessary\n    if isinstance(annotated_image, np.ndarray):\n        annotated_image = Image.fromarray(annotated_image)\n\n    if annotated_image is not None:\n        annotated_image.save(str(output_path))\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo.YOLOLayoutMapper","title":"YOLOLayoutMapper","text":"<pre><code>YOLOLayoutMapper()\n</code></pre> <p>               Bases: <code>BaseLayoutMapper</code></p> <p>Label mapper for YOLO layout detection model.</p> Source code in <code>omnidocs/tasks/layout_analysis/base.py</code> <pre><code>def __init__(self):\n    self._mapping: Dict[str, LayoutLabel] = {}\n    self._reverse_mapping: Dict[LayoutLabel, str] = {}\n    self._setup_mapping()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.florence","title":"omnidocs.tasks.layout_analysis.extractors.florence","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.florence.FlorenceLayoutDetector","title":"FlorenceLayoutDetector","text":"<pre><code>FlorenceLayoutDetector(device: Optional[str] = None, show_log: bool = False, trust_remote_code: bool = True, **kwargs)\n</code></pre> <p>               Bases: <code>BaseLayoutDetector</code></p> <p>Florence-based layout detection implementation.</p> <p>Initialize Florence Layout Detector.</p> Source code in <code>omnidocs/tasks/layout_analysis/extractors/florence.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    trust_remote_code: bool = True,\n    **kwargs\n):\n    \"\"\"Initialize Florence Layout Detector.\"\"\"\n    super().__init__(show_log=show_log)\n\n    # Initialize label mapper\n    self._label_mapper = FlorenceLayoutMapper()\n\n    logger.info(\"Initializing FlorenceLayoutDetector\")\n\n    if device:\n        self.device = device\n    logger.info(f\"Using device: {self.device}\")\n\n    try:\n        from transformers import AutoProcessor, AutoModelForCausalLM\n    except ImportError as ex:\n        logger.error(\"Failed to import transformers\")\n        raise ImportError(\n            \"transformers is not available. Please install it with: pip install transformers\"\n        ) from ex\n\n    # Initialize the model and processor\n    try:\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.MODEL_REPO,\n            trust_remote_code=trust_remote_code,\n            **kwargs\n        )\n        self.processor = AutoProcessor.from_pretrained(\n            self.MODEL_REPO,\n            trust_remote_code=trust_remote_code\n        )\n        self.model.to(self.device)\n        logger.success(\"Model initialized successfully\")\n    except Exception as e:\n        logger.error(\"Failed to initialize model\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.florence.FlorenceLayoutDetector.detect","title":"detect","text":"<pre><code>detect(input_path: Union[str, Path], max_new_tokens: int = 1024, do_sample: bool = False, num_beams: int = 3, **kwargs) -&gt; Tuple[Image.Image, LayoutOutput]\n</code></pre> <p>Run layout detection with standardized labels.</p> Source code in <code>omnidocs/tasks/layout_analysis/extractors/florence.py</code> <pre><code>@log_execution_time\ndef detect(\n    self,\n    input_path: Union[str, Path],\n    max_new_tokens: int = 1024,\n    do_sample: bool = False,\n    num_beams: int = 3,\n    **kwargs\n) -&gt; Tuple[Image.Image, LayoutOutput]:\n    \"\"\"Run layout detection with standardized labels.\"\"\"\n    try:\n        # Load and preprocess input\n        image = Image.open(input_path).convert(\"RGB\")\n\n        # Prepare inputs\n        prompt = \"&lt;OD&gt;\"\n        inputs = self.processor(\n            text=prompt,\n            images=image,\n            return_tensors=\"pt\"\n        ).to(self.device)\n\n        # Generate predictions\n        generated_ids = self.model.generate(\n            input_ids=inputs[\"input_ids\"],\n            pixel_values=inputs[\"pixel_values\"],\n            max_new_tokens=max_new_tokens,\n            do_sample=do_sample,\n            num_beams=num_beams,\n            **kwargs\n        )\n\n        # Decode and post-process\n        generated_text = self.processor.batch_decode(\n            generated_ids,\n            skip_special_tokens=False\n        )[0]\n\n        parsed_result = self.processor.post_process_generation(\n            generated_text,\n            task=\"&lt;OD&gt;\",\n            image_size=(image.width, image.height)\n        )\n\n        # Convert to standard format\n        layout_boxes = []\n        for bbox, label in zip(\n            parsed_result[\"&lt;OD&gt;\"][\"bboxes\"],\n            parsed_result[\"&lt;OD&gt;\"][\"labels\"]\n        ):\n            mapped_label = self.map_label(label.lower())\n            if mapped_label:\n                layout_boxes.append(\n                    LayoutBox(\n                        label=mapped_label,\n                        bbox=[float(coord) for coord in bbox],\n                        confidence=None  # Florence model doesn't provide confidence scores\n                    )\n                )\n\n        # Create annotated image\n        annotated_img = image.copy()\n        draw = ImageDraw.Draw(annotated_img)\n\n        # Draw boxes and labels\n        for box in layout_boxes:\n            color = self.color_map.get(box.label, 'gray')\n            coords = box.bbox\n            draw.rectangle(coords, outline=color, width=3)\n            draw.text((coords[0], coords[1]-20), box.label, fill=color)\n\n        return annotated_img, LayoutOutput(bboxes=layout_boxes)\n\n    except Exception as e:\n        logger.error(\"Error during prediction\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.florence.FlorenceLayoutDetector.visualize","title":"visualize","text":"<pre><code>visualize(detection_result: Tuple[Image, LayoutOutput], output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save annotated image and layout data to files.</p> <p>Parameters:</p> Name Type Description Default <code>detection_result</code> <code>Tuple[Image, LayoutOutput]</code> <p>Tuple containing (PIL Image, LayoutOutput)</p> required <code>output_path</code> <code>Union[str, Path]</code> <p>Path to save visualization</p> required Source code in <code>omnidocs/tasks/layout_analysis/extractors/florence.py</code> <pre><code>def visualize(\n    self,\n    detection_result: Tuple[Image.Image, LayoutOutput],\n    output_path: Union[str, Path],\n) -&gt; None:\n    \"\"\"\n    Save annotated image and layout data to files.\n\n    Args:\n        detection_result: Tuple containing (PIL Image, LayoutOutput)\n        output_path: Path to save visualization\n    \"\"\"\n    super().visualize(detection_result, output_path)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.florence.FlorenceLayoutMapper","title":"FlorenceLayoutMapper","text":"<pre><code>FlorenceLayoutMapper()\n</code></pre> <p>               Bases: <code>BaseLayoutMapper</code></p> <p>Label mapper for Florence layout detection model.</p> Source code in <code>omnidocs/tasks/layout_analysis/base.py</code> <pre><code>def __init__(self):\n    self._mapping: Dict[str, LayoutLabel] = {}\n    self._reverse_mapping: Dict[LayoutLabel, str] = {}\n    self._setup_mapping()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.paddle","title":"omnidocs.tasks.layout_analysis.extractors.paddle","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.paddle.PaddleLayoutDetector","title":"PaddleLayoutDetector","text":"<pre><code>PaddleLayoutDetector(device: Optional[str] = None, show_log: bool = False, **kwargs)\n</code></pre> <p>               Bases: <code>BaseLayoutDetector</code></p> <p>PaddleOCR-based layout detection implementation.</p> <p>Initialize PaddleOCR Layout Detector.</p> Source code in <code>omnidocs/tasks/layout_analysis/extractors/paddle.py</code> <pre><code>def __init__(\n    self, \n    device: Optional[str] = None,\n    show_log: bool = False,\n    **kwargs\n):\n    \"\"\"Initialize PaddleOCR Layout Detector.\"\"\"\n    super().__init__()\n\n    # Initialize label mapper\n    self._label_mapper = PaddleLayoutMapper()\n\n    # Log initialization\n    logger.info(\"Initializing PaddleLayoutDetector\")\n\n    # Set device if specified\n    if device:\n        self.device = device\n    logger.info(f\"Using device: {self.device}\")\n\n    try:\n        from paddleocr import PPStructure\n    except ImportError as ex:\n        logger.error(\"Failed to import paddleocr\")\n        raise ImportError(\n            \"paddleocr is not available. Please install it with: pip install paddleocr\"\n        ) from ex\n\n\n    # Initialize the model\n    try:\n        self.model = PPStructure(\n            table=True,\n            ocr=True,\n            show_log=show_log,\n            **kwargs\n        )\n        logger.success(\"Model initialized successfully\")\n    except Exception as e:\n        logger.error(\"Failed to initialize model\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.paddle.PaddleLayoutDetector.detect","title":"detect","text":"<pre><code>detect(input_path: Union[str, Path], **kwargs) -&gt; Tuple[Image.Image, LayoutOutput]\n</code></pre> <p>Run layout detection with standardized labels.</p> Source code in <code>omnidocs/tasks/layout_analysis/extractors/paddle.py</code> <pre><code>@log_execution_time\ndef detect(\n    self, \n    input_path: Union[str, Path], \n    **kwargs\n) -&gt; Tuple[Image.Image, LayoutOutput]:\n    \"\"\"Run layout detection with standardized labels.\"\"\"\n    try:\n        # Load and preprocess input\n        images = self.preprocess_input(input_path)\n\n        results = []\n        for img in images:\n            # Get detection results\n            det_result = self.model(img)\n\n            # Convert to PIL Image if needed\n            if isinstance(img, np.ndarray):\n                img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n\n            # Create annotated image\n            annotated_img = img.copy()\n            draw = ImageDraw.Draw(annotated_img)\n\n            # Convert detection results to LayoutBox objects with standardized labels\n            layout_boxes = []\n\n            for block in det_result:\n                # Extract coordinates and type\n                x1, y1, x2, y2 = block['bbox']\n                model_label = block['type']\n                mapped_label = self.map_label(model_label)\n\n                if mapped_label:  # Only include boxes with valid mapped labels\n                    layout_boxes.append(\n                        LayoutBox(\n                            label=mapped_label,\n                            bbox=[float(x1), float(y1), float(x2), float(y2)],\n                            confidence=block.get('confidence', None)\n                        )\n                    )\n\n                    # Draw with standardized colors\n                    color = self.color_map.get(mapped_label, 'gray')\n                    draw.rectangle([x1, y1, x2, y2], outline=color, width=3)\n                    draw.text((x1, y1-20), mapped_label, fill=color)\n\n            results.append((\n                annotated_img,\n                LayoutOutput(bboxes=layout_boxes)\n            ))\n\n        return results[0] if results else (None, LayoutOutput(bboxes=[]))\n\n    except Exception as e:\n        logger.error(\"Error during prediction\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.paddle.PaddleLayoutDetector.visualize","title":"visualize","text":"<pre><code>visualize(detection_result: Tuple[Image, LayoutOutput], output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save annotated image and layout data to files.</p> <p>Parameters:</p> Name Type Description Default <code>detection_result</code> <code>Tuple[Image, LayoutOutput]</code> <p>Tuple containing (PIL Image, LayoutOutput)</p> required <code>output_path</code> <code>Union[str, Path]</code> <p>Path to save visualization</p> required Source code in <code>omnidocs/tasks/layout_analysis/extractors/paddle.py</code> <pre><code>def visualize(\n    self,\n    detection_result: Tuple[Image.Image, LayoutOutput],\n    output_path: Union[str, Path],\n) -&gt; None:\n    \"\"\"\n    Save annotated image and layout data to files.\n\n    Args:\n        detection_result: Tuple containing (PIL Image, LayoutOutput)\n        output_path: Path to save visualization\n    \"\"\"\n    super().visualize(detection_result, output_path)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.paddle.PaddleLayoutMapper","title":"PaddleLayoutMapper","text":"<pre><code>PaddleLayoutMapper()\n</code></pre> <p>               Bases: <code>BaseLayoutMapper</code></p> <p>Label mapper for PaddleOCR layout detection model.</p> Source code in <code>omnidocs/tasks/layout_analysis/base.py</code> <pre><code>def __init__(self):\n    self._mapping: Dict[str, LayoutLabel] = {}\n    self._reverse_mapping: Dict[LayoutLabel, str] = {}\n    self._setup_mapping()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.rtdetr","title":"omnidocs.tasks.layout_analysis.extractors.rtdetr","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.rtdetr.RTDETRLayoutDetector","title":"RTDETRLayoutDetector","text":"<pre><code>RTDETRLayoutDetector(device: Optional[str] = None, show_log: bool = False, model_path: Optional[Union[str, Path]] = None, num_threads: Optional[int] = 4, use_cpu_only: bool = True)\n</code></pre> <p>               Bases: <code>BaseLayoutDetector</code></p> <p>RT-DETR-based layout detection implementation.</p> <p>Initialize RT-DETR Layout Detector with careful device handling.</p> Source code in <code>omnidocs/tasks/layout_analysis/extractors/rtdetr.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    model_path: Optional[Union[str, Path]] = None,\n    num_threads: Optional[int] = 4,\n    use_cpu_only: bool = True\n):\n    \"\"\"Initialize RT-DETR Layout Detector with careful device handling.\"\"\"\n    super().__init__(show_log=show_log)\n\n    self._label_mapper = RTDETRLayoutMapper()\n\n    if self.show_log:\n        logger.info(\"Initializing RTDETRLayoutDetector\")\n\n    # Set default paths\n    if model_path is None:\n        model_path = _MODELS_DIR / \"rtdetr_layout\" / self.MODEL_REPO.replace(\"/\", \"_\")\n\n    self.model_path = Path(model_path)\n    self.num_threads = num_threads\n\n    # Careful device handling\n    if use_cpu_only:\n        self.device = \"cpu\"\n        if self.show_log:\n            logger.info(\"Forced CPU usage due to use_cpu_only flag\")\n    elif device:\n        self.device = device\n        if self.show_log:\n            logger.info(f\"Using specified device: {device}\")\n    else:\n        # Check CUDA availability with error handling\n        try:\n            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            if self.show_log:\n                logger.info(f\"Automatically selected device: {self.device}\")\n        except Exception as e:\n            self.device = \"cpu\"\n            if self.show_log:\n                logger.warning(f\"Error checking CUDA availability: {e}. Defaulting to CPU\")\n\n    self.num_threads = num_threads or int(os.environ.get(\"OMP_NUM_THREADS\", 4))\n\n    # Set thread count for CPU operations\n    if self.device == \"cpu\":\n        torch.set_num_threads(self.num_threads)\n        if self.show_log:\n            logger.info(f\"Set CPU threads to {self.num_threads}\")\n\n    # Model parameters\n    self.image_size = 640\n    self.confidence_threshold = 0.6\n\n    # Check dependencies\n    self._check_dependencies()\n\n    # Download model if needed\n    if not self._model_exists():\n        if self.show_log:\n            logger.info(f\"Model not found at {self.model_path}, will download from HuggingFace\")\n        self._download_model()\n\n    # Load model\n    try:\n        self._load_model()\n        if self.show_log:\n            logger.success(\"Model initialized successfully\")\n    except Exception as e:\n        if self.show_log:\n            logger.error(\"Failed to initialize model\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.rtdetr.RTDETRLayoutDetector.detect","title":"detect","text":"<pre><code>detect(input_path: Union[str, Path], confidence_threshold: Optional[float] = None, **kwargs) -&gt; Tuple[Image.Image, LayoutOutput]\n</code></pre> <p>Run layout detection using RT-DETR Transformers model.</p> Source code in <code>omnidocs/tasks/layout_analysis/extractors/rtdetr.py</code> <pre><code>@log_execution_time\ndef detect(\n    self,\n    input_path: Union[str, Path],\n    confidence_threshold: Optional[float] = None,\n    **kwargs\n) -&gt; Tuple[Image.Image, LayoutOutput]:\n    \"\"\"Run layout detection using RT-DETR Transformers model.\"\"\"\n    if self.model is None:\n        raise RuntimeError(\"Model not loaded. Initialization failed.\")\n\n    try:\n        # Load and preprocess image\n        if isinstance(input_path, (str, Path)):\n            image = Image.open(input_path).convert(\"RGB\")\n        elif isinstance(input_path, Image.Image):\n            image = input_path.convert(\"RGB\")\n        elif isinstance(input_path, np.ndarray):\n            image = Image.fromarray(input_path).convert(\"RGB\")\n        else:\n            raise ValueError(\"Unsupported input type\")\n\n        # Preprocess the image using the image processor\n        resize = {\"height\": self.image_size, \"width\": self.image_size}\n        inputs = self.image_processor(\n            images=image,\n            return_tensors=\"pt\",\n            size=resize,\n        )\n\n        # Move inputs to the correct device\n        if self.device == \"cuda\":\n            inputs = {k: v.cuda() if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n\n        # Run inference\n        try:\n            with torch.no_grad():\n                outputs = self.model(**inputs)\n        except Exception as e:\n            raise RuntimeError(f\"Error during model inference: {e}\") from e \n\n        # Post-process results\n        threshold = confidence_threshold or self.confidence_threshold\n        results = self.image_processor.post_process_object_detection(\n            outputs,\n            target_sizes=torch.tensor([image.size[::-1]]),\n            threshold=threshold\n        )\n\n        # Process predictions\n        layout_boxes = []\n\n        for result in results:\n            for score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n                score_val = float(score.item())\n                label_idx = int(label_id.item())\n\n                # Get label from model config (add 1 because model config is 0-indexed)\n                model_label = self.model.config.id2label.get(label_idx + 1)\n                if not model_label:\n                    continue\n\n                # Map to standardized label\n                mapped_label = self.map_label(model_label)\n                if not mapped_label:\n                    continue\n\n                # Convert box coordinates (already in image space)\n                box = [round(i, 2) for i in box.tolist()]\n                left, top, right, bottom = box\n\n                layout_boxes.append(\n                    LayoutBox(\n                        label=mapped_label,\n                        bbox=[left, top, right, bottom],\n                        confidence=score_val\n                    )\n                )\n\n        # Create annotated image\n        annotated_img = image.copy()\n        draw = ImageDraw.Draw(annotated_img)\n\n        # Draw boxes with standardized colors\n        for box in layout_boxes:\n            color = self.color_map.get(box.label, 'gray')\n            coords = box.bbox\n            draw.rectangle(coords, outline=color, width=3)\n            draw.text((coords[0], coords[1]-20), box.label, fill=color)\n\n        return annotated_img, LayoutOutput(bboxes=layout_boxes)\n\n    except Exception as e:\n        if self.show_log:\n            logger.error(\"Error during prediction\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.rtdetr.RTDETRLayoutMapper","title":"RTDETRLayoutMapper","text":"<pre><code>RTDETRLayoutMapper()\n</code></pre> <p>               Bases: <code>BaseLayoutMapper</code></p> <p>Label mapper for RT-DETR layout detection model.</p> Source code in <code>omnidocs/tasks/layout_analysis/base.py</code> <pre><code>def __init__(self):\n    self._mapping: Dict[str, LayoutLabel] = {}\n    self._reverse_mapping: Dict[LayoutLabel, str] = {}\n    self._setup_mapping()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.surya","title":"omnidocs.tasks.layout_analysis.extractors.surya","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.surya.SuryaLayoutDetector","title":"SuryaLayoutDetector","text":"<pre><code>SuryaLayoutDetector(device: Optional[str] = None, show_log: bool = False, **kwargs)\n</code></pre> <p>               Bases: <code>BaseLayoutDetector</code></p> <p>Surya-based layout detection implementation.</p> <p>Initialize Surya Layout Detector.</p> Source code in <code>omnidocs/tasks/layout_analysis/extractors/surya.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    **kwargs\n):\n    \"\"\"Initialize Surya Layout Detector.\"\"\"\n    super().__init__(show_log=show_log)\n\n    # Initialize label mapper\n    self._label_mapper = SuryaLayoutMapper()\n\n    if self.show_log:\n        logger.info(\"Initializing SuryaLayoutDetector\")\n\n    # Set device if specified, otherwise use default from parent\n    if device:\n        self.device = device\n\n    if self.show_log:\n        logger.info(f\"Using device: {self.device}\")\n\n    try:\n        # Import required libraries - use new API\n        import surya\n        if self.show_log:\n            logger.info(f\"Found surya package at: {surya.__file__}\")\n    except ImportError as ex:\n        if self.show_log:\n            logger.error(\"Failed to import surya\")\n        raise ImportError(\n            \"surya is not available. Please install it with: pip install surya-ocr\"\n        ) from ex\n\n    try:\n        # Initialize detection and layout models using new API\n        from surya.layout import LayoutPredictor\n\n        self.layout_predictor = LayoutPredictor()\n\n        if self.show_log:\n            logger.success(\"Models initialized successfully\")\n\n    except Exception as e:\n        if self.show_log:\n            logger.error(\"Failed to initialize models\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.surya.SuryaLayoutDetector.detect","title":"detect","text":"<pre><code>detect(input_path: Union[str, Path], **kwargs) -&gt; Tuple[Image.Image, LayoutOutput]\n</code></pre> <p>Run layout detection with standardized labels.</p> Source code in <code>omnidocs/tasks/layout_analysis/extractors/surya.py</code> <pre><code>@log_execution_time\ndef detect(\n    self,\n    input_path: Union[str, Path],\n    **kwargs\n) -&gt; Tuple[Image.Image, LayoutOutput]:\n    \"\"\"Run layout detection with standardized labels.\"\"\"\n    try:\n        # Load and preprocess input\n        if isinstance(input_path, (str, Path)):\n            image = Image.open(input_path).convert(\"RGB\")\n        elif isinstance(input_path, Image.Image):\n            image = input_path.convert(\"RGB\")\n        elif isinstance(input_path, np.ndarray):\n            image = Image.fromarray(input_path).convert(\"RGB\")\n        else:\n            raise ValueError(\"Unsupported input type\")\n\n        # Run layout detection using new API\n        layout_predictions = self.layout_predictor([image])\n\n        # Process the layout prediction (take first since we only processed one image)\n        layout_pred = layout_predictions[0]\n\n        # Convert to standardized format\n        layout_boxes = []\n        for box in layout_pred.bboxes:\n            mapped_label = self.map_label(box.label)\n            if mapped_label:\n                layout_boxes.append(\n                    LayoutBox(\n                        label=mapped_label,\n                        bbox=box.bbox,  # Already in [x1, y1, x2, y2] format\n                        confidence=box.confidence\n                    )\n                )\n\n        # Create annotated image\n        annotated_img = image.copy()\n        draw = ImageDraw.Draw(annotated_img)\n\n        # Draw boxes with standardized colors\n        for box in layout_boxes:\n            color = self.color_map.get(box.label, 'gray')\n            coords = box.bbox\n            draw.rectangle(coords, outline=color, width=3)\n            draw.text((coords[0], coords[1]-20), box.label, fill=color)\n\n        # Create LayoutOutput with image size\n        layout_output = LayoutOutput(\n            bboxes=layout_boxes,\n            image_size=image.size\n        )\n\n        return annotated_img, layout_output\n\n    except Exception as e:\n        if self.show_log:\n            logger.error(\"Error during prediction\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.surya.SuryaLayoutDetector.visualize","title":"visualize","text":"<pre><code>visualize(detection_result: Tuple[Image, LayoutOutput], output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save annotated image and layout data to files.</p> <p>Parameters:</p> Name Type Description Default <code>detection_result</code> <code>Tuple[Image, LayoutOutput]</code> <p>Tuple containing (PIL Image, LayoutOutput)</p> required <code>output_path</code> <code>Union[str, Path]</code> <p>Path to save visualization</p> required Source code in <code>omnidocs/tasks/layout_analysis/extractors/surya.py</code> <pre><code>def visualize(\n    self,\n    detection_result: Tuple[Image.Image, LayoutOutput],\n    output_path: Union[str, Path],\n) -&gt; None:\n    \"\"\"\n    Save annotated image and layout data to files.\n\n    Args:\n        detection_result: Tuple containing (PIL Image, LayoutOutput)\n        output_path: Path to save visualization\n    \"\"\"\n    super().visualize(detection_result, output_path)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.layout_analysis.extractors.surya.SuryaLayoutMapper","title":"SuryaLayoutMapper","text":"<pre><code>SuryaLayoutMapper()\n</code></pre> <p>               Bases: <code>BaseLayoutMapper</code></p> <p>Label mapper for Surya layout detection model.</p> Source code in <code>omnidocs/tasks/layout_analysis/base.py</code> <pre><code>def __init__(self):\n    self._mapping: Dict[str, LayoutLabel] = {}\n    self._reverse_mapping: Dict[LayoutLabel, str] = {}\n    self._setup_mapping()\n</code></pre>"},{"location":"api_reference/python_api.html#text-extraction","title":"\ud83d\udcdd Text Extraction","text":"<p>Extract raw and structured text from PDFs and images using classic and deep learning methods.</p>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction","title":"omnidocs.tasks.text_extraction","text":"<p>Text extraction module for OmniDocs.</p> <p>This module provides base classes and implementations for text extraction from documents (PDFs, images, etc.).</p>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.BaseTextExtractor","title":"BaseTextExtractor","text":"<pre><code>BaseTextExtractor(device: Optional[str] = None, show_log: bool = False, engine_name: Optional[str] = None, extract_images: bool = False)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for text extraction models.</p> <p>Initialize the text extractor.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run model on ('cuda' or 'cpu')</p> <code>None</code> <code>show_log</code> <code>bool</code> <p>Whether to show detailed logs</p> <code>False</code> <code>engine_name</code> <code>Optional[str]</code> <p>Name of the text extraction engine</p> <code>None</code> <code>extract_images</code> <code>bool</code> <p>Whether to extract images alongside text</p> <code>False</code> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def __init__(self, \n             device: Optional[str] = None, \n             show_log: bool = False,\n             engine_name: Optional[str] = None,\n             extract_images: bool = False):\n    \"\"\"Initialize the text extractor.\n\n    Args:\n        device: Device to run model on ('cuda' or 'cpu')\n        show_log: Whether to show detailed logs\n        engine_name: Name of the text extraction engine\n        extract_images: Whether to extract images alongside text\n    \"\"\"\n    self.show_log = show_log\n    self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n    self.engine_name = engine_name or self.__class__.__name__.lower().replace('extractor', '')\n    self.extract_images = extract_images\n    self.model = None\n    self.model_path = None\n    self._label_mapper: Optional[BaseTextMapper] = None\n\n    # Initialize mapper if engine name is provided\n    if self.engine_name:\n        self._label_mapper = BaseTextMapper(self.engine_name)\n\n    if self.show_log:\n        logger.info(f\"Initializing {self.__class__.__name__}\")\n        logger.info(f\"Using device: {self.device}\")\n        logger.info(f\"Engine: {self.engine_name}\")\n        logger.info(f\"Extract images: {self.extract_images}\")\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.BaseTextExtractor.label_mapper","title":"label_mapper  <code>property</code>","text":"<pre><code>label_mapper: BaseTextMapper\n</code></pre> <p>Get the label mapper for this extractor.</p>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.BaseTextExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(input_path: Union[str, Path], **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text from input document.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input document</p> required <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>TextOutput containing extracted text</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(\n    self,\n    input_path: Union[str, Path],\n    **kwargs\n) -&gt; TextOutput:\n    \"\"\"Extract text from input document.\n\n    Args:\n        input_path: Path to input document\n        **kwargs: Additional model-specific parameters\n\n    Returns:\n        TextOutput containing extracted text\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.BaseTextExtractor.extract_all","title":"extract_all","text":"<pre><code>extract_all(input_paths: List[Union[str, Path]], **kwargs) -&gt; List[TextOutput]\n</code></pre> <p>Extract text from multiple documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_paths</code> <code>List[Union[str, Path]]</code> <p>List of document paths</p> required <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[TextOutput]</code> <p>List of TextOutput objects</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def extract_all(\n    self,\n    input_paths: List[Union[str, Path]],\n    **kwargs\n) -&gt; List[TextOutput]:\n    \"\"\"Extract text from multiple documents.\n\n    Args:\n        input_paths: List of document paths\n        **kwargs: Additional model-specific parameters\n\n    Returns:\n        List of TextOutput objects\n    \"\"\"\n    results = []\n    for input_path in input_paths:\n        try:\n            result = self.extract(input_path, **kwargs)\n            results.append(result)\n        except Exception as e:\n            if self.show_log:\n                logger.error(f\"Error processing {input_path}: {str(e)}\")\n            raise\n    return results\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.BaseTextExtractor.extract_from_pages","title":"extract_from_pages","text":"<pre><code>extract_from_pages(input_path: Union[str, Path], page_range: Optional[Tuple[int, int]] = None, **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text from specific pages of a document.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input document</p> required <code>page_range</code> <code>Optional[Tuple[int, int]]</code> <p>Optional tuple of (start_page, end_page) (1-based, inclusive)</p> <code>None</code> <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>TextOutput containing extracted text from specified pages</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def extract_from_pages(\n    self,\n    input_path: Union[str, Path],\n    page_range: Optional[Tuple[int, int]] = None,\n    **kwargs\n) -&gt; TextOutput:\n    \"\"\"Extract text from specific pages of a document.\n\n    Args:\n        input_path: Path to input document\n        page_range: Optional tuple of (start_page, end_page) (1-based, inclusive)\n        **kwargs: Additional model-specific parameters\n\n    Returns:\n        TextOutput containing extracted text from specified pages\n    \"\"\"\n    # Default implementation extracts all pages then filters\n    # Child classes can override for more efficient page-specific extraction\n    full_output = self.extract(input_path, **kwargs)\n\n    if page_range is None:\n        return full_output\n\n    start_page, end_page = page_range\n    filtered_blocks = [\n        block for block in full_output.text_blocks\n        if start_page &lt;= block.page_num &lt;= end_page\n    ]\n\n    # Rebuild full text from filtered blocks\n    full_text = '\\n'.join(block.text for block in filtered_blocks)\n\n    return TextOutput(\n        text_blocks=filtered_blocks,\n        full_text=full_text,\n        metadata=full_output.metadata,\n        source_info=full_output.source_info,\n        processing_time=full_output.processing_time,\n        page_count=end_page - start_page + 1\n    )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.BaseTextExtractor.extract_with_layout","title":"extract_with_layout","text":"<pre><code>extract_with_layout(input_path: Union[str, Path], layout_regions: Optional[List[Dict]] = None, **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text with optional layout information.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input document</p> required <code>layout_regions</code> <code>Optional[List[Dict]]</code> <p>Optional list of layout regions to focus extraction on</p> <code>None</code> <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>TextOutput containing extracted text</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def extract_with_layout(\n    self,\n    input_path: Union[str, Path],\n    layout_regions: Optional[List[Dict]] = None,\n    **kwargs\n) -&gt; TextOutput:\n    \"\"\"Extract text with optional layout information.\n\n    Args:\n        input_path: Path to input document\n        layout_regions: Optional list of layout regions to focus extraction on\n        **kwargs: Additional model-specific parameters\n\n    Returns:\n        TextOutput containing extracted text\n    \"\"\"\n    # Default implementation just calls extract, can be overridden by child classes\n    return self.extract(input_path, **kwargs)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.BaseTextExtractor.get_supported_formats","title":"get_supported_formats","text":"<pre><code>get_supported_formats() -&gt; List[str]\n</code></pre> <p>Get list of supported document formats.</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def get_supported_formats(self) -&gt; List[str]:\n    \"\"\"Get list of supported document formats.\"\"\"\n    # Default formats - child classes should override\n    return ['.txt', '.pdf']\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.BaseTextExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: Any, source_info: Optional[Dict] = None) -&gt; TextOutput\n</code></pre> <p>Convert raw text extraction output to standardized TextOutput format.</p> <p>Parameters:</p> Name Type Description Default <code>raw_output</code> <code>Any</code> <p>Raw output from text extraction engine</p> required <code>source_info</code> <code>Optional[Dict]</code> <p>Optional source document information</p> <code>None</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>Standardized TextOutput object</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def postprocess_output(self, raw_output: Any, source_info: Optional[Dict] = None) -&gt; TextOutput:\n    \"\"\"Convert raw text extraction output to standardized TextOutput format.\n\n    Args:\n        raw_output: Raw output from text extraction engine\n        source_info: Optional source document information\n\n    Returns:\n        Standardized TextOutput object\n    \"\"\"\n    raise NotImplementedError(\"Child classes must implement postprocess_output method\")\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.BaseTextExtractor.preprocess_input","title":"preprocess_input","text":"<pre><code>preprocess_input(input_path: Union[str, Path]) -&gt; Any\n</code></pre> <p>Preprocess input document for text extraction.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input document</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Preprocessed document object</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def preprocess_input(self, input_path: Union[str, Path]) -&gt; Any:\n    \"\"\"Preprocess input document for text extraction.\n\n    Args:\n        input_path: Path to input document\n\n    Returns:\n        Preprocessed document object\n    \"\"\"\n    # Default implementation - child classes should override for specific formats\n    return input_path\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.BaseTextMapper","title":"BaseTextMapper","text":"<pre><code>BaseTextMapper(engine_name: str)\n</code></pre> <p>Base class for mapping text extraction engine-specific outputs to standardized format.</p> <p>Initialize mapper for specific text extraction engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine_name</code> <code>str</code> <p>Name of the text extraction engine</p> required Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def __init__(self, engine_name: str):\n    \"\"\"Initialize mapper for specific text extraction engine.\n\n    Args:\n        engine_name: Name of the text extraction engine\n    \"\"\"\n    self.engine_name = engine_name.lower()\n    self._block_type_mapping: Dict[str, str] = {}\n    self._setup_block_type_mapping()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.BaseTextMapper.extract_font_info","title":"extract_font_info","text":"<pre><code>extract_font_info(raw_font_data: Any) -&gt; Dict[str, Any]\n</code></pre> <p>Extract and normalize font information.</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def extract_font_info(self, raw_font_data: Any) -&gt; Dict[str, Any]:\n    \"\"\"Extract and normalize font information.\"\"\"\n    font_info = {}\n\n    if isinstance(raw_font_data, dict):\n        font_info.update({\n            'font_name': raw_font_data.get('name', raw_font_data.get('font_name')),\n            'font_size': raw_font_data.get('size', raw_font_data.get('font_size')),\n            'bold': raw_font_data.get('bold', raw_font_data.get('is_bold', False)),\n            'italic': raw_font_data.get('italic', raw_font_data.get('is_italic', False)),\n            'color': raw_font_data.get('color', raw_font_data.get('font_color'))\n        })\n\n    return {k: v for k, v in font_info.items() if v is not None}\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.BaseTextMapper.normalize_bbox","title":"normalize_bbox","text":"<pre><code>normalize_bbox(bbox: List[float], page_width: int, page_height: int) -&gt; List[float]\n</code></pre> <p>Normalize bounding box coordinates to absolute values.</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def normalize_bbox(self, bbox: List[float], page_width: int, page_height: int) -&gt; List[float]:\n    \"\"\"Normalize bounding box coordinates to absolute values.\"\"\"\n    if all(0 &lt;= coord &lt;= 1 for coord in bbox):\n        return [\n            bbox[0] * page_width,\n            bbox[1] * page_height,\n            bbox[2] * page_width,\n            bbox[3] * page_height\n        ]\n    return bbox\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.BaseTextMapper.normalize_block_type","title":"normalize_block_type","text":"<pre><code>normalize_block_type(engine_type: str) -&gt; str\n</code></pre> <p>Convert engine-specific block type to standardized format.</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def normalize_block_type(self, engine_type: str) -&gt; str:\n    \"\"\"Convert engine-specific block type to standardized format.\"\"\"\n    return self._block_type_mapping.get(engine_type.lower(), engine_type)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.TextBlock","title":"TextBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for individual text block.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>str</code> <p>Text content</p> <code>bbox</code> <code>Optional[List[float]]</code> <p>Bounding box coordinates [x1, y1, x2, y2]</p> <code>confidence</code> <code>Optional[float]</code> <p>Confidence score for text extraction</p> <code>page_num</code> <code>int</code> <p>Page number (for multi-page documents)</p> <code>block_type</code> <code>Optional[str]</code> <p>Type of text block (paragraph, heading, list, etc.)</p> <code>font_info</code> <code>Optional[Dict[str, Any]]</code> <p>Optional font information</p> <code>reading_order</code> <code>Optional[int]</code> <p>Reading order index</p> <code>language</code> <code>Optional[str]</code> <p>Detected language of the text</p>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.TextBlock.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        'text': self.text,\n        'bbox': self.bbox,\n        'confidence': self.confidence,\n        'page_num': self.page_num,\n        'block_type': self.block_type,\n        'font_info': self.font_info,\n        'reading_order': self.reading_order,\n        'language': self.language\n    }\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.TextOutput","title":"TextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for text extraction results.</p> <p>Attributes:</p> Name Type Description <code>text_blocks</code> <code>List[TextBlock]</code> <p>List of extracted text blocks</p> <code>full_text</code> <code>str</code> <p>Combined text from all blocks</p> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata from extraction</p> <code>source_info</code> <code>Optional[Dict[str, Any]]</code> <p>Information about the source document</p> <code>processing_time</code> <code>Optional[float]</code> <p>Time taken for text extraction</p> <code>page_count</code> <code>int</code> <p>Number of pages in the document</p>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.TextOutput.get_sorted_by_reading_order","title":"get_sorted_by_reading_order","text":"<pre><code>get_sorted_by_reading_order() -&gt; List[TextBlock]\n</code></pre> <p>Get text blocks sorted by reading order.</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def get_sorted_by_reading_order(self) -&gt; List[TextBlock]:\n    \"\"\"Get text blocks sorted by reading order.\"\"\"\n    blocks_with_order = [block for block in self.text_blocks if block.reading_order is not None]\n    blocks_without_order = [block for block in self.text_blocks if block.reading_order is None]\n\n    # Sort blocks with reading order\n    blocks_with_order.sort(key=lambda x: (x.page_num, x.reading_order))\n\n    # Sort blocks without reading order by page and bbox\n    if blocks_without_order:\n        blocks_without_order.sort(key=lambda x: (\n            x.page_num,\n            x.bbox[1] if x.bbox else 0,  # Sort by y coordinate (top to bottom)\n            x.bbox[0] if x.bbox else 0   # Then by x coordinate (left to right)\n        ))\n\n    return blocks_with_order + blocks_without_order\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.TextOutput.get_text_by_confidence","title":"get_text_by_confidence","text":"<pre><code>get_text_by_confidence(min_confidence: float = 0.5) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by minimum confidence threshold.</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def get_text_by_confidence(self, min_confidence: float = 0.5) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by minimum confidence threshold.\"\"\"\n    return [block for block in self.text_blocks if block.confidence is None or block.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.TextOutput.get_text_by_page","title":"get_text_by_page","text":"<pre><code>get_text_by_page(page_num: int) -&gt; List[TextBlock]\n</code></pre> <p>Get text blocks from a specific page.</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def get_text_by_page(self, page_num: int) -&gt; List[TextBlock]:\n    \"\"\"Get text blocks from a specific page.\"\"\"\n    return [block for block in self.text_blocks if block.page_num == page_num]\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.TextOutput.get_text_by_type","title":"get_text_by_type","text":"<pre><code>get_text_by_type(block_type: str) -&gt; List[TextBlock]\n</code></pre> <p>Get text blocks of a specific type.</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def get_text_by_type(self, block_type: str) -&gt; List[TextBlock]:\n    \"\"\"Get text blocks of a specific type.\"\"\"\n    return [block for block in self.text_blocks if block.block_type == block_type]\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.TextOutput.save_json","title":"save_json","text":"<pre><code>save_json(output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save output to JSON file.</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def save_json(self, output_path: Union[str, Path]) -&gt; None:\n    \"\"\"Save output to JSON file.\"\"\"\n    import json\n    with open(output_path, 'w', encoding='utf-8') as f:\n        json.dump(self.to_dict(), f, indent=2, ensure_ascii=False)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.TextOutput.save_markdown","title":"save_markdown","text":"<pre><code>save_markdown(output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save text as markdown with basic formatting.</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def save_markdown(self, output_path: Union[str, Path]) -&gt; None:\n    \"\"\"Save text as markdown with basic formatting.\"\"\"\n    markdown_content = []\n\n    for block in self.get_sorted_by_reading_order():\n        if block.block_type == 'heading':\n            # Convert to markdown heading\n            markdown_content.append(f\"# {block.text}\\n\")\n        elif block.block_type == 'subheading':\n            markdown_content.append(f\"## {block.text}\\n\")\n        elif block.block_type == 'list':\n            # Convert to markdown list\n            lines = block.text.split('\\n')\n            for line in lines:\n                if line.strip():\n                    markdown_content.append(f\"- {line.strip()}\")\n            markdown_content.append(\"\")\n        else:\n            # Regular paragraph\n            markdown_content.append(f\"{block.text}\\n\")\n\n    with open(output_path, 'w', encoding='utf-8') as f:\n        f.write('\\n'.join(markdown_content))\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.TextOutput.save_text","title":"save_text","text":"<pre><code>save_text(output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save full text to a text file.</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def save_text(self, output_path: Union[str, Path]) -&gt; None:\n    \"\"\"Save full text to a text file.\"\"\"\n    with open(output_path, 'w', encoding='utf-8') as f:\n        f.write(self.full_text)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.TextOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        'text_blocks': [block.to_dict() for block in self.text_blocks],\n        'full_text': self.full_text,\n        'metadata': self.metadata,\n        'source_info': self.source_info,\n        'processing_time': self.processing_time,\n        'page_count': self.page_count\n    }\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pymupdf","title":"omnidocs.tasks.text_extraction.extractors.pymupdf","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pymupdf.PyMuPDFTextExtractor","title":"PyMuPDFTextExtractor","text":"<pre><code>PyMuPDFTextExtractor(device: Optional[str] = None, show_log: bool = False, extract_images: bool = False, extract_tables: bool = False, flags: int = 0, clip: Optional[tuple] = None)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Text extractor using PyMuPDF (fitz).</p> <p>Initialize PyMuPDF text extractor.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run on (not used for PyMuPDF)</p> <code>None</code> <code>show_log</code> <code>bool</code> <p>Whether to show detailed logs</p> <code>False</code> <code>extract_images</code> <code>bool</code> <p>Whether to extract images alongside text</p> <code>False</code> <code>extract_tables</code> <code>bool</code> <p>Whether to extract tables</p> <code>False</code> <code>flags</code> <code>int</code> <p>Text extraction flags (fitz.TEXT_PRESERVE_LIGATURES, etc.)</p> <code>0</code> <code>clip</code> <code>Optional[tuple]</code> <p>Optional clipping rectangle (x0, y0, x1, y1)</p> <code>None</code> Source code in <code>omnidocs/tasks/text_extraction/extractors/pymupdf.py</code> <pre><code>def __init__(self, \n             device: Optional[str] = None, \n             show_log: bool = False,\n             extract_images: bool = False,\n             extract_tables: bool = False,\n             flags: int = 0,\n             clip: Optional[tuple] = None):\n    \"\"\"Initialize PyMuPDF text extractor.\n\n    Args:\n        device: Device to run on (not used for PyMuPDF)\n        show_log: Whether to show detailed logs\n        extract_images: Whether to extract images alongside text\n        extract_tables: Whether to extract tables\n        flags: Text extraction flags (fitz.TEXT_PRESERVE_LIGATURES, etc.)\n        clip: Optional clipping rectangle (x0, y0, x1, y1)\n    \"\"\"\n    super().__init__(device, show_log, \"pymupdf\", extract_images)\n    self.extract_tables = extract_tables\n    self.flags = flags\n    self.clip = clip\n    self._label_mapper = PyMuPDFTextMapper()\n    self._load_model()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pymupdf.PyMuPDFTextExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path], use_layout: bool = True, **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text from document using PyMuPDF.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input document</p> required <code>use_layout</code> <code>bool</code> <p>Whether to use layout information for extraction</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>TextOutput containing extracted text</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pymupdf.py</code> <pre><code>def extract(\n    self,\n    input_path: Union[str, Path],\n    use_layout: bool = True,\n    **kwargs\n) -&gt; TextOutput:\n    \"\"\"Extract text from document using PyMuPDF.\n\n    Args:\n        input_path: Path to input document\n        use_layout: Whether to use layout information for extraction\n        **kwargs: Additional parameters\n\n    Returns:\n        TextOutput containing extracted text\n    \"\"\"\n    start_time = time.time()\n\n    # Preprocess input\n    input_path = self.preprocess_input(input_path)\n\n    if self.show_log:\n        logger.info(f\"Extracting text from {input_path}\")\n\n    try:\n        all_text_blocks = []\n\n        # Open document\n        doc = fitz.open(str(input_path))\n\n        try:\n            total_pages = len(doc)\n\n            for page_num in range(total_pages):\n                page = doc[page_num]\n\n                # Extract text blocks\n                if use_layout:\n                    page_blocks = self._extract_text_blocks(page)\n                else:\n                    page_blocks = self._extract_text_simple(page)\n\n                all_text_blocks.extend(page_blocks)\n\n                # Extract tables if requested\n                if self.extract_tables:\n                    table_blocks = self._extract_tables(page)\n                    all_text_blocks.extend(table_blocks)\n\n            # Create source info\n            source_info = {\n                'file_path': str(input_path),\n                'file_name': input_path.name,\n                'file_size': input_path.stat().st_size,\n                'engine': 'pymupdf',\n                'total_pages': total_pages,\n                'metadata': doc.metadata\n            }\n\n        finally:\n            doc.close()\n\n        # Post-process output\n        output = self.postprocess_output(all_text_blocks, source_info)\n        output.processing_time = time.time() - start_time\n\n        if self.show_log:\n            logger.info(f\"Extracted {len(output.text_blocks)} text blocks from {total_pages} pages in {output.processing_time:.2f}s\")\n\n        return output\n\n    except Exception as e:\n        logger.error(f\"Error extracting text from {input_path}: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pymupdf.PyMuPDFTextExtractor.extract_from_pages","title":"extract_from_pages","text":"<pre><code>extract_from_pages(input_path: Union[str, Path], page_range: Optional[tuple] = None, use_layout: bool = True, **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text from specific pages.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input document</p> required <code>page_range</code> <code>Optional[tuple]</code> <p>Optional tuple of (start_page, end_page) (1-based, inclusive)</p> <code>None</code> <code>use_layout</code> <code>bool</code> <p>Whether to use layout information</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>TextOutput containing extracted text from specified pages</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pymupdf.py</code> <pre><code>def extract_from_pages(\n    self,\n    input_path: Union[str, Path],\n    page_range: Optional[tuple] = None,\n    use_layout: bool = True,\n    **kwargs\n) -&gt; TextOutput:\n    \"\"\"Extract text from specific pages.\n\n    Args:\n        input_path: Path to input document\n        page_range: Optional tuple of (start_page, end_page) (1-based, inclusive)\n        use_layout: Whether to use layout information\n        **kwargs: Additional parameters\n\n    Returns:\n        TextOutput containing extracted text from specified pages\n    \"\"\"\n    start_time = time.time()\n\n    # Preprocess input\n    input_path = self.preprocess_input(input_path)\n\n    if self.show_log:\n        logger.info(f\"Extracting text from {input_path}, pages {page_range}\")\n\n    try:\n        all_text_blocks = []\n\n        # Open document\n        doc = fitz.open(str(input_path))\n\n        try:\n            total_pages = len(doc)\n\n            if page_range is None:\n                start_page, end_page = 1, total_pages\n            else:\n                start_page, end_page = page_range\n\n            # Convert to 0-based indexing\n            start_idx = max(0, start_page - 1)\n            end_idx = min(total_pages - 1, end_page - 1)\n\n            for page_num in range(start_idx, end_idx + 1):\n                page = doc[page_num]\n\n                # Extract text blocks\n                if use_layout:\n                    page_blocks = self._extract_text_blocks(page)\n                else:\n                    page_blocks = self._extract_text_simple(page)\n\n                all_text_blocks.extend(page_blocks)\n\n                # Extract tables if requested\n                if self.extract_tables:\n                    table_blocks = self._extract_tables(page)\n                    all_text_blocks.extend(table_blocks)\n\n            # Create source info\n            source_info = {\n                'file_path': str(input_path),\n                'file_name': input_path.name,\n                'file_size': input_path.stat().st_size,\n                'engine': 'pymupdf',\n                'total_pages': total_pages,\n                'page_range': page_range,\n                'metadata': doc.metadata\n            }\n\n        finally:\n            doc.close()\n\n        # Post-process output\n        output = self.postprocess_output(all_text_blocks, source_info)\n        output.processing_time = time.time() - start_time\n\n        if self.show_log:\n            logger.info(f\"Extracted {len(output.text_blocks)} text blocks in {output.processing_time:.2f}s\")\n\n        return output\n\n    except Exception as e:\n        logger.error(f\"Error extracting text from {input_path}: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pymupdf.PyMuPDFTextExtractor.get_supported_formats","title":"get_supported_formats","text":"<pre><code>get_supported_formats() -&gt; List[str]\n</code></pre> <p>Get list of supported document formats.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pymupdf.py</code> <pre><code>def get_supported_formats(self) -&gt; List[str]:\n    \"\"\"Get list of supported document formats.\"\"\"\n    return ['.pdf', '.xps', '.oxps', '.epub', '.mobi', '.fb2', '.cbz', '.svg']\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pymupdf.PyMuPDFTextExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: Any, source_info: Optional[Dict] = None) -&gt; TextOutput\n</code></pre> <p>Convert PyMuPDF output to standardized TextOutput format.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pymupdf.py</code> <pre><code>def postprocess_output(self, raw_output: Any, source_info: Optional[Dict] = None) -&gt; TextOutput:\n    \"\"\"Convert PyMuPDF output to standardized TextOutput format.\"\"\"\n    text_blocks = raw_output  # raw_output is already a list of TextBlocks\n\n    # Sort blocks by page and reading order\n    text_blocks.sort(key=lambda x: (x.page_num, x.reading_order or 0))\n\n    # Combine all text\n    full_text = '\\n\\n'.join(block.text for block in text_blocks if block.text.strip())\n\n    # Get metadata\n    metadata = {\n        'engine': 'pymupdf',\n        'extract_tables': self.extract_tables,\n        'flags': self.flags,\n        'clip': self.clip,\n        'total_blocks': len(text_blocks)\n    }\n\n    return TextOutput(\n        text_blocks=text_blocks,\n        full_text=full_text,\n        metadata=metadata,\n        source_info=source_info,\n        page_count=max(block.page_num for block in text_blocks) if text_blocks else 1\n    )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pymupdf.PyMuPDFTextExtractor.preprocess_input","title":"preprocess_input","text":"<pre><code>preprocess_input(input_path: Union[str, Path]) -&gt; Path\n</code></pre> <p>Preprocess input document.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pymupdf.py</code> <pre><code>def preprocess_input(self, input_path: Union[str, Path]) -&gt; Path:\n    \"\"\"Preprocess input document.\"\"\"\n    input_path = Path(input_path)\n    if not input_path.exists():\n        raise FileNotFoundError(f\"Input file not found: {input_path}\")\n\n    supported_formats = ['.pdf', '.xps', '.oxps', '.epub', '.mobi', '.fb2', '.cbz', '.svg']\n    if input_path.suffix.lower() not in supported_formats:\n        raise ValueError(f\"Unsupported format: {input_path.suffix}. Supported: {supported_formats}\")\n\n    return input_path\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pymupdf.PyMuPDFTextMapper","title":"PyMuPDFTextMapper","text":"<pre><code>PyMuPDFTextMapper()\n</code></pre> <p>               Bases: <code>BaseTextMapper</code></p> <p>Mapper for PyMuPDF text extraction output.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pymupdf.py</code> <pre><code>def __init__(self):\n    super().__init__(\"pymupdf\")\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pdfplumber","title":"omnidocs.tasks.text_extraction.extractors.pdfplumber","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pdfplumber.PdfplumberTextExtractor","title":"PdfplumberTextExtractor","text":"<pre><code>PdfplumberTextExtractor(device: Optional[str] = None, show_log: bool = False, extract_images: bool = False, extract_tables: bool = False, use_layout: bool = True)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Text extractor using pdfplumber.</p> <p>Initialize pdfplumber text extractor.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run on (not used for pdfplumber)</p> <code>None</code> <code>show_log</code> <code>bool</code> <p>Whether to show detailed logs</p> <code>False</code> <code>extract_images</code> <code>bool</code> <p>Whether to extract images alongside text</p> <code>False</code> <code>extract_tables</code> <code>bool</code> <p>Whether to extract tables</p> <code>False</code> <code>use_layout</code> <code>bool</code> <p>Whether to use layout information for text extraction</p> <code>True</code> Source code in <code>omnidocs/tasks/text_extraction/extractors/pdfplumber.py</code> <pre><code>def __init__(self, \n             device: Optional[str] = None, \n             show_log: bool = False,\n             extract_images: bool = False,\n             extract_tables: bool = False,\n             use_layout: bool = True):\n    \"\"\"Initialize pdfplumber text extractor.\n\n    Args:\n        device: Device to run on (not used for pdfplumber)\n        show_log: Whether to show detailed logs\n        extract_images: Whether to extract images alongside text\n        extract_tables: Whether to extract tables\n        use_layout: Whether to use layout information for text extraction\n    \"\"\"\n    super().__init__(device, show_log, \"pdfplumber\", extract_images)\n    self.extract_tables = extract_tables\n    self.use_layout = use_layout\n    self._label_mapper = PdfplumberTextMapper()\n    self._load_model()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pdfplumber.PdfplumberTextExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path], **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text from PDF using pdfplumber.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input PDF</p> required <code>**kwargs</code> <p>Additional parameters (ignored for pdfplumber)</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>TextOutput containing extracted text</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pdfplumber.py</code> <pre><code>def extract(\n    self,\n    input_path: Union[str, Path],\n    **kwargs\n) -&gt; TextOutput:\n    \"\"\"Extract text from PDF using pdfplumber.\n\n    Args:\n        input_path: Path to input PDF\n        **kwargs: Additional parameters (ignored for pdfplumber)\n\n    Returns:\n        TextOutput containing extracted text\n    \"\"\"\n    start_time = time.time()\n\n    # Preprocess input\n    input_path = self.preprocess_input(input_path)\n\n    if self.show_log:\n        logger.info(f\"Extracting text from {input_path}\")\n\n    try:\n        all_text_blocks = []\n\n        with pdfplumber.open(input_path) as pdf:\n            total_pages = len(pdf.pages)\n\n            for page in pdf.pages:\n                if self.use_layout:\n                    page_blocks = self._extract_text_with_layout(page)\n                else:\n                    page_blocks = self._extract_text_simple(page)\n\n                all_text_blocks.extend(page_blocks)\n\n                # Extract tables if requested\n                if self.extract_tables:\n                    table_blocks = self._extract_tables(page)\n                    all_text_blocks.extend(table_blocks)\n\n        # Create source info\n        source_info = {\n            'file_path': str(input_path),\n            'file_name': input_path.name,\n            'file_size': input_path.stat().st_size,\n            'engine': 'pdfplumber',\n            'total_pages': total_pages\n        }\n\n        # Post-process output\n        output = self.postprocess_output(all_text_blocks, source_info)\n        output.processing_time = time.time() - start_time\n\n        if self.show_log:\n            logger.info(f\"Extracted {len(output.text_blocks)} text blocks from {total_pages} pages in {output.processing_time:.2f}s\")\n\n        return output\n\n    except Exception as e:\n        logger.error(f\"Error extracting text from {input_path}: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pdfplumber.PdfplumberTextExtractor.get_supported_formats","title":"get_supported_formats","text":"<pre><code>get_supported_formats() -&gt; List[str]\n</code></pre> <p>Get list of supported document formats.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pdfplumber.py</code> <pre><code>def get_supported_formats(self) -&gt; List[str]:\n    \"\"\"Get list of supported document formats.\"\"\"\n    return ['.pdf']\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pdfplumber.PdfplumberTextExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: Any, source_info: Optional[Dict] = None) -&gt; TextOutput\n</code></pre> <p>Convert pdfplumber output to standardized TextOutput format.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pdfplumber.py</code> <pre><code>def postprocess_output(self, raw_output: Any, source_info: Optional[Dict] = None) -&gt; TextOutput:\n    \"\"\"Convert pdfplumber output to standardized TextOutput format.\"\"\"\n    text_blocks = raw_output  # raw_output is already a list of TextBlocks\n\n    # Sort blocks by page and reading order\n    text_blocks.sort(key=lambda x: (x.page_num, x.reading_order or 0))\n\n    # Combine all text\n    full_text = '\\n\\n'.join(block.text for block in text_blocks)\n\n    # Get metadata\n    metadata = {\n        'engine': 'pdfplumber',\n        'extract_tables': self.extract_tables,\n        'use_layout': self.use_layout,\n        'total_blocks': len(text_blocks)\n    }\n\n    return TextOutput(\n        text_blocks=text_blocks,\n        full_text=full_text,\n        metadata=metadata,\n        source_info=source_info,\n        page_count=max(block.page_num for block in text_blocks) if text_blocks else 1\n    )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pdfplumber.PdfplumberTextExtractor.preprocess_input","title":"preprocess_input","text":"<pre><code>preprocess_input(input_path: Union[str, Path]) -&gt; Path\n</code></pre> <p>Preprocess input document.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pdfplumber.py</code> <pre><code>def preprocess_input(self, input_path: Union[str, Path]) -&gt; Path:\n    \"\"\"Preprocess input document.\"\"\"\n    input_path = Path(input_path)\n    if not input_path.exists():\n        raise FileNotFoundError(f\"Input file not found: {input_path}\")\n\n    if input_path.suffix.lower() != '.pdf':\n        raise ValueError(f\"pdfplumber only supports PDF files. Got: {input_path.suffix}\")\n\n    return input_path\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pdfplumber.PdfplumberTextMapper","title":"PdfplumberTextMapper","text":"<pre><code>PdfplumberTextMapper()\n</code></pre> <p>               Bases: <code>BaseTextMapper</code></p> <p>Mapper for pdfplumber text extraction output.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pdfplumber.py</code> <pre><code>def __init__(self):\n    super().__init__(\"pdfplumber\")\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pypdf2","title":"omnidocs.tasks.text_extraction.extractors.pypdf2","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pypdf2.PyPDF2TextExtractor","title":"PyPDF2TextExtractor","text":"<pre><code>PyPDF2TextExtractor(device: Optional[str] = None, show_log: bool = False, extract_images: bool = False, ignore_images: bool = True, extract_forms: bool = False)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Text extractor using PyPDF2.</p> <p>Initialize PyPDF2 text extractor.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run on (not used for PyPDF2)</p> <code>None</code> <code>show_log</code> <code>bool</code> <p>Whether to show detailed logs</p> <code>False</code> <code>extract_images</code> <code>bool</code> <p>Whether to extract images alongside text</p> <code>False</code> <code>ignore_images</code> <code>bool</code> <p>Whether to ignore images during text extraction</p> <code>True</code> <code>extract_forms</code> <code>bool</code> <p>Whether to extract form fields</p> <code>False</code> Source code in <code>omnidocs/tasks/text_extraction/extractors/pypdf2.py</code> <pre><code>def __init__(self, \n             device: Optional[str] = None, \n             show_log: bool = False,\n             extract_images: bool = False,\n             ignore_images: bool = True,\n             extract_forms: bool = False):\n    \"\"\"Initialize PyPDF2 text extractor.\n\n    Args:\n        device: Device to run on (not used for PyPDF2)\n        show_log: Whether to show detailed logs\n        extract_images: Whether to extract images alongside text\n        ignore_images: Whether to ignore images during text extraction\n        extract_forms: Whether to extract form fields\n    \"\"\"\n    super().__init__(device, show_log, \"pypdf2\", extract_images)\n    self.ignore_images = ignore_images\n    self.extract_forms = extract_forms\n    self._label_mapper = PyPDF2TextMapper()\n    self._load_model()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pypdf2.PyPDF2TextExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path], password: Optional[str] = None, **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text from PDF using PyPDF2.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input PDF</p> required <code>password</code> <code>Optional[str]</code> <p>Optional password for encrypted PDFs</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters (ignored for PyPDF2)</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>TextOutput containing extracted text</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pypdf2.py</code> <pre><code>def extract(\n    self,\n    input_path: Union[str, Path],\n    password: Optional[str] = None,\n    **kwargs\n) -&gt; TextOutput:\n    \"\"\"Extract text from PDF using PyPDF2.\n\n    Args:\n        input_path: Path to input PDF\n        password: Optional password for encrypted PDFs\n        **kwargs: Additional parameters (ignored for PyPDF2)\n\n    Returns:\n        TextOutput containing extracted text\n    \"\"\"\n    start_time = time.time()\n\n    # Preprocess input\n    input_path = self.preprocess_input(input_path)\n\n    if self.show_log:\n        logger.info(f\"Extracting text from {input_path}\")\n\n    try:\n        all_text_blocks = []\n\n        # Open PDF\n        with open(input_path, 'rb') as file:\n            reader = PdfReader(file)\n\n            # Check if PDF is encrypted\n            if reader.is_encrypted:\n                if password:\n                    if not reader.decrypt(password):\n                        raise ValueError(\"Invalid password for encrypted PDF\")\n                else:\n                    raise ValueError(\"PDF is encrypted but no password provided\")\n\n            total_pages = len(reader.pages)\n\n            # Extract text from each page\n            for page_num, page in enumerate(reader.pages, 1):\n                page_blocks = self._extract_page_text(page, page_num)\n                all_text_blocks.extend(page_blocks)\n\n            # Extract form fields if requested\n            if self.extract_forms:\n                form_blocks = self._extract_form_fields(reader)\n                all_text_blocks.extend(form_blocks)\n\n            # Get PDF metadata\n            pdf_metadata = self._get_pdf_metadata(reader)\n\n            # Create source info\n            source_info = {\n                'file_path': str(input_path),\n                'file_name': input_path.name,\n                'file_size': input_path.stat().st_size,\n                'engine': 'pypdf2',\n                'total_pages': total_pages,\n                'is_encrypted': reader.is_encrypted,\n                'pdf_metadata': pdf_metadata\n            }\n\n        # Post-process output\n        output = self.postprocess_output(all_text_blocks, source_info)\n        output.processing_time = time.time() - start_time\n\n        if self.show_log:\n            logger.info(f\"Extracted {len(output.text_blocks)} text blocks from {total_pages} pages in {output.processing_time:.2f}s\")\n\n        return output\n\n    except Exception as e:\n        logger.error(f\"Error extracting text from {input_path}: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pypdf2.PyPDF2TextExtractor.extract_from_pages","title":"extract_from_pages","text":"<pre><code>extract_from_pages(input_path: Union[str, Path], page_range: Optional[tuple] = None, password: Optional[str] = None, **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text from specific pages.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input PDF</p> required <code>page_range</code> <code>Optional[tuple]</code> <p>Optional tuple of (start_page, end_page) (1-based, inclusive)</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>Optional password for encrypted PDFs</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>TextOutput containing extracted text from specified pages</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pypdf2.py</code> <pre><code>def extract_from_pages(\n    self,\n    input_path: Union[str, Path],\n    page_range: Optional[tuple] = None,\n    password: Optional[str] = None,\n    **kwargs\n) -&gt; TextOutput:\n    \"\"\"Extract text from specific pages.\n\n    Args:\n        input_path: Path to input PDF\n        page_range: Optional tuple of (start_page, end_page) (1-based, inclusive)\n        password: Optional password for encrypted PDFs\n        **kwargs: Additional parameters\n\n    Returns:\n        TextOutput containing extracted text from specified pages\n    \"\"\"\n    start_time = time.time()\n\n    # Preprocess input\n    input_path = self.preprocess_input(input_path)\n\n    if self.show_log:\n        logger.info(f\"Extracting text from {input_path}, pages {page_range}\")\n\n    try:\n        all_text_blocks = []\n\n        # Open PDF\n        with open(input_path, 'rb') as file:\n            reader = PdfReader(file)\n\n            # Check if PDF is encrypted\n            if reader.is_encrypted:\n                if password:\n                    if not reader.decrypt(password):\n                        raise ValueError(\"Invalid password for encrypted PDF\")\n                else:\n                    raise ValueError(\"PDF is encrypted but no password provided\")\n\n            total_pages = len(reader.pages)\n\n            if page_range is None:\n                start_page, end_page = 1, total_pages\n            else:\n                start_page, end_page = page_range\n\n            # Validate page range\n            start_page = max(1, start_page)\n            end_page = min(total_pages, end_page)\n\n            # Extract text from specified pages\n            for page_num in range(start_page, end_page + 1):\n                page = reader.pages[page_num - 1]  # Convert to 0-based index\n                page_blocks = self._extract_page_text(page, page_num)\n                all_text_blocks.extend(page_blocks)\n\n            # Get PDF metadata\n            pdf_metadata = self._get_pdf_metadata(reader)\n\n            # Create source info\n            source_info = {\n                'file_path': str(input_path),\n                'file_name': input_path.name,\n                'file_size': input_path.stat().st_size,\n                'engine': 'pypdf2',\n                'total_pages': total_pages,\n                'page_range': page_range,\n                'is_encrypted': reader.is_encrypted,\n                'pdf_metadata': pdf_metadata\n            }\n\n        # Post-process output\n        output = self.postprocess_output(all_text_blocks, source_info)\n        output.processing_time = time.time() - start_time\n\n        if self.show_log:\n            logger.info(f\"Extracted {len(output.text_blocks)} text blocks in {output.processing_time:.2f}s\")\n\n        return output\n\n    except Exception as e:\n        logger.error(f\"Error extracting text from {input_path}: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pypdf2.PyPDF2TextExtractor.extract_with_password","title":"extract_with_password","text":"<pre><code>extract_with_password(input_path: Union[str, Path], password: str, **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text from password-protected PDF.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input PDF</p> required <code>password</code> <code>str</code> <p>Password for encrypted PDF</p> required <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>TextOutput containing extracted text</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pypdf2.py</code> <pre><code>def extract_with_password(\n    self,\n    input_path: Union[str, Path],\n    password: str,\n    **kwargs\n) -&gt; TextOutput:\n    \"\"\"Extract text from password-protected PDF.\n\n    Args:\n        input_path: Path to input PDF\n        password: Password for encrypted PDF\n        **kwargs: Additional parameters\n\n    Returns:\n        TextOutput containing extracted text\n    \"\"\"\n    return self.extract(input_path, password=password, **kwargs)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pypdf2.PyPDF2TextExtractor.get_supported_formats","title":"get_supported_formats","text":"<pre><code>get_supported_formats() -&gt; List[str]\n</code></pre> <p>Get list of supported document formats.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pypdf2.py</code> <pre><code>def get_supported_formats(self) -&gt; List[str]:\n    \"\"\"Get list of supported document formats.\"\"\"\n    return ['.pdf']\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pypdf2.PyPDF2TextExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: Any, source_info: Optional[Dict] = None) -&gt; TextOutput\n</code></pre> <p>Convert PyPDF2 output to standardized TextOutput format.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pypdf2.py</code> <pre><code>def postprocess_output(self, raw_output: Any, source_info: Optional[Dict] = None) -&gt; TextOutput:\n    \"\"\"Convert PyPDF2 output to standardized TextOutput format.\"\"\"\n    text_blocks = raw_output  # raw_output is already a list of TextBlocks\n\n    # Sort blocks by page and reading order\n    text_blocks.sort(key=lambda x: (x.page_num, x.reading_order or 0))\n\n    # Combine all text\n    full_text = '\\n\\n'.join(block.text for block in text_blocks if block.text.strip())\n\n    # Get metadata\n    metadata = {\n        'engine': 'pypdf2',\n        'ignore_images': self.ignore_images,\n        'extract_forms': self.extract_forms,\n        'total_blocks': len(text_blocks)\n    }\n\n    # Make everything JSON serializable\n    metadata = sanitize_for_json(metadata)\n    source_info = sanitize_for_json(source_info)\n\n    return TextOutput(\n        text_blocks=text_blocks,\n        full_text=full_text,\n        metadata=metadata,\n        source_info=source_info,\n        page_count=max(block.page_num for block in text_blocks) if text_blocks else 1\n    )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pypdf2.PyPDF2TextExtractor.preprocess_input","title":"preprocess_input","text":"<pre><code>preprocess_input(input_path: Union[str, Path]) -&gt; Path\n</code></pre> <p>Preprocess input document.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pypdf2.py</code> <pre><code>def preprocess_input(self, input_path: Union[str, Path]) -&gt; Path:\n    \"\"\"Preprocess input document.\"\"\"\n    input_path = Path(input_path)\n    if not input_path.exists():\n        raise FileNotFoundError(f\"Input file not found: {input_path}\")\n\n    if input_path.suffix.lower() != '.pdf':\n        raise ValueError(f\"PyPDF2 only supports PDF files. Got: {input_path.suffix}\")\n\n    return input_path\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pypdf2.PyPDF2TextMapper","title":"PyPDF2TextMapper","text":"<pre><code>PyPDF2TextMapper()\n</code></pre> <p>               Bases: <code>BaseTextMapper</code></p> <p>Mapper for PyPDF2 text extraction output.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pypdf2.py</code> <pre><code>def __init__(self):\n    super().__init__(\"pypdf2\")\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pypdf2.sanitize_for_json","title":"sanitize_for_json","text":"<pre><code>sanitize_for_json(obj: Any) -&gt; Any\n</code></pre> <p>Recursively convert PyPDF2 objects (like IndirectObject) to JSON-serializable types.</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>Input object that might contain non-serializable types</p> required <p>Returns:</p> Type Description <code>Any</code> <p>JSON-serializable version of the input object</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pypdf2.py</code> <pre><code>def sanitize_for_json(obj: Any) -&gt; Any:\n    \"\"\"\n    Recursively convert PyPDF2 objects (like IndirectObject) to JSON-serializable types.\n\n    Args:\n        obj: Input object that might contain non-serializable types\n\n    Returns:\n        JSON-serializable version of the input object\n    \"\"\"\n    if obj is None:\n        return None\n\n    # Handle common collection types recursively\n    if isinstance(obj, dict):\n        return {k: sanitize_for_json(v) for k, v in obj.items()}\n    elif isinstance(obj, list):\n        return [sanitize_for_json(item) for item in obj]\n    elif isinstance(obj, tuple):\n        return tuple(sanitize_for_json(item) for item in obj)\n\n    # Try to determine if this is a PyPDF2 IndirectObject or similar custom type\n    # that's not JSON-serializable\n    try:\n        # This will work for built-in types that are JSON-serializable\n        if isinstance(obj, (str, int, float, bool)):\n            return obj\n\n        # Check if it's a custom class from PyPDF2\n        class_name = obj.__class__.__name__\n        if \"PyPDF2\" in str(obj.__class__) or class_name in [\n            \"IndirectObject\", \"DictionaryObject\", \"ArrayObject\", \n            \"PdfObject\", \"NullObject\", \"NameObject\"\n        ]:\n            return str(obj)\n\n        # If we got here, it might be a normal object, let's try to serialize it\n        return obj\n    except Exception:\n        # If all else fails, convert to string\n        return str(obj)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pdftext","title":"omnidocs.tasks.text_extraction.extractors.pdftext","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pdftext.PdftextTextExtractor","title":"PdftextTextExtractor","text":"<pre><code>PdftextTextExtractor(device: Optional[str] = None, show_log: bool = False, extract_images: bool = False, keep_layout: bool = False, physical_layout: bool = False)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Text extractor using pdftext.</p> <p>Initialize pdftext text extractor.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run on (not used for pdftext)</p> <code>None</code> <code>show_log</code> <code>bool</code> <p>Whether to show detailed logs</p> <code>False</code> <code>extract_images</code> <code>bool</code> <p>Whether to extract images alongside text</p> <code>False</code> <code>keep_layout</code> <code>bool</code> <p>Whether to keep original layout formatting</p> <code>False</code> <code>physical_layout</code> <code>bool</code> <p>Whether to use physical layout analysis</p> <code>False</code> Source code in <code>omnidocs/tasks/text_extraction/extractors/pdftext.py</code> <pre><code>def __init__(self, \n             device: Optional[str] = None, \n             show_log: bool = False,\n             extract_images: bool = False,\n             keep_layout: bool = False,\n             physical_layout: bool = False):\n    \"\"\"Initialize pdftext text extractor.\n\n    Args:\n        device: Device to run on (not used for pdftext)\n        show_log: Whether to show detailed logs\n        extract_images: Whether to extract images alongside text\n        keep_layout: Whether to keep original layout formatting\n        physical_layout: Whether to use physical layout analysis\n    \"\"\"\n    super().__init__(device, show_log, \"pdftext\", extract_images)\n    self.keep_layout = keep_layout\n    self.physical_layout = physical_layout\n    self._label_mapper = PdftextTextMapper()\n    self._load_model()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pdftext.PdftextTextExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path], **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text from PDF using pdftext.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input PDF</p> required <code>**kwargs</code> <p>Additional parameters (ignored for pdftext)</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>TextOutput containing extracted text</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pdftext.py</code> <pre><code>def extract(\n    self,\n    input_path: Union[str, Path],\n    **kwargs\n) -&gt; TextOutput:\n    \"\"\"Extract text from PDF using pdftext.\n\n    Args:\n        input_path: Path to input PDF\n        **kwargs: Additional parameters (ignored for pdftext)\n\n    Returns:\n        TextOutput containing extracted text\n    \"\"\"\n    start_time = time.time()\n\n    # Preprocess input\n    input_path = self.preprocess_input(input_path)\n\n    if self.show_log:\n        logger.info(f\"Extracting text from {input_path}\")\n\n    try:\n        # Extract text blocks\n        text_blocks = self._extract_text_by_page(input_path)\n\n        # Create source info\n        source_info = {\n            'file_path': str(input_path),\n            'file_name': input_path.name,\n            'file_size': input_path.stat().st_size,\n            'engine': 'pdftext'\n        }\n\n        # Post-process output\n        output = self.postprocess_output(text_blocks, source_info)\n        output.processing_time = time.time() - start_time\n\n        if self.show_log:\n            logger.info(f\"Extracted {len(output.text_blocks)} text blocks in {output.processing_time:.2f}s\")\n\n        return output\n\n    except Exception as e:\n        logger.error(f\"Error extracting text from {input_path}: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pdftext.PdftextTextExtractor.extract_from_pages","title":"extract_from_pages","text":"<pre><code>extract_from_pages(input_path: Union[str, Path], page_range: Optional[tuple] = None, **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text from specific pages.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input PDF</p> required <code>page_range</code> <code>Optional[tuple]</code> <p>Optional tuple of (start_page, end_page) (1-based, inclusive)</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>TextOutput containing extracted text from specified pages</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pdftext.py</code> <pre><code>def extract_from_pages(\n    self,\n    input_path: Union[str, Path],\n    page_range: Optional[tuple] = None,\n    **kwargs\n) -&gt; TextOutput:\n    \"\"\"Extract text from specific pages.\n\n    Args:\n        input_path: Path to input PDF\n        page_range: Optional tuple of (start_page, end_page) (1-based, inclusive)\n        **kwargs: Additional parameters\n\n    Returns:\n        TextOutput containing extracted text from specified pages\n    \"\"\"\n    start_time = time.time()\n\n    # Preprocess input\n    input_path = self.preprocess_input(input_path)\n\n    if self.show_log:\n        logger.info(f\"Extracting text from {input_path}, pages {page_range}\")\n\n    try:\n        text_blocks = []\n\n        if page_range is None:\n            # Extract all pages\n            text_blocks = self._extract_text_by_page(input_path)\n        else:\n            start_page, end_page = page_range\n\n            for page_num in range(start_page, end_page + 1):\n                try:\n                    page_text = pdftext.pdf_text(\n                        str(input_path), \n                        page_num=page_num,\n                        keep_layout=self.keep_layout,\n                        physical_layout=self.physical_layout\n                    )\n\n                    if page_text and page_text.strip():\n                        paragraphs = page_text.split('\\n\\n')\n\n                        for para_idx, paragraph in enumerate(paragraphs):\n                            if paragraph.strip():\n                                block = TextBlock(\n                                    text=paragraph.strip(),\n                                    bbox=None,\n                                    confidence=1.0,\n                                    page_num=page_num,\n                                    block_type='paragraph',\n                                    reading_order=para_idx\n                                )\n                                text_blocks.append(block)\n\n                except Exception as e:\n                    logger.warning(f\"Error extracting page {page_num}: {str(e)}\")\n                    continue\n\n        # Create source info\n        source_info = {\n            'file_path': str(input_path),\n            'file_name': input_path.name,\n            'file_size': input_path.stat().st_size,\n            'engine': 'pdftext',\n            'page_range': page_range\n        }\n\n        # Post-process output\n        output = self.postprocess_output(text_blocks, source_info)\n        output.processing_time = time.time() - start_time\n\n        if self.show_log:\n            logger.info(f\"Extracted {len(output.text_blocks)} text blocks in {output.processing_time:.2f}s\")\n\n        return output\n\n    except Exception as e:\n        logger.error(f\"Error extracting text from {input_path}: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pdftext.PdftextTextExtractor.get_supported_formats","title":"get_supported_formats","text":"<pre><code>get_supported_formats() -&gt; List[str]\n</code></pre> <p>Get list of supported document formats.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pdftext.py</code> <pre><code>def get_supported_formats(self) -&gt; List[str]:\n    \"\"\"Get list of supported document formats.\"\"\"\n    return ['.pdf']\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pdftext.PdftextTextExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: Any, source_info: Optional[Dict] = None) -&gt; TextOutput\n</code></pre> <p>Convert pdftext output to standardized TextOutput format.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pdftext.py</code> <pre><code>def postprocess_output(self, raw_output: Any, source_info: Optional[Dict] = None) -&gt; TextOutput:\n    \"\"\"Convert pdftext output to standardized TextOutput format.\"\"\"\n    text_blocks = raw_output  # raw_output is already a list of TextBlocks\n\n    # Sort blocks by page and reading order\n    text_blocks.sort(key=lambda x: (x.page_num, x.reading_order or 0))\n\n    # Combine all text\n    full_text = '\\n\\n'.join(block.text for block in text_blocks if block.text.strip())\n\n    # Get metadata\n    metadata = {\n        'engine': 'pdftext',\n        'keep_layout': self.keep_layout,\n        'physical_layout': self.physical_layout,\n        'total_blocks': len(text_blocks)\n    }\n\n    return TextOutput(\n        text_blocks=text_blocks,\n        full_text=full_text,\n        metadata=metadata,\n        source_info=source_info,\n        page_count=max(block.page_num for block in text_blocks) if text_blocks else 1\n    )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pdftext.PdftextTextExtractor.preprocess_input","title":"preprocess_input","text":"<pre><code>preprocess_input(input_path: Union[str, Path]) -&gt; Path\n</code></pre> <p>Preprocess input document.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pdftext.py</code> <pre><code>def preprocess_input(self, input_path: Union[str, Path]) -&gt; Path:\n    \"\"\"Preprocess input document.\"\"\"\n    input_path = Path(input_path)\n    if not input_path.exists():\n        raise FileNotFoundError(f\"Input file not found: {input_path}\")\n\n    if input_path.suffix.lower() != '.pdf':\n        raise ValueError(f\"pdftext only supports PDF files. Got: {input_path.suffix}\")\n\n    return input_path\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.pdftext.PdftextTextMapper","title":"PdftextTextMapper","text":"<pre><code>PdftextTextMapper()\n</code></pre> <p>               Bases: <code>BaseTextMapper</code></p> <p>Mapper for pdftext text extraction output.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/pdftext.py</code> <pre><code>def __init__(self):\n    super().__init__(\"pdftext\")\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.surya_text","title":"omnidocs.tasks.text_extraction.extractors.surya_text","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.surya_text.SuryaTextExtractor","title":"SuryaTextExtractor","text":"<pre><code>SuryaTextExtractor(device: Optional[str] = None, show_log: bool = False, extract_images: bool = False, model_path: Optional[Union[str, Path]] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Surya-based text extraction implementation for images and documents.</p> <p>Initialize Surya Text Extractor.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/surya_text.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    extract_images: bool = False,\n    model_path: Optional[Union[str, Path]] = None,\n    **kwargs\n):\n    \"\"\"Initialize Surya Text Extractor.\"\"\"\n    super().__init__(device=device, show_log=show_log, engine_name='surya', extract_images=extract_images)\n\n    self._label_mapper = SuryaTextMapper()\n\n    if self.show_log:\n        logger.info(\"Initializing SuryaTextExtractor\")\n\n    # Set device if specified, otherwise use default from parent\n    if device:\n        self.device = device\n\n    if self.show_log:\n        logger.info(f\"Using device: {self.device}\")\n\n    # Set default paths\n    if model_path is None:\n        model_path = _MODELS_DIR / \"surya_text\"\n\n    self.model_path = Path(model_path)\n\n    # Check dependencies and load model\n    self._check_dependencies()\n    self._load_model()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.surya_text.SuryaTextExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text using Surya OCR.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/surya_text.py</code> <pre><code>@log_execution_time\ndef extract(\n    self,\n    input_path: Union[str, Path, Image.Image],\n    **kwargs\n) -&gt; TextOutput:\n    \"\"\"Extract text using Surya OCR.\"\"\"\n    start_time = time.time()\n\n    try:\n        # Preprocess input\n        images = self.preprocess_input(input_path)\n\n        predictions = []\n\n        for img in images:\n            # Run text detection and recognition\n            try:\n                from surya.common.surya.schema import TaskNames\n\n                # Use recognition predictor for text extraction\n                prediction = self.rec_predictor(\n                    [img],\n                    task_names=[TaskNames.ocr_with_boxes],\n                    det_predictor=self.det_predictor,\n                    math_mode=False  # Standard text mode\n                )\n\n                if prediction and len(prediction) &gt; 0:\n                    predictions.append(prediction[0])\n\n            except Exception as e:\n                if self.show_log:\n                    logger.warning(f\"Error processing image with Surya: {e}\")\n                continue\n\n        # Prepare source info\n        source_info = {\n            'source_path': str(input_path) if not isinstance(input_path, Image.Image) else 'PIL_Image',\n            'num_images': len(images),\n            'processing_time': time.time() - start_time\n        }\n\n        # Convert to standardized format\n        result = self.postprocess_output({\n            'predictions': predictions,\n            'processing_info': {\n                'total_images': len(images),\n                'successful_predictions': len(predictions)\n            }\n        }, source_info)\n\n        if self.show_log:\n            logger.info(f\"Extracted {len(result.text_blocks)} text blocks using Surya\")\n\n        return result\n\n    except Exception:\n        if self.show_log:\n            logger.error(\"Error during Surya text extraction\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.surya_text.SuryaTextExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: Any, source_info: Optional[Dict] = None) -&gt; TextOutput\n</code></pre> <p>Convert Surya output to standardized TextOutput format.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/surya_text.py</code> <pre><code>def postprocess_output(self, raw_output: Any, source_info: Optional[Dict] = None) -&gt; TextOutput:\n    \"\"\"Convert Surya output to standardized TextOutput format.\"\"\"\n    text_blocks = []\n    full_text_parts = []\n\n    if 'predictions' in raw_output:\n        for page_idx, prediction in enumerate(raw_output['predictions']):\n            if hasattr(prediction, 'text_lines'):\n                for line_idx, text_line in enumerate(prediction.text_lines):\n                    # Create text block\n                    block = TextBlock(\n                        text=text_line.text.strip(),\n                        bbox=text_line.bbox if hasattr(text_line, 'bbox') else None,\n                        confidence=getattr(text_line, 'confidence', 1.0),\n                        page_num=page_idx + 1,\n                        block_type='text_line',\n                        reading_order=line_idx\n                    )\n                    text_blocks.append(block)\n                    full_text_parts.append(text_line.text.strip())\n\n    # Build metadata\n    metadata = {\n        'engine': 'surya',\n        'total_blocks': len(text_blocks),\n        'processing_info': raw_output.get('processing_info', {})\n    }\n\n    if source_info:\n        metadata.update(source_info)\n\n    return TextOutput(\n        text_blocks=text_blocks,\n        full_text='\\n'.join(full_text_parts),\n        metadata=metadata,\n        source_info=source_info,\n        page_count=len(raw_output.get('predictions', []))\n    )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.surya_text.SuryaTextExtractor.preprocess_input","title":"preprocess_input","text":"<pre><code>preprocess_input(input_path: Union[str, Path, Image]) -&gt; List[Image.Image]\n</code></pre> <p>Preprocess input for Surya text extraction.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/surya_text.py</code> <pre><code>def preprocess_input(self, input_path: Union[str, Path, Image.Image]) -&gt; List[Image.Image]:\n    \"\"\"Preprocess input for Surya text extraction.\"\"\"\n    if isinstance(input_path, Image.Image):\n        return [input_path.convert(\"RGB\")]\n    elif isinstance(input_path, (str, Path)):\n        # Handle image files\n        if str(input_path).lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp')):\n            image = Image.open(input_path).convert(\"RGB\")\n            return [image]\n        else:\n            # For PDF files, we'd need to convert to images first\n            # This is a simplified implementation - you might want to use pdf2image\n            raise ValueError(f\"Unsupported file type: {input_path}. Surya text extractor works with images.\")\n    else:\n        raise ValueError(\"Unsupported input type for Surya text extractor\")\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.surya_text.SuryaTextMapper","title":"SuryaTextMapper","text":"<pre><code>SuryaTextMapper()\n</code></pre> <p>               Bases: <code>BaseTextMapper</code></p> <p>Label mapper for Surya text model output.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/surya_text.py</code> <pre><code>def __init__(self):\n    super().__init__('surya')\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.docling_parse","title":"omnidocs.tasks.text_extraction.extractors.docling_parse","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.docling_parse.DoclingTextExtractor","title":"DoclingTextExtractor","text":"<pre><code>DoclingTextExtractor(device: Optional[str] = None, show_log: bool = False, extract_images: bool = False, ocr_enabled: bool = True, table_structure_enabled: bool = True)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Text extractor using Docling.</p> <p>Initialize Docling text extractor.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run on (not used for Docling)</p> <code>None</code> <code>show_log</code> <code>bool</code> <p>Whether to show detailed logs</p> <code>False</code> <code>extract_images</code> <code>bool</code> <p>Whether to extract images alongside text</p> <code>False</code> <code>ocr_enabled</code> <code>bool</code> <p>Whether to enable OCR for scanned documents</p> <code>True</code> <code>table_structure_enabled</code> <code>bool</code> <p>Whether to enable table structure detection</p> <code>True</code> Source code in <code>omnidocs/tasks/text_extraction/extractors/docling_parse.py</code> <pre><code>def __init__(self, \n             device: Optional[str] = None, \n             show_log: bool = False,\n             extract_images: bool = False,\n             ocr_enabled: bool = True,\n             table_structure_enabled: bool = True):\n    \"\"\"Initialize Docling text extractor.\n\n    Args:\n        device: Device to run on (not used for Docling)\n        show_log: Whether to show detailed logs\n        extract_images: Whether to extract images alongside text\n        ocr_enabled: Whether to enable OCR for scanned documents\n        table_structure_enabled: Whether to enable table structure detection\n    \"\"\"\n    super().__init__(device, show_log, \"docling\", extract_images)\n    self.ocr_enabled = ocr_enabled\n    self.table_structure_enabled = table_structure_enabled\n    self._label_mapper = DoclingTextMapper()\n    self._load_model()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.docling_parse.DoclingTextExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path], **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text from document using Docling.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input document</p> required <code>**kwargs</code> <p>Additional parameters (ignored for Docling)</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>TextOutput containing extracted text</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/docling_parse.py</code> <pre><code>def extract(\n    self,\n    input_path: Union[str, Path],\n    **kwargs\n) -&gt; TextOutput:\n    \"\"\"Extract text from document using Docling.\n\n    Args:\n        input_path: Path to input document\n        **kwargs: Additional parameters (ignored for Docling)\n\n    Returns:\n        TextOutput containing extracted text\n    \"\"\"\n    start_time = time.time()\n\n    # Preprocess input\n    input_path = self.preprocess_input(input_path)\n\n    if self.show_log:\n        logger.info(f\"Extracting text from {input_path}\")\n\n    try:\n        # Convert document\n        result = self.model.convert(input_path)\n\n        # Create source info\n        source_info = {\n            'file_path': str(input_path),\n            'file_name': input_path.name,\n            'file_size': input_path.stat().st_size,\n            'engine': 'docling'\n        }\n\n        # Post-process output\n        output = self.postprocess_output(result, source_info)\n        output.processing_time = time.time() - start_time\n\n        if self.show_log:\n            logger.info(f\"Extracted {len(output.text_blocks)} text blocks in {output.processing_time:.2f}s\")\n\n        return output\n\n    except Exception as e:\n        logger.error(f\"Error extracting text from {input_path}: {str(e)}\")\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.docling_parse.DoclingTextExtractor.get_supported_formats","title":"get_supported_formats","text":"<pre><code>get_supported_formats() -&gt; List[str]\n</code></pre> <p>Get list of supported document formats.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/docling_parse.py</code> <pre><code>def get_supported_formats(self) -&gt; List[str]:\n    \"\"\"Get list of supported document formats.\"\"\"\n    return ['.pdf', '.docx', '.pptx', '.html', '.md']\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.docling_parse.DoclingTextExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: Any, source_info: Optional[Dict] = None) -&gt; TextOutput\n</code></pre> <p>Convert Docling output to standardized TextOutput format.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/docling_parse.py</code> <pre><code>def postprocess_output(self, raw_output: Any, source_info: Optional[Dict] = None) -&gt; TextOutput:\n    \"\"\"Convert Docling output to standardized TextOutput format.\"\"\"\n    text_blocks = []\n\n    # Process document elements\n    for element in raw_output.document.texts:\n        # Get bounding box if available\n        bbox = None\n        if hasattr(element, 'prov') and element.prov:\n            for prov in element.prov:\n                if hasattr(prov, 'bbox'):\n                    bbox = [prov.bbox.l, prov.bbox.t, prov.bbox.r, prov.bbox.b]\n                    break\n\n        # Get page number\n        page_num = 1\n        if hasattr(element, 'prov') and element.prov:\n            for prov in element.prov:\n                if hasattr(prov, 'page'):\n                    page_num = prov.page + 1  # Convert to 1-based\n                    break\n\n        # Create text block\n        block = TextBlock(\n            text=element.text,\n            bbox=bbox,\n            confidence=1.0,  # Docling doesn't provide confidence scores\n            page_num=page_num,\n            block_type=self._label_mapper.normalize_block_type(element.label),\n            reading_order=getattr(element, 'reading_order', None)\n        )\n        text_blocks.append(block)\n\n    # Sort blocks by reading order\n    text_blocks.sort(key=lambda x: (x.page_num, x.reading_order or 0))\n\n    # Combine all text\n    full_text = '\\n\\n'.join(block.text for block in text_blocks)\n\n    # Get metadata\n    metadata = {\n        'engine': 'docling',\n        'ocr_enabled': self.ocr_enabled,\n        'table_structure_enabled': self.table_structure_enabled,\n        'total_elements': len(text_blocks)\n    }\n\n    return TextOutput(\n        text_blocks=text_blocks,\n        full_text=full_text,\n        metadata=metadata,\n        source_info=source_info,\n        page_count=max(block.page_num for block in text_blocks) if text_blocks else 1\n    )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.docling_parse.DoclingTextExtractor.preprocess_input","title":"preprocess_input","text":"<pre><code>preprocess_input(input_path: Union[str, Path]) -&gt; Path\n</code></pre> <p>Preprocess input document.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/docling_parse.py</code> <pre><code>def preprocess_input(self, input_path: Union[str, Path]) -&gt; Path:\n    \"\"\"Preprocess input document.\"\"\"\n    input_path = Path(input_path)\n    if not input_path.exists():\n        raise FileNotFoundError(f\"Input file not found: {input_path}\")\n\n    supported_formats = ['.pdf', '.docx', '.pptx', '.html', '.md']\n    if input_path.suffix.lower() not in supported_formats:\n        raise ValueError(f\"Unsupported format: {input_path.suffix}. Supported: {supported_formats}\")\n\n    return input_path\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.text_extraction.extractors.docling_parse.DoclingTextMapper","title":"DoclingTextMapper","text":"<pre><code>DoclingTextMapper()\n</code></pre> <p>               Bases: <code>BaseTextMapper</code></p> <p>Mapper for Docling text extraction output.</p> Source code in <code>omnidocs/tasks/text_extraction/extractors/docling_parse.py</code> <pre><code>def __init__(self):\n    super().__init__(\"docling\")\n</code></pre>"},{"location":"api_reference/python_api.html#math-expression-extraction","title":"\ud83d\udd22 Math Expression Extraction","text":"<p>Recognize and extract LaTeX math expressions from images and PDFs.</p>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction","title":"omnidocs.tasks.math_expression_extraction","text":"<p>Math expression extraction module for OmniDocs.</p> <p>This module provides base classes and implementations for mathematical expression extraction and LaTeX recognition from images and documents.</p>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.BaseLatexExtractor","title":"BaseLatexExtractor","text":"<pre><code>BaseLatexExtractor(device: Optional[str] = None, show_log: bool = False)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for LaTeX expression extraction models.</p> <p>Initialize the LaTeX extractor.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run model on ('cuda' or 'cpu')</p> <code>None</code> <code>show_log</code> <code>bool</code> <p>Whether to show detailed logs</p> <code>False</code> Source code in <code>omnidocs/tasks/math_expression_extraction/base.py</code> <pre><code>def __init__(self, device: Optional[str] = None, show_log: bool = False):\n    \"\"\"Initialize the LaTeX extractor.\n\n    Args:\n        device: Device to run model on ('cuda' or 'cpu')\n        show_log: Whether to show detailed logs\n    \"\"\"\n    self.show_log = show_log\n    self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n    self.model = None\n    self.model_path = None\n    self._label_mapper: Optional[BaseLatexMapper] = None\n\n    if self.show_log:\n        logger.info(f\"Initializing {self.__class__.__name__}\")\n        logger.info(f\"Using device: {self.device}\")\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.BaseLatexExtractor.label_mapper","title":"label_mapper  <code>property</code>","text":"<pre><code>label_mapper: BaseLatexMapper\n</code></pre> <p>Get the label mapper for this extractor.</p>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.BaseLatexExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; LatexOutput\n</code></pre> <p>Extract LaTeX expressions from input image.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path, Image]</code> <p>Path to input image or image data</p> required <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>LatexOutput</code> <p>LatexOutput containing extracted expressions</p> Source code in <code>omnidocs/tasks/math_expression_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(\n    self,\n    input_path: Union[str, Path, Image.Image],\n    **kwargs\n) -&gt; LatexOutput:\n    \"\"\"Extract LaTeX expressions from input image.\n\n    Args:\n        input_path: Path to input image or image data\n        **kwargs: Additional model-specific parameters\n\n    Returns:\n        LatexOutput containing extracted expressions\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.BaseLatexExtractor.extract_all","title":"extract_all","text":"<pre><code>extract_all(input_paths: List[Union[str, Path, Image]], **kwargs) -&gt; List[LatexOutput]\n</code></pre> <p>Extract LaTeX from multiple images.</p> <p>Parameters:</p> Name Type Description Default <code>input_paths</code> <code>List[Union[str, Path, Image]]</code> <p>List of image paths or image data</p> required <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[LatexOutput]</code> <p>List of LatexOutput objects</p> Source code in <code>omnidocs/tasks/math_expression_extraction/base.py</code> <pre><code>def extract_all(\n    self,\n    input_paths: List[Union[str, Path, Image.Image]],\n    **kwargs\n) -&gt; List[LatexOutput]:\n    \"\"\"Extract LaTeX from multiple images.\n\n    Args:\n        input_paths: List of image paths or image data\n        **kwargs: Additional model-specific parameters\n\n    Returns:\n        List of LatexOutput objects\n    \"\"\"\n    results = []\n    for input_path in input_paths:\n        try:\n            result = self.extract(input_path, **kwargs)\n            results.append(result)\n        except Exception as e:\n            if self.show_log:\n                logger.error(f\"Error processing {input_path}: {str(e)}\")\n            raise\n    return results\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.BaseLatexExtractor.map_expression","title":"map_expression","text":"<pre><code>map_expression(expression: str) -&gt; str\n</code></pre> <p>Map model-specific LaTeX to standardized format.</p> Source code in <code>omnidocs/tasks/math_expression_extraction/base.py</code> <pre><code>def map_expression(self, expression: str) -&gt; str:\n    \"\"\"Map model-specific LaTeX to standardized format.\"\"\"\n    if self._label_mapper is None:\n        return expression\n    return self._label_mapper.to_standard(expression)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.BaseLatexExtractor.preprocess_input","title":"preprocess_input","text":"<pre><code>preprocess_input(input_path: Union[str, Path, Image, ndarray]) -&gt; List[Image.Image]\n</code></pre> <p>Convert input to list of PIL Images.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path, Image, ndarray]</code> <p>Input image path or image data</p> required <p>Returns:</p> Type Description <code>List[Image]</code> <p>List of PIL Images</p> Source code in <code>omnidocs/tasks/math_expression_extraction/base.py</code> <pre><code>def preprocess_input(self, input_path: Union[str, Path, Image.Image, np.ndarray]) -&gt; List[Image.Image]:\n    \"\"\"Convert input to list of PIL Images.\n\n    Args:\n        input_path: Input image path or image data\n\n    Returns:\n        List of PIL Images\n    \"\"\"\n    if isinstance(input_path, (str, Path)):\n        image = Image.open(input_path).convert('RGB')\n        return [image]\n    elif isinstance(input_path, Image.Image):\n        return [input_path.convert('RGB')]\n    elif isinstance(input_path, np.ndarray):\n        return [Image.fromarray(cv2.cvtColor(input_path, cv2.COLOR_BGR2RGB))]\n    else:\n        raise ValueError(f\"Unsupported input type: {type(input_path)}\")\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.BaseLatexMapper","title":"BaseLatexMapper","text":"<pre><code>BaseLatexMapper()\n</code></pre> <p>Base class for mapping model-specific outputs to standardized format.</p> Source code in <code>omnidocs/tasks/math_expression_extraction/base.py</code> <pre><code>def __init__(self):\n    self._mapping: Dict[str, str] = {}\n    self._reverse_mapping: Dict[str, str] = {}\n    self._setup_mapping()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.BaseLatexMapper.from_standard","title":"from_standard","text":"<pre><code>from_standard(standard_latex: str) -&gt; str\n</code></pre> <p>Convert standardized LaTeX to model-specific format.</p> Source code in <code>omnidocs/tasks/math_expression_extraction/base.py</code> <pre><code>def from_standard(self, standard_latex: str) -&gt; str:\n    \"\"\"Convert standardized LaTeX to model-specific format.\"\"\"\n    return self._reverse_mapping.get(standard_latex, standard_latex)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.BaseLatexMapper.to_standard","title":"to_standard","text":"<pre><code>to_standard(model_output: str) -&gt; str\n</code></pre> <p>Convert model-specific LaTeX to standardized format.</p> Source code in <code>omnidocs/tasks/math_expression_extraction/base.py</code> <pre><code>def to_standard(self, model_output: str) -&gt; str:\n    \"\"\"Convert model-specific LaTeX to standardized format.\"\"\"\n    return self._mapping.get(model_output, model_output)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.LatexOutput","title":"LatexOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for extracted LaTeX expressions.</p> <p>Attributes:</p> Name Type Description <code>expressions</code> <code>List[str]</code> <p>List of extracted LaTeX expressions</p> <code>confidences</code> <code>Optional[List[float]]</code> <p>Optional confidence scores for each expression</p> <code>bboxes</code> <code>Optional[List[List[float]]]</code> <p>Optional bounding boxes for each expression</p> <code>source_img_size</code> <code>Optional[Tuple[int, int]]</code> <p>Optional tuple of source image dimensions</p>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.LatexOutput.save_json","title":"save_json","text":"<pre><code>save_json(output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save output to JSON file.</p> Source code in <code>omnidocs/tasks/math_expression_extraction/base.py</code> <pre><code>def save_json(self, output_path: Union[str, Path]) -&gt; None:\n    \"\"\"Save output to JSON file.\"\"\"\n    import json\n    with open(output_path, 'w') as f:\n        json.dump(self.to_dict(), f, indent=2)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.LatexOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/math_expression_extraction/base.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        'expressions': self.expressions,\n        'confidences': self.confidences,\n        'bboxes': self.bboxes,\n        'source_img_size': self.source_img_size\n    }\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.extractors.donut","title":"omnidocs.tasks.math_expression_extraction.extractors.donut","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.extractors.donut.DonutExtractor","title":"DonutExtractor","text":"<pre><code>DonutExtractor(device: Optional[str] = None, show_log: bool = False, model_name: str = 'naver-clova-ix/donut-base-finetuned-cord-v2', model_path: Optional[Union[str, Path]] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseLatexExtractor</code></p> <p>Donut (NAVER CLOVA) based expression extraction implementation.</p> <p>Initialize Donut Extractor.</p> Source code in <code>omnidocs/tasks/math_expression_extraction/extractors/donut.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    model_name: str = \"naver-clova-ix/donut-base-finetuned-cord-v2\",\n    model_path: Optional[Union[str, Path]] = None,\n    **kwargs\n):\n    \"\"\"Initialize Donut Extractor.\"\"\"\n    super().__init__(device=device, show_log=show_log)\n\n    self._label_mapper = DonutMapper()\n    self.model_name = model_name\n\n    # Set default paths\n    if model_path is None:\n        model_path = _MODELS_DIR / \"donut_models\" / model_name.replace(\"/\", \"_\")\n\n    self.model_path = Path(model_path)\n\n    # Check dependencies\n    self._check_dependencies()\n\n    # Download model if needed\n    if not self._model_exists():\n        if self.show_log:\n            logger.info(f\"Model not found at {self.model_path}, will download from HuggingFace\")\n        self._download_model()\n\n    try:\n        self._load_model()\n        if self.show_log:\n            logger.success(\"Donut model initialized successfully\")\n    except Exception as e:\n        logger.error(\"Failed to initialize Donut model\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.extractors.donut.DonutExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; LatexOutput\n</code></pre> <p>Extract LaTeX expressions using Donut.</p> Source code in <code>omnidocs/tasks/math_expression_extraction/extractors/donut.py</code> <pre><code>@log_execution_time\ndef extract(\n    self,\n    input_path: Union[str, Path, Image.Image],\n    **kwargs\n) -&gt; LatexOutput:\n    \"\"\"Extract LaTeX expressions using Donut.\"\"\"\n    try:\n        # Preprocess input\n        images = self.preprocess_input(input_path)\n\n        expressions = []\n        for img in images:\n            # Prepare image for Donut\n            pixel_values = self.processor(img, return_tensors=\"pt\").pixel_values\n            pixel_values = pixel_values.to(self.device)\n\n            # Prepare task prompt (adjust based on your specific task)\n            task_prompt = \"&lt;s_cord-v2&gt;\"  # Default CORD v2 task(this is used for receipt/invoice parsing)\n            decoder_input_ids = self.processor.tokenizer(\n                task_prompt, \n                add_special_tokens=False, \n                return_tensors=\"pt\" #returns pytorch tensor \n            ).input_ids\n            decoder_input_ids = decoder_input_ids.to(self.device)\n\n            # Generate\n            with torch.no_grad():\n                outputs = self.model.generate(\n                    pixel_values,\n                    decoder_input_ids=decoder_input_ids,\n                    max_length=self.model.decoder.config.max_position_embeddings,\n                    early_stopping=True,\n                    pad_token_id=self.processor.tokenizer.pad_token_id,\n                    eos_token_id=self.processor.tokenizer.eos_token_id,\n                    use_cache=True,\n                    num_beams=1,\n                    bad_words_ids=[[self.processor.tokenizer.unk_token_id]],\n                    return_dict_in_generate=True,\n                )\n\n            # Decode output\n            #converts the generated token IDs back into a string\n            sequence = self.processor.batch_decode(outputs.sequences)[0]\n            #removes any pos and eos \n            sequence = sequence.replace(self.processor.tokenizer.eos_token, \"\").replace(self.processor.tokenizer.pad_token, \"\")\n            #removes task prompt \n            sequence = sequence.replace(task_prompt, \"\")\n\n            # Extract math content from JSON-like output\n            math_content = self._extract_math_from_json(sequence)\n\n            # Map to standard format\n            mapped_expr = self.map_expression(math_content)\n            expressions.append(mapped_expr)\n\n        return LatexOutput(\n            expressions=expressions,\n            source_img_size=images[0].size if images else None\n        )\n\n    except Exception as e:\n        logger.error(\"Error during Donut extraction\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.extractors.donut.DonutMapper","title":"DonutMapper","text":"<pre><code>DonutMapper()\n</code></pre> <p>               Bases: <code>BaseLatexMapper</code></p> <p>Label mapper for Donut model output.</p> Source code in <code>omnidocs/tasks/math_expression_extraction/base.py</code> <pre><code>def __init__(self):\n    self._mapping: Dict[str, str] = {}\n    self._reverse_mapping: Dict[str, str] = {}\n    self._setup_mapping()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.extractors.nougat","title":"omnidocs.tasks.math_expression_extraction.extractors.nougat","text":"<p>Nougat (Neural Optical Understanding for Academic Documents) LaTeX Expression Extractor</p> <p>This module provides LaTeX expression extraction using Facebook's Nougat model via Hugging Face transformers.</p>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.extractors.nougat.NougatExtractor","title":"NougatExtractor","text":"<pre><code>NougatExtractor(model_type: str = 'small', device: Optional[str] = None, show_log: bool = False, model_path: Optional[str] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseLatexExtractor</code></p> <p>Nougat (Neural Optical Understanding for Academic Documents) based expression extraction.</p> <p>Initialize Nougat Extractor.</p> Source code in <code>omnidocs/tasks/math_expression_extraction/extractors/nougat.py</code> <pre><code>def __init__(\n    self,\n    model_type: str = \"small\",\n    device: Optional[str] = None,\n    show_log: bool = False,\n    model_path: Optional[str] = None,\n    **kwargs\n):\n    \"\"\"Initialize Nougat Extractor.\"\"\"\n    super().__init__(device=device, show_log=show_log)\n\n    self._label_mapper = NougatMapper()\n    self.model_type = model_type\n\n    # Set default model path if not provided\n    if model_path is None:\n        model_path = _MODELS_DIR / f\"nougat_{model_type}\"\n    self.model_path = Path(model_path)\n\n    # Check dependencies\n    self._check_dependencies()\n\n    try:\n        # Check if model exists locally, download if needed\n        if not self._model_exists():\n            if self.show_log:\n                logger.info(\"Model not found locally, will download from Hugging Face\")\n            self._download_model()\n        else:\n            if self.show_log:\n                logger.info(\"Model found locally, using that version\")\n\n        self._load_model()\n        if self.show_log:\n            logger.success(\"Nougat model initialized successfully\")\n    except Exception as e:\n        logger.error(\"Failed to initialize Nougat model\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.extractors.nougat.NougatExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; LatexOutput\n</code></pre> <p>Extract LaTeX expressions using Nougat.</p> Source code in <code>omnidocs/tasks/math_expression_extraction/extractors/nougat.py</code> <pre><code>@log_execution_time\ndef extract(\n    self,\n    input_path: Union[str, Path, Image.Image],\n    **kwargs\n) -&gt; LatexOutput:\n    \"\"\"Extract LaTeX expressions using Nougat.\"\"\"\n    try:\n        # Preprocess input\n        images = self.preprocess_input(input_path)\n\n        all_expressions = []\n        for img in images:\n            # Add padding to make it look more like a document page\n            from PIL import ImageOps\n            padded_image = ImageOps.expand(img, border=100, fill='white')\n\n            # Process image with Nougat processor\n            pixel_values = self.processor(padded_image, return_tensors=\"pt\").pixel_values\n            pixel_values = pixel_values.to(self.device)\n\n            # Generate text using the model\n            with torch.no_grad():\n                outputs = self.model.generate(\n                    pixel_values,\n                    max_length=512,\n                    num_beams=1,  # Use greedy decoding for faster inference\n                    do_sample=False,\n                    early_stopping=False\n                )\n\n            # Decode the generated text\n            generated_text = self.processor.batch_decode(outputs, skip_special_tokens=True)[0]\n\n            # Extract mathematical expressions from the text\n            expressions = self._extract_math_expressions(generated_text)\n\n            # Map expressions to standard format\n            mapped_expressions = [self.map_expression(expr) for expr in expressions]\n            all_expressions.extend(mapped_expressions)\n\n        return LatexOutput(\n            expressions=all_expressions,\n            source_img_size=images[0].size if images else None\n        )\n\n    except Exception as e:\n        logger.error(\"Error during Nougat extraction\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.extractors.nougat.NougatMapper","title":"NougatMapper","text":"<pre><code>NougatMapper()\n</code></pre> <p>               Bases: <code>BaseLatexMapper</code></p> <p>Label mapper for Nougat model output.</p> Source code in <code>omnidocs/tasks/math_expression_extraction/base.py</code> <pre><code>def __init__(self):\n    self._mapping: Dict[str, str] = {}\n    self._reverse_mapping: Dict[str, str] = {}\n    self._setup_mapping()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.extractors.surya_math","title":"omnidocs.tasks.math_expression_extraction.extractors.surya_math","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.extractors.surya_math.SuryaMathExtractor","title":"SuryaMathExtractor","text":"<pre><code>SuryaMathExtractor(device: Optional[str] = None, show_log: bool = False, model_path: Optional[Union[str, Path]] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseLatexExtractor</code></p> <p>Surya-based mathematical expression extraction implementation.</p> <p>Initialize Surya Math Extractor.</p> Source code in <code>omnidocs/tasks/math_expression_extraction/extractors/surya_math.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    model_path: Optional[Union[str, Path]] = None,\n    **kwargs\n):\n    \"\"\"Initialize Surya Math Extractor.\"\"\"\n    super().__init__(device=device, show_log=show_log)\n\n    self._label_mapper = SuryaMathMapper()\n\n    if self.show_log:\n        logger.info(\"Initializing SuryaMathExtractor\")\n\n    # Set device if specified, otherwise use default from parent\n    if device:\n        self.device = device\n\n    if self.show_log:\n        logger.info(f\"Using device: {self.device}\")\n\n    # Set default paths\n    if model_path is None:\n        model_path = _MODELS_DIR / \"surya_math\"\n\n    self.model_path = Path(model_path)\n\n    # Check dependencies and load model\n    self._check_dependencies()\n    self._load_model()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.extractors.surya_math.SuryaMathExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; LatexOutput\n</code></pre> <p>Extract LaTeX expressions using Surya.</p> Source code in <code>omnidocs/tasks/math_expression_extraction/extractors/surya_math.py</code> <pre><code>@log_execution_time\ndef extract(\n    self,\n    input_path: Union[str, Path, Image.Image],\n    **kwargs\n) -&gt; LatexOutput:\n    \"\"\"Extract LaTeX expressions using Surya.\"\"\"\n    try:\n        # Preprocess input\n        images = self.preprocess_input(input_path)\n\n        expressions = []\n        confidences = []\n        bboxes = []\n\n        for img in images:\n            # Convert PIL to RGB if needed\n            if isinstance(img, Image.Image):\n                img_rgb = img.convert(\"RGB\")\n            else:\n                img_rgb = Image.fromarray(img).convert(\"RGB\")\n\n            # Run math detection and recognition\n            try:\n                # Import TaskNames for proper task specification\n                from surya.common.surya.schema import TaskNames\n\n                # Use recognition predictor with math mode enabled\n                predictions = self.rec_predictor(\n                    [img_rgb],\n                    task_names=[TaskNames.ocr_with_boxes],\n                    det_predictor=self.det_predictor,\n                    math_mode=True  # Enable math mode for LaTeX output\n                )\n\n                # Process predictions\n                if predictions and len(predictions) &gt; 0:\n                    prediction = predictions[0]\n\n                    # Extract text regions that contain math\n                    for text_line in prediction.text_lines:\n                        text_content = text_line.text.strip()\n\n                        # Check if this looks like math content\n                        if self._is_math_content(text_content):\n                            # Map to standard format\n                            mapped_expr = self.map_expression(text_content)\n                            expressions.append(mapped_expr)\n\n                            # Add confidence if available\n                            if hasattr(text_line, 'confidence'):\n                                confidences.append(text_line.confidence)\n                            else:\n                                confidences.append(1.0)\n\n                            # Add bounding box if available\n                            if hasattr(text_line, 'bbox'):\n                                bboxes.append(text_line.bbox)\n                            else:\n                                bboxes.append([0, 0, img_rgb.width, img_rgb.height])\n\n            except Exception as e:\n                if self.show_log:\n                    logger.warning(f\"Error processing image with Surya: {e}\")\n                # Fallback: return empty result for this image\n                continue\n\n        return LatexOutput(\n            expressions=expressions,\n            confidences=confidences if confidences else None,\n            bboxes=bboxes if bboxes else None,\n            source_img_size=images[0].size if images else None\n        )\n\n    except Exception as e:\n        if self.show_log:\n            logger.error(\"Error during Surya math extraction\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.extractors.surya_math.SuryaMathMapper","title":"SuryaMathMapper","text":"<pre><code>SuryaMathMapper()\n</code></pre> <p>               Bases: <code>BaseLatexMapper</code></p> <p>Label mapper for Surya math model output.</p> Source code in <code>omnidocs/tasks/math_expression_extraction/base.py</code> <pre><code>def __init__(self):\n    self._mapping: Dict[str, str] = {}\n    self._reverse_mapping: Dict[str, str] = {}\n    self._setup_mapping()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.extractors.unimernet","title":"omnidocs.tasks.math_expression_extraction.extractors.unimernet","text":"<p>UniMERNet (Universal Mathematical Expression Recognition Network) extractor for LaTeX expressions.</p>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.extractors.unimernet.UniMERNetExtractor","title":"UniMERNetExtractor","text":"<pre><code>UniMERNetExtractor(model_path: Optional[str] = None, cfg_path: Optional[str] = None, device: Optional[str] = None, show_log: bool = False, **kwargs)\n</code></pre> <p>               Bases: <code>BaseLatexExtractor</code></p> <p>UniMERNet (Universal Mathematical Expression Recognition Network) based expression extraction.</p> <p>Initialize UniMERNet Extractor.</p> Source code in <code>omnidocs/tasks/math_expression_extraction/extractors/unimernet.py</code> <pre><code>def __init__(\n    self,\n    model_path: Optional[str] = None,\n    cfg_path: Optional[str] = None,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    **kwargs\n):\n    \"\"\"Initialize UniMERNet Extractor.\"\"\"\n    super().__init__(device=device, show_log=show_log)\n\n    self._label_mapper = UniMERNetMapper()\n\n    # Set default paths\n    if model_path is None:\n        model_path = \"omnidocs/models/unimernet_base\"\n    if cfg_path is None:\n        cfg_path = str(Path(__file__).parent / \"UniMERNet\" / \"configs\" / \"demo.yaml\")\n\n    self.model_path = Path(model_path)\n    self.cfg_path = Path(cfg_path)\n\n    # Check dependencies\n    self._check_dependencies()\n\n    # Download model if needed\n    if not self.model_path.exists():\n        self._download_model()\n\n    try:\n        self._load_model()\n        if self.show_log:\n            logger.success(\"UniMERNet model initialized successfully\")\n    except Exception as e:\n        logger.error(\"Failed to initialize UniMERNet model\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.extractors.unimernet.UniMERNetExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; LatexOutput\n</code></pre> <p>Extract LaTeX expressions using UniMERNet.</p> Source code in <code>omnidocs/tasks/math_expression_extraction/extractors/unimernet.py</code> <pre><code>@log_execution_time\ndef extract(\n    self,\n    input_path: Union[str, Path, Image.Image],\n    **kwargs\n) -&gt; LatexOutput:\n    \"\"\"Extract LaTeX expressions using UniMERNet.\"\"\"\n    try:\n        # Preprocess input\n        images = self.preprocess_input(input_path)\n\n        expressions = []\n        for img in images:\n            # Process image with UniMERNet\n            image_tensor = self.vis_processor(img).unsqueeze(0).to(self.device)\n\n            # Generate LaTeX\n            with torch.no_grad():\n                output = self.model.generate({\"image\": image_tensor})\n                pred = output[\"pred_str\"][0]\n\n            # Map to standard format\n            mapped_expr = self.map_expression(pred)\n            expressions.append(mapped_expr)\n\n        return LatexOutput(\n            expressions=expressions,\n            source_img_size=images[0].size if images else None\n        )\n\n    except Exception as e:\n        logger.error(\"Error during UniMERNet extraction\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.math_expression_extraction.extractors.unimernet.UniMERNetMapper","title":"UniMERNetMapper","text":"<pre><code>UniMERNetMapper()\n</code></pre> <p>               Bases: <code>BaseLatexMapper</code></p> <p>Label mapper for UniMERNet model output.</p> Source code in <code>omnidocs/tasks/math_expression_extraction/base.py</code> <pre><code>def __init__(self):\n    self._mapping: Dict[str, str] = {}\n    self._reverse_mapping: Dict[str, str] = {}\n    self._setup_mapping()\n</code></pre>"},{"location":"api_reference/python_api.html#ocr-optical-character-recognition","title":"\ud83d\uddb9 OCR (Optical Character Recognition)","text":"<p>Extract text from scanned documents and images using OCR models.</p>"},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.paddle","title":"omnidocs.tasks.ocr_extraction.extractors.paddle","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.paddle.PaddleOCRExtractor","title":"PaddleOCRExtractor","text":"<pre><code>PaddleOCRExtractor(device: Optional[str] = None, show_log: bool = False, languages: Optional[List[str]] = None, use_angle_cls: bool = True, use_gpu: bool = True, drop_score: float = 0.5, model_path: Optional[str] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>PaddleOCR based text extraction implementation.</p> <p>Initialize PaddleOCR Extractor.</p> Source code in <code>omnidocs/tasks/ocr_extraction/extractors/paddle.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    languages: Optional[List[str]] = None,\n    use_angle_cls: bool = True,\n    use_gpu: bool = True,\n    drop_score: float = 0.5,\n    model_path: Optional[str] = None,\n    **kwargs\n):\n    \"\"\"Initialize PaddleOCR Extractor.\"\"\"\n    super().__init__(\n        device=device, \n        show_log=show_log, \n        languages=languages or ['en'],\n        engine_name='paddle'\n    )\n\n    self.use_angle_cls = use_angle_cls\n    self.use_gpu = use_gpu\n    self.drop_score = drop_score\n    self._label_mapper = PaddleOCRMapper()\n\n    # Set default paths\n    if model_path is None:\n        model_path = \"omnidocs/models/paddleocr\"\n    self.model_path = Path(model_path)\n\n    # Check dependencies first\n    self._check_dependencies()\n\n    # Set up model directory and download if needed\n    if self.model_path.exists() and any(self.model_path.iterdir()):\n        if self.show_log:\n            logger.info(f\"Using existing PaddleOCR models from: {self.model_path}\")\n    elif not self.model_path.exists():\n        self._download_model()\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.paddle.PaddleOCRExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; OCROutput\n</code></pre> <p>Extract text using PaddleOCR.</p> Source code in <code>omnidocs/tasks/ocr_extraction/extractors/paddle.py</code> <pre><code>@log_execution_time\ndef extract(\n    self,\n    input_path: Union[str, Path, Image.Image],\n    **kwargs\n) -&gt; OCROutput:\n    \"\"\"Extract text using PaddleOCR.\"\"\"\n    try:\n        # Preprocess input\n        images = self.preprocess_input(input_path)\n        img = images[0]\n\n        # Convert PIL to cv2 format if needed\n        if isinstance(img, Image.Image):\n            img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n\n        # Perform OCR\n        result = self.model.ocr(img, cls=self.use_angle_cls)\n\n        # Convert to standardized format\n        texts = self._process_ocr_results(result)\n        full_text_parts = [text.text for text in texts]\n\n        img_size = img.shape[:2][::-1]  # (width, height)\n\n        ocr_output = OCROutput(\n            texts=texts,\n            full_text=' '.join(full_text_parts),\n            source_img_size=img_size\n        )\n\n        if self.show_log:\n            logger.info(f\"Extracted {len(texts)} text regions\")\n\n        return ocr_output\n\n    except Exception as e:\n        logger.error(\"Error during PaddleOCR extraction\", exc_info=True)\n        return OCROutput(\n            texts=[],\n            full_text=\"\",\n            source_img_size=None,\n            processing_time=None,\n            metadata={\"error\": str(e)}\n        )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.paddle.PaddleOCRExtractor.predict","title":"predict","text":"<pre><code>predict(img, **kwargs)\n</code></pre> <p>Predict method for compatibility with original interface.</p> Source code in <code>omnidocs/tasks/ocr_extraction/extractors/paddle.py</code> <pre><code>def predict(self, img, **kwargs):\n    \"\"\"Predict method for compatibility with original interface.\"\"\"\n    try:\n        result = self.extract(img, **kwargs)\n\n        # Convert to original format\n        ocr_res = []\n        for text_obj in result.texts:\n            # Convert bbox back to points format\n            x0, y0, x1, y1 = text_obj.bbox\n            points = [[x0, y0], [x1, y0], [x1, y1], [x0, y1]]\n            poly = [coord for point in points for coord in point]\n\n            ocr_res.append({\n                \"category_type\": \"text\",\n                'poly': poly,\n                'score': text_obj.confidence,\n                'text': text_obj.text,\n            })\n\n        return ocr_res\n\n    except Exception as e:\n        logger.error(\"Error during prediction\", exc_info=True)\n        return []\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.paddle.PaddleOCRExtractor.preprocess_image","title":"preprocess_image","text":"<pre><code>preprocess_image(image, alpha_color=(255, 255, 255), inv=False, bin=False)\n</code></pre> <p>Preprocess image for OCR.</p> Source code in <code>omnidocs/tasks/ocr_extraction/extractors/paddle.py</code> <pre><code>def preprocess_image(self, image, alpha_color=(255, 255, 255), inv=False, bin=False):\n    \"\"\"Preprocess image for OCR.\"\"\"\n    image = alpha_to_color(image, alpha_color)\n    if inv:\n        image = cv2.bitwise_not(image)\n    if bin:\n        image = binarize_img(image)\n    return image\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.paddle.PaddleOCRMapper","title":"PaddleOCRMapper","text":"<pre><code>PaddleOCRMapper()\n</code></pre> <p>               Bases: <code>BaseOCRMapper</code></p> <p>Label mapper for PaddleOCR model output.</p> Source code in <code>omnidocs/tasks/ocr_extraction/extractors/paddle.py</code> <pre><code>def __init__(self):\n    super().__init__('paddleocr')\n    self._mapping = {\n        'en': 'en',\n        'ch': 'ch',\n        'chinese_cht': 'chinese_cht',\n        'ta': 'ta',\n        'te': 'te',\n        'ka': 'ka',\n        'ja': 'japan',\n        'ko': 'korean',\n        'hi': 'hi',\n        'ar': 'ar',\n        'cyrillic': 'cyrillic',\n        'devanagari': 'devanagari',\n        'fr': 'fr',\n        'de': 'german',\n        'es': 'es',\n        'pt': 'pt',\n        'ru': 'ru',\n        'it': 'it',\n    }\n    self._reverse_mapping = {v: k for k, v in self._mapping.items()}\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.paddle.alpha_to_color","title":"alpha_to_color","text":"<pre><code>alpha_to_color(img, alpha_color=(255, 255, 255))\n</code></pre> <p>Convert transparent pixels to specified color.</p> Source code in <code>omnidocs/tasks/ocr_extraction/extractors/paddle.py</code> <pre><code>def alpha_to_color(img, alpha_color=(255, 255, 255)):\n    \"\"\"Convert transparent pixels to specified color.\"\"\"\n    if len(img.shape) == 4:  # RGBA\n        alpha_channel = img[:, :, 3]\n        rgb_channels = img[:, :, :3]\n        transparent_mask = alpha_channel == 0\n\n        for i in range(3):\n            rgb_channels[:, :, i][transparent_mask] = alpha_color[i]\n\n        return rgb_channels\n    return img\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.paddle.binarize_img","title":"binarize_img","text":"<pre><code>binarize_img(img)\n</code></pre> <p>Convert image to binary (black and white).</p> Source code in <code>omnidocs/tasks/ocr_extraction/extractors/paddle.py</code> <pre><code>def binarize_img(img):\n    \"\"\"Convert image to binary (black and white).\"\"\"\n    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) if len(img.shape) == 3 else img\n    _, binary = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n    return cv2.cvtColor(binary, cv2.COLOR_GRAY2BGR)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.paddle.points_to_bbox","title":"points_to_bbox","text":"<pre><code>points_to_bbox(points)\n</code></pre> <p>Change polygon(shape: N * 8) to bbox(shape: N * 4).</p> Source code in <code>omnidocs/tasks/ocr_extraction/extractors/paddle.py</code> <pre><code>def points_to_bbox(points):\n    \"\"\"Change polygon(shape: N * 8) to bbox(shape: N * 4).\"\"\"\n    x_coords = [p[0] for p in points]\n    y_coords = [p[1] for p in points]\n    return [min(x_coords), min(y_coords), max(x_coords), max(y_coords)]\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.tesseract_ocr","title":"omnidocs.tasks.ocr_extraction.extractors.tesseract_ocr","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.tesseract_ocr.TesseractOCRExtractor","title":"TesseractOCRExtractor","text":"<pre><code>TesseractOCRExtractor(device: Optional[str] = None, show_log: bool = False, languages: Optional[List[str]] = None, psm: int = 6, oem: int = 3, config: str = '', **kwargs)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>Tesseract OCR based text extraction implementation.</p> <p>Initialize Tesseract OCR Extractor.</p> Source code in <code>omnidocs/tasks/ocr_extraction/extractors/tesseract_ocr.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    languages: Optional[List[str]] = None,\n    psm: int = 6,\n    oem: int = 3,\n    config: str = \"\",\n    **kwargs\n):\n    \"\"\"Initialize Tesseract OCR Extractor.\"\"\"\n    super().__init__(\n        device=device, \n        show_log=show_log, \n        languages=languages or ['en'],\n        engine_name='tesseract'\n    )\n\n    self.psm = psm  # Page segmentation mode\n    self.oem = oem  # OCR engine mode\n    self.config = config\n    self._label_mapper = TesseractOCRMapper()\n\n    try:\n        import pytesseract\n        from pytesseract import Output\n        self.pytesseract = pytesseract\n        self.Output = Output\n\n        # Set Tesseract executable path for Windows\n        import os\n        tesseract_paths = [\n            r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\",\n            r\"C:\\Program Files (x86)\\Tesseract-OCR\\tesseract.exe\",\n            r\"C:\\Users\\{}\\AppData\\Local\\Tesseract-OCR\\tesseract.exe\".format(os.getenv('USERNAME', '')),\n        ]\n\n        for path in tesseract_paths:\n            if os.path.exists(path):\n                pytesseract.pytesseract.tesseract_cmd = path\n                if self.show_log:\n                    logger.info(f\"Found Tesseract at: {path}\")\n                break\n        else:\n            # Try to find in PATH\n            import shutil\n            tesseract_cmd = shutil.which('tesseract')\n            if tesseract_cmd:\n                pytesseract.pytesseract.tesseract_cmd = tesseract_cmd\n                if self.show_log:\n                    logger.info(f\"Found Tesseract in PATH: {tesseract_cmd}\")\n\n    except ImportError as e:\n        logger.error(\"Failed to import pytesseract\")\n        raise ImportError(\n            \"pytesseract is not available. Please install it with: pip install pytesseract\"\n        ) from e\n\n    self._load_model()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.tesseract_ocr.TesseractOCRExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; OCROutput\n</code></pre> <p>Extract text using Tesseract OCR.</p> Source code in <code>omnidocs/tasks/ocr_extraction/extractors/tesseract_ocr.py</code> <pre><code>@log_execution_time\ndef extract(\n    self,\n    input_path: Union[str, Path, Image.Image],\n    **kwargs\n) -&gt; OCROutput:\n    \"\"\"Extract text using Tesseract OCR.\"\"\"\n    try:\n        # Preprocess input\n        images = self.preprocess_input(input_path)\n        img = images[0]\n\n        # Convert PIL to numpy array\n        img_array = np.array(img)\n\n        # Run OCR with detailed output\n        raw_output = self.pytesseract.image_to_data(\n            img_array,\n            lang=self.lang_string,\n            config=self.tesseract_config,\n            output_type=self.Output.DICT\n        )\n\n        # Convert to standardized format\n        result = self.postprocess_output(raw_output, img.size)\n\n        if self.show_log:\n            logger.info(f\"Extracted {len(result.texts)} text regions\")\n\n        return result\n\n    except Exception as e:\n        logger.error(\"Error during Tesseract extraction\", exc_info=True)\n        return OCROutput(\n            texts=[],\n            full_text=\"\",\n            source_img_size=None,\n            processing_time=None,\n            metadata={\"error\": str(e)}\n        )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.tesseract_ocr.TesseractOCRExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: dict, img_size: Tuple[int, int]) -&gt; OCROutput\n</code></pre> <p>Convert Tesseract output to standardized OCROutput format.</p> Source code in <code>omnidocs/tasks/ocr_extraction/extractors/tesseract_ocr.py</code> <pre><code>def postprocess_output(self, raw_output: dict, img_size: Tuple[int, int]) -&gt; OCROutput:\n    \"\"\"Convert Tesseract output to standardized OCROutput format.\"\"\"\n    texts = []\n    full_text_parts = []\n\n    n_boxes = len(raw_output['text'])\n\n    for i in range(n_boxes):\n        text = raw_output['text'][i].strip()\n\n        if not text:\n            continue\n\n        confidence = float(raw_output['conf'][i])\n\n        if confidence &lt; 0:\n            continue\n\n        x = int(raw_output['left'][i])\n        y = int(raw_output['top'][i])\n        w = int(raw_output['width'][i])\n        h = int(raw_output['height'][i])\n        bbox = [float(x), float(y), float(x + w), float(y + h)]\n\n        # Create polygon from bbox\n        polygon = [[float(x), float(y)], [float(x + w), float(y)], \n                   [float(x + w), float(y + h)], [float(x), float(y + h)]]\n\n        detected_lang = self.detect_text_language(text)\n\n        ocr_text = OCRText(\n            text=text,\n            confidence=confidence / 100.0,\n            bbox=bbox,\n            polygon=polygon,\n            language=detected_lang,\n            reading_order=i\n        )\n\n        texts.append(ocr_text)\n        full_text_parts.append(text)\n\n    return OCROutput(\n        texts=texts,\n        full_text=' '.join(full_text_parts),\n        source_img_size=img_size\n    )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.tesseract_ocr.TesseractOCRMapper","title":"TesseractOCRMapper","text":"<pre><code>TesseractOCRMapper()\n</code></pre> <p>               Bases: <code>BaseOCRMapper</code></p> <p>Label mapper for Tesseract OCR model output.</p> Source code in <code>omnidocs/tasks/ocr_extraction/extractors/tesseract_ocr.py</code> <pre><code>def __init__(self):\n    super().__init__('tesseract')\n    self._setup_mapping()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.easy_ocr","title":"omnidocs.tasks.ocr_extraction.extractors.easy_ocr","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.easy_ocr.EasyOCRExtractor","title":"EasyOCRExtractor","text":"<pre><code>EasyOCRExtractor(device: Optional[str] = None, show_log: bool = False, languages: Optional[List[str]] = None, gpu: bool = True, **kwargs)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>EasyOCR based text extraction implementation.</p> <p>Initialize EasyOCR Extractor.</p> Source code in <code>omnidocs/tasks/ocr_extraction/extractors/easy_ocr.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    languages: Optional[List[str]] = None,\n    gpu: bool = True,\n    **kwargs\n):\n    \"\"\"Initialize EasyOCR Extractor.\"\"\"\n    super().__init__(\n        device=device, \n        show_log=show_log, \n        languages=languages or ['en'],\n        engine_name='easyocr'\n    )\n\n    self.gpu = gpu \n    self._label_mapper = EasyOCRMapper()\n\n    # Set default model path\n    self.model_path = Path(\"omnidocs/models/easyocr\")\n\n    # Check dependencies\n    self._check_dependencies()\n\n    # Download model if needed\n    if not self.model_path.exists():\n        self._download_model()\n\n    self._load_model()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.easy_ocr.EasyOCRExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], detail: int = 1, paragraph: bool = False, width_ths: float = 0.7, height_ths: float = 0.7, **kwargs) -&gt; OCROutput\n</code></pre> <p>Extract text using EasyOCR.</p> Source code in <code>omnidocs/tasks/ocr_extraction/extractors/easy_ocr.py</code> <pre><code>@log_execution_time\ndef extract(\n    self,\n    input_path: Union[str, Path, Image.Image],\n    detail: int = 1,  # Changed default to 1 for bbox and confidence\n    paragraph: bool = False,\n    width_ths: float = 0.7,\n    height_ths: float = 0.7,\n    **kwargs\n) -&gt; OCROutput:\n    \"\"\"Extract text using EasyOCR.\"\"\"\n    try:\n        # Preprocess input\n        images = self.preprocess_input(input_path)\n        img = images[0]\n\n        # Convert PIL to numpy array\n        img_array = np.array(img)\n\n        # Run OCR\n        raw_output = self.model.readtext(\n            img_array,\n            detail=detail,\n            paragraph=paragraph,\n            width_ths=width_ths,\n            height_ths=height_ths,\n            **kwargs\n        )\n\n        # Convert to standardized format\n        result = self.postprocess_output(raw_output, img.size)\n\n        if self.show_log:\n            logger.info(f\"Extracted {len(result.texts)} text regions\")\n\n        return result\n\n    except Exception as e:\n        logger.error(\"Error during EasyOCR extraction\", exc_info=True)\n        return OCROutput(\n            texts=[],\n            full_text=\"\",\n            source_img_size=None,\n            processing_time=None,\n            metadata={\"error\": str(e)}\n        )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.easy_ocr.EasyOCRExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: List, img_size: Tuple[int, int]) -&gt; OCROutput\n</code></pre> <p>Convert EasyOCR output to standardized OCROutput format.</p> Source code in <code>omnidocs/tasks/ocr_extraction/extractors/easy_ocr.py</code> <pre><code>def postprocess_output(self, raw_output: List, img_size: Tuple[int, int]) -&gt; OCROutput:\n    \"\"\"Convert EasyOCR output to standardized OCROutput format.\"\"\"\n    texts = []\n    full_text_parts = []\n\n    for i, detection in enumerate(raw_output):\n        if isinstance(detection, str):\n            text = detection\n            confidence = 0.9\n            bbox = [0, 0, img_size[0], img_size[1]]\n            polygon = [[0, 0], [img_size[0], 0], [img_size[0], img_size[1]], [0, img_size[1]]]\n        elif isinstance(detection, (list, tuple)) and len(detection) == 3:\n            bbox_coords, text, confidence = detection\n\n            bbox_array = np.array(bbox_coords)\n            x1, y1 = bbox_array.min(axis=0)\n            x2, y2 = bbox_array.max(axis=0)\n            bbox = [float(x1), float(y1), float(x2), float(y2)]\n\n            polygon = [[float(x), float(y)] for x, y in bbox_coords]\n        else:\n            continue\n\n        detected_lang = self.detect_text_language(text)\n\n        ocr_text = OCRText(\n            text=text,\n            confidence=float(confidence),\n            bbox=bbox,\n            polygon=polygon,\n            language=detected_lang,\n            reading_order=i\n        )\n\n        texts.append(ocr_text)\n        full_text_parts.append(text)\n\n    return OCROutput(\n        texts=texts,\n        full_text=' '.join(full_text_parts),\n        source_img_size=img_size\n    )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.easy_ocr.EasyOCRMapper","title":"EasyOCRMapper","text":"<pre><code>EasyOCRMapper()\n</code></pre> <p>               Bases: <code>BaseOCRMapper</code></p> <p>Label mapper for EasyOCR model output.</p> Source code in <code>omnidocs/tasks/ocr_extraction/extractors/easy_ocr.py</code> <pre><code>def __init__(self):\n    super().__init__('easyocr')\n    self._setup_mapping()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.surya_ocr","title":"omnidocs.tasks.ocr_extraction.extractors.surya_ocr","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.surya_ocr.SuryaOCRExtractor","title":"SuryaOCRExtractor","text":"<pre><code>SuryaOCRExtractor(device: Optional[str] = None, show_log: bool = False, languages: Optional[List[str]] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>Surya OCR based text extraction implementation.</p> <p>Initialize Surya OCR Extractor.</p> Source code in <code>omnidocs/tasks/ocr_extraction/extractors/surya_ocr.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    languages: Optional[List[str]] = None,\n    **kwargs\n):\n    \"\"\"Initialize Surya OCR Extractor.\"\"\"\n    super().__init__(\n        device=device, \n        show_log=show_log, \n        languages=languages or ['en'],\n        engine_name='surya'\n    )\n\n    self._label_mapper = SuryaOCRMapper()\n\n    # Set default model path\n    self.model_path = Path(\"omnidocs/models/surya\")\n\n    # Check dependencies\n    self._check_dependencies()\n\n    # Download model if needed\n    if not self.model_path.exists():\n        self._download_model()\n\n    self._load_model()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.surya_ocr.SuryaOCRExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; OCROutput\n</code></pre> <p>Extract text using Surya OCR.</p> Source code in <code>omnidocs/tasks/ocr_extraction/extractors/surya_ocr.py</code> <pre><code>@log_execution_time\ndef extract(\n    self,\n    input_path: Union[str, Path, Image.Image],\n    **kwargs\n) -&gt; OCROutput:\n    \"\"\"Extract text using Surya OCR.\"\"\"\n    try:\n        # Preprocess input\n        images = self.preprocess_input(input_path)\n        img = images[0]\n\n        # Map languages to Surya format\n        surya_languages = []\n        for lang in self.languages:\n            mapped_lang = self._label_mapper.from_standard_language(lang)\n            surya_languages.append(mapped_lang)\n\n        # Use the new Predictor-based API\n        predictions = None\n\n        if hasattr(self, 'use_new_api') and self.use_new_api:\n            # Use the new Predictor-based API based on surya scripts\n            try:\n                # Convert image to RGB if needed (function expects a list)\n                img_rgb_list = self.convert_if_not_rgb([img])\n                img_rgb = img_rgb_list[0]\n\n                # Import TaskNames for proper task specification\n                from surya.common.surya.schema import TaskNames\n\n                # Call RecognitionPredictor directly with det_predictor parameter\n                # This is how it's done in surya/scripts/ocr_text.py\n                predictions = self.rec_predictor(\n                    [img_rgb],\n                    task_names=[TaskNames.ocr_with_boxes],\n                    det_predictor=self.det_predictor,\n                    math_mode=False\n                )\n\n            except Exception as e:\n                if self.show_log:\n                    logger.warning(f\"New API failed: {e}\")\n\n        else:\n            # Fallback to old API (shouldn't happen with current version)\n            if hasattr(self, 'run_ocr'):\n                try:\n                    predictions = self.run_ocr(\n                        [img],\n                        [surya_languages],\n                        self.det_model,\n                        self.det_processor,\n                        self.rec_model,\n                        self.rec_processor\n                    )\n                except Exception as e:\n                    if self.show_log:\n                        logger.warning(f\"run_ocr failed: {e}\")\n\n        if predictions is None:\n            raise RuntimeError(\"Failed to run OCR with available Surya API functions\")\n\n        # Convert to standardized format\n        result = self.postprocess_output(predictions, img.size)\n\n        if self.show_log:\n            logger.info(f\"Extracted {len(result.texts)} text regions\")\n\n        return result\n\n    except Exception as e:\n        logger.error(\"Error during Surya OCR extraction\", exc_info=True)\n        return OCROutput(\n            texts=[],\n            full_text=\"\",\n            source_img_size=None,\n            processing_time=None,\n            metadata={\"error\": str(e)}\n        )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.surya_ocr.SuryaOCRExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: Union[List, Any], img_size: Tuple[int, int]) -&gt; OCROutput\n</code></pre> <p>Convert Surya OCR output to standardized OCROutput format.</p> Source code in <code>omnidocs/tasks/ocr_extraction/extractors/surya_ocr.py</code> <pre><code>def postprocess_output(self, raw_output: Union[List, Any], img_size: Tuple[int, int]) -&gt; OCROutput:\n    \"\"\"Convert Surya OCR output to standardized OCROutput format.\"\"\"\n    texts = []\n    full_text_parts = []\n\n    if not raw_output:\n        return OCROutput(\n            texts=[],\n            full_text=\"\",\n            source_img_size=img_size\n        )\n\n    try:\n        # Handle different output formats from different Surya versions\n        if isinstance(raw_output, list) and len(raw_output) &gt; 0:\n            prediction = raw_output[0]\n\n            # Check for different attribute names based on version\n            text_lines = None\n            if hasattr(prediction, 'text_lines'):\n                text_lines = prediction.text_lines\n            elif hasattr(prediction, 'bboxes') and hasattr(prediction, 'text'):\n                # Handle case where we have separate bboxes and text\n                if hasattr(prediction, 'text') and isinstance(prediction.text, list):\n                    text_lines = []\n                    for i, (bbox, text) in enumerate(zip(prediction.bboxes, prediction.text)):\n                        # Create a mock text_line object\n                        class MockTextLine:\n                            def __init__(self, text, bbox):\n                                self.text = text\n                                self.bbox = bbox\n                                self.confidence = 0.9  # Default confidence\n                        text_lines.append(MockTextLine(text, bbox))\n\n            if text_lines:\n                for i, text_line in enumerate(text_lines):\n                    if hasattr(text_line, 'text') and hasattr(text_line, 'bbox'):\n                        text = text_line.text.strip() if text_line.text else \"\"\n                        if not text:\n                            continue\n\n                        bbox = text_line.bbox\n                        # Ensure bbox is in the correct format [x1, y1, x2, y2]\n                        if len(bbox) &gt;= 4:\n                            bbox_list = [float(bbox[0]), float(bbox[1]), float(bbox[2]), float(bbox[3])]\n                        else:\n                            continue\n\n                        # Create polygon from bbox\n                        polygon = [\n                            [float(bbox[0]), float(bbox[1])], \n                            [float(bbox[2]), float(bbox[1])],\n                            [float(bbox[2]), float(bbox[3])], \n                            [float(bbox[0]), float(bbox[3])]\n                        ]\n\n                        confidence = getattr(text_line, 'confidence', 0.9)\n                        detected_lang = self.detect_text_language(text)\n\n                        ocr_text = OCRText(\n                            text=text,\n                            confidence=float(confidence),\n                            bbox=bbox_list,\n                            polygon=polygon,\n                            language=detected_lang,\n                            reading_order=i\n                        )\n\n                        texts.append(ocr_text)\n                        full_text_parts.append(text)\n\n    except Exception as e:\n        logger.error(f\"Error processing Surya OCR output: {e}\", exc_info=True)\n\n    return OCROutput(\n        texts=texts,\n        full_text=' '.join(full_text_parts),\n        source_img_size=img_size\n    )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.ocr_extraction.extractors.surya_ocr.SuryaOCRMapper","title":"SuryaOCRMapper","text":"<pre><code>SuryaOCRMapper()\n</code></pre> <p>               Bases: <code>BaseOCRMapper</code></p> <p>Label mapper for Surya OCR model output.</p> Source code in <code>omnidocs/tasks/ocr_extraction/extractors/surya_ocr.py</code> <pre><code>def __init__(self):\n    super().__init__('surya')\n    self._setup_mapping()\n</code></pre>"},{"location":"api_reference/python_api.html#table-extraction","title":"\ud83d\udcca Table Extraction","text":"<p>Extract tabular data from PDFs and images using classic and deep learning models.</p>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction","title":"omnidocs.tasks.table_extraction","text":"<p>Table extraction module for OmniDocs.</p> <p>This module provides base classes and implementations for table detection and extraction from images and documents.</p>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.BaseTableExtractor","title":"BaseTableExtractor","text":"<pre><code>BaseTableExtractor(device: Optional[str] = None, show_log: bool = False, engine_name: Optional[str] = None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for table extraction models.</p> <p>Initialize the table extractor.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run model on ('cuda' or 'cpu')</p> <code>None</code> <code>show_log</code> <code>bool</code> <p>Whether to show detailed logs</p> <code>False</code> <code>engine_name</code> <code>Optional[str]</code> <p>Name of the table extraction engine</p> <code>None</code> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def __init__(self, \n             device: Optional[str] = None, \n             show_log: bool = False,\n             engine_name: Optional[str] = None):\n    \"\"\"Initialize the table extractor.\n\n    Args:\n        device: Device to run model on ('cuda' or 'cpu')\n        show_log: Whether to show detailed logs\n        engine_name: Name of the table extraction engine\n    \"\"\"\n    self.show_log = show_log\n    self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n    self.engine_name = engine_name or self.__class__.__name__.lower().replace('extractor', '')\n    self.model = None\n    self.model_path = None\n    self._label_mapper: Optional[BaseTableMapper] = None\n\n    # Initialize mapper if engine name is provided\n    if self.engine_name:\n        self._label_mapper = BaseTableMapper(self.engine_name)\n\n    if self.show_log:\n        logger.info(f\"Initializing {self.__class__.__name__}\")\n        logger.info(f\"Using device: {self.device}\")\n        logger.info(f\"Engine: {self.engine_name}\")\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.BaseTableExtractor.label_mapper","title":"label_mapper  <code>property</code>","text":"<pre><code>label_mapper: BaseTableMapper\n</code></pre> <p>Get the label mapper for this extractor.</p>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.BaseTableExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; TableOutput\n</code></pre> <p>Extract tables from input image.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path, Image]</code> <p>Path to input image or image data</p> required <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>TableOutput</code> <p>TableOutput containing extracted tables</p> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(\n    self,\n    input_path: Union[str, Path, Image.Image],\n    **kwargs\n) -&gt; TableOutput:\n    \"\"\"Extract tables from input image.\n\n    Args:\n        input_path: Path to input image or image data\n        **kwargs: Additional model-specific parameters\n\n    Returns:\n        TableOutput containing extracted tables\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.BaseTableExtractor.extract_all","title":"extract_all","text":"<pre><code>extract_all(input_paths: List[Union[str, Path, Image]], **kwargs) -&gt; List[TableOutput]\n</code></pre> <p>Extract tables from multiple images.</p> <p>Parameters:</p> Name Type Description Default <code>input_paths</code> <code>List[Union[str, Path, Image]]</code> <p>List of image paths or image data</p> required <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[TableOutput]</code> <p>List of TableOutput objects</p> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def extract_all(\n    self,\n    input_paths: List[Union[str, Path, Image.Image]],\n    **kwargs\n) -&gt; List[TableOutput]:\n    \"\"\"Extract tables from multiple images.\n\n    Args:\n        input_paths: List of image paths or image data\n        **kwargs: Additional model-specific parameters\n\n    Returns:\n        List of TableOutput objects\n    \"\"\"\n    results = []\n    for input_path in input_paths:\n        try:\n            result = self.extract(input_path, **kwargs)\n            results.append(result)\n        except Exception as e:\n            if self.show_log:\n                logger.error(f\"Error processing {input_path}: {str(e)}\")\n            raise\n    return results\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.BaseTableExtractor.extract_with_layout","title":"extract_with_layout","text":"<pre><code>extract_with_layout(input_path: Union[str, Path, Image], layout_regions: Optional[List[Dict]] = None, **kwargs) -&gt; TableOutput\n</code></pre> <p>Extract tables with optional layout information.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path, Image]</code> <p>Path to input image or image data</p> required <code>layout_regions</code> <code>Optional[List[Dict]]</code> <p>Optional list of layout regions containing tables</p> <code>None</code> <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>TableOutput</code> <p>TableOutput containing extracted tables</p> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def extract_with_layout(\n    self,\n    input_path: Union[str, Path, Image.Image],\n    layout_regions: Optional[List[Dict]] = None,\n    **kwargs\n) -&gt; TableOutput:\n    \"\"\"Extract tables with optional layout information.\n\n    Args:\n        input_path: Path to input image or image data\n        layout_regions: Optional list of layout regions containing tables\n        **kwargs: Additional model-specific parameters\n\n    Returns:\n        TableOutput containing extracted tables\n    \"\"\"\n    # Default implementation just calls extract, can be overridden by child classes\n    return self.extract(input_path, **kwargs)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.BaseTableExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: Any, img_size: Tuple[int, int]) -&gt; TableOutput\n</code></pre> <p>Convert raw table extraction output to standardized TableOutput format.</p> <p>Parameters:</p> Name Type Description Default <code>raw_output</code> <code>Any</code> <p>Raw output from table extraction engine</p> required <code>img_size</code> <code>Tuple[int, int]</code> <p>Original image size (width, height)</p> required <p>Returns:</p> Type Description <code>TableOutput</code> <p>Standardized TableOutput object</p> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def postprocess_output(self, raw_output: Any, img_size: Tuple[int, int]) -&gt; TableOutput:\n    \"\"\"Convert raw table extraction output to standardized TableOutput format.\n\n    Args:\n        raw_output: Raw output from table extraction engine\n        img_size: Original image size (width, height)\n\n    Returns:\n        Standardized TableOutput object\n    \"\"\"\n    raise NotImplementedError(\"Child classes must implement postprocess_output method\")\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.BaseTableExtractor.preprocess_input","title":"preprocess_input","text":"<pre><code>preprocess_input(input_path: Union[str, Path, Image, ndarray]) -&gt; List[Image.Image]\n</code></pre> <p>Convert input to list of PIL Images.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path, Image, ndarray]</code> <p>Input image path or image data</p> required <p>Returns:</p> Type Description <code>List[Image]</code> <p>List of PIL Images</p> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def preprocess_input(self, input_path: Union[str, Path, Image.Image, np.ndarray]) -&gt; List[Image.Image]:\n    \"\"\"Convert input to list of PIL Images.\n\n    Args:\n        input_path: Input image path or image data\n\n    Returns:\n        List of PIL Images\n    \"\"\"\n    if isinstance(input_path, (str, Path)):\n        image = Image.open(input_path).convert('RGB')\n        return [image]\n    elif isinstance(input_path, Image.Image):\n        return [input_path.convert('RGB')]\n    elif isinstance(input_path, np.ndarray):\n        return [Image.fromarray(cv2.cvtColor(input_path, cv2.COLOR_BGR2RGB))]\n    else:\n        raise ValueError(f\"Unsupported input type: {type(input_path)}\")\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.BaseTableExtractor.visualize","title":"visualize","text":"<pre><code>visualize(table_result: TableOutput, image_path: Union[str, Path, Image], output_path: str = 'visualized_tables.png', table_color: str = 'red', cell_color: str = 'blue', box_width: int = 2, show_text: bool = False, text_color: str = 'green', font_size: int = 12, show_table_ids: bool = True) -&gt; None\n</code></pre> <p>Visualize table extraction results by drawing bounding boxes on the original image.</p> <p>This method allows users to easily see which extractor is working better by visualizing the detected tables and cells with bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>table_result</code> <code>TableOutput</code> <p>TableOutput containing extracted tables</p> required <code>image_path</code> <code>Union[str, Path, Image]</code> <p>Path to original image or PIL Image object</p> required <code>output_path</code> <code>str</code> <p>Path to save the annotated image</p> <code>'visualized_tables.png'</code> <code>table_color</code> <code>str</code> <p>Color for table bounding boxes</p> <code>'red'</code> <code>cell_color</code> <code>str</code> <p>Color for cell bounding boxes</p> <code>'blue'</code> <code>box_width</code> <code>int</code> <p>Width of bounding box lines</p> <code>2</code> <code>show_text</code> <code>bool</code> <p>Whether to overlay cell text</p> <code>False</code> <code>text_color</code> <code>str</code> <p>Color for text overlay</p> <code>'green'</code> <code>font_size</code> <code>int</code> <p>Font size for text overlay</p> <code>12</code> <code>show_table_ids</code> <code>bool</code> <p>Whether to show table IDs</p> <code>True</code> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def visualize(self,\n              table_result: 'TableOutput',\n              image_path: Union[str, Path, Image.Image],\n              output_path: str = \"visualized_tables.png\",\n              table_color: str = 'red',\n              cell_color: str = 'blue',\n              box_width: int = 2,\n              show_text: bool = False,\n              text_color: str = 'green',\n              font_size: int = 12,\n              show_table_ids: bool = True) -&gt; None:\n    \"\"\"Visualize table extraction results by drawing bounding boxes on the original image.\n\n    This method allows users to easily see which extractor is working better\n    by visualizing the detected tables and cells with bounding boxes.\n\n    Args:\n        table_result: TableOutput containing extracted tables\n        image_path: Path to original image or PIL Image object\n        output_path: Path to save the annotated image\n        table_color: Color for table bounding boxes\n        cell_color: Color for cell bounding boxes\n        box_width: Width of bounding box lines\n        show_text: Whether to overlay cell text\n        text_color: Color for text overlay\n        font_size: Font size for text overlay\n        show_table_ids: Whether to show table IDs\n    \"\"\"\n    try:\n        from PIL import Image, ImageDraw, ImageFont\n\n        # Handle different input types\n        if isinstance(image_path, (str, Path)):\n            image_path = Path(image_path)\n\n            # Check if it's a PDF file\n            if image_path.suffix.lower() == '.pdf':\n                # Convert PDF to image\n                image = self._convert_pdf_to_image(image_path)\n            else:\n                # Regular image file\n                image = Image.open(image_path).convert(\"RGB\")\n        elif isinstance(image_path, Image.Image):\n            image = image_path.convert(\"RGB\")\n        else:\n            raise ValueError(f\"Unsupported image input type: {type(image_path)}\")\n\n        # Create a copy to draw on\n        annotated_image = image.copy()\n        draw = ImageDraw.Draw(annotated_image)\n\n        # Just use original coordinates - no transformation needed\n\n        # Try to load a font for text overlay\n        font = None\n        if show_text or show_table_ids:\n            try:\n                # Try to use a better font if available\n                font = ImageFont.truetype(\"arial.ttf\", font_size)\n            except (OSError, IOError):\n                try:\n                    # Fallback to default font\n                    font = ImageFont.load_default()\n                except:\n                    font = None\n\n        # Draw tables and cells if table results exist\n        if hasattr(table_result, \"tables\") and table_result.tables:\n            for table_idx, table in enumerate(table_result.tables):\n                # Draw table bounding box\n                if table.bbox and len(table.bbox) == 4:\n                    x1, y1, x2, y2 = table.bbox\n                    draw.rectangle(\n                        [(x1, y1), (x2, y2)],\n                        outline=table_color,\n                        width=box_width + 1\n                    )\n\n                    # Draw table ID (only if requested)\n                    if show_table_ids and font:\n                        table_id = getattr(table, 'table_id', f'Table {table_idx}')\n                        draw.text((x1, y1 - font_size - 2), table_id,\n                                fill=table_color, font=font)\n\n                # Draw cell bounding boxes\n                if hasattr(table, \"cells\") and table.cells:\n                    for cell in table.cells:\n                        if cell.bbox and len(cell.bbox) == 4:\n                            x1, y1, x2, y2 = cell.bbox\n\n                            # Draw cell rectangle - no text overlay\n                            draw.rectangle(\n                                [(x1, y1), (x2, y2)],\n                                outline=cell_color,\n                                width=box_width\n                            )\n\n        # Save the annotated image\n        annotated_image.save(output_path)\n\n        if self.show_log:\n            logger.info(f\"Table visualization saved to {output_path}\")\n            num_tables = len(table_result.tables) if table_result.tables else 0\n            total_cells = sum(len(table.cells) for table in table_result.tables) if table_result.tables else 0\n            logger.info(f\"Visualized {num_tables} tables with {total_cells} cells\")\n\n    except Exception as e:\n        error_msg = f\"Error creating table visualization: {str(e)}\"\n        if self.show_log:\n            logger.error(error_msg)\n        raise RuntimeError(error_msg)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.BaseTableExtractor.visualize_from_json","title":"visualize_from_json","text":"<pre><code>visualize_from_json(image_path: Union[str, Path, Image], json_path: Union[str, Path], output_path: str = 'visualized_tables_from_json.png', **kwargs) -&gt; None\n</code></pre> <p>Load table extraction results from JSON file and visualize them.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>Union[str, Path, Image]</code> <p>Path to original image, PDF file, or PIL Image object</p> required <code>json_path</code> <code>Union[str, Path]</code> <p>Path to JSON file containing table extraction results</p> required <code>output_path</code> <code>str</code> <p>Path to save the annotated image</p> <code>'visualized_tables_from_json.png'</code> <code>**kwargs</code> <p>Additional arguments passed to visualize method</p> <code>{}</code> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def visualize_from_json(self,\n                       image_path: Union[str, Path, Image.Image],\n                       json_path: Union[str, Path],\n                       output_path: str = \"visualized_tables_from_json.png\",\n                       **kwargs) -&gt; None:\n    \"\"\"\n    Load table extraction results from JSON file and visualize them.\n\n    Args:\n        image_path: Path to original image, PDF file, or PIL Image object\n        json_path: Path to JSON file containing table extraction results\n        output_path: Path to save the annotated image\n        **kwargs: Additional arguments passed to visualize method\n    \"\"\"\n    import json\n\n    try:\n        # Load table results from JSON\n        with open(json_path, 'r', encoding='utf-8') as f:\n            data = json.load(f)\n\n        # Reconstruct TableOutput from JSON data\n        tables = []\n        if isinstance(data, list):\n            # Handle list of tables format\n            for table_data in data:\n                cells = []\n                if 'cells' in table_data:\n                    for cell_data in table_data['cells']:\n                        cell = TableCell(**cell_data)\n                        cells.append(cell)\n\n                table = Table(\n                    cells=cells,\n                    num_rows=table_data.get('num_rows', 0),\n                    num_cols=table_data.get('num_cols', 0),\n                    bbox=table_data.get('bbox'),\n                    confidence=table_data.get('confidence'),\n                    table_id=table_data.get('table_id', ''),\n                    structure_confidence=table_data.get('structure_confidence')\n                )\n                tables.append(table)\n\n        # Create TableOutput object\n        table_result = TableOutput(\n            tables=tables,\n            source_img_size=data[0].get('source_img_size') if data else None,\n            metadata=data[0].get('metadata', {}) if data else {}\n        )\n\n        # Visualize the loaded results\n        self.visualize(table_result, image_path, output_path, **kwargs)\n\n    except Exception as e:\n        error_msg = f\"Error loading and visualizing tables from JSON: {str(e)}\"\n        if self.show_log:\n            logger.error(error_msg)\n        raise RuntimeError(error_msg)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.BaseTableMapper","title":"BaseTableMapper","text":"<pre><code>BaseTableMapper(engine_name: str)\n</code></pre> <p>Base class for mapping table extraction engine-specific outputs to standardized format.</p> <p>Initialize mapper for specific table extraction engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine_name</code> <code>str</code> <p>Name of the table extraction engine</p> required Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def __init__(self, engine_name: str):\n    \"\"\"Initialize mapper for specific table extraction engine.\n\n    Args:\n        engine_name: Name of the table extraction engine\n    \"\"\"\n    self.engine_name = engine_name.lower()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.BaseTableMapper.detect_header_rows","title":"detect_header_rows","text":"<pre><code>detect_header_rows(cells: List[TableCell]) -&gt; List[TableCell]\n</code></pre> <p>Detect and mark header cells based on position and formatting.</p> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def detect_header_rows(self, cells: List[TableCell]) -&gt; List[TableCell]:\n    \"\"\"Detect and mark header cells based on position and formatting.\"\"\"\n    # Simple heuristic: first row is likely header\n    if not cells:\n        return cells\n\n    first_row_cells = [cell for cell in cells if cell.row == 0]\n    for cell in first_row_cells:\n        cell.is_header = True\n\n    return cells\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.BaseTableMapper.normalize_bbox","title":"normalize_bbox","text":"<pre><code>normalize_bbox(bbox: List[float], img_width: int, img_height: int) -&gt; List[float]\n</code></pre> <p>Normalize bounding box coordinates to absolute pixel values.</p> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def normalize_bbox(self, bbox: List[float], img_width: int, img_height: int) -&gt; List[float]:\n    \"\"\"Normalize bounding box coordinates to absolute pixel values.\"\"\"\n    if all(0 &lt;= coord &lt;= 1 for coord in bbox):\n        return [\n            bbox[0] * img_width,\n            bbox[1] * img_height,\n            bbox[2] * img_width,\n            bbox[3] * img_height\n        ]\n    return bbox\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.Table","title":"Table","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for extracted table.</p> <p>Attributes:</p> Name Type Description <code>cells</code> <code>List[TableCell]</code> <p>List of table cells</p> <code>num_rows</code> <code>int</code> <p>Number of rows in the table</p> <code>num_cols</code> <code>int</code> <p>Number of columns in the table</p> <code>bbox</code> <code>Optional[List[float]]</code> <p>Bounding box of the entire table [x1, y1, x2, y2]</p> <code>confidence</code> <code>Optional[float]</code> <p>Overall table detection confidence</p> <code>table_id</code> <code>Optional[str]</code> <p>Optional table identifier</p> <code>caption</code> <code>Optional[str]</code> <p>Optional table caption</p> <code>structure_confidence</code> <code>Optional[float]</code> <p>Confidence score for table structure detection</p>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.Table.to_csv","title":"to_csv","text":"<pre><code>to_csv() -&gt; str\n</code></pre> <p>Convert table to CSV format.</p> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def to_csv(self) -&gt; str:\n    \"\"\"Convert table to CSV format.\"\"\"\n    import csv\n    import io\n\n    # Create a grid to store cell values\n    grid = [[''] * self.num_cols for _ in range(self.num_rows)]\n\n    # Fill the grid with cell values\n    for cell in self.cells:\n        for r in range(cell.row, cell.row + cell.rowspan):\n            for c in range(cell.col, cell.col + cell.colspan):\n                if r &lt; self.num_rows and c &lt; self.num_cols:\n                    grid[r][c] = cell.text\n\n    # Convert to CSV\n    output = io.StringIO()\n    writer = csv.writer(output)\n    writer.writerows(grid)\n    return output.getvalue()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.Table.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        'cells': [cell.to_dict() for cell in self.cells],\n        'num_rows': self.num_rows,\n        'num_cols': self.num_cols,\n        'bbox': self.bbox,\n        'confidence': self.confidence,\n        'table_id': self.table_id,\n        'caption': self.caption,\n        'structure_confidence': self.structure_confidence\n    }\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.Table.to_html","title":"to_html","text":"<pre><code>to_html() -&gt; str\n</code></pre> <p>Convert table to HTML format.</p> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def to_html(self) -&gt; str:\n    \"\"\"Convert table to HTML format.\"\"\"\n    html = ['&lt;table&gt;']\n\n    # Create a grid to track cell positions and spans\n    grid = [[None for _ in range(self.num_cols)] for _ in range(self.num_rows)]\n\n    # Mark occupied cells\n    for cell in self.cells:\n        for r in range(cell.row, cell.row + cell.rowspan):\n            for c in range(cell.col, cell.col + cell.colspan):\n                if r &lt; self.num_rows and c &lt; self.num_cols:\n                    grid[r][c] = cell if r == cell.row and c == cell.col else 'occupied'\n\n    # Generate HTML rows\n    for row_idx in range(self.num_rows):\n        html.append('  &lt;tr&gt;')\n        for col_idx in range(self.num_cols):\n            cell_data = grid[row_idx][col_idx]\n            if isinstance(cell_data, TableCell):\n                tag = 'th' if cell_data.is_header else 'td'\n                attrs = []\n                if cell_data.rowspan &gt; 1:\n                    attrs.append(f'rowspan=\"{cell_data.rowspan}\"')\n                if cell_data.colspan &gt; 1:\n                    attrs.append(f'colspan=\"{cell_data.colspan}\"')\n                attr_str = ' ' + ' '.join(attrs) if attrs else ''\n                html.append(f'    &lt;{tag}{attr_str}&gt;{cell_data.text}&lt;/{tag}&gt;')\n            elif cell_data is None:\n                html.append('    &lt;td&gt;&lt;/td&gt;')\n            # Skip 'occupied' cells as they're part of a span\n        html.append('  &lt;/tr&gt;')\n\n    html.append('&lt;/table&gt;')\n    return '\\n'.join(html)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.TableCell","title":"TableCell","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for individual table cell.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>str</code> <p>Cell text content</p> <code>row</code> <code>int</code> <p>Row index (0-based)</p> <code>col</code> <code>int</code> <p>Column index (0-based)</p> <code>rowspan</code> <code>int</code> <p>Number of rows the cell spans</p> <code>colspan</code> <code>int</code> <p>Number of columns the cell spans</p> <code>bbox</code> <code>Optional[List[float]]</code> <p>Bounding box coordinates [x1, y1, x2, y2]</p> <code>confidence</code> <code>Optional[float]</code> <p>Confidence score for cell detection</p> <code>is_header</code> <code>bool</code> <p>Whether the cell is a header cell</p>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.TableCell.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        'text': self.text,\n        'row': self.row,\n        'col': self.col,\n        'rowspan': self.rowspan,\n        'colspan': self.colspan,\n        'bbox': self.bbox,\n        'confidence': self.confidence,\n        'is_header': self.is_header\n    }\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.TableOutput","title":"TableOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for table extraction results.</p> <p>Attributes:</p> Name Type Description <code>tables</code> <code>List[Table]</code> <p>List of extracted tables</p> <code>source_img_size</code> <code>Optional[Tuple[int, int]]</code> <p>Original image dimensions (width, height)</p> <code>processing_time</code> <code>Optional[float]</code> <p>Time taken for table extraction</p> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata from the extraction engine</p>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.TableOutput.get_tables_by_confidence","title":"get_tables_by_confidence","text":"<pre><code>get_tables_by_confidence(min_confidence: float = 0.5) -&gt; List[Table]\n</code></pre> <p>Filter tables by minimum confidence threshold.</p> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def get_tables_by_confidence(self, min_confidence: float = 0.5) -&gt; List[Table]:\n    \"\"\"Filter tables by minimum confidence threshold.\"\"\"\n    return [table for table in self.tables if table.confidence is None or table.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.TableOutput.save_json","title":"save_json","text":"<pre><code>save_json(output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save output to JSON file.</p> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def save_json(self, output_path: Union[str, Path]) -&gt; None:\n    \"\"\"Save output to JSON file.\"\"\"\n    import json\n    with open(output_path, 'w', encoding='utf-8') as f:\n        json.dump(self.to_dict(), f, indent=2, ensure_ascii=False)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.TableOutput.save_tables_as_csv","title":"save_tables_as_csv","text":"<pre><code>save_tables_as_csv(output_dir: Union[str, Path]) -&gt; List[Path]\n</code></pre> <p>Save all tables as separate CSV files.</p> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def save_tables_as_csv(self, output_dir: Union[str, Path]) -&gt; List[Path]:\n    \"\"\"Save all tables as separate CSV files.\"\"\"\n    output_dir = Path(output_dir)\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    saved_files = []\n    for i, table in enumerate(self.tables):\n        filename = f\"table_{table.table_id or i}.csv\"\n        file_path = output_dir / filename\n        with open(file_path, 'w', encoding='utf-8') as f:\n            f.write(table.to_csv())\n        saved_files.append(file_path)\n\n    return saved_files\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.TableOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        'tables': [table.to_dict() for table in self.tables],\n        'source_img_size': self.source_img_size,\n        'processing_time': self.processing_time,\n        'metadata': self.metadata\n    }\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.camelot","title":"omnidocs.tasks.table_extraction.extractors.camelot","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.camelot.CamelotExtractor","title":"CamelotExtractor","text":"<pre><code>CamelotExtractor(device: Optional[str] = None, show_log: bool = False, method: str = 'lattice', pages: str = '1', flavor: str = 'lattice', **kwargs)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>Camelot based table extraction implementation.</p> <p>TODO: Bbox coordinate transformation from PDF to image space is still broken. Current issues: - Coordinate transformation accuracy issues between PDF points and image pixels - Cell bbox estimation doesn't account for actual cell sizes from Camelot - Need better integration with Camelot's internal coordinate data - Grid-based estimation fallback is inaccurate for real table layouts</p> <p>Initialize Camelot Table Extractor.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/camelot.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    method: str = 'lattice',\n    pages: str = '1',\n    flavor: str = 'lattice',\n    **kwargs\n):\n    \"\"\"Initialize Camelot Table Extractor.\"\"\"\n    super().__init__(\n        device=device,\n        show_log=show_log,\n        engine_name='camelot'\n    )\n\n    self._label_mapper = CamelotMapper()\n    self.method = method\n    self.pages = pages\n    self.flavor = flavor\n\n    try:\n        import camelot\n        self.camelot = camelot\n\n    except ImportError as e:\n        logger.error(\"Failed to import Camelot\")\n        raise ImportError(\n            \"Camelot is not available. Please install it with: pip install camelot-py[cv]\"\n        ) from e\n\n    self._load_model()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.camelot.CamelotExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; TableOutput\n</code></pre> <p>Extract tables using Camelot.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/camelot.py</code> <pre><code>@log_execution_time\ndef extract(\n    self,\n    input_path: Union[str, Path, Image.Image],\n    **kwargs\n) -&gt; TableOutput:\n    \"\"\"Extract tables using Camelot.\"\"\"\n    try:\n        # Camelot works with PDF files\n        if isinstance(input_path, (str, Path)):\n            pdf_path = Path(input_path)\n            if pdf_path.suffix.lower() != '.pdf':\n                raise ValueError(\"Camelot only works with PDF files\")\n\n            # Extract tables from PDF\n            tables = self.camelot.read_pdf(\n                str(pdf_path),\n                pages=self.pages,\n                flavor=self.flavor,\n                **kwargs\n            )\n\n            # Get image size (estimate from first page)\n            try:\n                images = self._convert_pdf_to_image(pdf_path)\n                img_size = images[0].size if images else (612, 792)  # Default PDF size\n            except:\n                img_size = (612, 792)  # Default PDF size\n\n        else:\n            raise ValueError(\"Camelot requires PDF file path, not image data\")\n\n        # Convert to standardized format\n        result = self.postprocess_output(tables, img_size)\n\n        if self.show_log:\n            logger.info(f\"Extracted {len(result.tables)} tables using Camelot\")\n\n        return result\n\n    except Exception as e:\n        logger.error(\"Error during Camelot extraction\", exc_info=True)\n        return TableOutput(\n            tables=[],\n            source_img_size=None,\n            processing_time=None,\n            metadata={\"error\": str(e)}\n        )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.camelot.CamelotExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: Any, img_size: Tuple[int, int]) -&gt; TableOutput\n</code></pre> <p>Convert Camelot output to standardized TableOutput format.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/camelot.py</code> <pre><code>def postprocess_output(self, raw_output: Any, img_size: Tuple[int, int]) -&gt; TableOutput:\n    \"\"\"Convert Camelot output to standardized TableOutput format.\"\"\"\n    tables = []\n\n    for i, camelot_table in enumerate(raw_output):\n        # Get table data\n        df = camelot_table.df\n\n        # Convert DataFrame to cells\n        cells = []\n        num_rows, num_cols = df.shape\n\n        for row_idx in range(num_rows):\n            for col_idx in range(num_cols):\n                cell_text = str(df.iloc[row_idx, col_idx]).strip()\n\n                # Create cell with basic info\n                cell = TableCell(\n                    text=cell_text,\n                    row=row_idx,\n                    col=col_idx,\n                    rowspan=1,\n                    colspan=1,\n                    confidence=camelot_table.accuracy / 100.0,  # Convert percentage to decimal\n                    is_header=(row_idx == 0)  # Assume first row is header\n                )\n                cells.append(cell)\n\n        # Get table bounding box if available\n        bbox = None\n        if hasattr(camelot_table, '_bbox'):\n            bbox = list(camelot_table._bbox)\n\n        # Create table object\n        table = Table(\n            cells=cells,\n            num_rows=num_rows,\n            num_cols=num_cols,\n            bbox=bbox,\n            confidence=camelot_table.accuracy / 100.0,\n            table_id=f\"table_{i}\",\n            structure_confidence=camelot_table.accuracy / 100.0\n        )\n\n        tables.append(table)\n\n    return TableOutput(\n        tables=tables,\n        source_img_size=img_size,\n        metadata={\n            'engine': 'camelot',\n            'method': self.method,\n            'flavor': self.flavor\n        }\n    )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.camelot.CamelotExtractor.predict","title":"predict","text":"<pre><code>predict(pdf_path: Union[str, Path], **kwargs)\n</code></pre> <p>Predict method for compatibility with original interface.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/camelot.py</code> <pre><code>def predict(self, pdf_path: Union[str, Path], **kwargs):\n    \"\"\"Predict method for compatibility with original interface.\"\"\"\n    try:\n        result = self.extract(pdf_path, **kwargs)\n\n        # Convert to original format\n        table_res = []\n        for table in result.tables:\n            table_data = {\n                \"table_id\": table.table_id,\n                \"bbox\": table.bbox,\n                \"confidence\": table.confidence,\n                \"cells\": [cell.to_dict() for cell in table.cells],\n                \"num_rows\": table.num_rows,\n                \"num_cols\": table.num_cols\n            }\n            table_res.append(table_data)\n\n        return table_res\n\n    except Exception as e:\n        logger.error(\"Error during Camelot prediction\", exc_info=True)\n        return []\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.camelot.CamelotMapper","title":"CamelotMapper","text":"<pre><code>CamelotMapper()\n</code></pre> <p>               Bases: <code>BaseTableMapper</code></p> <p>Label mapper for Camelot table extraction output.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/camelot.py</code> <pre><code>def __init__(self):\n    super().__init__('camelot')\n    self._setup_mapping()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.pdfplumber","title":"omnidocs.tasks.table_extraction.extractors.pdfplumber","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.pdfplumber.PDFPlumberExtractor","title":"PDFPlumberExtractor","text":"<pre><code>PDFPlumberExtractor(device: Optional[str] = None, show_log: bool = False, table_settings: Optional[Dict] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>PDFPlumber based table extraction implementation.</p> <p>Initialize PDFPlumber Table Extractor.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/pdfplumber.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    table_settings: Optional[Dict] = None,\n    **kwargs\n):\n    \"\"\"Initialize PDFPlumber Table Extractor.\"\"\"\n    super().__init__(\n        device=device,\n        show_log=show_log,\n        engine_name='pdfplumber'\n    )\n\n    self._label_mapper = PDFPlumberMapper()\n    self.table_settings = table_settings or self._label_mapper._table_settings\n\n    try:\n        import pdfplumber\n        self.pdfplumber = pdfplumber\n\n    except ImportError as e:\n        logger.error(\"Failed to import PDFPlumber\")\n        raise ImportError(\n            \"PDFPlumber is not available. Please install it with: pip install pdfplumber\"\n        ) from e\n\n    self._load_model()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.pdfplumber.PDFPlumberExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; TableOutput\n</code></pre> <p>Extract tables using PDFPlumber.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/pdfplumber.py</code> <pre><code>@log_execution_time\ndef extract(\n    self,\n    input_path: Union[str, Path, Image.Image],\n    **kwargs\n) -&gt; TableOutput:\n    \"\"\"Extract tables using PDFPlumber.\"\"\"\n    try:\n        # PDFPlumber works with PDF files\n        if isinstance(input_path, (str, Path)):\n            pdf_path = Path(input_path)\n            if pdf_path.suffix.lower() != '.pdf':\n                raise ValueError(\"PDFPlumber only works with PDF files\")\n\n            all_tables = []\n\n            # Open PDF and extract tables from all pages\n            with self.pdfplumber.open(str(pdf_path)) as pdf:\n                for page in pdf.pages:\n                    page_tables = self._extract_tables_from_page(page)\n                    all_tables.extend(page_tables)\n\n            # Get image size and PDF size for coordinate transformation\n            try:\n                # Get actual PDF page size first\n                import fitz\n                doc = fitz.open(str(pdf_path))\n                page = doc[0]\n                pdf_size = (page.rect.width, page.rect.height)\n                doc.close()\n\n                # Convert PDF to image to get actual image size\n                images = self._convert_pdf_to_image(pdf_path)\n                img_size = images[0].size if images else pdf_size\n            except:\n                pdf_size = (612, 792)  # Default PDF size\n                img_size = (612, 792)  # Default image size\n\n        else:\n            raise ValueError(\"PDFPlumber requires PDF file path, not image data\")\n\n        # Convert to standardized format\n        result = self.postprocess_output(all_tables, img_size, pdf_size)\n\n        if self.show_log:\n            logger.info(f\"Extracted {len(result.tables)} tables using PDFPlumber\")\n\n        return result\n\n    except Exception as e:\n        logger.error(\"Error during PDFPlumber extraction\", exc_info=True)\n        return TableOutput(\n            tables=[],\n            source_img_size=None,\n            processing_time=None,\n            metadata={\"error\": str(e)}\n        )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.pdfplumber.PDFPlumberExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: List[Dict], img_size: Tuple[int, int], pdf_size: Tuple[int, int] = None) -&gt; TableOutput\n</code></pre> <p>Convert PDFPlumber output to standardized TableOutput format.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/pdfplumber.py</code> <pre><code>def postprocess_output(\n    self,\n    raw_output: List[Dict],\n    img_size: Tuple[int, int],\n    pdf_size: Tuple[int, int] = None,\n) -&gt; TableOutput:\n    \"\"\"Convert PDFPlumber output to standardized TableOutput format.\"\"\"\n    tables: List[Table] = []\n\n    for i, table_data in enumerate(raw_output):\n        table_bbox = table_data.get(\"bbox\")\n        if table_bbox is None:\n            table_bbox = [0, 0, img_size[0], img_size[1]]\n\n        if pdf_size:\n            table_bbox_img = self._transform_pdf_to_image_coords(\n                table_bbox, pdf_size, img_size\n            )\n        else:\n            table_bbox_img = table_bbox\n\n        # Get max row/col indexes to know dimensions\n        max_row = max(c[\"row\"] for c in table_data[\"cells\"])\n        max_col = max(c[\"col\"] for c in table_data[\"cells\"])\n        num_rows = max_row + 1\n        num_cols = max_col + 1\n\n        # Pre-compute equally spaced cell rectangles inside the table bbox\n        x0, y0, x1, y1 = table_bbox_img\n        cell_w = (x1 - x0) / num_cols\n        cell_h = (y1 - y0) / num_rows\n\n        cells: List[TableCell] = []\n        for c in table_data[\"cells\"]:\n            r, cidx = c[\"row\"], c[\"col\"]\n\n            # exact rectangle in image space\n            cx0 = x0 + cidx * cell_w\n            cy0 = y0 + r * cell_h\n            cx1 = cx0 + cell_w\n            cy1 = cy0 + cell_h\n            cell_bbox_img = [cx0, cy0, cx1, cy1]\n\n            cells.append(\n                TableCell(\n                    text=c[\"text\"].strip(),\n                    row=r,\n                    col=cidx,\n                    rowspan=c.get(\"rowspan\", 1),\n                    colspan=c.get(\"colspan\", 1),\n                    bbox=cell_bbox_img,\n                    confidence=0.9,\n                    is_header=(r == 0),\n                )\n            )\n\n        tables.append(\n            Table(\n                cells=cells,\n                num_rows=num_rows,\n                num_cols=num_cols,\n                bbox=table_bbox_img,\n                confidence=0.9,\n                table_id=f\"table_{i}\",\n                structure_confidence=0.9,\n            )\n        )\n\n    return TableOutput(\n        tables=tables,\n        source_img_size=img_size,\n        metadata={\"engine\": \"pdfplumber\", \"table_settings\": self.table_settings},\n    )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.pdfplumber.PDFPlumberExtractor.predict","title":"predict","text":"<pre><code>predict(pdf_path: Union[str, Path], **kwargs)\n</code></pre> <p>Predict method for compatibility with original interface.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/pdfplumber.py</code> <pre><code>def predict(self, pdf_path: Union[str, Path], **kwargs):\n    \"\"\"Predict method for compatibility with original interface.\"\"\"\n    try:\n        result = self.extract(pdf_path, **kwargs)\n\n        # Convert to original format\n        table_res = []\n        for table in result.tables:\n            table_data = {\n                \"table_id\": table.table_id,\n                \"bbox\": table.bbox,\n                \"confidence\": table.confidence,\n                \"cells\": [cell.to_dict() for cell in table.cells],\n                \"num_rows\": table.num_rows,\n                \"num_cols\": table.num_cols\n            }\n            table_res.append(table_data)\n\n        return table_res\n\n    except Exception as e:\n        logger.error(\"Error during PDFPlumber prediction\", exc_info=True)\n        return []\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.pdfplumber.PDFPlumberMapper","title":"PDFPlumberMapper","text":"<pre><code>PDFPlumberMapper()\n</code></pre> <p>               Bases: <code>BaseTableMapper</code></p> <p>Label mapper for PDFPlumber table extraction output.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/pdfplumber.py</code> <pre><code>def __init__(self):\n    super().__init__('pdfplumber')\n    self._setup_mapping()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.surya_table","title":"omnidocs.tasks.table_extraction.extractors.surya_table","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.surya_table.SuryaTableExtractor","title":"SuryaTableExtractor","text":"<pre><code>SuryaTableExtractor(device: Optional[str] = None, show_log: bool = False, model_path: Optional[Union[str, Path]] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>Surya-based table extraction implementation.</p> <p>Initialize Surya Table Extractor.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/surya_table.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    model_path: Optional[Union[str, Path]] = None,\n    **kwargs\n):\n    \"\"\"Initialize Surya Table Extractor.\"\"\"\n    super().__init__(device=device, show_log=show_log, engine_name='surya')\n\n    self._label_mapper = SuryaTableMapper()\n\n    if self.show_log:\n        logger.info(\"Initializing SuryaTableExtractor\")\n\n    # Set device if specified, otherwise use default from parent\n    if device:\n        self.device = device\n\n    if self.show_log:\n        logger.info(f\"Using device: {self.device}\")\n\n    # Set default paths\n    if model_path is None:\n        model_path = _MODELS_DIR / \"surya_table\"\n\n    self.model_path = Path(model_path)\n\n    # Check dependencies and load model\n    self._check_dependencies()\n    self._load_model()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.surya_table.SuryaTableExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; TableOutput\n</code></pre> <p>Extract tables using Surya.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/surya_table.py</code> <pre><code>@log_execution_time\ndef extract(\n    self,\n    input_path: Union[str, Path, Image.Image],\n    **kwargs\n) -&gt; TableOutput:\n    \"\"\"Extract tables using Surya.\"\"\"\n    try:\n        # Preprocess input\n        images = self.preprocess_input(input_path)\n        image = images[0]\n        img_size = image.size\n\n        # Convert PIL to RGB if needed\n        if isinstance(image, Image.Image):\n            img_rgb = image.convert(\"RGB\")\n        else:\n            img_rgb = Image.fromarray(image).convert(\"RGB\")\n\n        # Step 1: Use layout detection to find table regions\n        layout_predictions = self.layout_predictor([img_rgb])\n\n        tables_data = []\n\n        if layout_predictions and len(layout_predictions) &gt; 0:\n            layout_pred = layout_predictions[0]\n\n            # Find table regions from layout\n            table_regions = []\n            for bbox_obj in layout_pred.bboxes:\n                if hasattr(bbox_obj, 'label') and 'table' in bbox_obj.label.lower():\n                    table_regions.append({\n                        'bbox': bbox_obj.bbox,\n                        'confidence': getattr(bbox_obj, 'confidence', 1.0)\n                    })\n\n            # Step 2: For each table region, extract text and structure\n            for table_region in table_regions:\n                bbox = table_region['bbox']\n\n                # Crop table region\n                table_img = img_rgb.crop(bbox)\n\n                # Step 3: Run OCR on table region\n                try:\n                    from surya.common.surya.schema import TaskNames\n\n                    # Use recognition predictor for table text extraction\n                    predictions = self.rec_predictor(\n                        [table_img],\n                        task_names=[TaskNames.ocr_with_boxes],\n                        det_predictor=self.det_predictor,\n                        math_mode=False\n                    )\n\n                    # Process OCR results into table structure\n                    if predictions and len(predictions) &gt; 0:\n                        prediction = predictions[0]\n\n                        # Extract text lines and organize into table structure\n                        cells = self._organize_text_into_table(prediction.text_lines, bbox)\n\n                        table_data = {\n                            'bbox': bbox,\n                            'confidence': table_region['confidence'],\n                            'cells': cells,\n                            'num_rows': len(set(c['row'] for c in cells)) if cells else 0,\n                            'num_cols': len(set(c['col'] for c in cells)) if cells else 0\n                        }\n                        tables_data.append(table_data)\n\n                except Exception as e:\n                    if self.show_log:\n                        logger.warning(f\"Error processing table region: {e}\")\n                    continue\n\n        # Convert to standardized format\n        result = self.postprocess_output({'tables': tables_data}, img_size)\n\n        if self.show_log:\n            logger.info(f\"Extracted {len(result.tables)} tables using Surya\")\n\n        return result\n\n    except Exception as e:\n        if self.show_log:\n            logger.error(\"Error during Surya table extraction\", exc_info=True)\n        raise\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.surya_table.SuryaTableExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: Any, img_size: Tuple[int, int]) -&gt; TableOutput\n</code></pre> <p>Convert Surya output to standardized TableOutput format.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/surya_table.py</code> <pre><code>def postprocess_output(self, raw_output: Any, img_size: Tuple[int, int]) -&gt; TableOutput:\n    \"\"\"Convert Surya output to standardized TableOutput format.\"\"\"\n    tables = []\n\n    if 'tables' in raw_output:\n        for table_idx, table_data in enumerate(raw_output['tables']):\n            # Extract table cells with proper mapping\n            cells = []\n\n            # Handle different possible structures from Surya\n            if 'cells' in table_data:\n                # Direct cell data\n                for cell_data in table_data['cells']:\n                    cell = self._create_table_cell(cell_data, table_idx)\n                    if cell:\n                        cells.append(cell)\n            elif 'text_lines' in table_data:\n                # Convert text lines to cells\n                cells = self._text_lines_to_cells(table_data['text_lines'], table_data.get('bbox', [0, 0, img_size[0], img_size[1]]))\n\n            if cells:\n                # Calculate table dimensions\n                num_rows = max(c.row for c in cells) + 1 if cells else 0\n                num_cols = max(c.col for c in cells) + 1 if cells else 0\n\n                # Create table\n                table = Table(\n                    cells=cells,\n                    bbox=table_data.get('bbox', [0, 0, img_size[0], img_size[1]]),\n                    confidence=table_data.get('confidence', 1.0),\n                    num_rows=num_rows,\n                    num_cols=num_cols,\n                    table_id=f\"surya_table_{table_idx}\"\n                )\n                tables.append(table)\n\n    return TableOutput(\n        tables=tables,\n        source_img_size=img_size,\n        metadata={'engine': 'surya', 'raw_output': raw_output}\n    )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.surya_table.SuryaTableMapper","title":"SuryaTableMapper","text":"<pre><code>SuryaTableMapper()\n</code></pre> <p>               Bases: <code>BaseTableMapper</code></p> <p>Label mapper for Surya table model output.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/surya_table.py</code> <pre><code>def __init__(self):\n    super().__init__('surya')\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.tabula","title":"omnidocs.tasks.table_extraction.extractors.tabula","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.tabula.TabulaExtractor","title":"TabulaExtractor","text":"<pre><code>TabulaExtractor(device: Optional[str] = None, show_log: bool = False, method: str = 'lattice', pages: Optional[Union[str, List[int]]] = None, multiple_tables: bool = True, guess: bool = True, area: Optional[List[float]] = None, columns: Optional[List[float]] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>Tabula based table extraction implementation.</p> <p>Initialize Tabula Table Extractor.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/tabula.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    method: str = 'lattice',\n    pages: Optional[Union[str, List[int]]] = None,\n    multiple_tables: bool = True,\n    guess: bool = True,\n    area: Optional[List[float]] = None,\n    columns: Optional[List[float]] = None,\n    **kwargs\n):\n    \"\"\"Initialize Tabula Table Extractor.\"\"\"\n    super().__init__(\n        device=device,\n        show_log=show_log,\n        engine_name='tabula'\n    )\n\n    self._label_mapper = TabulaMapper()\n    self.method = method\n    self.pages = pages or 'all'\n    self.multiple_tables = multiple_tables\n    self.guess = guess\n    self.area = area\n    self.columns = columns\n\n    try:\n        import tabula\n        self.tabula = tabula\n\n    except ImportError as e:\n        logger.error(\"Failed to import Tabula\")\n        raise ImportError(\n            \"Tabula is not available. Please install it with: pip install tabula-py\"\n        ) from e\n\n    self._load_model()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.tabula.TabulaExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; TableOutput\n</code></pre> <p>Extract tables using Tabula.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/tabula.py</code> <pre><code>@log_execution_time\ndef extract(\n    self,\n    input_path: Union[str, Path, Image.Image],\n    **kwargs\n) -&gt; TableOutput:\n    \"\"\"Extract tables using Tabula.\"\"\"\n    try:\n        # Tabula works with PDF files\n        if isinstance(input_path, (str, Path)):\n            pdf_path = Path(input_path)\n            if pdf_path.suffix.lower() != '.pdf':\n                raise ValueError(\"Tabula only works with PDF files\")\n\n            # Prepare extraction options\n            options = self._prepare_tabula_options(**kwargs)\n\n            # Extract tables from PDF\n            try:\n                tables_list = self.tabula.read_pdf(str(pdf_path), **options)\n\n                # Ensure we have a list of DataFrames\n                if not isinstance(tables_list, list):\n                    tables_list = [tables_list]\n\n            except Exception as e:\n                if self.show_log:\n                    logger.error(f\"Tabula extraction failed: {str(e)}\")\n                tables_list = []\n\n            # Get image size and PDF size for coordinate transformation\n            try:\n                # Get actual PDF page size first\n                import fitz\n                doc = fitz.open(str(pdf_path))\n                page = doc[0]\n                pdf_size = (page.rect.width, page.rect.height)\n                doc.close()\n\n                # Convert PDF to image to get actual image size\n                images = self._convert_pdf_to_image(pdf_path)\n                img_size = images[0].size if images else pdf_size\n            except:\n                pdf_size = (612, 792)  # Default PDF size\n                img_size = (612, 792)  # Default image size\n\n        else:\n            raise ValueError(\"Tabula requires PDF file path, not image data\")\n\n        # Convert to standardized format\n        result = self.postprocess_output(tables_list, img_size, pdf_size)\n\n        if self.show_log:\n            logger.info(f\"Extracted {len(result.tables)} tables using Tabula\")\n\n        return result\n\n    except Exception as e:\n        logger.error(\"Error during Tabula extraction\", exc_info=True)\n        return TableOutput(\n            tables=[],\n            source_img_size=None,\n            processing_time=None,\n            metadata={\"error\": str(e)}\n        )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.tabula.TabulaExtractor.extract_with_area","title":"extract_with_area","text":"<pre><code>extract_with_area(input_path: Union[str, Path], area: List[float], **kwargs) -&gt; TableOutput\n</code></pre> <p>Extract tables from specific area of PDF.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/tabula.py</code> <pre><code>def extract_with_area(\n    self,\n    input_path: Union[str, Path],\n    area: List[float],\n    **kwargs\n) -&gt; TableOutput:\n    \"\"\"Extract tables from specific area of PDF.\"\"\"\n    original_area = self.area\n    self.area = area\n\n    try:\n        result = self.extract(input_path, **kwargs)\n        return result\n    finally:\n        self.area = original_area\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.tabula.TabulaExtractor.extract_with_columns","title":"extract_with_columns","text":"<pre><code>extract_with_columns(input_path: Union[str, Path], columns: List[float], **kwargs) -&gt; TableOutput\n</code></pre> <p>Extract tables with specified column positions.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/tabula.py</code> <pre><code>def extract_with_columns(\n    self,\n    input_path: Union[str, Path],\n    columns: List[float],\n    **kwargs\n) -&gt; TableOutput:\n    \"\"\"Extract tables with specified column positions.\"\"\"\n    original_columns = self.columns\n    self.columns = columns\n\n    try:\n        result = self.extract(input_path, **kwargs)\n        return result\n    finally:\n        self.columns = original_columns\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.tabula.TabulaExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: List, img_size: Tuple[int, int], pdf_size: Tuple[int, int] = None) -&gt; TableOutput\n</code></pre> <p>Convert Tabula output to standardized TableOutput format.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/tabula.py</code> <pre><code>def postprocess_output(self, raw_output: List, img_size: Tuple[int, int], pdf_size: Tuple[int, int] = None) -&gt; TableOutput:\n    \"\"\"Convert Tabula output to standardized TableOutput format.\"\"\"\n    tables = []\n\n    for i, df in enumerate(raw_output):\n        if df.empty:\n            continue\n\n        # Get table dimensions\n        num_rows, num_cols = df.shape\n\n        # Estimate table bbox\n        bbox = self._estimate_table_bbox(df, img_size)\n\n        # Transform PDF coordinates to image coordinates if needed\n        if pdf_size and bbox:\n            bbox = self._transform_pdf_to_image_coords(bbox, pdf_size, img_size)\n\n        # Convert DataFrame to cells with estimated bboxes\n        cells = self._dataframe_to_cells(df, i, bbox)\n\n        # Create table object\n        table = Table(\n            cells=cells,\n            num_rows=num_rows,\n            num_cols=num_cols,\n            bbox=bbox,\n            confidence=None,  # Tabula doesn't provide confidence\n            table_id=f\"table_{i}\",\n            structure_confidence=None\n        )\n\n        tables.append(table)\n\n    return TableOutput(\n        tables=tables,\n        source_img_size=img_size,\n        metadata={\n            'engine': 'tabula',\n            'method': self.method,\n            'pages': self.pages,\n            'multiple_tables': self.multiple_tables,\n            'guess': self.guess\n        }\n    )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.tabula.TabulaExtractor.predict","title":"predict","text":"<pre><code>predict(pdf_path: Union[str, Path], **kwargs)\n</code></pre> <p>Predict method for compatibility with original interface.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/tabula.py</code> <pre><code>def predict(self, pdf_path: Union[str, Path], **kwargs):\n    \"\"\"Predict method for compatibility with original interface.\"\"\"\n    try:\n        result = self.extract(pdf_path, **kwargs)\n\n        # Convert to original format\n        table_res = []\n        for table in result.tables:\n            table_data = {\n                \"table_id\": table.table_id,\n                \"bbox\": table.bbox,\n                \"confidence\": table.confidence,\n                \"cells\": [cell.to_dict() for cell in table.cells],\n                \"num_rows\": table.num_rows,\n                \"num_cols\": table.num_cols\n            }\n            table_res.append(table_data)\n\n        return table_res\n\n    except Exception as e:\n        logger.error(\"Error during Tabula prediction\", exc_info=True)\n        return []\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.tabula.TabulaMapper","title":"TabulaMapper","text":"<pre><code>TabulaMapper()\n</code></pre> <p>               Bases: <code>BaseTableMapper</code></p> <p>Label mapper for Tabula table extraction output.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/tabula.py</code> <pre><code>def __init__(self):\n    super().__init__('tabula')\n    self._setup_mapping()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.table_transformer","title":"omnidocs.tasks.table_extraction.extractors.table_transformer","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.table_transformer.TableTransformerExtractor","title":"TableTransformerExtractor","text":"<pre><code>TableTransformerExtractor(device: Optional[str] = None, show_log: bool = False, detection_model_path: Optional[str] = None, structure_model_path: Optional[str] = None, detection_threshold: float = 0.7, structure_threshold: float = 0.7, **kwargs)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>Table Transformer based table extraction implementation.</p> <p>Initialize Table Transformer Extractor.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/table_transformer.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    detection_model_path: Optional[str] = None,\n    structure_model_path: Optional[str] = None,\n    detection_threshold: float = 0.7,\n    structure_threshold: float = 0.7,\n    **kwargs\n):\n    \"\"\"Initialize Table Transformer Extractor.\"\"\"\n    super().__init__(\n        device=device,\n        show_log=show_log,\n        engine_name='table_transformer'\n    )\n\n    self._label_mapper = TableTransformerMapper()\n\n    # Set default paths if not provided\n    self.detection_model_path = Path(detection_model_path) if detection_model_path else \\\n        Path(self._label_mapper._model_configs['detection']['local_path'])\n    self.structure_model_path = Path(structure_model_path) if structure_model_path else \\\n        Path(self._label_mapper._model_configs['structure']['local_path'])\n\n    self.detection_threshold = detection_threshold\n    self.structure_threshold = structure_threshold\n\n    # Check dependencies\n    self._check_dependencies()\n\n    # Download model if needed (sets up model sources)\n    self._download_model()\n\n    # Load models\n    self._load_model()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.table_transformer.TableTransformerExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; TableOutput\n</code></pre> <p>Extract tables using Table Transformer.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/table_transformer.py</code> <pre><code>@log_execution_time\ndef extract(\n    self,\n    input_path: Union[str, Path, Image.Image],\n    **kwargs\n) -&gt; TableOutput:\n    \"\"\"Extract tables using Table Transformer.\"\"\"\n    try:\n        # Preprocess input\n        images = self.preprocess_input(input_path)\n        image = images[0]\n        img_size = image.size\n\n        # Detect tables\n        detected_tables = self._detect_tables(image)\n\n        if not detected_tables:\n            if self.show_log:\n                logger.info(\"No tables detected in the image\")\n            return TableOutput(\n                tables=[],\n                source_img_size=img_size,\n                metadata={'engine': 'table_transformer', 'message': 'No tables detected'}\n            )\n\n        # Analyze structure for each detected table\n        table_results = []\n        for table_detection in detected_tables:\n            structure_data = self._analyze_table_structure(image, table_detection['bbox'])\n            cells = self._create_table_cells(structure_data)\n\n            table_results.append({\n                'bbox': table_detection['bbox'],\n                'confidence': table_detection['confidence'],\n                'cells': cells,\n                'structure_confidence': np.mean([e['confidence'] for e in structure_data['elements']]) if structure_data['elements'] else 0.0\n            })\n\n        # Convert to standardized format\n        result = self._create_table_output(table_results, img_size)\n\n        if self.show_log:\n            logger.info(f\"Extracted {len(result.tables)} tables using Table Transformer\")\n\n        return result\n\n    except Exception as e:\n        logger.error(\"Error during Table Transformer extraction\", exc_info=True)\n        return TableOutput(\n            tables=[],\n            source_img_size=None,\n            processing_time=None,\n            metadata={\"error\": str(e)}\n        )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.table_transformer.TableTransformerExtractor.predict","title":"predict","text":"<pre><code>predict(input_path: Union[str, Path, Image], **kwargs)\n</code></pre> <p>Predict method for compatibility with original interface.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/table_transformer.py</code> <pre><code>def predict(self, input_path: Union[str, Path, Image.Image], **kwargs):\n    \"\"\"Predict method for compatibility with original interface.\"\"\"\n    try:\n        result = self.extract(input_path, **kwargs)\n\n        # Convert to original format\n        return [\n            {\n                \"table_id\": table.table_id,\n                \"bbox\": table.bbox,\n                \"confidence\": table.confidence,\n                \"cells\": [cell.to_dict() for cell in table.cells],\n                \"num_rows\": table.num_rows,\n                \"num_cols\": table.num_cols\n            }\n            for table in result.tables\n        ]\n\n    except Exception as e:\n        logger.error(\"Error during Table Transformer prediction\", exc_info=True)\n        return []\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.table_transformer.TableTransformerMapper","title":"TableTransformerMapper","text":"<pre><code>TableTransformerMapper()\n</code></pre> <p>               Bases: <code>BaseTableMapper</code></p> <p>Label mapper for Table Transformer model output.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/table_transformer.py</code> <pre><code>def __init__(self):\n    super().__init__('table_transformer')\n    self._setup_mapping()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.tableformer","title":"omnidocs.tasks.table_extraction.extractors.tableformer","text":""},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.tableformer.TableFormerExtractor","title":"TableFormerExtractor","text":"<pre><code>TableFormerExtractor(device: Optional[str] = None, show_log: bool = False, model_path: Optional[str] = None, model_type: str = 'structure', confidence_threshold: float = 0.7, max_size: int = 1000, **kwargs)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>TableFormer based table extraction implementation.</p> <p>Initialize TableFormer Extractor.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/tableformer.py</code> <pre><code>def __init__(\n    self,\n    device: Optional[str] = None,\n    show_log: bool = False,\n    model_path: Optional[str] = None,\n    model_type: str = 'structure',\n    confidence_threshold: float = 0.7,\n    max_size: int = 1000,\n    **kwargs\n):\n    \"\"\"Initialize TableFormer Extractor.\"\"\"\n    super().__init__(\n        device=device,\n        show_log=show_log,\n        engine_name='tableformer'\n    )\n\n    self._label_mapper = TableFormerMapper()\n    self.model_type = model_type\n    self.confidence_threshold = confidence_threshold\n    self.max_size = max_size\n\n    # Set default model paths\n    if model_path is None:\n        model_path = f\"omnidocs/models/tableformer_{model_type}\"\n\n    self.model_path = Path(model_path)\n\n    # Check dependencies\n    self._check_dependencies()\n\n    # Try to load from local path first, fallback to HuggingFace\n    if self.model_path.exists() and any(self.model_path.iterdir()):\n        if self.show_log:\n            logger.info(f\"Found local {self.model_type} model at: {self.model_path}\")\n        self.model_name_or_path = str(self.model_path)\n    else:\n        # Get HuggingFace model name from config\n        hf_model_name = self._label_mapper._model_configs[self.model_type]['model_name']\n        if self.show_log:\n            logger.info(f\"Local {self.model_type} model not found, will download from HuggingFace: {hf_model_name}\")\n\n        # Download model if needed\n        if not self.model_path.exists():\n            self._download_model()\n\n        self.model_name_or_path = hf_model_name\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.tableformer.TableFormerExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; TableOutput\n</code></pre> <p>Extract tables using TableFormer.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/tableformer.py</code> <pre><code>@log_execution_time\ndef extract(\n    self,\n    input_path: Union[str, Path, Image.Image],\n    **kwargs\n) -&gt; TableOutput:\n    \"\"\"Extract tables using TableFormer.\"\"\"\n    try:\n        # Preprocess input\n        images = self.preprocess_input(input_path)\n        image = images[0]\n        img_size = image.size\n\n        # Detect table structure\n        detections = self._detect_table_structure(image)\n\n        if not detections:\n            if self.show_log:\n                logger.info(\"No table structure detected in the image\")\n            return TableOutput(\n                tables=[],\n                source_img_size=img_size,\n                metadata={'engine': 'tableformer', 'message': 'No table structure detected'}\n            )\n\n        # Convert to standardized format\n        result = self.postprocess_output({'detections': detections}, img_size)\n\n        if self.show_log:\n            logger.info(f\"Extracted {len(result.tables)} tables using TableFormer\")\n\n        return result\n\n    except Exception as e:\n        logger.error(\"Error during TableFormer extraction\", exc_info=True)\n        return TableOutput(\n            tables=[],\n            source_img_size=None,\n            processing_time=None,\n            metadata={\"error\": str(e)}\n        )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.tableformer.TableFormerExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: Dict, img_size: Tuple[int, int]) -&gt; TableOutput\n</code></pre> <p>Convert TableFormer output to standardized TableOutput format.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/tableformer.py</code> <pre><code>def postprocess_output(self, raw_output: Dict, img_size: Tuple[int, int]) -&gt; TableOutput:\n    \"\"\"Convert TableFormer output to standardized TableOutput format.\"\"\"\n    tables = []\n\n    # Extract table from detections\n    detections = raw_output.get('detections', [])\n    if detections:\n        table = self._create_table_from_detections(detections, img_size)\n        tables.append(table)\n\n    return TableOutput(\n        tables=tables,\n        source_img_size=img_size,\n        metadata={\n            'engine': 'tableformer',\n            'model_name': self.model_name_or_path,\n            'confidence_threshold': self.confidence_threshold,\n            'max_size': self.max_size\n        }\n    )\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.tableformer.TableFormerExtractor.predict","title":"predict","text":"<pre><code>predict(input_path: Union[str, Path, Image], **kwargs)\n</code></pre> <p>Predict method for compatibility with original interface.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/tableformer.py</code> <pre><code>def predict(self, input_path: Union[str, Path, Image.Image], **kwargs):\n    \"\"\"Predict method for compatibility with original interface.\"\"\"\n    try:\n        result = self.extract(input_path, **kwargs)\n\n        # Convert to original format\n        table_res = []\n        for table in result.tables:\n            table_data = {\n                \"table_id\": table.table_id,\n                \"bbox\": table.bbox,\n                \"confidence\": table.confidence,\n                \"cells\": [cell.to_dict() for cell in table.cells],\n                \"num_rows\": table.num_rows,\n                \"num_cols\": table.num_cols\n            }\n            table_res.append(table_data)\n\n        return table_res\n\n    except Exception as e:\n        logger.error(\"Error during TableFormer prediction\", exc_info=True)\n        return []\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.tasks.table_extraction.extractors.tableformer.TableFormerMapper","title":"TableFormerMapper","text":"<pre><code>TableFormerMapper()\n</code></pre> <p>               Bases: <code>BaseTableMapper</code></p> <p>Label mapper for TableFormer model output.</p> Source code in <code>omnidocs/tasks/table_extraction/extractors/tableformer.py</code> <pre><code>def __init__(self):\n    super().__init__('tableformer')\n    self._setup_mapping()\n</code></pre>"},{"location":"api_reference/python_api.html#utilities-helpers","title":"\ud83d\udee0\ufe0f Utilities &amp; Helpers","text":"<p>Common utility functions, data structures, and helpers used throughout OmniDocs.</p>"},{"location":"api_reference/python_api.html#omnidocs.utils","title":"omnidocs.utils","text":"<p>Utilities module for OmniDocs.</p> <p>This module provides common utilities used across different tasks and components.</p>"},{"location":"api_reference/python_api.html#omnidocs.utils.GlobalLanguageMapper","title":"GlobalLanguageMapper","text":"<pre><code>GlobalLanguageMapper()\n</code></pre> <p>Global language mapper that handles different OCR engine formats.</p> Source code in <code>omnidocs/utils/language.py</code> <pre><code>def __init__(self):\n    self._engine_mappings: Dict[str, Dict[str, str]] = {}\n    self._setup_default_mappings()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.utils.GlobalLanguageMapper.from_standard","title":"from_standard","text":"<pre><code>from_standard(engine_name: str, standard_code: str) -&gt; str\n</code></pre> <p>Convert standard language code to engine-specific format.</p> <p>Parameters:</p> Name Type Description Default <code>engine_name</code> <code>str</code> <p>Name of the OCR engine</p> required <code>standard_code</code> <code>str</code> <p>Standard ISO 639-1 language code</p> required <p>Returns:</p> Type Description <code>str</code> <p>Engine-specific language code</p> Source code in <code>omnidocs/utils/language.py</code> <pre><code>def from_standard(self, engine_name: str, standard_code: str) -&gt; str:\n    \"\"\"Convert standard language code to engine-specific format.\n\n    Args:\n        engine_name: Name of the OCR engine\n        standard_code: Standard ISO 639-1 language code\n\n    Returns:\n        Engine-specific language code\n    \"\"\"\n    if engine_name not in self._engine_mappings:\n        return standard_code\n\n    mapping = self._engine_mappings[engine_name]\n    reverse_mapping = {v: k for k, v in mapping.items()}\n    return reverse_mapping.get(standard_code.lower(), standard_code)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.utils.GlobalLanguageMapper.get_engine_codes","title":"get_engine_codes","text":"<pre><code>get_engine_codes(engine_name: str) -&gt; List[str]\n</code></pre> <p>Get list of engine-specific language codes.</p> <p>Parameters:</p> Name Type Description Default <code>engine_name</code> <code>str</code> <p>Name of the OCR engine</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of engine-specific language codes</p> Source code in <code>omnidocs/utils/language.py</code> <pre><code>def get_engine_codes(self, engine_name: str) -&gt; List[str]:\n    \"\"\"Get list of engine-specific language codes.\n\n    Args:\n        engine_name: Name of the OCR engine\n\n    Returns:\n        List of engine-specific language codes\n    \"\"\"\n    if engine_name not in self._engine_mappings:\n        return []\n\n    return list(self._engine_mappings[engine_name].keys())\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.utils.GlobalLanguageMapper.get_supported_engines","title":"get_supported_engines","text":"<pre><code>get_supported_engines() -&gt; List[str]\n</code></pre> <p>Get list of supported OCR engines.</p> Source code in <code>omnidocs/utils/language.py</code> <pre><code>def get_supported_engines(self) -&gt; List[str]:\n    \"\"\"Get list of supported OCR engines.\"\"\"\n    return list(self._engine_mappings.keys())\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.utils.GlobalLanguageMapper.get_supported_languages","title":"get_supported_languages","text":"<pre><code>get_supported_languages(engine_name: str) -&gt; List[str]\n</code></pre> <p>Get list of supported languages for a specific engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine_name</code> <code>str</code> <p>Name of the OCR engine</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of standard language codes supported by the engine</p> Source code in <code>omnidocs/utils/language.py</code> <pre><code>def get_supported_languages(self, engine_name: str) -&gt; List[str]:\n    \"\"\"Get list of supported languages for a specific engine.\n\n    Args:\n        engine_name: Name of the OCR engine\n\n    Returns:\n        List of standard language codes supported by the engine\n    \"\"\"\n    if engine_name not in self._engine_mappings:\n        return []\n\n    return list(self._engine_mappings[engine_name].values())\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.utils.GlobalLanguageMapper.register_engine_mapping","title":"register_engine_mapping","text":"<pre><code>register_engine_mapping(engine_name: str, mapping: Dict[str, str]) -&gt; None\n</code></pre> <p>Register a new engine's language mapping.</p> <p>Parameters:</p> Name Type Description Default <code>engine_name</code> <code>str</code> <p>Name of the OCR engine</p> required <code>mapping</code> <code>Dict[str, str]</code> <p>Dictionary mapping engine codes to standard codes</p> required Source code in <code>omnidocs/utils/language.py</code> <pre><code>def register_engine_mapping(self, engine_name: str, mapping: Dict[str, str]) -&gt; None:\n    \"\"\"Register a new engine's language mapping.\n\n    Args:\n        engine_name: Name of the OCR engine\n        mapping: Dictionary mapping engine codes to standard codes\n    \"\"\"\n    self._engine_mappings[engine_name] = mapping\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.utils.GlobalLanguageMapper.to_standard","title":"to_standard","text":"<pre><code>to_standard(engine_name: str, engine_code: str) -&gt; str\n</code></pre> <p>Convert engine-specific language code to standard format.</p> <p>Parameters:</p> Name Type Description Default <code>engine_name</code> <code>str</code> <p>Name of the OCR engine</p> required <code>engine_code</code> <code>str</code> <p>Engine-specific language code</p> required <p>Returns:</p> Type Description <code>str</code> <p>Standard ISO 639-1 language code</p> Source code in <code>omnidocs/utils/language.py</code> <pre><code>def to_standard(self, engine_name: str, engine_code: str) -&gt; str:\n    \"\"\"Convert engine-specific language code to standard format.\n\n    Args:\n        engine_name: Name of the OCR engine\n        engine_code: Engine-specific language code\n\n    Returns:\n        Standard ISO 639-1 language code\n    \"\"\"\n    if engine_name not in self._engine_mappings:\n        return engine_code\n\n    mapping = self._engine_mappings[engine_name]\n    return mapping.get(engine_code.lower(), engine_code)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.utils.LanguageCode","title":"LanguageCode","text":"<p>               Bases: <code>Enum</code></p> <p>Standard ISO 639-1 language codes supported by OmniDocs.</p>"},{"location":"api_reference/python_api.html#omnidocs.utils.LanguageCode.get_all_codes","title":"get_all_codes  <code>classmethod</code>","text":"<pre><code>get_all_codes() -&gt; List[str]\n</code></pre> <p>Get all supported language codes.</p> Source code in <code>omnidocs/utils/language.py</code> <pre><code>@classmethod\ndef get_all_codes(cls) -&gt; List[str]:\n    \"\"\"Get all supported language codes.\"\"\"\n    return [lang.value for lang in cls]\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.utils.LanguageCode.is_valid_code","title":"is_valid_code  <code>classmethod</code>","text":"<pre><code>is_valid_code(code: str) -&gt; bool\n</code></pre> <p>Check if a language code is valid.</p> Source code in <code>omnidocs/utils/language.py</code> <pre><code>@classmethod\ndef is_valid_code(cls, code: str) -&gt; bool:\n    \"\"\"Check if a language code is valid.\"\"\"\n    return code.lower() in [lang.value.lower() for lang in cls]\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.utils.LanguageDetector","title":"LanguageDetector","text":"<p>Simple language detection utilities.</p>"},{"location":"api_reference/python_api.html#omnidocs.utils.LanguageDetector.detect_script","title":"detect_script  <code>classmethod</code>","text":"<pre><code>detect_script(text: str) -&gt; Optional[str]\n</code></pre> <p>Detect the primary script/language of the given text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to analyze</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Detected language code or None if unable to detect</p> Source code in <code>omnidocs/utils/language.py</code> <pre><code>@classmethod\ndef detect_script(cls, text: str) -&gt; Optional[str]:\n    \"\"\"Detect the primary script/language of the given text.\n\n    Args:\n        text: Input text to analyze\n\n    Returns:\n        Detected language code or None if unable to detect\n    \"\"\"\n    if not text:\n        return None\n\n    # Count characters for each language\n    language_scores = {}\n\n    for char in text:\n        char_code = ord(char)\n        for language, ranges in cls.LANGUAGE_RANGES.items():\n            for start, end in ranges:\n                if start &lt;= char_code &lt;= end:\n                    language_scores[language] = language_scores.get(language, 0) + 1\n                    break\n\n    if not language_scores:\n        # Default to English for Latin script\n        return LanguageCode.ENGLISH.value\n\n    # Return language with highest score\n    return max(language_scores, key=language_scores.get)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.utils.LanguageDetector.is_mixed_script","title":"is_mixed_script  <code>classmethod</code>","text":"<pre><code>is_mixed_script(text: str, threshold: float = 0.1) -&gt; bool\n</code></pre> <p>Check if text contains mixed scripts.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Input text to analyze</p> required <code>threshold</code> <code>float</code> <p>Minimum ratio for considering a script significant</p> <code>0.1</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if text contains multiple scripts above threshold</p> Source code in <code>omnidocs/utils/language.py</code> <pre><code>@classmethod\ndef is_mixed_script(cls, text: str, threshold: float = 0.1) -&gt; bool:\n    \"\"\"Check if text contains mixed scripts.\n\n    Args:\n        text: Input text to analyze\n        threshold: Minimum ratio for considering a script significant\n\n    Returns:\n        True if text contains multiple scripts above threshold\n    \"\"\"\n    if not text:\n        return False\n\n    language_scores = {}\n    total_chars = 0\n\n    for char in text:\n        if char.isalnum():  # Only count alphanumeric characters\n            total_chars += 1\n            char_code = ord(char)\n            for language, ranges in cls.LANGUAGE_RANGES.items():\n                for start, end in ranges:\n                    if start &lt;= char_code &lt;= end:\n                        language_scores[language] = language_scores.get(language, 0) + 1\n                        break\n\n    if total_chars == 0:\n        return False\n\n    # Check how many languages exceed the threshold\n    significant_languages = sum(\n        1 for score in language_scores.values()\n        if score / total_chars &gt;= threshold\n    )\n\n    return significant_languages &gt; 1\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.utils.detect_language","title":"detect_language","text":"<pre><code>detect_language(text: str) -&gt; Optional[str]\n</code></pre> <p>Convenience function to detect language from text.</p> Source code in <code>omnidocs/utils/language.py</code> <pre><code>def detect_language(text: str) -&gt; Optional[str]:\n    \"\"\"Convenience function to detect language from text.\"\"\"\n    return LanguageDetector.detect_script(text)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.utils.get_all_supported_languages","title":"get_all_supported_languages","text":"<pre><code>get_all_supported_languages() -&gt; List[str]\n</code></pre> <p>Get all language codes supported by OmniDocs.</p> Source code in <code>omnidocs/utils/language.py</code> <pre><code>def get_all_supported_languages() -&gt; List[str]:\n    \"\"\"Get all language codes supported by OmniDocs.\"\"\"\n    return LanguageCode.get_all_codes()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.utils.get_language_mapper","title":"get_language_mapper","text":"<pre><code>get_language_mapper() -&gt; GlobalLanguageMapper\n</code></pre> <p>Get the global language mapper instance.</p> Source code in <code>omnidocs/utils/language.py</code> <pre><code>def get_language_mapper() -&gt; GlobalLanguageMapper:\n    \"\"\"Get the global language mapper instance.\"\"\"\n    return global_language_mapper\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.utils.get_logger","title":"get_logger","text":"<pre><code>get_logger(name: str, level: Union[str, int] = logging.INFO, log_file: Optional[Union[str, Path]] = None, include_path: bool = True) -&gt; logging.Logger\n</code></pre> <p>Get a configured logger instance.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the logger</p> required <code>level</code> <code>Union[str, int]</code> <p>Logging level</p> <code>INFO</code> <code>log_file</code> <code>Optional[Union[str, Path]]</code> <p>Optional file path to save logs</p> <code>None</code> <code>include_path</code> <code>bool</code> <p>Whether to include full path in log messages</p> <code>True</code> <p>Returns:</p> Type Description <code>Logger</code> <p>Configured logger instance</p> Source code in <code>omnidocs/utils/logging.py</code> <pre><code>def get_logger(\n    name: str,\n    level: Union[str, int] = logging.INFO,\n    log_file: Optional[Union[str, Path]] = None,\n    include_path: bool = True,\n) -&gt; logging.Logger:\n    \"\"\"\n    Get a configured logger instance.\n\n    Args:\n        name: Name of the logger\n        level: Logging level\n        log_file: Optional file path to save logs\n        include_path: Whether to include full path in log messages\n\n    Returns:\n        Configured logger instance\n    \"\"\"\n    # Create logger\n    logger = logging.getLogger(name)\n    logger.setLevel(level)\n\n    # Remove existing handlers\n    logger.handlers.clear()\n\n    # Create console handler with rich support\n    console_handler = RichHandler(\n        console=console,\n        show_time=False,\n        show_path=False,\n        rich_tracebacks=True,\n        tracebacks_show_locals=True,\n    )\n    console_handler.setFormatter(CustomFormatter(include_path=include_path))\n    logger.addHandler(console_handler)\n\n    # Add file handler if log_file is specified\n    if log_file:\n        log_file = Path(log_file)\n        log_file.parent.mkdir(parents=True, exist_ok=True)\n\n        file_handler = logging.FileHandler(log_file)\n        file_handler.setFormatter(CustomFormatter(include_path=True))\n        logger.addHandler(file_handler)\n\n    return logger\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.utils.get_model_path","title":"get_model_path","text":"<pre><code>get_model_path(extractor_name: str, model_name: str) -&gt; Path\n</code></pre> <p>Get standardized model path for a specific extractor and model.</p> <p>Parameters:</p> Name Type Description Default <code>extractor_name</code> <code>str</code> <p>Name of the extractor (e.g., 'donut', 'nougat')</p> required <code>model_name</code> <code>str</code> <p>Name/ID of the model (e.g., 'naver-clova-ix/donut-base')</p> required <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Full path where the model should be stored</p> Source code in <code>omnidocs/utils/model_config.py</code> <pre><code>def get_model_path(extractor_name: str, model_name: str) -&gt; Path:\n    \"\"\"\n    Get standardized model path for a specific extractor and model.\n\n    Args:\n        extractor_name: Name of the extractor (e.g., 'donut', 'nougat')\n        model_name: Name/ID of the model (e.g., 'naver-clova-ix/donut-base')\n\n    Returns:\n        Path: Full path where the model should be stored\n    \"\"\"\n    models_dir = get_models_directory()\n    # Replace slashes in model names to create valid directory names\n    safe_model_name = model_name.replace(\"/\", \"_\")\n    return models_dir / extractor_name / safe_model_name\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.utils.get_models_directory","title":"get_models_directory","text":"<pre><code>get_models_directory() -&gt; Path\n</code></pre> <p>Get the models directory, setting up environment if needed.</p> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The models directory path</p> Source code in <code>omnidocs/utils/model_config.py</code> <pre><code>def get_models_directory() -&gt; Path:\n    \"\"\"\n    Get the models directory, setting up environment if needed.\n\n    Returns:\n        Path: The models directory path\n    \"\"\"\n    return setup_model_environment()\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.utils.is_supported_language","title":"is_supported_language","text":"<pre><code>is_supported_language(code: str) -&gt; bool\n</code></pre> <p>Check if a language code is supported by OmniDocs.</p> Source code in <code>omnidocs/utils/language.py</code> <pre><code>def is_supported_language(code: str) -&gt; bool:\n    \"\"\"Check if a language code is supported by OmniDocs.\"\"\"\n    return LanguageCode.is_valid_code(code)\n</code></pre>"},{"location":"api_reference/python_api.html#omnidocs.utils.setup_model_environment","title":"setup_model_environment","text":"<pre><code>setup_model_environment() -&gt; Path\n</code></pre> <p>Setup model environment variables once for the entire application.</p> <p>This function: 1. Calculates the omnidocs models directory dynamically 2. Creates the directory if it doesn't exist 3. Sets HuggingFace environment variables to use our models directory 4. Uses a flag to prevent multiple setups</p> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>The models directory path</p> Source code in <code>omnidocs/utils/model_config.py</code> <pre><code>def setup_model_environment() -&gt; Path:\n    \"\"\"\n    Setup model environment variables once for the entire application.\n\n    This function:\n    1. Calculates the omnidocs models directory dynamically\n    2. Creates the directory if it doesn't exist\n    3. Sets HuggingFace environment variables to use our models directory\n    4. Uses a flag to prevent multiple setups\n\n    Returns:\n        Path: The models directory path\n    \"\"\"\n    # Check if already setup to prevent multiple calls\n    if 'OMNIDOCS_MODELS_SETUP' in os.environ:\n        # Return the already configured models directory\n        return Path(os.environ[\"HF_HOME\"])\n\n    # Calculate omnidocs root dynamically\n    current_file = Path(__file__)\n    omnidocs_root = current_file.parent.parent  # Go up to omnidocs/ root\n    models_dir = omnidocs_root / \"models\"\n    models_dir.mkdir(exist_ok=True)\n\n    # Set environment variables for HuggingFace to use our models directory\n    os.environ[\"HF_HOME\"] = str(models_dir)\n    os.environ[\"TRANSFORMERS_CACHE\"] = str(models_dir)\n    os.environ[\"HF_HUB_CACHE\"] = str(models_dir)\n\n    # Set flag to prevent re-setup\n    os.environ[\"OMNIDOCS_MODELS_SETUP\"] = \"true\"\n\n    return models_dir\n</code></pre>"},{"location":"api_reference/python_api.html#usage-tips","title":"\ud83e\uddd1\u200d\ud83d\udcbb Usage Tips","text":"<ul> <li>All extractors follow a consistent interface: <code>extractor = ...Extractor(); result = extractor.extract(input)</code></li> <li>Results are returned as structured objects (e.g., <code>TableOutput</code>, <code>TextOutput</code>, etc.)</li> <li>See the Getting Started guide for real-world examples.</li> <li>For advanced configuration, check each extractor\u2019s docstring for parameters and options.</li> </ul>"},{"location":"api_reference/python_api.html#more-resources","title":"\ud83d\udcda More Resources","text":"<ul> <li>Project README</li> <li>All Tutorials</li> <li>Open an Issue</li> </ul>"},{"location":"api_reference/tasks/layout_analysis.html","title":"\ud83d\udcd0 Layout Analysis","text":"<p>This section documents the API for layout analysis tasks, including various extractors for detecting and analyzing document structure.</p>"},{"location":"api_reference/tasks/layout_analysis.html#overview","title":"Overview","text":"<p>Layout analysis in OmniDocs focuses on identifying and categorizing different regions within a document, such as text blocks, images, tables, and figures. This is crucial for understanding the document's overall structure and reading order.</p>"},{"location":"api_reference/tasks/layout_analysis.html#available-extractors","title":"Available Extractors","text":""},{"location":"api_reference/tasks/layout_analysis.html#doclayoutyoloextractor","title":"DocLayoutYOLOExtractor","text":"<p>A layout detection model based on YOLO-v10, designed for diverse document types.</p>"},{"location":"api_reference/tasks/layout_analysis.html#omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo.YOLOLayoutDetector","title":"omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo.YOLOLayoutDetector","text":"<pre><code>YOLOLayoutDetector(device: Optional[str] = None, show_log: bool = False, model_path: Optional[Union[str, Path]] = None)\n</code></pre> <p>               Bases: <code>BaseLayoutDetector</code></p> <p>YOLO-based layout detection implementation.</p> <p>Initialize YOLO Layout Detector.</p>"},{"location":"api_reference/tasks/layout_analysis.html#usage-example","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo import YOLOLayoutDetector\n\nextractor = YOLOLayoutDetector()\nresult = extractor.extract(\"document.pdf\")\nprint(f\"Detected {len(result.layouts)} layout elements.\")\n</code></pre>"},{"location":"api_reference/tasks/layout_analysis.html#florencelayoutextractor","title":"FlorenceLayoutExtractor","text":"<p>A fine-tuned model for document layout analysis, improving bounding box accuracy in document images.</p>"},{"location":"api_reference/tasks/layout_analysis.html#omnidocs.tasks.layout_analysis.extractors.florence.FlorenceLayoutDetector","title":"omnidocs.tasks.layout_analysis.extractors.florence.FlorenceLayoutDetector","text":"<pre><code>FlorenceLayoutDetector(device: Optional[str] = None, show_log: bool = False, trust_remote_code: bool = True, **kwargs)\n</code></pre> <p>               Bases: <code>BaseLayoutDetector</code></p> <p>Florence-based layout detection implementation.</p> <p>Initialize Florence Layout Detector.</p>"},{"location":"api_reference/tasks/layout_analysis.html#usage-example_1","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.layout_analysis.extractors.florence import FlorenceLayoutDetector\n\nextractor = FlorenceLayoutDetector()\nresult = extractor.extract(\"image.png\")\nprint(f\"Detected {len(result.layouts)} layout elements.\")\n</code></pre>"},{"location":"api_reference/tasks/layout_analysis.html#paddlelayoutextractor","title":"PaddleLayoutExtractor","text":"<p>An OCR tool that supports multiple languages and provides layout detection capabilities.</p>"},{"location":"api_reference/tasks/layout_analysis.html#omnidocs.tasks.layout_analysis.extractors.paddle.PaddleLayoutDetector","title":"omnidocs.tasks.layout_analysis.extractors.paddle.PaddleLayoutDetector","text":"<pre><code>PaddleLayoutDetector(device: Optional[str] = None, show_log: bool = False, **kwargs)\n</code></pre> <p>               Bases: <code>BaseLayoutDetector</code></p> <p>PaddleOCR-based layout detection implementation.</p> <p>Initialize PaddleOCR Layout Detector.</p>"},{"location":"api_reference/tasks/layout_analysis.html#usage-example_2","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.layout_analysis.extractors.paddle import PaddleLayoutDetector\n\nextractor = PaddleLayoutDetector()\nresult = extractor.extract(\"image.png\")\nprint(f\"Detected {len(result.layouts)} layout elements.\")\n</code></pre>"},{"location":"api_reference/tasks/layout_analysis.html#rtdetrlayoutextractor","title":"RTDETRLayoutExtractor","text":"<p>Implementation of RT-DETR, a real-time detection transformer focusing on object detection tasks.</p>"},{"location":"api_reference/tasks/layout_analysis.html#omnidocs.tasks.layout_analysis.extractors.rtdetr.RTDETRLayoutDetector","title":"omnidocs.tasks.layout_analysis.extractors.rtdetr.RTDETRLayoutDetector","text":"<pre><code>RTDETRLayoutDetector(device: Optional[str] = None, show_log: bool = False, model_path: Optional[Union[str, Path]] = None, num_threads: Optional[int] = 4, use_cpu_only: bool = True)\n</code></pre> <p>               Bases: <code>BaseLayoutDetector</code></p> <p>RT-DETR-based layout detection implementation.</p> <p>Initialize RT-DETR Layout Detector with careful device handling.</p>"},{"location":"api_reference/tasks/layout_analysis.html#usage-example_3","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.layout_analysis.extractors.rtdetr import RTDETRLayoutDetector\n\nextractor = RTDETRLayoutDetector()\nresult = extractor.extract(\"image.png\")\nprint(f\"Detected {len(result.layouts)} layout elements.\")\n</code></pre>"},{"location":"api_reference/tasks/layout_analysis.html#suryalayoutextractor","title":"SuryaLayoutExtractor","text":"<p>OCR and layout analysis tool supporting 90+ languages, including reading order and table recognition.</p>"},{"location":"api_reference/tasks/layout_analysis.html#omnidocs.tasks.layout_analysis.extractors.surya.SuryaLayoutDetector","title":"omnidocs.tasks.layout_analysis.extractors.surya.SuryaLayoutDetector","text":"<pre><code>SuryaLayoutDetector(device: Optional[str] = None, show_log: bool = False, **kwargs)\n</code></pre> <p>               Bases: <code>BaseLayoutDetector</code></p> <p>Surya-based layout detection implementation.</p> <p>Initialize Surya Layout Detector.</p>"},{"location":"api_reference/tasks/layout_analysis.html#usage-example_4","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.layout_analysis.extractors.surya import SuryaLayoutDetector\n\nextractor = SuryaLayoutDetector()\nresult = extractor.extract(\"document.pdf\")\nprint(f\"Detected {len(result.layouts)} layout elements.\")\n</code></pre>"},{"location":"api_reference/tasks/layout_analysis.html#layoutoutput","title":"LayoutOutput","text":"<p>The standardized output format for layout analysis results.</p>"},{"location":"api_reference/tasks/layout_analysis.html#omnidocs.tasks.layout_analysis.base.LayoutOutput","title":"omnidocs.tasks.layout_analysis.base.LayoutOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for all detected layout boxes in an image.</p> <p>Attributes:</p> Name Type Description <code>bboxes</code> <code>List[LayoutBox]</code> <p>List of detected LayoutBox objects</p> <code>page_number</code> <code>Optional[int]</code> <p>Optional page number for multi-page documents</p> <code>image_size</code> <code>Optional[Tuple[int, int]]</code> <p>Optional tuple of (width, height) of the processed image</p>"},{"location":"api_reference/tasks/layout_analysis.html#omnidocs.tasks.layout_analysis.base.LayoutOutput.save_json","title":"save_json","text":"<pre><code>save_json(output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save layout output to JSON file.</p>"},{"location":"api_reference/tasks/layout_analysis.html#omnidocs.tasks.layout_analysis.base.LayoutOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p>"},{"location":"api_reference/tasks/layout_analysis.html#key-properties","title":"Key Properties","text":"<ul> <li><code>layouts</code> (List[LayoutElement]): List of detected layout elements.</li> <li><code>source_file</code> (str): Path to the processed file.</li> <li><code>source_img_size</code> (Tuple[int, int]): Dimensions of the source image.</li> </ul>"},{"location":"api_reference/tasks/layout_analysis.html#key-methods","title":"Key Methods","text":"<ul> <li><code>save_json(output_path)</code>: Save results to a JSON file.</li> <li><code>visualize(image_path, output_path)</code>: Visualize layout elements on the source image.</li> </ul>"},{"location":"api_reference/tasks/layout_analysis.html#layoutelement","title":"LayoutElement","text":"<p>Represents a single detected layout element.</p>"},{"location":"api_reference/tasks/layout_analysis.html#omnidocs.tasks.layout_analysis.base.BaseLayoutMapper","title":"omnidocs.tasks.layout_analysis.base.BaseLayoutMapper","text":"<pre><code>BaseLayoutMapper()\n</code></pre> <p>Base class for layout label mapping.</p>"},{"location":"api_reference/tasks/layout_analysis.html#omnidocs.tasks.layout_analysis.base.BaseLayoutMapper.from_standard","title":"from_standard","text":"<pre><code>from_standard(layout_label: LayoutLabel) -&gt; Optional[str]\n</code></pre> <p>Convert standardized LayoutLabel to model-specific label.</p>"},{"location":"api_reference/tasks/layout_analysis.html#omnidocs.tasks.layout_analysis.base.BaseLayoutMapper.to_standard","title":"to_standard","text":"<pre><code>to_standard(model_label: str) -&gt; Optional[LayoutLabel]\n</code></pre> <p>Convert model-specific label to standardized LayoutLabel.</p>"},{"location":"api_reference/tasks/layout_analysis.html#attributes","title":"Attributes","text":"<ul> <li><code>type</code> (str): Type of the element (e.g., 'text', 'title', 'table', 'figure').</li> <li><code>bbox</code> (List[float]): Bounding box coordinates [x1, y1, x2, y2].</li> <li><code>text_content</code> (Optional[str]): Text content if applicable.</li> <li><code>confidence</code> (Optional[float]): Confidence score of the detection.</li> </ul>"},{"location":"api_reference/tasks/layout_analysis.html#baselayoutextractor","title":"BaseLayoutExtractor","text":"<p>The abstract base class for all layout analysis extractors.</p>"},{"location":"api_reference/tasks/layout_analysis.html#omnidocs.tasks.layout_analysis.base.BaseLayoutDetector","title":"omnidocs.tasks.layout_analysis.base.BaseLayoutDetector","text":"<pre><code>BaseLayoutDetector(show_log: bool = False)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for all layout detection models.</p>"},{"location":"api_reference/tasks/layout_analysis.html#omnidocs.tasks.layout_analysis.base.BaseLayoutDetector.preprocess_input","title":"preprocess_input","text":"<pre><code>preprocess_input(input_path: Union[str, Path]) -&gt; List[np.ndarray]\n</code></pre> <p>Convert input to processable format.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input image or PDF</p> required <p>Returns:</p> Type Description <code>List[ndarray]</code> <p>List of preprocessed images as numpy arrays</p>"},{"location":"api_reference/tasks/layout_analysis.html#omnidocs.tasks.layout_analysis.base.BaseLayoutDetector.visualize","title":"visualize","text":"<pre><code>visualize(detection_result: Tuple[Image, LayoutOutput], output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save annotated image to file.</p> <p>Parameters:</p> Name Type Description Default <code>detection_result</code> <code>Tuple[Image, LayoutOutput]</code> <p>Tuple containing (PIL Image, LayoutOutput)</p> required <code>output_path</code> <code>Union[str, Path]</code> <p>Path to save visualization</p> required"},{"location":"api_reference/tasks/layout_analysis.html#layoutmapper","title":"LayoutMapper","text":"<p>Handles mapping of layout labels and normalization of bounding boxes.</p>"},{"location":"api_reference/tasks/layout_analysis.html#omnidocs.tasks.layout_analysis.base.BaseLayoutMapper","title":"omnidocs.tasks.layout_analysis.base.BaseLayoutMapper","text":"<pre><code>BaseLayoutMapper()\n</code></pre> <p>Base class for layout label mapping.</p>"},{"location":"api_reference/tasks/layout_analysis.html#omnidocs.tasks.layout_analysis.base.BaseLayoutMapper.from_standard","title":"from_standard","text":"<pre><code>from_standard(layout_label: LayoutLabel) -&gt; Optional[str]\n</code></pre> <p>Convert standardized LayoutLabel to model-specific label.</p>"},{"location":"api_reference/tasks/layout_analysis.html#omnidocs.tasks.layout_analysis.base.BaseLayoutMapper.to_standard","title":"to_standard","text":"<pre><code>to_standard(model_label: str) -&gt; Optional[LayoutLabel]\n</code></pre> <p>Convert model-specific label to standardized LayoutLabel.</p>"},{"location":"api_reference/tasks/layout_analysis.html#related-resources","title":"Related Resources","text":"<ul> <li>Layout Analysis Overview</li> <li>Core Classes</li> <li>Utilities</li> </ul>"},{"location":"api_reference/tasks/math_expression.html","title":"\ud83d\udd22 Math Expression Extraction","text":"<p>This section documents the API for mathematical expression extraction tasks, providing various extractors to recognize and retrieve LaTeX math from documents.</p>"},{"location":"api_reference/tasks/math_expression.html#overview","title":"Overview","text":"<p>Math expression extraction in OmniDocs focuses on converting mathematical formulas and equations found in documents (e.g., academic papers, textbooks) into a machine-readable format, typically LaTeX. This enables further processing, rendering, or indexing of mathematical content.</p>"},{"location":"api_reference/tasks/math_expression.html#available-extractors","title":"Available Extractors","text":""},{"location":"api_reference/tasks/math_expression.html#donutextractor","title":"DonutExtractor","text":"<p>NAVER CLOVA Donut model for math/LaTeX extraction.</p>"},{"location":"api_reference/tasks/math_expression.html#omnidocs.tasks.math_expression_extraction.extractors.donut.DonutExtractor","title":"omnidocs.tasks.math_expression_extraction.extractors.donut.DonutExtractor","text":"<pre><code>DonutExtractor(device: Optional[str] = None, show_log: bool = False, model_name: str = 'naver-clova-ix/donut-base-finetuned-cord-v2', model_path: Optional[Union[str, Path]] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseLatexExtractor</code></p> <p>Donut (NAVER CLOVA) based expression extraction implementation.</p> <p>Initialize Donut Extractor.</p>"},{"location":"api_reference/tasks/math_expression.html#omnidocs.tasks.math_expression_extraction.extractors.donut.DonutExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; LatexOutput\n</code></pre> <p>Extract LaTeX expressions using Donut.</p>"},{"location":"api_reference/tasks/math_expression.html#usage-example","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.math_expression_extraction.extractors.donut import DonutExtractor\n\nextractor = DonutExtractor()\nresult = extractor.extract(\"math_document.pdf\")\nprint(f\"Extracted LaTeX: {result.full_text[:200]}...\")\n</code></pre>"},{"location":"api_reference/tasks/math_expression.html#nougatextractor","title":"NougatExtractor","text":"<p>Facebook's Nougat model for LaTeX extraction from academic documents.</p>"},{"location":"api_reference/tasks/math_expression.html#omnidocs.tasks.math_expression_extraction.extractors.nougat.NougatExtractor","title":"omnidocs.tasks.math_expression_extraction.extractors.nougat.NougatExtractor","text":"<pre><code>NougatExtractor(model_type: str = 'small', device: Optional[str] = None, show_log: bool = False, model_path: Optional[str] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseLatexExtractor</code></p> <p>Nougat (Neural Optical Understanding for Academic Documents) based expression extraction.</p> <p>Initialize Nougat Extractor.</p>"},{"location":"api_reference/tasks/math_expression.html#omnidocs.tasks.math_expression_extraction.extractors.nougat.NougatExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; LatexOutput\n</code></pre> <p>Extract LaTeX expressions using Nougat.</p>"},{"location":"api_reference/tasks/math_expression.html#usage-example_1","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.math_expression_extraction.extractors.nougat import NougatExtractor\n\nextractor = NougatExtractor()\nresult = extractor.extract(\"academic_paper.pdf\")\nprint(f\"Extracted LaTeX: {result.full_text[:200]}...\")\n</code></pre>"},{"location":"api_reference/tasks/math_expression.html#suryamathextractor","title":"SuryaMathExtractor","text":"<p>Surya-based mathematical expression extraction.</p>"},{"location":"api_reference/tasks/math_expression.html#omnidocs.tasks.math_expression_extraction.extractors.surya_math.SuryaMathExtractor","title":"omnidocs.tasks.math_expression_extraction.extractors.surya_math.SuryaMathExtractor","text":"<pre><code>SuryaMathExtractor(device: Optional[str] = None, show_log: bool = False, model_path: Optional[Union[str, Path]] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseLatexExtractor</code></p> <p>Surya-based mathematical expression extraction implementation.</p> <p>Initialize Surya Math Extractor.</p>"},{"location":"api_reference/tasks/math_expression.html#omnidocs.tasks.math_expression_extraction.extractors.surya_math.SuryaMathExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; LatexOutput\n</code></pre> <p>Extract LaTeX expressions using Surya.</p>"},{"location":"api_reference/tasks/math_expression.html#usage-example_2","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.math_expression_extraction.extractors.surya_math import SuryaMathExtractor\n\nextractor = SuryaMathExtractor()\nresult = extractor.extract(\"math_image.png\")\nprint(f\"Extracted LaTeX: {result.full_text[:200]}...\")\n</code></pre>"},{"location":"api_reference/tasks/math_expression.html#unimernetextractor","title":"UniMERNetExtractor","text":"<p>Universal Mathematical Expression Recognition Network.</p>"},{"location":"api_reference/tasks/math_expression.html#omnidocs.tasks.math_expression_extraction.extractors.unimernet.UniMERNetExtractor","title":"omnidocs.tasks.math_expression_extraction.extractors.unimernet.UniMERNetExtractor","text":"<pre><code>UniMERNetExtractor(model_path: Optional[str] = None, cfg_path: Optional[str] = None, device: Optional[str] = None, show_log: bool = False, **kwargs)\n</code></pre> <p>               Bases: <code>BaseLatexExtractor</code></p> <p>UniMERNet (Universal Mathematical Expression Recognition Network) based expression extraction.</p> <p>Initialize UniMERNet Extractor.</p>"},{"location":"api_reference/tasks/math_expression.html#omnidocs.tasks.math_expression_extraction.extractors.unimernet.UniMERNetExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; LatexOutput\n</code></pre> <p>Extract LaTeX expressions using UniMERNet.</p>"},{"location":"api_reference/tasks/math_expression.html#usage-example_3","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.math_expression_extraction.extractors.unimernet import UniMERNetExtractor\n\nextractor = UniMERNetExtractor()\nresult = extractor.extract(\"math_equation.png\")\nprint(f\"Extracted LaTeX: {result.full_text[:200]}...\")\n</code></pre>"},{"location":"api_reference/tasks/math_expression.html#mathoutput","title":"MathOutput","text":"<p>The standardized output format for mathematical expression extraction results.</p>"},{"location":"api_reference/tasks/math_expression.html#omnidocs.tasks.math_expression_extraction.base.LatexOutput","title":"omnidocs.tasks.math_expression_extraction.base.LatexOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for extracted LaTeX expressions.</p> <p>Attributes:</p> Name Type Description <code>expressions</code> <code>List[str]</code> <p>List of extracted LaTeX expressions</p> <code>confidences</code> <code>Optional[List[float]]</code> <p>Optional confidence scores for each expression</p> <code>bboxes</code> <code>Optional[List[List[float]]]</code> <p>Optional bounding boxes for each expression</p> <code>source_img_size</code> <code>Optional[Tuple[int, int]]</code> <p>Optional tuple of source image dimensions</p>"},{"location":"api_reference/tasks/math_expression.html#omnidocs.tasks.math_expression_extraction.base.LatexOutput.save_json","title":"save_json","text":"<pre><code>save_json(output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save output to JSON file.</p>"},{"location":"api_reference/tasks/math_expression.html#omnidocs.tasks.math_expression_extraction.base.LatexOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p>"},{"location":"api_reference/tasks/math_expression.html#key-properties","title":"Key Properties","text":"<ul> <li><code>expressions</code> (List[MathExpression]): List of detected mathematical expressions.</li> <li><code>full_text</code> (str): Combined LaTeX string of all expressions.</li> <li><code>source_file</code> (str): Path to the processed file.</li> </ul>"},{"location":"api_reference/tasks/math_expression.html#key-methods","title":"Key Methods","text":"<ul> <li><code>save_json(output_path)</code>: Save results to a JSON file.</li> </ul>"},{"location":"api_reference/tasks/math_expression.html#attributes","title":"Attributes","text":"<ul> <li><code>latex</code> (str): The extracted LaTeX string.</li> <li><code>bbox</code> (List[float]): Bounding box coordinates [x1, y1, x2, y2].</li> <li><code>confidence</code> (Optional[float]): Confidence score of the extraction.</li> <li><code>page_number</code> (int): The page number where the expression is found.</li> </ul>"},{"location":"api_reference/tasks/math_expression.html#basemathextractor","title":"BaseMathExtractor","text":"<p>The abstract base class for all mathematical expression extractors.</p>"},{"location":"api_reference/tasks/math_expression.html#omnidocs.tasks.math_expression_extraction.base.BaseLatexExtractor","title":"omnidocs.tasks.math_expression_extraction.base.BaseLatexExtractor","text":"<pre><code>BaseLatexExtractor(device: Optional[str] = None, show_log: bool = False)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for LaTeX expression extraction models.</p> <p>Initialize the LaTeX extractor.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run model on ('cuda' or 'cpu')</p> <code>None</code> <code>show_log</code> <code>bool</code> <p>Whether to show detailed logs</p> <code>False</code>"},{"location":"api_reference/tasks/math_expression.html#omnidocs.tasks.math_expression_extraction.base.BaseLatexExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; LatexOutput\n</code></pre> <p>Extract LaTeX expressions from input image.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path, Image]</code> <p>Path to input image or image data</p> required <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>LatexOutput</code> <p>LatexOutput containing extracted expressions</p>"},{"location":"api_reference/tasks/math_expression.html#omnidocs.tasks.math_expression_extraction.base.BaseLatexExtractor.preprocess_input","title":"preprocess_input","text":"<pre><code>preprocess_input(input_path: Union[str, Path, Image, ndarray]) -&gt; List[Image.Image]\n</code></pre> <p>Convert input to list of PIL Images.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path, Image, ndarray]</code> <p>Input image path or image data</p> required <p>Returns:</p> Type Description <code>List[Image]</code> <p>List of PIL Images</p>"},{"location":"api_reference/tasks/math_expression.html#related-resources","title":"Related Resources","text":"<ul> <li>Math Expression Overview</li> <li>Donut Tutorial</li> <li>Nougat Tutorial</li> <li>SuryaMath Tutorial</li> <li>UniMERNet Tutorial</li> <li>Core Classes</li> <li>Utilities</li> </ul>"},{"location":"api_reference/tasks/ocr.html","title":"\ud83d\uddb9 OCR (Optical Character Recognition)","text":"<p>This section documents the API for OCR tasks, providing various extractors to recognize and extract text from images and scanned documents.</p>"},{"location":"api_reference/tasks/ocr.html#overview","title":"Overview","text":"<p>OCR in OmniDocs enables the conversion of images (e.g., scanned documents, photos) into machine-readable text. It supports multiple engines, allowing you to choose the best balance of speed, accuracy, and language support for your needs.</p>"},{"location":"api_reference/tasks/ocr.html#available-extractors","title":"Available Extractors","text":""},{"location":"api_reference/tasks/ocr.html#easyocrextractor","title":"EasyOCRExtractor","text":"<p>A simple and easy-to-use OCR library that supports multiple languages and is built on PyTorch.</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.extractors.easy_ocr.EasyOCRExtractor","title":"omnidocs.tasks.ocr_extraction.extractors.easy_ocr.EasyOCRExtractor","text":"<pre><code>EasyOCRExtractor(device: Optional[str] = None, show_log: bool = False, languages: Optional[List[str]] = None, gpu: bool = True, **kwargs)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>EasyOCR based text extraction implementation.</p> <p>Initialize EasyOCR Extractor.</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.extractors.easy_ocr.EasyOCRExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], detail: int = 1, paragraph: bool = False, width_ths: float = 0.7, height_ths: float = 0.7, **kwargs) -&gt; OCROutput\n</code></pre> <p>Extract text using EasyOCR.</p>"},{"location":"api_reference/tasks/ocr.html#usage-example","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.ocr_extraction.extractors.easy_ocr import EasyOCRExtractor\n\nextractor = EasyOCRExtractor(languages=['en'])\nresult = extractor.extract(\"scanned_document.png\")\nprint(f\"Extracted text: {result.full_text[:200]}...\")\n</code></pre>"},{"location":"api_reference/tasks/ocr.html#paddleocrextractor","title":"PaddleOCRExtractor","text":"<p>An OCR tool that supports multiple languages and provides layout detection capabilities.</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.extractors.paddle.PaddleOCRExtractor","title":"omnidocs.tasks.ocr_extraction.extractors.paddle.PaddleOCRExtractor","text":"<pre><code>PaddleOCRExtractor(device: Optional[str] = None, show_log: bool = False, languages: Optional[List[str]] = None, use_angle_cls: bool = True, use_gpu: bool = True, drop_score: float = 0.5, model_path: Optional[str] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>PaddleOCR based text extraction implementation.</p> <p>Initialize PaddleOCR Extractor.</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.extractors.paddle.PaddleOCRExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; OCROutput\n</code></pre> <p>Extract text using PaddleOCR.</p>"},{"location":"api_reference/tasks/ocr.html#usage-example_1","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.ocr_extraction.extractors.paddle import PaddleOCRExtractor\n\nextractor = PaddleOCRExtractor(languages=['en'])\nresult = extractor.extract(\"scanned_document.png\")\nprint(f\"Extracted text: {result.full_text[:200]}...\")\n</code></pre>"},{"location":"api_reference/tasks/ocr.html#suryaocrextractor","title":"SuryaOCRExtractor","text":"<p>A modern, high-accuracy OCR engine, part of the Surya library, with strong support for Indian languages.</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.extractors.surya_ocr.SuryaOCRExtractor","title":"omnidocs.tasks.ocr_extraction.extractors.surya_ocr.SuryaOCRExtractor","text":"<pre><code>SuryaOCRExtractor(device: Optional[str] = None, show_log: bool = False, languages: Optional[List[str]] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>Surya OCR based text extraction implementation.</p> <p>Initialize Surya OCR Extractor.</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.extractors.surya_ocr.SuryaOCRExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; OCROutput\n</code></pre> <p>Extract text using Surya OCR.</p>"},{"location":"api_reference/tasks/ocr.html#usage-example_2","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.ocr_extraction.extractors.surya_ocr import SuryaOCRExtractor\n\nextractor = SuryaOCRExtractor(languages=['en'])\nresult = extractor.extract(\"scanned_document.png\")\nprint(f\"Extracted text: {result.full_text[:200]}...\")\n</code></pre>"},{"location":"api_reference/tasks/ocr.html#tesseractocrextractor","title":"TesseractOCRExtractor","text":"<p>An open-source OCR engine that supports multiple languages and is widely used for text extraction from images.</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.extractors.tesseract_ocr.TesseractOCRExtractor","title":"omnidocs.tasks.ocr_extraction.extractors.tesseract_ocr.TesseractOCRExtractor","text":"<pre><code>TesseractOCRExtractor(device: Optional[str] = None, show_log: bool = False, languages: Optional[List[str]] = None, psm: int = 6, oem: int = 3, config: str = '', **kwargs)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>Tesseract OCR based text extraction implementation.</p> <p>Initialize Tesseract OCR Extractor.</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.extractors.tesseract_ocr.TesseractOCRExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; OCROutput\n</code></pre> <p>Extract text using Tesseract OCR.</p>"},{"location":"api_reference/tasks/ocr.html#usage-example_3","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.ocr_extraction.extractors.tesseract_ocr import TesseractOCRExtractor\n\nextractor = TesseractOCRExtractor(languages=['eng']) # Tesseract uses 'eng' for English\nresult = extractor.extract(\"scanned_document.png\")\nprint(f\"Extracted text: {result.full_text[:200]}...\")\n</code></pre>"},{"location":"api_reference/tasks/ocr.html#ocroutput","title":"OCROutput","text":"<p>The standardized output format for OCR results.</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.OCROutput","title":"omnidocs.tasks.ocr_extraction.base.OCROutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for OCR extraction results.</p> <p>Attributes:</p> Name Type Description <code>texts</code> <code>List[OCRText]</code> <p>List of detected text objects</p> <code>full_text</code> <code>str</code> <p>Combined text from all detections</p> <code>source_img_size</code> <code>Optional[Tuple[int, int]]</code> <p>Original image dimensions (width, height)</p> <code>processing_time</code> <code>Optional[float]</code> <p>Time taken for OCR processing</p> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata from the OCR engine</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.OCROutput.get_sorted_by_reading_order","title":"get_sorted_by_reading_order","text":"<pre><code>get_sorted_by_reading_order() -&gt; List[OCRText]\n</code></pre> <p>Get texts sorted by reading order (top-to-bottom, left-to-right if no reading_order).</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.OCROutput.get_text_by_confidence","title":"get_text_by_confidence","text":"<pre><code>get_text_by_confidence(min_confidence: float = 0.5) -&gt; List[OCRText]\n</code></pre> <p>Filter texts by minimum confidence threshold.</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.OCROutput.save_json","title":"save_json","text":"<pre><code>save_json(output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save output to JSON file.</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.OCROutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p>"},{"location":"api_reference/tasks/ocr.html#key-properties","title":"Key Properties","text":"<ul> <li><code>texts</code> (List[OCRText]): List of individual text regions detected.</li> <li><code>full_text</code> (str): The combined text from all detected regions.</li> <li><code>source_img_size</code> (Tuple[int, int]): Dimensions of the source image.</li> </ul>"},{"location":"api_reference/tasks/ocr.html#key-methods","title":"Key Methods","text":"<ul> <li><code>save_json(output_path)</code>: Save results to a JSON file.</li> <li><code>visualize(image_path, output_path)</code>: Visualize OCR results with bounding boxes on the source image.</li> <li><code>get_text_by_confidence(min_confidence)</code>: Filter text regions by confidence score.</li> <li><code>get_sorted_by_reading_order()</code>: Sort text regions by reading order.</li> </ul>"},{"location":"api_reference/tasks/ocr.html#ocrtext","title":"OCRText","text":"<p>Represents a single text region detected by OCR.</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.OCRText","title":"omnidocs.tasks.ocr_extraction.base.OCRText","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for individual OCR text detection.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>str</code> <p>Extracted text content</p> <code>confidence</code> <code>Optional[float]</code> <p>Confidence score for the text detection</p> <code>bbox</code> <code>Optional[List[float]]</code> <p>Bounding box coordinates [x1, y1, x2, y2]</p> <code>polygon</code> <code>Optional[List[List[float]]]</code> <p>Optional polygon coordinates for irregular text regions</p> <code>language</code> <code>Optional[str]</code> <p>Detected language code (e.g., 'en', 'zh', 'fr')</p> <code>reading_order</code> <code>Optional[int]</code> <p>Optional reading order index for text sequencing</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.OCRText.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p>"},{"location":"api_reference/tasks/ocr.html#attributes","title":"Attributes","text":"<ul> <li><code>text</code> (str): The recognized text content.</li> <li><code>confidence</code> (float): Confidence score of the recognition (0.0-1.0).</li> <li><code>bbox</code> (List[float]): Bounding box coordinates [x1, y1, x2, y2].</li> <li><code>polygon</code> (List[List[float]]): Precise polygon coordinates of the text region.</li> <li><code>language</code> (Optional[str]): Detected language code.</li> <li><code>reading_order</code> (int): Reading order index of the text region.</li> </ul>"},{"location":"api_reference/tasks/ocr.html#baseocrextractor","title":"BaseOCRExtractor","text":"<p>The abstract base class for all OCR extractors.</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor","title":"omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor","text":"<pre><code>BaseOCRExtractor(device: Optional[str] = None, show_log: bool = False, languages: Optional[List[str]] = None, engine_name: Optional[str] = None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for OCR text extraction models.</p> <p>Initialize the OCR extractor.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run model on ('cuda' or 'cpu')</p> <code>None</code> <code>show_log</code> <code>bool</code> <p>Whether to show detailed logs</p> <code>False</code> <code>languages</code> <code>Optional[List[str]]</code> <p>List of language codes to support (e.g., ['en', 'zh'])</p> <code>None</code> <code>engine_name</code> <code>Optional[str]</code> <p>Name of the OCR engine for language mapping</p> <code>None</code>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; OCROutput\n</code></pre> <p>Extract text from input image.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path, Image]</code> <p>Path to input image or image data</p> required <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>OCROutput</code> <p>OCROutput containing extracted text</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.extract_all","title":"extract_all","text":"<pre><code>extract_all(input_paths: List[Union[str, Path, Image]], **kwargs) -&gt; List[OCROutput]\n</code></pre> <p>Extract text from multiple images.</p> <p>Parameters:</p> Name Type Description Default <code>input_paths</code> <code>List[Union[str, Path, Image]]</code> <p>List of image paths or image data</p> required <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[OCROutput]</code> <p>List of OCROutput objects</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.extract_with_layout","title":"extract_with_layout","text":"<pre><code>extract_with_layout(input_path: Union[str, Path, Image], layout_regions: Optional[List[Dict]] = None, **kwargs) -&gt; OCROutput\n</code></pre> <p>Extract text with optional layout information.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path, Image]</code> <p>Path to input image or image data</p> required <code>layout_regions</code> <code>Optional[List[Dict]]</code> <p>Optional list of layout regions to focus OCR on</p> <code>None</code> <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>OCROutput</code> <p>OCROutput containing extracted text</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.preprocess_input","title":"preprocess_input","text":"<pre><code>preprocess_input(input_path: Union[str, Path, Image, ndarray]) -&gt; List[Image.Image]\n</code></pre> <p>Convert input to list of PIL Images.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path, Image, ndarray]</code> <p>Input image path or image data</p> required <p>Returns:</p> Type Description <code>List[Image]</code> <p>List of PIL Images</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: Any, img_size: Tuple[int, int]) -&gt; OCROutput\n</code></pre> <p>Convert raw OCR output to standardized OCROutput format.</p> <p>Parameters:</p> Name Type Description Default <code>raw_output</code> <code>Any</code> <p>Raw output from OCR engine</p> required <code>img_size</code> <code>Tuple[int, int]</code> <p>Original image size (width, height)</p> required <p>Returns:</p> Type Description <code>OCROutput</code> <p>Standardized OCROutput object</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.visualize","title":"visualize","text":"<pre><code>visualize(ocr_result: OCROutput, image_path: Union[str, Path, Image], output_path: str = 'visualized.png', box_color: str = 'red', box_width: int = 2, show_text: bool = False, text_color: str = 'blue', font_size: int = 12) -&gt; None\n</code></pre> <p>Visualize OCR results by drawing bounding boxes on the original image.</p> <p>This method allows users to easily see which extractor is working better by visualizing the detected text regions with bounding boxes.</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.get_supported_languages","title":"get_supported_languages","text":"<pre><code>get_supported_languages() -&gt; List[str]\n</code></pre> <p>Get list of supported language codes.</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.set_languages","title":"set_languages","text":"<pre><code>set_languages(languages: List[str]) -&gt; None\n</code></pre> <p>Update supported languages for OCR extraction.</p>"},{"location":"api_reference/tasks/ocr.html#baseocrmapper","title":"BaseOCRMapper","text":"<p>Handles language code mapping and normalization for OCR engines.</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.BaseOCRMapper","title":"omnidocs.tasks.ocr_extraction.base.BaseOCRMapper","text":"<pre><code>BaseOCRMapper(engine_name: str)\n</code></pre> <p>Base class for mapping OCR engine-specific outputs to standardized format.</p> <p>Initialize mapper for specific OCR engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine_name</code> <code>str</code> <p>Name of the OCR engine (e.g., 'tesseract', 'paddle', 'easyocr')</p> required"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.BaseOCRMapper.detect_text_language","title":"detect_text_language","text":"<pre><code>detect_text_language(text: str) -&gt; Optional[str]\n</code></pre> <p>Detect language of extracted text.</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.BaseOCRMapper.from_standard_language","title":"from_standard_language","text":"<pre><code>from_standard_language(standard_language: str) -&gt; str\n</code></pre> <p>Convert standard ISO 639-1 language code to engine-specific format.</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.BaseOCRMapper.get_supported_languages","title":"get_supported_languages","text":"<pre><code>get_supported_languages() -&gt; List[str]\n</code></pre> <p>Get list of supported languages for this engine.</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.BaseOCRMapper.normalize_bbox","title":"normalize_bbox","text":"<pre><code>normalize_bbox(bbox: List[float], img_width: int, img_height: int) -&gt; List[float]\n</code></pre> <p>Normalize bounding box coordinates to absolute pixel values.</p>"},{"location":"api_reference/tasks/ocr.html#omnidocs.tasks.ocr_extraction.base.BaseOCRMapper.to_standard_language","title":"to_standard_language","text":"<pre><code>to_standard_language(engine_language: str) -&gt; str\n</code></pre> <p>Convert engine-specific language code to standard ISO 639-1.</p>"},{"location":"api_reference/tasks/ocr.html#related-resources","title":"Related Resources","text":"<ul> <li>OCR Overview</li> <li>EasyOCR Tutorial</li> <li>PaddleOCR Tutorial</li> <li>Surya OCR Tutorial</li> <li>Tesseract Tutorial</li> <li>Core Classes</li> <li>Utilities</li> </ul>"},{"location":"api_reference/tasks/table_extraction.html","title":"\ud83d\udcca Table Extraction","text":"<p>This section documents the API for table extraction tasks, providing various extractors to retrieve tabular data from documents.</p>"},{"location":"api_reference/tasks/table_extraction.html#overview","title":"Overview","text":"<p>Table extraction in OmniDocs focuses on accurately identifying and extracting structured data from tables within PDFs and images. This is crucial for converting unstructured document data into usable formats like DataFrames.</p>"},{"location":"api_reference/tasks/table_extraction.html#available-extractors","title":"Available Extractors","text":""},{"location":"api_reference/tasks/table_extraction.html#camelotextractor","title":"CamelotExtractor","text":"<p>Accurate table extraction from PDFs, supporting both lattice (for tables with lines) and stream (for tables without lines) modes.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.extractors.camelot.CamelotExtractor","title":"omnidocs.tasks.table_extraction.extractors.camelot.CamelotExtractor","text":"<pre><code>CamelotExtractor(device: Optional[str] = None, show_log: bool = False, method: str = 'lattice', pages: str = '1', flavor: str = 'lattice', **kwargs)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>Camelot based table extraction implementation.</p> <p>TODO: Bbox coordinate transformation from PDF to image space is still broken. Current issues: - Coordinate transformation accuracy issues between PDF points and image pixels - Cell bbox estimation doesn't account for actual cell sizes from Camelot - Need better integration with Camelot's internal coordinate data - Grid-based estimation fallback is inaccurate for real table layouts</p> <p>Initialize Camelot Table Extractor.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.extractors.camelot.CamelotExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; TableOutput\n</code></pre> <p>Extract tables using Camelot.</p>"},{"location":"api_reference/tasks/table_extraction.html#usage-example","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.table_extraction.extractors.camelot import CamelotExtractor\n\nextractor = CamelotExtractor(flavor='lattice') # or 'stream'\nresult = extractor.extract(\"document.pdf\")\nfor i, table in enumerate(result.tables):\n    print(f\"Table {i+1} shape: {table.df.shape}\")\n    print(table.df.head())\n</code></pre>"},{"location":"api_reference/tasks/table_extraction.html#pdfplumbertableextractor","title":"PDFPlumberTableExtractor","text":"<p>A lightweight and fast PDF table extraction library.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.extractors.pdfplumber.PDFPlumberExtractor","title":"omnidocs.tasks.table_extraction.extractors.pdfplumber.PDFPlumberExtractor","text":"<pre><code>PDFPlumberExtractor(device: Optional[str] = None, show_log: bool = False, table_settings: Optional[Dict] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>PDFPlumber based table extraction implementation.</p> <p>Initialize PDFPlumber Table Extractor.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.extractors.pdfplumber.PDFPlumberExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; TableOutput\n</code></pre> <p>Extract tables using PDFPlumber.</p>"},{"location":"api_reference/tasks/table_extraction.html#usage-example_1","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.table_extraction.extractors.pdfplumber import PDFPlumberExtractor\n\nextractor = PDFPlumberExtractor()\nresult = extractor.extract(\"document.pdf\")\nfor i, table in enumerate(result.tables):\n    print(f\"Table {i+1} shape: {table.df.shape}\")\n</code></pre>"},{"location":"api_reference/tasks/table_extraction.html#ppstructuretableextractor","title":"PPStructureTableExtractor","text":"<p>An OCR tool that supports multiple languages and provides table recognition capabilities.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.extractors.ppstructure.PPStructureExtractor","title":"omnidocs.tasks.table_extraction.extractors.ppstructure.PPStructureExtractor","text":"<pre><code>PPStructureExtractor(device: Optional[str] = None, show_log: bool = False, languages: Optional[List[str]] = None, use_gpu: bool = True, layout_model: Optional[str] = None, table_model: Optional[str] = None, return_ocr_result_in_table: bool = True, **kwargs)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>PaddleOCR PPStructure based table extraction implementation.</p> <p>Initialize PPStructure Table Extractor.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.extractors.ppstructure.PPStructureExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; TableOutput\n</code></pre> <p>Extract tables using PPStructure.</p>"},{"location":"api_reference/tasks/table_extraction.html#usage-example_2","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.table_extraction.extractors.ppstructure import PPStructureExtractor\n\nextractor = PPStructureExtractor()\nresult = extractor.extract(\"image.png\")\nfor i, table in enumerate(result.tables):\n    print(f\"Table {i+1} shape: {table.df.shape}\")\n</code></pre>"},{"location":"api_reference/tasks/table_extraction.html#suryatableextractor","title":"SuryaTableExtractor","text":"<p>Deep learning-based table structure recognition, part of the Surya library.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.extractors.surya_table.SuryaTableExtractor","title":"omnidocs.tasks.table_extraction.extractors.surya_table.SuryaTableExtractor","text":"<pre><code>SuryaTableExtractor(device: Optional[str] = None, show_log: bool = False, model_path: Optional[Union[str, Path]] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>Surya-based table extraction implementation.</p> <p>Initialize Surya Table Extractor.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.extractors.surya_table.SuryaTableExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; TableOutput\n</code></pre> <p>Extract tables using Surya.</p>"},{"location":"api_reference/tasks/table_extraction.html#usage-example_3","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.table_extraction.extractors.surya_table import SuryaTableExtractor\n\nextractor = SuryaTableExtractor()\nresult = extractor.extract(\"document.pdf\")\nfor i, table in enumerate(result.tables):\n    print(f\"Table {i+1} shape: {table.df.shape}\")\n</code></pre>"},{"location":"api_reference/tasks/table_extraction.html#tabletransformerextractor","title":"TableTransformerExtractor","text":"<p>A transformer-based model for table detection and extraction.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.extractors.table_transformer.TableTransformerExtractor","title":"omnidocs.tasks.table_extraction.extractors.table_transformer.TableTransformerExtractor","text":"<pre><code>TableTransformerExtractor(device: Optional[str] = None, show_log: bool = False, detection_model_path: Optional[str] = None, structure_model_path: Optional[str] = None, detection_threshold: float = 0.7, structure_threshold: float = 0.7, **kwargs)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>Table Transformer based table extraction implementation.</p> <p>Initialize Table Transformer Extractor.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.extractors.table_transformer.TableTransformerExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; TableOutput\n</code></pre> <p>Extract tables using Table Transformer.</p>"},{"location":"api_reference/tasks/table_extraction.html#usage-example_4","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.table_extraction.extractors.table_transformer import TableTransformerExtractor\n\nextractor = TableTransformerExtractor()\nresult = extractor.extract(\"image.png\")\nfor i, table in enumerate(result.tables):\n    print(f\"Table {i+1} shape: {table.df.shape}\")\n</code></pre>"},{"location":"api_reference/tasks/table_extraction.html#tableformerextractor","title":"TableFormerExtractor","text":"<p>An advanced deep learning model for table structure parsing.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.extractors.tableformer.TableFormerExtractor","title":"omnidocs.tasks.table_extraction.extractors.tableformer.TableFormerExtractor","text":"<pre><code>TableFormerExtractor(device: Optional[str] = None, show_log: bool = False, model_path: Optional[str] = None, model_type: str = 'structure', confidence_threshold: float = 0.7, max_size: int = 1000, **kwargs)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>TableFormer based table extraction implementation.</p> <p>Initialize TableFormer Extractor.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.extractors.tableformer.TableFormerExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; TableOutput\n</code></pre> <p>Extract tables using TableFormer.</p>"},{"location":"api_reference/tasks/table_extraction.html#usage-example_5","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.table_extraction.extractors.tableformer import TableFormerExtractor\n\nextractor = TableFormerExtractor()\nresult = extractor.extract(\"document.pdf\")\nfor i, table in enumerate(result.tables):\n    print(f\"Table {i+1} shape: {table.df.shape}\")\n</code></pre>"},{"location":"api_reference/tasks/table_extraction.html#tabulaextractor","title":"TabulaExtractor","text":"<p>A Java-based tool for extracting tables from PDFs. Requires Java runtime installed.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.extractors.tabula.TabulaExtractor","title":"omnidocs.tasks.table_extraction.extractors.tabula.TabulaExtractor","text":"<pre><code>TabulaExtractor(device: Optional[str] = None, show_log: bool = False, method: str = 'lattice', pages: Optional[Union[str, List[int]]] = None, multiple_tables: bool = True, guess: bool = True, area: Optional[List[float]] = None, columns: Optional[List[float]] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>Tabula based table extraction implementation.</p> <p>Initialize Tabula Table Extractor.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.extractors.tabula.TabulaExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; TableOutput\n</code></pre> <p>Extract tables using Tabula.</p>"},{"location":"api_reference/tasks/table_extraction.html#usage-example_6","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.table_extraction.extractors.tabula import TabulaExtractor\n\nextractor = TabulaExtractor()\nresult = extractor.extract(\"document.pdf\")\nfor i, table in enumerate(result.tables):\n    print(f\"Table {i+1} shape: {table.df.shape}\")\n</code></pre>"},{"location":"api_reference/tasks/table_extraction.html#tableoutput","title":"TableOutput","text":"<p>The standardized output format for table extraction results.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.base.TableOutput","title":"omnidocs.tasks.table_extraction.base.TableOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for table extraction results.</p> <p>Attributes:</p> Name Type Description <code>tables</code> <code>List[Table]</code> <p>List of extracted tables</p> <code>source_img_size</code> <code>Optional[Tuple[int, int]]</code> <p>Original image dimensions (width, height)</p> <code>processing_time</code> <code>Optional[float]</code> <p>Time taken for table extraction</p> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata from the extraction engine</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.base.TableOutput.get_tables_by_confidence","title":"get_tables_by_confidence","text":"<pre><code>get_tables_by_confidence(min_confidence: float = 0.5) -&gt; List[Table]\n</code></pre> <p>Filter tables by minimum confidence threshold.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.base.TableOutput.save_json","title":"save_json","text":"<pre><code>save_json(output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save output to JSON file.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.base.TableOutput.save_tables_as_csv","title":"save_tables_as_csv","text":"<pre><code>save_tables_as_csv(output_dir: Union[str, Path]) -&gt; List[Path]\n</code></pre> <p>Save all tables as separate CSV files.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.base.TableOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p>"},{"location":"api_reference/tasks/table_extraction.html#key-properties","title":"Key Properties","text":"<ul> <li><code>tables</code> (List[Table]): List of extracted tables.</li> <li><code>source_file</code> (str): Path to the processed file.</li> <li><code>processing_time</code> (Optional[float]): Time taken for extraction.</li> </ul>"},{"location":"api_reference/tasks/table_extraction.html#key-methods","title":"Key Methods","text":"<ul> <li><code>save_json(output_path)</code>: Save results metadata to a JSON file.</li> <li><code>save_tables_as_csv(output_dir)</code>: Save all extracted tables as individual CSV files.</li> <li><code>get_tables_by_confidence(min_confidence)</code>: Filter tables by confidence score.</li> </ul>"},{"location":"api_reference/tasks/table_extraction.html#table","title":"Table","text":"<p>Represents a single extracted table.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.base.Table","title":"omnidocs.tasks.table_extraction.base.Table","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for extracted table.</p> <p>Attributes:</p> Name Type Description <code>cells</code> <code>List[TableCell]</code> <p>List of table cells</p> <code>num_rows</code> <code>int</code> <p>Number of rows in the table</p> <code>num_cols</code> <code>int</code> <p>Number of columns in the table</p> <code>bbox</code> <code>Optional[List[float]]</code> <p>Bounding box of the entire table [x1, y1, x2, y2]</p> <code>confidence</code> <code>Optional[float]</code> <p>Overall table detection confidence</p> <code>table_id</code> <code>Optional[str]</code> <p>Optional table identifier</p> <code>caption</code> <code>Optional[str]</code> <p>Optional table caption</p> <code>structure_confidence</code> <code>Optional[float]</code> <p>Confidence score for table structure detection</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.base.Table.to_csv","title":"to_csv","text":"<pre><code>to_csv() -&gt; str\n</code></pre> <p>Convert table to CSV format.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.base.Table.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.base.Table.to_html","title":"to_html","text":"<pre><code>to_html() -&gt; str\n</code></pre> <p>Convert table to HTML format.</p>"},{"location":"api_reference/tasks/table_extraction.html#attributes","title":"Attributes","text":"<ul> <li><code>df</code> (pandas.DataFrame): The extracted table data as a DataFrame.</li> <li><code>bbox</code> (List[float]): Bounding box coordinates of the table.</li> <li><code>page_number</code> (int): The page number where the table is found.</li> <li><code>confidence</code> (Optional[float]): Confidence score of the table extraction.</li> </ul>"},{"location":"api_reference/tasks/table_extraction.html#key-methods_1","title":"Key Methods","text":"<ul> <li><code>to_csv()</code>: Convert the table DataFrame to a CSV string.</li> <li><code>to_html()</code>: Convert the table DataFrame to an HTML string.</li> </ul>"},{"location":"api_reference/tasks/table_extraction.html#basetableextractor","title":"BaseTableExtractor","text":"<p>The abstract base class for all table extraction extractors.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.base.BaseTableExtractor","title":"omnidocs.tasks.table_extraction.base.BaseTableExtractor","text":"<pre><code>BaseTableExtractor(device: Optional[str] = None, show_log: bool = False, engine_name: Optional[str] = None)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for table extraction models.</p> <p>Initialize the table extractor.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run model on ('cuda' or 'cpu')</p> <code>None</code> <code>show_log</code> <code>bool</code> <p>Whether to show detailed logs</p> <code>False</code> <code>engine_name</code> <code>Optional[str]</code> <p>Name of the table extraction engine</p> <code>None</code>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.base.BaseTableExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; TableOutput\n</code></pre> <p>Extract tables from input image.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path, Image]</code> <p>Path to input image or image data</p> required <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>TableOutput</code> <p>TableOutput containing extracted tables</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.base.BaseTableExtractor.preprocess_input","title":"preprocess_input","text":"<pre><code>preprocess_input(input_path: Union[str, Path, Image, ndarray]) -&gt; List[Image.Image]\n</code></pre> <p>Convert input to list of PIL Images.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path, Image, ndarray]</code> <p>Input image path or image data</p> required <p>Returns:</p> Type Description <code>List[Image]</code> <p>List of PIL Images</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.base.BaseTableExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: Any, img_size: Tuple[int, int]) -&gt; TableOutput\n</code></pre> <p>Convert raw table extraction output to standardized TableOutput format.</p> <p>Parameters:</p> Name Type Description Default <code>raw_output</code> <code>Any</code> <p>Raw output from table extraction engine</p> required <code>img_size</code> <code>Tuple[int, int]</code> <p>Original image size (width, height)</p> required <p>Returns:</p> Type Description <code>TableOutput</code> <p>Standardized TableOutput object</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.base.BaseTableExtractor.visualize","title":"visualize","text":"<pre><code>visualize(table_result: TableOutput, image_path: Union[str, Path, Image], output_path: str = 'visualized_tables.png', table_color: str = 'red', cell_color: str = 'blue', box_width: int = 2, show_text: bool = False, text_color: str = 'green', font_size: int = 12, show_table_ids: bool = True) -&gt; None\n</code></pre> <p>Visualize table extraction results by drawing bounding boxes on the original image.</p> <p>This method allows users to easily see which extractor is working better by visualizing the detected tables and cells with bounding boxes.</p> <p>Parameters:</p> Name Type Description Default <code>table_result</code> <code>TableOutput</code> <p>TableOutput containing extracted tables</p> required <code>image_path</code> <code>Union[str, Path, Image]</code> <p>Path to original image or PIL Image object</p> required <code>output_path</code> <code>str</code> <p>Path to save the annotated image</p> <code>'visualized_tables.png'</code> <code>table_color</code> <code>str</code> <p>Color for table bounding boxes</p> <code>'red'</code> <code>cell_color</code> <code>str</code> <p>Color for cell bounding boxes</p> <code>'blue'</code> <code>box_width</code> <code>int</code> <p>Width of bounding box lines</p> <code>2</code> <code>show_text</code> <code>bool</code> <p>Whether to overlay cell text</p> <code>False</code> <code>text_color</code> <code>str</code> <p>Color for text overlay</p> <code>'green'</code> <code>font_size</code> <code>int</code> <p>Font size for text overlay</p> <code>12</code> <code>show_table_ids</code> <code>bool</code> <p>Whether to show table IDs</p> <code>True</code>"},{"location":"api_reference/tasks/table_extraction.html#tablemapper","title":"TableMapper","text":"<p>Handles mapping of table-related labels and normalization of bounding boxes.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.base.BaseTableMapper","title":"omnidocs.tasks.table_extraction.base.BaseTableMapper","text":"<pre><code>BaseTableMapper(engine_name: str)\n</code></pre> <p>Base class for mapping table extraction engine-specific outputs to standardized format.</p> <p>Initialize mapper for specific table extraction engine.</p> <p>Parameters:</p> Name Type Description Default <code>engine_name</code> <code>str</code> <p>Name of the table extraction engine</p> required"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.base.BaseTableMapper.detect_header_rows","title":"detect_header_rows","text":"<pre><code>detect_header_rows(cells: List[TableCell]) -&gt; List[TableCell]\n</code></pre> <p>Detect and mark header cells based on position and formatting.</p>"},{"location":"api_reference/tasks/table_extraction.html#omnidocs.tasks.table_extraction.base.BaseTableMapper.normalize_bbox","title":"normalize_bbox","text":"<pre><code>normalize_bbox(bbox: List[float], img_width: int, img_height: int) -&gt; List[float]\n</code></pre> <p>Normalize bounding box coordinates to absolute pixel values.</p>"},{"location":"api_reference/tasks/table_extraction.html#related-resources","title":"Related Resources","text":"<ul> <li>Table Extraction Overview</li> <li>Camelot Tutorial</li> <li>PDFPlumber Tutorial</li> <li>Surya Table Tutorial</li> <li>Tabula Tutorial</li> <li>TableTransformer Tutorial</li> <li>TableFormer Tutorial</li> <li>Core Classes</li> <li>Utilities</li> </ul>"},{"location":"api_reference/tasks/text_extraction.html","title":"\ud83d\udcdd Text Extraction","text":"<p>This section documents the API for text extraction tasks, providing various extractors to retrieve textual content from documents.</p>"},{"location":"api_reference/tasks/text_extraction.html#overview","title":"Overview","text":"<p>Text extraction in OmniDocs focuses on accurately pulling out text from different document formats (PDFs, images, etc.), often preserving layout and structural information. This is a fundamental step for many document understanding applications.</p>"},{"location":"api_reference/tasks/text_extraction.html#available-extractors","title":"Available Extractors","text":""},{"location":"api_reference/tasks/text_extraction.html#doclingparseextractor","title":"DoclingParseExtractor","text":"<p>A unified parsing library for PDF, DOCX, PPTX, HTML, and MD, with OCR and structure capabilities.</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.extractors.docling_parse.DoclingTextExtractor","title":"omnidocs.tasks.text_extraction.extractors.docling_parse.DoclingTextExtractor","text":"<pre><code>DoclingTextExtractor(device: Optional[str] = None, show_log: bool = False, extract_images: bool = False, ocr_enabled: bool = True, table_structure_enabled: bool = True)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Text extractor using Docling.</p> <p>Initialize Docling text extractor.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run on (not used for Docling)</p> <code>None</code> <code>show_log</code> <code>bool</code> <p>Whether to show detailed logs</p> <code>False</code> <code>extract_images</code> <code>bool</code> <p>Whether to extract images alongside text</p> <code>False</code> <code>ocr_enabled</code> <code>bool</code> <p>Whether to enable OCR for scanned documents</p> <code>True</code> <code>table_structure_enabled</code> <code>bool</code> <p>Whether to enable table structure detection</p> <code>True</code>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.extractors.docling_parse.DoclingTextExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path], **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text from document using Docling.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input document</p> required <code>**kwargs</code> <p>Additional parameters (ignored for Docling)</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>TextOutput containing extracted text</p>"},{"location":"api_reference/tasks/text_extraction.html#usage-example","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.text_extraction.extractors.docling_parse import DoclingTextExtractor\n\nextractor = DoclingTextExtractor()\nresult = extractor.extract(\"document.pdf\")\nprint(f\"Extracted text: {result.full_text[:200]}...\")\n</code></pre>"},{"location":"api_reference/tasks/text_extraction.html#pdfplumbertextextractor","title":"PDFPlumberTextExtractor","text":"<p>A library for extracting text and tables from PDFs with layout details.</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.extractors.pdfplumber.PdfplumberTextExtractor","title":"omnidocs.tasks.text_extraction.extractors.pdfplumber.PdfplumberTextExtractor","text":"<pre><code>PdfplumberTextExtractor(device: Optional[str] = None, show_log: bool = False, extract_images: bool = False, extract_tables: bool = False, use_layout: bool = True)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Text extractor using pdfplumber.</p> <p>Initialize pdfplumber text extractor.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run on (not used for pdfplumber)</p> <code>None</code> <code>show_log</code> <code>bool</code> <p>Whether to show detailed logs</p> <code>False</code> <code>extract_images</code> <code>bool</code> <p>Whether to extract images alongside text</p> <code>False</code> <code>extract_tables</code> <code>bool</code> <p>Whether to extract tables</p> <code>False</code> <code>use_layout</code> <code>bool</code> <p>Whether to use layout information for text extraction</p> <code>True</code>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.extractors.pdfplumber.PdfplumberTextExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path], **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text from PDF using pdfplumber.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input PDF</p> required <code>**kwargs</code> <p>Additional parameters (ignored for pdfplumber)</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>TextOutput containing extracted text</p>"},{"location":"api_reference/tasks/text_extraction.html#usage-example_1","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.text_extraction.extractors.pdfplumber import PdfplumberTextExtractor\n\nextractor = PdfplumberTextExtractor()\nresult = extractor.extract(\"document.pdf\")\nprint(f\"Extracted text: {result.full_text[:200]}...\")\n</code></pre>"},{"location":"api_reference/tasks/text_extraction.html#pdftextextractor","title":"PDFTextExtractor","text":"<p>A simple, fast PDF text extraction with layout options.</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.extractors.pdftext.PdftextTextExtractor","title":"omnidocs.tasks.text_extraction.extractors.pdftext.PdftextTextExtractor","text":"<pre><code>PdftextTextExtractor(device: Optional[str] = None, show_log: bool = False, extract_images: bool = False, keep_layout: bool = False, physical_layout: bool = False)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Text extractor using pdftext.</p> <p>Initialize pdftext text extractor.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run on (not used for pdftext)</p> <code>None</code> <code>show_log</code> <code>bool</code> <p>Whether to show detailed logs</p> <code>False</code> <code>extract_images</code> <code>bool</code> <p>Whether to extract images alongside text</p> <code>False</code> <code>keep_layout</code> <code>bool</code> <p>Whether to keep original layout formatting</p> <code>False</code> <code>physical_layout</code> <code>bool</code> <p>Whether to use physical layout analysis</p> <code>False</code>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.extractors.pdftext.PdftextTextExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path], **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text from PDF using pdftext.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input PDF</p> required <code>**kwargs</code> <p>Additional parameters (ignored for pdftext)</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>TextOutput containing extracted text</p>"},{"location":"api_reference/tasks/text_extraction.html#usage-example_2","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.text_extraction.extractors.pdftext import PdftextTextExtractor\n\nextractor = PdftextTextExtractor()\nresult = extractor.extract(\"document.pdf\")\nprint(f\"Extracted text: {result.full_text[:200]}...\")\n</code></pre>"},{"location":"api_reference/tasks/text_extraction.html#pymupdftextextractor","title":"PyMuPDFTextExtractor","text":"<p>A fast, multi-format text extraction library with layout and font information.</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.extractors.pymupdf.PyMuPDFTextExtractor","title":"omnidocs.tasks.text_extraction.extractors.pymupdf.PyMuPDFTextExtractor","text":"<pre><code>PyMuPDFTextExtractor(device: Optional[str] = None, show_log: bool = False, extract_images: bool = False, extract_tables: bool = False, flags: int = 0, clip: Optional[tuple] = None)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Text extractor using PyMuPDF (fitz).</p> <p>Initialize PyMuPDF text extractor.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run on (not used for PyMuPDF)</p> <code>None</code> <code>show_log</code> <code>bool</code> <p>Whether to show detailed logs</p> <code>False</code> <code>extract_images</code> <code>bool</code> <p>Whether to extract images alongside text</p> <code>False</code> <code>extract_tables</code> <code>bool</code> <p>Whether to extract tables</p> <code>False</code> <code>flags</code> <code>int</code> <p>Text extraction flags (fitz.TEXT_PRESERVE_LIGATURES, etc.)</p> <code>0</code> <code>clip</code> <code>Optional[tuple]</code> <p>Optional clipping rectangle (x0, y0, x1, y1)</p> <code>None</code>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.extractors.pymupdf.PyMuPDFTextExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path], use_layout: bool = True, **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text from document using PyMuPDF.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input document</p> required <code>use_layout</code> <code>bool</code> <p>Whether to use layout information for extraction</p> <code>True</code> <code>**kwargs</code> <p>Additional parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>TextOutput containing extracted text</p>"},{"location":"api_reference/tasks/text_extraction.html#usage-example_3","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.text_extraction.extractors.pymupdf import PyMuPDFTextExtractor\n\nextractor = PyMuPDFTextExtractor()\nresult = extractor.extract(\"document.pdf\")\nprint(f\"Extracted text: {result.full_text[:200]}...\")\n</code></pre>"},{"location":"api_reference/tasks/text_extraction.html#pypdf2textextractor","title":"PyPDF2TextExtractor","text":"<p>A pure Python library for extracting text from PDFs, supporting encrypted PDFs and form fields.</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.extractors.pypdf2.PyPDF2TextExtractor","title":"omnidocs.tasks.text_extraction.extractors.pypdf2.PyPDF2TextExtractor","text":"<pre><code>PyPDF2TextExtractor(device: Optional[str] = None, show_log: bool = False, extract_images: bool = False, ignore_images: bool = True, extract_forms: bool = False)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Text extractor using PyPDF2.</p> <p>Initialize PyPDF2 text extractor.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run on (not used for PyPDF2)</p> <code>None</code> <code>show_log</code> <code>bool</code> <p>Whether to show detailed logs</p> <code>False</code> <code>extract_images</code> <code>bool</code> <p>Whether to extract images alongside text</p> <code>False</code> <code>ignore_images</code> <code>bool</code> <p>Whether to ignore images during text extraction</p> <code>True</code> <code>extract_forms</code> <code>bool</code> <p>Whether to extract form fields</p> <code>False</code>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.extractors.pypdf2.PyPDF2TextExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path], password: Optional[str] = None, **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text from PDF using PyPDF2.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input PDF</p> required <code>password</code> <code>Optional[str]</code> <p>Optional password for encrypted PDFs</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters (ignored for PyPDF2)</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>TextOutput containing extracted text</p>"},{"location":"api_reference/tasks/text_extraction.html#usage-example_4","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.text_extraction.extractors.pypdf2 import PyPDF2TextExtractor\n\nextractor = PyPDF2TextExtractor()\nresult = extractor.extract(\"document.pdf\")\nprint(f\"Extracted text: {result.full_text[:200]}...\")\n</code></pre>"},{"location":"api_reference/tasks/text_extraction.html#suryatextextractor","title":"SuryaTextExtractor","text":"<p>Surya-based text extraction for images and documents.</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.extractors.surya_text.SuryaTextExtractor","title":"omnidocs.tasks.text_extraction.extractors.surya_text.SuryaTextExtractor","text":"<pre><code>SuryaTextExtractor(device: Optional[str] = None, show_log: bool = False, extract_images: bool = False, model_path: Optional[Union[str, Path]] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Surya-based text extraction implementation for images and documents.</p> <p>Initialize Surya Text Extractor.</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.extractors.surya_text.SuryaTextExtractor.extract","title":"extract","text":"<pre><code>extract(input_path: Union[str, Path, Image], **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text using Surya OCR.</p>"},{"location":"api_reference/tasks/text_extraction.html#usage-example_5","title":"Usage Example","text":"<pre><code>from omnidocs.tasks.text_extraction.extractors.surya_text import SuryaTextExtractor\n\nextractor = SuryaTextExtractor()\nresult = extractor.extract(\"image.png\")\nprint(f\"Extracted text: {result.full_text[:200]}...\")\n</code></pre>"},{"location":"api_reference/tasks/text_extraction.html#textoutput","title":"TextOutput","text":"<p>The standardized output format for text extraction results.</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.base.TextOutput","title":"omnidocs.tasks.text_extraction.base.TextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for text extraction results.</p> <p>Attributes:</p> Name Type Description <code>text_blocks</code> <code>List[TextBlock]</code> <p>List of extracted text blocks</p> <code>full_text</code> <code>str</code> <p>Combined text from all blocks</p> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata from extraction</p> <code>source_info</code> <code>Optional[Dict[str, Any]]</code> <p>Information about the source document</p> <code>processing_time</code> <code>Optional[float]</code> <p>Time taken for text extraction</p> <code>page_count</code> <code>int</code> <p>Number of pages in the document</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.base.TextOutput.get_sorted_by_reading_order","title":"get_sorted_by_reading_order","text":"<pre><code>get_sorted_by_reading_order() -&gt; List[TextBlock]\n</code></pre> <p>Get text blocks sorted by reading order.</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.base.TextOutput.get_text_by_confidence","title":"get_text_by_confidence","text":"<pre><code>get_text_by_confidence(min_confidence: float = 0.5) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by minimum confidence threshold.</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.base.TextOutput.get_text_by_page","title":"get_text_by_page","text":"<pre><code>get_text_by_page(page_num: int) -&gt; List[TextBlock]\n</code></pre> <p>Get text blocks from a specific page.</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.base.TextOutput.get_text_by_type","title":"get_text_by_type","text":"<pre><code>get_text_by_type(block_type: str) -&gt; List[TextBlock]\n</code></pre> <p>Get text blocks of a specific type.</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.base.TextOutput.save_json","title":"save_json","text":"<pre><code>save_json(output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save output to JSON file.</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.base.TextOutput.save_markdown","title":"save_markdown","text":"<pre><code>save_markdown(output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save text as markdown with basic formatting.</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.base.TextOutput.save_text","title":"save_text","text":"<pre><code>save_text(output_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save full text to a text file.</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.base.TextOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p>"},{"location":"api_reference/tasks/text_extraction.html#key-properties","title":"Key Properties","text":"<ul> <li><code>text_blocks</code> (List[TextBlock]): List of extracted text blocks with positions.</li> <li><code>full_text</code> (str): The complete extracted text content.</li> <li><code>source_file</code> (str): Path to the processed file.</li> </ul>"},{"location":"api_reference/tasks/text_extraction.html#key-methods","title":"Key Methods","text":"<ul> <li><code>save_json(output_path)</code>: Save results to a JSON file.</li> </ul>"},{"location":"api_reference/tasks/text_extraction.html#textblock","title":"TextBlock","text":"<p>Represents a single block of text with its bounding box.</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.base.TextBlock","title":"omnidocs.tasks.text_extraction.base.TextBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Container for individual text block.</p> <p>Attributes:</p> Name Type Description <code>text</code> <code>str</code> <p>Text content</p> <code>bbox</code> <code>Optional[List[float]]</code> <p>Bounding box coordinates [x1, y1, x2, y2]</p> <code>confidence</code> <code>Optional[float]</code> <p>Confidence score for text extraction</p> <code>page_num</code> <code>int</code> <p>Page number (for multi-page documents)</p> <code>block_type</code> <code>Optional[str]</code> <p>Type of text block (paragraph, heading, list, etc.)</p> <code>font_info</code> <code>Optional[Dict[str, Any]]</code> <p>Optional font information</p> <code>reading_order</code> <code>Optional[int]</code> <p>Reading order index</p> <code>language</code> <code>Optional[str]</code> <p>Detected language of the text</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.base.TextBlock.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p>"},{"location":"api_reference/tasks/text_extraction.html#attributes","title":"Attributes","text":"<ul> <li><code>text</code> (str): The text content of the block.</li> <li><code>bbox</code> (List[float]): Bounding box coordinates [x1, y1, x2, y2].</li> <li><code>page_number</code> (int): The page number where the text block is found.</li> </ul>"},{"location":"api_reference/tasks/text_extraction.html#basetextextractor","title":"BaseTextExtractor","text":"<p>The abstract base class for all text extraction extractors.</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.base.BaseTextExtractor","title":"omnidocs.tasks.text_extraction.base.BaseTextExtractor","text":"<pre><code>BaseTextExtractor(device: Optional[str] = None, show_log: bool = False, engine_name: Optional[str] = None, extract_images: bool = False)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Base class for text extraction models.</p> <p>Initialize the text extractor.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>Optional[str]</code> <p>Device to run model on ('cuda' or 'cpu')</p> <code>None</code> <code>show_log</code> <code>bool</code> <p>Whether to show detailed logs</p> <code>False</code> <code>engine_name</code> <code>Optional[str]</code> <p>Name of the text extraction engine</p> <code>None</code> <code>extract_images</code> <code>bool</code> <p>Whether to extract images alongside text</p> <code>False</code>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.base.BaseTextExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(input_path: Union[str, Path], **kwargs) -&gt; TextOutput\n</code></pre> <p>Extract text from input document.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input document</p> required <code>**kwargs</code> <p>Additional model-specific parameters</p> <code>{}</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>TextOutput containing extracted text</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.base.BaseTextExtractor.preprocess_input","title":"preprocess_input","text":"<pre><code>preprocess_input(input_path: Union[str, Path]) -&gt; Any\n</code></pre> <p>Preprocess input document for text extraction.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>Union[str, Path]</code> <p>Path to input document</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Preprocessed document object</p>"},{"location":"api_reference/tasks/text_extraction.html#omnidocs.tasks.text_extraction.base.BaseTextExtractor.postprocess_output","title":"postprocess_output","text":"<pre><code>postprocess_output(raw_output: Any, source_info: Optional[Dict] = None) -&gt; TextOutput\n</code></pre> <p>Convert raw text extraction output to standardized TextOutput format.</p> <p>Parameters:</p> Name Type Description Default <code>raw_output</code> <code>Any</code> <p>Raw output from text extraction engine</p> required <code>source_info</code> <code>Optional[Dict]</code> <p>Optional source document information</p> <code>None</code> <p>Returns:</p> Type Description <code>TextOutput</code> <p>Standardized TextOutput object</p>"},{"location":"api_reference/tasks/text_extraction.html#related-resources","title":"Related Resources","text":"<ul> <li>Text Extraction Overview</li> <li>Text Extractors Tutorial</li> <li>Core Classes</li> <li>Utilities</li> </ul>"},{"location":"getting_started/getting_started.html","title":"Getting Started with OmniDocs","text":"<p>Welcome to OmniDocs! This guide will help you get up and running with powerful document AI extraction in just a few steps.</p>"},{"location":"getting_started/getting_started.html#what-is-omnidocs","title":"\ud83d\ude80 What is OmniDocs?","text":"<p>OmniDocs is a unified Python library for extracting tables, text, math, and OCR data from PDFs and images using state-of-the-art models and classic tools\u2014all with a simple, consistent API.</p>"},{"location":"getting_started/getting_started.html#installation","title":"\ud83d\udee0\ufe0f Installation","text":"<p>Choose your preferred method:</p> <ul> <li>PyPI (Recommended): <pre><code>pip install omnidocs\n</code></pre></li> <li>uv pip (Fastest): <pre><code>uv pip install omnidocs\n</code></pre></li> <li>From Source: <pre><code>git clone https://github.com/adithya-s-k/OmniDocs.git\ncd OmniDocs\npip install . \nor \nuv sync \n</code></pre></li> <li>Conda (if available): <pre><code>conda install -c conda-forge omnidocs\n</code></pre></li> </ul>"},{"location":"getting_started/getting_started.html#setting-up-your-environment","title":"\ud83c\udfd7\ufe0f Setting Up Your Environment","text":"<p>It's best to use a virtual environment: <pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre></p>"},{"location":"getting_started/getting_started.html#quick-example","title":"\ud83d\udcc4 Quick Example","text":"<p>Extract tables from a PDF in just a few lines: <pre><code>from omnidocs.tasks.table_extraction.extractors.camelot import CamelotExtractor\nextractor = CamelotExtractor()\nresults = extractor.extract(\"sample.pdf\")\nprint(results.tables[0].df)  # Print first table as DataFrame\n</code></pre></p>"},{"location":"getting_started/getting_started.html#explore-tutorials","title":"\ud83d\udcda Explore Tutorials","text":"<ul> <li>Table Extraction</li> <li>Text Extraction</li> <li>Math Extraction</li> <li>OCR Extraction</li> </ul>"},{"location":"getting_started/getting_started.html#need-help","title":"\ud83e\uddd1\u200d\ud83d\udcbb Need Help?","text":"<ul> <li>See the API Reference</li> <li>Open an issue on GitHub</li> </ul> <p>Happy Document AI-ing! \ud83c\udf89</p>"},{"location":"getting_started/installation.html","title":"Installation","text":"<p>OmniDocs can be installed in several ways depending on your environment and preferences. Choose the method that works best for you.</p>"},{"location":"getting_started/installation.html#1-install-from-pypi-recommended","title":"1. Install from PyPI (Recommended)","text":"<pre><code>pip install omnidocs\n</code></pre>"},{"location":"getting_started/installation.html#2-install-with-uv-fast-modern-python-package-manager","title":"2. Install with uv (fast, modern Python package manager)","text":"<p>If you have uv installed  (<code>pip install uv</code>) you can:</p> <pre><code>uv pip install omnidocs\n</code></pre> <p>Or, to create a new virtual environment and install:</p> <pre><code>uv venv .venv\nuv pip install omnidocs\n</code></pre> <p>Or, to sync all dependencies from a lock file (if provided):</p> <pre><code>uv sync\n</code></pre>"},{"location":"getting_started/installation.html#3-install-from-source-latest-development-version","title":"3. Install from Source (Latest Development Version)","text":"<pre><code>git clone https://github.com/adithya-s-k/OmniDocs.git\ncd OmniDocs\n# Option 1: pip install (classic)\npip install .\n# Option 2: uv sync (fast, reproducible)\nuv venv .venv\nuv sync\n</code></pre>"},{"location":"getting_started/installation.html#4-optional-create-a-virtual-environment","title":"4. (Optional) Create a Virtual Environment","text":"<p>It's recommended to use a virtual environment to avoid dependency conflicts:</p>"},{"location":"getting_started/installation.html#python-m-venv-venv-source-venvbinactivate-on-windows-venvscriptsactivate","title":"<pre><code>python -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n</code></pre>","text":"<p>For more details, see the GitHub repository or the Getting Started guide.</p>"},{"location":"getting_started/ocr_test.html","title":"Ocr test","text":"In\u00a0[\u00a0]: Copied! <pre>def test_ocr_extraction():\n    from omnidocs.tasks.ocr_extraction.extractors.paddle import PaddleOCRExtractor\n    from omnidocs.tasks.ocr_extraction.extractors.tesseract_ocr import TesseractOCRExtractor\n    from omnidocs.tasks.ocr_extraction.extractors.easy_ocr import EasyOCRExtractor\n    from omnidocs.tasks.ocr_extraction.extractors.surya_ocr import SuryaOCRExtractor\n    from pathlib import Path\n    extractors = [PaddleOCRExtractor, TesseractOCRExtractor, EasyOCRExtractor, SuryaOCRExtractor]\n    image_path = Path(\"Omnidocs\") / \"tests\" / \"ocr_extraction\" / \"assets\" / \"invoice.jpg\"\n    \n    for extractor_cls in extractors:\n        print(f\"\\nTesting {extractor_cls.__name__}\")\n        print(\"-\" * 40)\n        \n        try:\n            extractor = extractor_cls()\n            result = extractor.extract(image_path)\n            print(f\"Text length: {len(result.full_text)} chars\")\n        \n            vis_path = f\"visualized_{extractor_cls.__name__}.png\"\n            extractor.visualize(result, image_path, vis_path)\n\n\n            #load and visualize, if already saved as json\n            #extractor.visualize_from_json(\"image.jpg\", \"results.json\", \"viz.png\")\n\n\n            # with custom styling\n            extractor.visualize(\n                result, \n                image_path, \n                f\"styled_{extractor_cls.__name__}.png\",\n                box_color='green',\n                box_width=3,\n                show_text=True,\n                text_color='red'\n            )\n            \n            print(\"SUCCESS\")\n        except Exception as e:\n            print(f\"ERROR: {e}\")\n</pre> def test_ocr_extraction():     from omnidocs.tasks.ocr_extraction.extractors.paddle import PaddleOCRExtractor     from omnidocs.tasks.ocr_extraction.extractors.tesseract_ocr import TesseractOCRExtractor     from omnidocs.tasks.ocr_extraction.extractors.easy_ocr import EasyOCRExtractor     from omnidocs.tasks.ocr_extraction.extractors.surya_ocr import SuryaOCRExtractor     from pathlib import Path     extractors = [PaddleOCRExtractor, TesseractOCRExtractor, EasyOCRExtractor, SuryaOCRExtractor]     image_path = Path(\"Omnidocs\") / \"tests\" / \"ocr_extraction\" / \"assets\" / \"invoice.jpg\"          for extractor_cls in extractors:         print(f\"\\nTesting {extractor_cls.__name__}\")         print(\"-\" * 40)                  try:             extractor = extractor_cls()             result = extractor.extract(image_path)             print(f\"Text length: {len(result.full_text)} chars\")                      vis_path = f\"visualized_{extractor_cls.__name__}.png\"             extractor.visualize(result, image_path, vis_path)               #load and visualize, if already saved as json             #extractor.visualize_from_json(\"image.jpg\", \"results.json\", \"viz.png\")               # with custom styling             extractor.visualize(                 result,                  image_path,                  f\"styled_{extractor_cls.__name__}.png\",                 box_color='green',                 box_width=3,                 show_text=True,                 text_color='red'             )                          print(\"SUCCESS\")         except Exception as e:             print(f\"ERROR: {e}\") In\u00a0[\u00a0]: Copied! <pre>test_ocr_extraction()\n</pre> test_ocr_extraction()"},{"location":"getting_started/quickstart.html","title":"Quick Start Guide","text":"<p>Get up and running with OmniDocs in minutes! This guide will walk you through installation and your first document processing tasks.</p>"},{"location":"getting_started/quickstart.html#installation","title":"\ud83d\ude80 Installation","text":""},{"location":"getting_started/quickstart.html#option-1-pypi-recommended","title":"Option 1: PyPI (Recommended)","text":"<pre><code>pip install omnidocs\n</code></pre>"},{"location":"getting_started/quickstart.html#option-2-from-source","title":"Option 2: From Source","text":"<pre><code>git clone https://github.com/adithya-s-k/OmniDocs.git\ncd OmniDocs\npip install -e .\n</code></pre>"},{"location":"getting_started/quickstart.html#option-3-with-uv-fastest","title":"Option 3: With uv (Fastest)","text":"<pre><code>uv pip install omnidocs\n</code></pre>"},{"location":"getting_started/quickstart.html#your-first-document-processing","title":"\ud83c\udfaf Your First Document Processing","text":""},{"location":"getting_started/quickstart.html#1-extract-text-with-ocr","title":"1. Extract Text with OCR","text":"<pre><code>from omnidocs.tasks.ocr_extraction.extractors.easy_ocr import EasyOCRExtractor\n\n# Initialize extractor\nextractor = EasyOCRExtractor(languages=['en'])\n\n# Extract text from image\nresult = extractor.extract(\"path/to/your/image.png\")\n\n# Print extracted text\nprint(\"Extracted Text:\")\nprint(result.full_text)\n\n# Access individual text regions\nfor text_region in result.texts:\n    print(f\"Text: {text_region.text}\")\n    print(f\"Confidence: {text_region.confidence:.2f}\")\n    print(f\"Bounding Box: {text_region.bbox}\")\n</code></pre>"},{"location":"getting_started/quickstart.html#2-extract-tables-from-pdf","title":"2. Extract Tables from PDF","text":"<pre><code>from omnidocs.tasks.table_extraction.extractors.camelot import CamelotExtractor\n\n# Initialize table extractor\nextractor = CamelotExtractor()\n\n# Extract tables\nresult = extractor.extract(\"path/to/your/document.pdf\")\n\n# Print number of tables found\nprint(f\"Found {len(result.tables)} tables\")\n\n# Access first table as DataFrame\nif result.tables:\n    first_table = result.tables[0]\n    print(\"First table:\")\n    print(first_table.df.head())\n\n    # Save as CSV\n    first_table.df.to_csv(\"extracted_table.csv\", index=False)\n</code></pre>"},{"location":"getting_started/quickstart.html#3-extract-mathematical-expressions","title":"3. Extract Mathematical Expressions","text":"<pre><code>from omnidocs.tasks.math_expression_extraction.extractors.nougat import NougatExtractor\n\n# Initialize math extractor\nextractor = NougatExtractor()\n\n# Extract math expressions\nresult = extractor.extract(\"path/to/academic/paper.pdf\")\n\n# Print extracted LaTeX\nprint(\"Extracted LaTeX:\")\nprint(result.full_text)\n</code></pre>"},{"location":"getting_started/quickstart.html#4-comprehensive-text-extraction","title":"4. Comprehensive Text Extraction","text":"<pre><code>from omnidocs.tasks.text_extraction.extractors.pymupdf import PyMuPDFExtractor\n\n# Initialize text extractor\nextractor = PyMuPDFExtractor()\n\n# Extract text with layout information\nresult = extractor.extract(\"path/to/document.pdf\")\n\n# Print extracted text\nprint(\"Document Text:\")\nprint(result.full_text)\n\n# Access text blocks with positions\nfor text_block in result.text_blocks:\n    print(f\"Block: {text_block.text[:50]}...\")\n    print(f\"Position: {text_block.bbox}\")\n</code></pre>"},{"location":"getting_started/quickstart.html#switching-between-extractors","title":"\ud83d\udd04 Switching Between Extractors","text":"<p>One of OmniDocs' key features is the ability to easily switch between different extractors:</p> <pre><code># Try different OCR engines\nfrom omnidocs.tasks.ocr_extraction.extractors import (\n    EasyOCRExtractor, TesseractOCRExtractor, PaddleOCRExtractor\n)\n\nextractors = [\n    EasyOCRExtractor(languages=['en']),\n    TesseractOCRExtractor(languages=['eng']),\n    PaddleOCRExtractor(languages=['en'])\n]\n\nimage_path = \"path/to/image.png\"\n\nfor extractor in extractors:\n    result = extractor.extract(image_path)\n    print(f\"{extractor.__class__.__name__}: {result.full_text[:100]}...\")\n</code></pre>"},{"location":"getting_started/quickstart.html#batch-processing","title":"\ud83d\udcca Batch Processing","text":"<p>Process multiple files efficiently:</p> <pre><code>import os\nfrom omnidocs.tasks.table_extraction.extractors.camelot import CamelotExtractor\n\nextractor = CamelotExtractor()\npdf_folder = \"path/to/pdf/folder\"\n\nresults = {}\nfor filename in os.listdir(pdf_folder):\n    if filename.lower().endswith('.pdf'):\n        file_path = os.path.join(pdf_folder, filename)\n        try:\n            result = extractor.extract(file_path)\n            results[filename] = len(result.tables)\n            print(f\"\u2705 {filename}: {len(result.tables)} tables\")\n        except Exception as e:\n            print(f\"\u274c {filename}: Error - {e}\")\n\nprint(f\"\\nProcessed {len(results)} files successfully\")\n</code></pre>"},{"location":"getting_started/quickstart.html#visualization","title":"\ud83c\udfa8 Visualization","text":"<p>Visualize extraction results:</p> <pre><code>from omnidocs.tasks.ocr_extraction.extractors.easy_ocr import EasyOCRExtractor\n\nextractor = EasyOCRExtractor(languages=['en'])\nresult = extractor.extract(\"path/to/image.png\")\n\n# Visualize OCR results with bounding boxes\nextractor.visualize(\n    result=result,\n    image_path=\"path/to/image.png\",\n    output_path=\"ocr_visualization.png\",\n    show_text=True,\n    show_confidence=True\n)\n</code></pre>"},{"location":"getting_started/quickstart.html#configuration-examples","title":"\ud83d\udd27 Configuration Examples","text":""},{"location":"getting_started/quickstart.html#custom-ocr-configuration","title":"Custom OCR Configuration","text":"<pre><code>from omnidocs.tasks.ocr_extraction.extractors.easy_ocr import EasyOCRExtractor\n\nextractor = EasyOCRExtractor(\n    languages=['en', 'fr', 'de'],  # Multiple languages\n    device='cuda',                 # Use GPU if available\n    show_log=True                  # Enable logging\n)\n</code></pre>"},{"location":"getting_started/quickstart.html#custom-table-extraction","title":"Custom Table Extraction","text":"<pre><code>from omnidocs.tasks.table_extraction.extractors.camelot import CamelotExtractor\n\nextractor = CamelotExtractor(\n    flavor='lattice',              # Use lattice parsing\n    pages='all',                   # Process all pages\n    table_areas=None,              # Auto-detect table areas\n    columns=None                   # Auto-detect columns\n)\n</code></pre>"},{"location":"getting_started/quickstart.html#common-issues-solutions","title":"\ud83d\udea8 Common Issues &amp; Solutions","text":""},{"location":"getting_started/quickstart.html#issue-import-errors","title":"Issue: Import Errors","text":"<pre><code># Install missing dependencies\npip install torch torchvision  # For deep learning models\npip install camelot-py[cv]      # For Camelot\npip install easyocr            # For EasyOCR\n</code></pre>"},{"location":"getting_started/quickstart.html#issue-cudagpu-problems","title":"Issue: CUDA/GPU Problems","text":"<pre><code># Force CPU usage if GPU issues\nextractor = EasyOCRExtractor(device='cpu')\n</code></pre>"},{"location":"getting_started/quickstart.html#issue-language-not-supported","title":"Issue: Language Not Supported","text":"<pre><code># Check supported languages\nextractor = EasyOCRExtractor()\nprint(extractor.get_supported_languages())\n</code></pre>"},{"location":"getting_started/quickstart.html#next-steps","title":"\ud83d\udcda Next Steps","text":"<p>Now that you're up and running:</p> <ol> <li>Explore Tutorials: Check out detailed task-specific tutorials</li> <li>Read API Reference: Dive into the complete API documentation</li> <li>Join Community: Report issues or contribute on GitHub</li> </ol>"},{"location":"getting_started/quickstart.html#real-world-examples","title":"\ud83c\udfaf Real-World Examples","text":""},{"location":"getting_started/quickstart.html#document-processing-pipeline","title":"Document Processing Pipeline","text":"<pre><code>from omnidocs.tasks.ocr_extraction.extractors.easy_ocr import EasyOCRExtractor\nfrom omnidocs.tasks.table_extraction.extractors.camelot import CamelotExtractor\n\ndef process_document(file_path):\n    \"\"\"Complete document processing pipeline\"\"\"\n    results = {}\n\n    # Extract text with OCR\n    ocr_extractor = EasyOCRExtractor(languages=['en'])\n    ocr_result = ocr_extractor.extract(file_path)\n    results['text'] = ocr_result.full_text\n\n    # Extract tables\n    table_extractor = CamelotExtractor()\n    table_result = table_extractor.extract(file_path)\n    results['tables'] = [table.df for table in table_result.tables]\n\n    return results\n\n# Process a document\ndocument_data = process_document(\"business_report.pdf\")\nprint(f\"Extracted {len(document_data['text'])} characters of text\")\nprint(f\"Found {len(document_data['tables'])} tables\")\n</code></pre> <p>You\u2019re now ready to build document-AI applications with OmniDocs.</p>"},{"location":"getting_started/table_test.html","title":"Table test","text":"In\u00a0[\u00a0]: Copied! <pre>from omnidocs.tasks.table_extraction.extractors import (\n    CamelotExtractor,\n    PDFPlumberExtractor,\n    TabulaExtractor,\n    TableTransformerExtractor,\n    TableFormerExtractor\n)\n</pre> from omnidocs.tasks.table_extraction.extractors import (     CamelotExtractor,     PDFPlumberExtractor,     TabulaExtractor,     TableTransformerExtractor,     TableFormerExtractor ) In\u00a0[\u00a0]: Copied! <pre>def test_table_extraction():\n    from omnidocs.tasks.table_extraction.extractors import (\n        CamelotExtractor,\n        PDFPlumberExtractor,\n        TabulaExtractor,\n        TableTransformerExtractor,\n        TableFormerExtractor\n    )\n    \n    # PDF extractors\n    pdf_extractors = [CamelotExtractor, PDFPlumberExtractor, TabulaExtractor]\n    pdf_path = \"C:\\\\Users\\\\laxma\\\\OneDrive\\\\Desktop\\\\CogLab\\\\11-07-2025\\\\Omnidocs\\\\tests\\\\table_extraction\\\\assets\\\\table_document.pdf\"\n    \n    # Image extractors  \n    image_extractors = [TableTransformerExtractor, TableFormerExtractor]\n    image_path = \"C:\\\\Users\\\\laxma\\\\OneDrive\\\\Desktop\\\\CogLab\\\\11-07-2025\\\\Omnidocs\\\\tests\\\\table_extraction\\\\assets\\\\table_image.png\"\n    \n    # For visualization: use image file even for PDF extractors\n    # (We extract from PDF but visualize on the corresponding image)\n    viz_image_path = image_path\n    \n    print(\"Testing PDF table extractors...\")\n    print(\"Note: Extracting from PDF but visualizing on corresponding image\")\n    for extractor_cls in pdf_extractors:\n        print(f\"\\nTesting {extractor_cls.__name__}\")\n        print(\"-\" * 40)\n        \n        try:\n            extractor = extractor_cls(show_log=True)\n            result = extractor.extract(pdf_path)\n            print(f\"Extracted {len(result.tables)} table(s)\")\n            \n            if result.tables:\n                # Basic visualization - clean boxes only\n                vis_path = f\"visualized_{extractor_cls.__name__}.png\"\n                extractor.visualize(result, viz_image_path, vis_path)\n                \n                # Custom styling - clean colored boxes\n                extractor.visualize(\n                    result, \n                    viz_image_path, \n                    f\"styled_{extractor_cls.__name__}.png\",\n                    table_color='red',\n                    cell_color='blue', \n                    box_width=3,\n                    show_text=False,  # No text overlay\n                    show_table_ids=True\n                )\n            \n            print(\"SUCCESS\")\n        except Exception as e:\n            print(f\"ERROR: {e}\")\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"Testing Image table extractors...\")\n    for extractor_cls in image_extractors:\n        print(f\"\\nTesting {extractor_cls.__name__}\")\n        print(\"-\" * 40)\n        \n        try:\n            extractor = extractor_cls(show_log=True)\n            result = extractor.extract(image_path)\n            print(f\"Extracted {len(result.tables)} table(s)\")\n            \n            if result.tables:\n                # Basic visualization\n                vis_path = f\"visualized_{extractor_cls.__name__}.png\"\n                extractor.visualize(result, image_path, vis_path)\n                \n                # Custom styling\n                extractor.visualize(\n                    result, \n                    image_path, \n                    f\"styled_{extractor_cls.__name__}.png\",\n                    table_color='purple',\n                    cell_color='orange', \n                    box_width=2,\n                    show_text=True,\n                    text_color='black',\n                    show_table_ids=True\n                )\n            \n            print(\"SUCCESS\")\n        except Exception as e:\n            print(f\"ERROR: {e}\")\n</pre> def test_table_extraction():     from omnidocs.tasks.table_extraction.extractors import (         CamelotExtractor,         PDFPlumberExtractor,         TabulaExtractor,         TableTransformerExtractor,         TableFormerExtractor     )          # PDF extractors     pdf_extractors = [CamelotExtractor, PDFPlumberExtractor, TabulaExtractor]     pdf_path = \"C:\\\\Users\\\\laxma\\\\OneDrive\\\\Desktop\\\\CogLab\\\\11-07-2025\\\\Omnidocs\\\\tests\\\\table_extraction\\\\assets\\\\table_document.pdf\"          # Image extractors       image_extractors = [TableTransformerExtractor, TableFormerExtractor]     image_path = \"C:\\\\Users\\\\laxma\\\\OneDrive\\\\Desktop\\\\CogLab\\\\11-07-2025\\\\Omnidocs\\\\tests\\\\table_extraction\\\\assets\\\\table_image.png\"          # For visualization: use image file even for PDF extractors     # (We extract from PDF but visualize on the corresponding image)     viz_image_path = image_path          print(\"Testing PDF table extractors...\")     print(\"Note: Extracting from PDF but visualizing on corresponding image\")     for extractor_cls in pdf_extractors:         print(f\"\\nTesting {extractor_cls.__name__}\")         print(\"-\" * 40)                  try:             extractor = extractor_cls(show_log=True)             result = extractor.extract(pdf_path)             print(f\"Extracted {len(result.tables)} table(s)\")                          if result.tables:                 # Basic visualization - clean boxes only                 vis_path = f\"visualized_{extractor_cls.__name__}.png\"                 extractor.visualize(result, viz_image_path, vis_path)                                  # Custom styling - clean colored boxes                 extractor.visualize(                     result,                      viz_image_path,                      f\"styled_{extractor_cls.__name__}.png\",                     table_color='red',                     cell_color='blue',                      box_width=3,                     show_text=False,  # No text overlay                     show_table_ids=True                 )                          print(\"SUCCESS\")         except Exception as e:             print(f\"ERROR: {e}\")          print(\"\\n\" + \"=\"*50)     print(\"Testing Image table extractors...\")     for extractor_cls in image_extractors:         print(f\"\\nTesting {extractor_cls.__name__}\")         print(\"-\" * 40)                  try:             extractor = extractor_cls(show_log=True)             result = extractor.extract(image_path)             print(f\"Extracted {len(result.tables)} table(s)\")                          if result.tables:                 # Basic visualization                 vis_path = f\"visualized_{extractor_cls.__name__}.png\"                 extractor.visualize(result, image_path, vis_path)                                  # Custom styling                 extractor.visualize(                     result,                      image_path,                      f\"styled_{extractor_cls.__name__}.png\",                     table_color='purple',                     cell_color='orange',                      box_width=2,                     show_text=True,                     text_color='black',                     show_table_ids=True                 )                          print(\"SUCCESS\")         except Exception as e:             print(f\"ERROR: {e}\") In\u00a0[\u00a0]: Copied! <pre>test_table_extraction()\n</pre> test_table_extraction() In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tasks/layout_analysis/overview.html","title":"Layout Analysis in OmniDocs","text":"<p>Layout analysis is the process of detecting and classifying regions (text, tables, images, etc.) in documents or images. OmniDocs provides a unified interface to several state-of-the-art layout detection backends, making it easy to experiment, compare, and integrate them into your workflows.</p>"},{"location":"tasks/layout_analysis/overview.html#layout-analysis","title":"Layout Analysis?","text":"<p>Layout analysis breaks a document page into its logical components\u2014like paragraphs, tables, figures, and headers\u2014by predicting bounding boxes and labels for each region. This is a crucial first step for downstream tasks like OCR, table extraction, and document understanding.</p>"},{"location":"tasks/layout_analysis/overview.html#supported-layout-detectors","title":"\ud83e\udde9 Supported Layout Detectors","text":"<p>OmniDocs supports multiple layout detection engines, each with its own strengths:</p> Detector Backend/Model Highlights Paddle PaddleOCR Layout Fast, robust, easy to use, good for scanned docs RTDETR RT-DETR Real-time, transformer-based, accurate Surya Surya Layout Modern, high-accuracy, Indian docs friendly YOLO YOLOv8/YOLOv5 Fast, customizable, works on many layouts Florence Florence Layout (If available) Large foundation model, generalizes well <p>Tip: You can easily switch between detectors by changing a single import/class name.</p>"},{"location":"tasks/layout_analysis/overview.html#how-to-use","title":"\ud83d\udcdd How to Use","text":"<p>All layout detectors follow the same API pattern:</p> <pre><code>from omnidocs.tasks.layout_analysis.extractors.paddle import PaddleLayoutDetector\n\ndetector = PaddleLayoutDetector(device='cpu')\nimage_path = \"path/to/your/document.png\"\nannotated_image, layout_output = detector.detect(image_path)\n\n# Visualize results\ndetector.visualize((annotated_image, layout_output), \"output.png\")\n</code></pre> <ul> <li>Change <code>PaddleLayoutDetector</code> to any other detector (e.g., <code>RTDETRLayoutDetector</code>, <code>SuryaLayoutDetector</code>, <code>YOLOLayoutDetector</code>) to use a different backend.</li> <li>All detectors return both the annotated image and a structured output with bounding boxes and labels.</li> </ul>"},{"location":"tasks/layout_analysis/overview.html#example-notebooks","title":"\ud83d\udcd2 Example Notebooks","text":"<p>See the tutorial notebooks for hands-on examples: - Paddle Layout Analysis - YOLO Layout Analysis - RTDETR Layout Analysis - Surya Layout Analysis</p> <p>Each notebook demonstrates: - How to initialize and use the detector - How to visualize results - How to interpret the output</p>"},{"location":"tasks/layout_analysis/overview.html#advanced-tips","title":"\ud83d\udee0\ufe0f Advanced Tips","text":"<ul> <li>Device Selection: Most detectors support <code>device='cpu'</code> or <code>device='cuda'</code> for GPU acceleration.</li> <li>Custom Models: For YOLO and Surya, you can plug in your own trained weights.</li> <li>Batch Processing: Use Python loops or scripts to process folders of images.</li> <li>Output Structure: All detectors return bounding boxes, class labels, and (optionally) confidence scores.</li> </ul>"},{"location":"tasks/layout_analysis/overview.html#next-steps","title":"\ud83d\udd17 Next Steps","text":"<ul> <li>Try out the notebooks with your own documents.</li> <li>Read the API Reference for advanced usage and customization.</li> <li>Explore downstream tasks like OCR and table extraction using the detected layouts.</li> </ul> <p>OmniDocs makes layout analysis accessible, reproducible, and extensible, no matter which backend you choose. </p>"},{"location":"tasks/layout_analysis/tutorials/florence.html","title":"Florence","text":"In\u00a0[\u00a0]: Copied!"},{"location":"tasks/layout_analysis/tutorials/florence.html#florence-layout-analysis","title":"Florence Layout Analysis\u00b6","text":"<pre>from omnidocs.tasks.layout_analysis import FlorenceLayoutAnalyzer\n</pre>"},{"location":"tasks/layout_analysis/tutorials/paddle.html","title":"PaddleOCR","text":"In\u00a0[\u00a0]: Copied! <pre>from omnidocs.tasks.layout_analysis.extractors.paddle import PaddleLayoutDetector\n</pre> from omnidocs.tasks.layout_analysis.extractors.paddle import PaddleLayoutDetector <p>Once we import, we call the PaddleLayout detector and pass</p> In\u00a0[\u00a0]: Copied! <pre>detector = PaddleLayoutDetector(device='cpu',show_log=True)\nimage_path = \"assets/news_paper.png\"\nannotated_image, layout_output = detector.detect(image_path)\nprint(f\"Detected {len(layout_output.bboxes)} elements\")\n\nprint(\"Saving visualization...\")\noutput_path = \"tests/layout_detectors/output/paddle_result.png\"\ndetector.visualize((annotated_image, layout_output), output_path)\nprint(f\"\u2713 Saved visualization to {output_path}\")\n</pre> detector = PaddleLayoutDetector(device='cpu',show_log=True) image_path = \"assets/news_paper.png\" annotated_image, layout_output = detector.detect(image_path) print(f\"Detected {len(layout_output.bboxes)} elements\")  print(\"Saving visualization...\") output_path = \"tests/layout_detectors/output/paddle_result.png\" detector.visualize((annotated_image, layout_output), output_path) print(f\"\u2713 Saved visualization to {output_path}\") In\u00a0[\u00a0]: Copied! <pre>from IPython.display import Image, display\n# Display in notebook\ndisplay(Image(output_path))\n</pre> from IPython.display import Image, display # Display in notebook display(Image(output_path))"},{"location":"tasks/layout_analysis/tutorials/paddle.html#paddleocr-layout-analysis","title":"PaddleOCR Layout Analysis\u00b6","text":"<p>Here we use PaddleOCR to detect analyze, its a example, and how it will ideally run.</p>"},{"location":"tasks/layout_analysis/tutorials/rtdetr.html","title":"RT-DETR","text":"In\u00a0[9]: Copied! <pre>from omnidocs.tasks.layout_analysis.extractors.rtdetr import RTDETRLayoutDetector\n</pre> from omnidocs.tasks.layout_analysis.extractors.rtdetr import RTDETRLayoutDetector In\u00a0[10]: Copied! <pre>detector = RTDETRLayoutDetector(show_log=True)\nimage_path = \"assets/news_paper.png\"\n\nannotated_image, layout_output = detector.detect(image_path)\nprint(f\"Detected {len(layout_output.bboxes)} elements\")\n</pre> detector = RTDETRLayoutDetector(show_log=True) image_path = \"assets/news_paper.png\"  annotated_image, layout_output = detector.detect(image_path) print(f\"Detected {len(layout_output.bboxes)} elements\") <pre>INFO     [timestamp]2025-07-29 20:13:59[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.rtdetr[/] |    \n         [function]rtdetr.py:67[/] | [info]Initializing RTDETRLayoutDetector[/]                                    \n</pre> <pre>INFO     [timestamp]2025-07-29 20:13:59[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.rtdetr[/] |    \n         [function]rtdetr.py:67[/] | [info]Initializing RTDETRLayoutDetector[/]                                    \n</pre> <pre>INFO     [timestamp]2025-07-29 20:13:59[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.rtdetr[/] |    \n         [function]rtdetr.py:80[/] | [info]Forced CPU usage due to use_cpu_only flag[/]                            \n</pre> <pre>INFO     [timestamp]2025-07-29 20:13:59[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.rtdetr[/] |    \n         [function]rtdetr.py:80[/] | [info]Forced CPU usage due to use_cpu_only flag[/]                            \n</pre> <pre>INFO     [timestamp]2025-07-29 20:13:59[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.rtdetr[/] |    \n         [function]rtdetr.py:102[/] | [info]Set CPU threads to 4[/]                                                \n</pre> <pre>INFO     [timestamp]2025-07-29 20:13:59[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.rtdetr[/] |    \n         [function]rtdetr.py:102[/] | [info]Set CPU threads to 4[/]                                                \n</pre> <pre>INFO     [timestamp]2025-07-29 20:13:59[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.rtdetr[/] |    \n         [function]rtdetr.py:192[/] | [info]Loading RT-DETR model from local path...[/]                            \n</pre> <pre>INFO     [timestamp]2025-07-29 20:13:59[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.rtdetr[/] |    \n         [function]rtdetr.py:192[/] | [info]Loading RT-DETR model from local path...[/]                            \n</pre> <pre>SUCCESS  [timestamp]2025-07-29 20:14:00[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.rtdetr[/] |    \n         [function]logging.py:196[/] | [success]RT-DETR model loaded successfully[/]                               \n</pre> <pre>SUCCESS  [timestamp]2025-07-29 20:14:00[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.rtdetr[/] |    \n         [function]logging.py:196[/] | [success]RT-DETR model loaded successfully[/]                               \n</pre> <pre>INFO     [timestamp]2025-07-29 20:14:00[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.rtdetr[/] |    \n         [function]rtdetr.py:209[/] | [info]Model ready on device: cpu[/]                                          \n</pre> <pre>INFO     [timestamp]2025-07-29 20:14:00[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.rtdetr[/] |    \n         [function]rtdetr.py:209[/] | [info]Model ready on device: cpu[/]                                          \n</pre> <pre>SUCCESS  [timestamp]2025-07-29 20:14:00[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.rtdetr[/] |    \n         [function]logging.py:196[/] | [success]Model initialized successfully[/]                                  \n</pre> <pre>SUCCESS  [timestamp]2025-07-29 20:14:00[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.rtdetr[/] |    \n         [function]logging.py:196[/] | [success]Model initialized successfully[/]                                  \n</pre> <pre>INFO     [timestamp]2025-07-29 20:14:02[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.rtdetr[/] |    \n         [function]logging.py:150[/] | [info]detect completed in 1.94s[/]                                          \n</pre> <pre>INFO     [timestamp]2025-07-29 20:14:02[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.rtdetr[/] |    \n         [function]logging.py:150[/] | [info]detect completed in 1.94s[/]                                          \n</pre> <pre>Detected 98 elements\n</pre> In\u00a0[\u00a0]: Copied! <pre>output_path = \"output/rtdetr_result.png\"\ndetector.visualize((annotated_image, layout_output), output_path)\nprint(f\"Saved visualization and json to {output_path}\")\n</pre> output_path = \"output/rtdetr_result.png\" detector.visualize((annotated_image, layout_output), output_path) print(f\"Saved visualization and json to {output_path}\") <pre>Saved visualization to output/rtdetr_result.png\n</pre> In\u00a0[13]: Copied! <pre>from IPython.display import Image, display\n# Display in notebook\ndisplay(Image(output_path))\n</pre> from IPython.display import Image, display # Display in notebook display(Image(output_path)) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tasks/layout_analysis/tutorials/rtdetr.html#rt-detr-layout-analysis","title":"RT-DETR Layout Analysis\u00b6","text":"<p>Here we use RT-DETR to detect analyze, its a example, and how it will ideally run.</p>"},{"location":"tasks/layout_analysis/tutorials/surya.html","title":"Surya","text":"In\u00a0[2]: Copied! <pre>from omnidocs.tasks.layout_analysis.extractors.surya import SuryaLayoutDetector\n</pre> from omnidocs.tasks.layout_analysis.extractors.surya import SuryaLayoutDetector <pre>c:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n</pre> In\u00a0[6]: Copied! <pre>detector = SuryaLayoutDetector(show_log=True)\nimage_path = \"assets/news_paper.png\"\nannotated_image, layout_output = detector.detect(image_path)\nprint(f\"Detected {len(layout_output.bboxes)} elements\")\n</pre> detector = SuryaLayoutDetector(show_log=True) image_path = \"assets/news_paper.png\" annotated_image, layout_output = detector.detect(image_path) print(f\"Detected {len(layout_output.bboxes)} elements\") <pre>INFO     [timestamp]2025-07-29 20:16:09[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.surya[/] |     \n         [function]surya.py:55[/] | [info]Initializing SuryaLayoutDetector[/]                                      \n</pre> <pre>INFO     [timestamp]2025-07-29 20:16:09[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.surya[/] |     \n         [function]surya.py:55[/] | [info]Initializing SuryaLayoutDetector[/]                                      \n</pre> <pre>INFO     [timestamp]2025-07-29 20:16:09[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.surya[/] |     \n         [function]surya.py:62[/] | [info]Using device: cuda[/]                                                    \n</pre> <pre>INFO     [timestamp]2025-07-29 20:16:09[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.surya[/] |     \n         [function]surya.py:62[/] | [info]Using device: cuda[/]                                                    \n</pre> <pre>INFO     [timestamp]2025-07-29 20:16:09[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.surya[/] |     \n         [function]surya.py:68[/] | [info]Found surya package at:                                                  \n         c:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\surya\\__init__.py[/]     \n</pre> <pre>INFO     [timestamp]2025-07-29 20:16:09[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.surya[/] |     \n         [function]surya.py:68[/] | [info]Found surya package at:                                                  \n         c:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\surya\\__init__.py[/]     \n</pre> <pre>SUCCESS  [timestamp]2025-07-29 20:16:11[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.surya[/] |     \n         [function]logging.py:196[/] | [success]Models initialized successfully[/]                                 \n</pre> <pre>SUCCESS  [timestamp]2025-07-29 20:16:11[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.surya[/] |     \n         [function]logging.py:196[/] | [success]Models initialized successfully[/]                                 \n</pre> <pre>Recognizing layout: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:05&lt;00:00,  5.97s/it]\n</pre> <pre>INFO     [timestamp]2025-07-29 20:16:17[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.surya[/] |     \n         [function]logging.py:150[/] | [info]detect completed in 6.17s[/]                                          \n</pre> <pre>INFO     [timestamp]2025-07-29 20:16:17[/] | [logger.name]omnidocs.tasks.layout_analysis.extractors.surya[/] |     \n         [function]logging.py:150[/] | [info]detect completed in 6.17s[/]                                          \n</pre> <pre>Detected 65 elements\n</pre> In\u00a0[\u00a0]: Copied! <pre>output_path = \"output/surya_result.png\"\ndetector.visualize((annotated_image, layout_output), output_path)\nprint(f\"Saved visualization and json to {output_path}\")\n</pre> output_path = \"output/surya_result.png\" detector.visualize((annotated_image, layout_output), output_path) print(f\"Saved visualization and json to {output_path}\") <pre>Saved visualization to output/surya_result.png\n</pre> In\u00a0[8]: Copied! <pre>from IPython.display import Image, display\n# Display in notebook\ndisplay(Image(output_path))\n</pre> from IPython.display import Image, display # Display in notebook display(Image(output_path))"},{"location":"tasks/layout_analysis/tutorials/surya.html#surya-layout-analysis","title":"Surya Layout Analysis\u00b6","text":"<pre>Here we use RT-DETR to detect analyze, its a example, and how it will ideally run. \n</pre>"},{"location":"tasks/layout_analysis/tutorials/yolo.html","title":"YOLO Layout","text":"In\u00a0[2]: Copied! <pre>from omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo import YOLOLayoutDetector\n</pre> from omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo import YOLOLayoutDetector <pre>c:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n</pre> In\u00a0[5]: Copied! <pre>detector = YOLOLayoutDetector(show_log=True)\nimage_path = \"assets/news_paper.png\"\nannotated_image, layout_output = detector.detect(image_path)\nprint(f\"Detected {len(layout_output.bboxes)} elements\")\n</pre> detector = YOLOLayoutDetector(show_log=True) image_path = \"assets/news_paper.png\" annotated_image, layout_output = detector.detect(image_path) print(f\"Detected {len(layout_output.bboxes)} elements\") <pre>INFO     [timestamp]2025-07-29 20:19:30[/] |                                                                       \n         [logger.name]omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo[/] |                               \n         [function]doc_layout_yolo.py:53[/] | [info]Initializing YOLOLayoutDetector[/]                             \n</pre> <pre>INFO     [timestamp]2025-07-29 20:19:30[/] |                                                                       \n         [logger.name]omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo[/] |                               \n         [function]doc_layout_yolo.py:53[/] | [info]Initializing YOLOLayoutDetector[/]                             \n</pre> <pre>INFO     [timestamp]2025-07-29 20:19:30[/] |                                                                       \n         [logger.name]omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo[/] |                               \n         [function]doc_layout_yolo.py:58[/] | [info]Using device: cuda[/]                                          \n</pre> <pre>INFO     [timestamp]2025-07-29 20:19:30[/] |                                                                       \n         [logger.name]omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo[/] |                               \n         [function]doc_layout_yolo.py:58[/] | [info]Using device: cuda[/]                                          \n</pre> <pre>INFO     [timestamp]2025-07-29 20:19:30[/] |                                                                       \n         [logger.name]omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo[/] |                               \n         [function]doc_layout_yolo.py:66[/] | [info]Model directory:                                               \n         C:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\omnidocs\\models\\yolo_layout\\juliozhao_DocLayout\n         -YOLO-DocStructBench[/]                                                                                   \n</pre> <pre>INFO     [timestamp]2025-07-29 20:19:30[/] |                                                                       \n         [logger.name]omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo[/] |                               \n         [function]doc_layout_yolo.py:66[/] | [info]Model directory:                                               \n         C:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\omnidocs\\models\\yolo_layout\\juliozhao_DocLayout\n         -YOLO-DocStructBench[/]                                                                                   \n</pre> <pre>INFO     [timestamp]2025-07-29 20:19:30[/] |                                                                       \n         [logger.name]omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo[/] |                               \n         [function]doc_layout_yolo.py:141[/] | [info]Loading YOLO model from                                       \n         C:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\omnidocs\\models\\yolo_layout\\juliozhao_DocLayout\n         -YOLO-DocStructBench\\doclayout_yolo_docstructbench_imgsz1024.pt[/]                                        \n</pre> <pre>INFO     [timestamp]2025-07-29 20:19:30[/] |                                                                       \n         [logger.name]omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo[/] |                               \n         [function]doc_layout_yolo.py:141[/] | [info]Loading YOLO model from                                       \n         C:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\omnidocs\\models\\yolo_layout\\juliozhao_DocLayout\n         -YOLO-DocStructBench\\doclayout_yolo_docstructbench_imgsz1024.pt[/]                                        \n</pre> <pre>SUCCESS  [timestamp]2025-07-29 20:19:30[/] |                                                                       \n         [logger.name]omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo[/] | [function]logging.py:196[/] | \n         [success]YOLO model loaded successfully on cuda[/]                                                        \n</pre> <pre>SUCCESS  [timestamp]2025-07-29 20:19:30[/] |                                                                       \n         [logger.name]omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo[/] | [function]logging.py:196[/] | \n         [success]YOLO model loaded successfully on cuda[/]                                                        \n</pre> <pre>SUCCESS  [timestamp]2025-07-29 20:19:30[/] |                                                                       \n         [logger.name]omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo[/] | [function]logging.py:196[/] | \n         [success]Model initialized successfully[/]                                                                \n</pre> <pre>SUCCESS  [timestamp]2025-07-29 20:19:30[/] |                                                                       \n         [logger.name]omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo[/] | [function]logging.py:196[/] | \n         [success]Model initialized successfully[/]                                                                \n</pre> <pre>\n0: 1024x672 17 titles, 59 plain texts, 2 abandons, 7 figures, 1 figure_caption, 274.8ms\nSpeed: 12.0ms preprocess, 274.8ms inference, 219.8ms postprocess per image at shape (1, 3, 1024, 672)\n</pre> <pre>INFO     [timestamp]2025-07-29 20:19:32[/] |                                                                       \n         [logger.name]omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo[/] | [function]logging.py:150[/] | \n         [info]detect completed in 2.25s[/]                                                                        \n</pre> <pre>INFO     [timestamp]2025-07-29 20:19:32[/] |                                                                       \n         [logger.name]omnidocs.tasks.layout_analysis.extractors.doc_layout_yolo[/] | [function]logging.py:150[/] | \n         [info]detect completed in 2.25s[/]                                                                        \n</pre> <pre>Detected 84 elements\n</pre> In\u00a0[\u00a0]: Copied! <pre>output_path = \"output/yolo_result.png\"\ndetector.visualize((annotated_image, layout_output), output_path)\nprint(f\"Saved visualization to {output_path}\")\n</pre> output_path = \"output/yolo_result.png\" detector.visualize((annotated_image, layout_output), output_path) print(f\"Saved visualization to {output_path}\") <pre>Saved visualization &amp; json to output/yolo_result.png\n</pre> In\u00a0[7]: Copied! <pre>from IPython.display import Image, display\n# Display in notebook\ndisplay(Image(output_path))\n</pre> from IPython.display import Image, display # Display in notebook display(Image(output_path)) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tasks/layout_analysis/tutorials/yolo.html#doclayout-yolo","title":"DocLayout YOLO\u00b6","text":"<p>Here we use yolo layout to detect analyze, its a example, and how it will ideally run.</p>"},{"location":"tasks/math_extraction/math_latex_extraction.html","title":"Mathematical LaTeX Expression Extraction with OmniDocs","text":""},{"location":"tasks/math_extraction/math_latex_extraction.html#overview","title":"Overview","text":"<p>OmniDocs provides multiple specialized extractors for LaTeX mathematical expression recognition. Each extractor is implemented as a standalone class that can be imported and used independently, following a consistent interface pattern.</p>"},{"location":"tasks/math_extraction/math_latex_extraction.html#available-latex-extractors","title":"Available LaTeX Extractors","text":""},{"location":"tasks/math_extraction/math_latex_extraction.html#1-donutextractor","title":"1. DonutExtractor","text":"<p>Document Understanding Transformer for mathematical expressions.</p> <pre><code>from omnidocs.tasks.math_expression_extraction.extractors.donut import DonutExtractor\n\n# Initialize extractor\nextractor = DonutExtractor(device='cuda', show_log=True)\n\n# Extract from image\nresult = extractor.extract(\"path/to/math_equation_image.png\")\nprint(result.expressions[0])  # Prints LaTeX code\n</code></pre>"},{"location":"tasks/math_extraction/math_latex_extraction.html#2-nougatextractor","title":"2. NougatExtractor","text":"<p>Neural Optical Understanding for Academic Documents.</p> <pre><code>from omnidocs.tasks.math_expression_extraction.extractors.nougat import NougatExtractor\n\n# Initialize Nougat extractor\nextractor = NougatExtractor(device='cuda', show_log=True)\n\n# Extract from academic paper image\nresult = extractor.extract(\"paper_with_equations.png\")\nprint(result.expressions[0])  # Mathematical expressions in LaTeX\n</code></pre>"},{"location":"tasks/math_extraction/math_latex_extraction.html#3-suryamathextractor","title":"3. SuryaMathExtractor","text":"<p>Lightweight and fast extractor for mathematical expressions.</p> <pre><code>from omnidocs.tasks.math_expression_extraction.extractors.surya_math import SuryaMathExtractor\n\n# Initialize SuryaMath extractor\nextractor = SuryaMathExtractor(device='cuda', show_log=True)\n\n# Extract from image\nresult = extractor.extract(\"path/to/math_equation_image.png\")\nprint(result.expressions[0])  # Prints LaTeX code\n</code></pre>"},{"location":"tasks/math_extraction/math_latex_extraction.html#4-unimernetextractor","title":"4. UniMERNetExtractor","text":"<p>Universal Mathematical Expression Recognition Network.</p> <pre><code>from omnidocs.tasks.math_expression_extraction.extractors.unimernet import UniMERNetExtractor\n\n# Initialize UniMERNet extractor\nextractor = UniMERNetExtractor(device='cuda', show_log=True)\n\n# Extract from image\nresult = extractor.extract(\"path/to/math_equation_image.png\")\nprint(result.expressions[0])  # Prints LaTeX code\n</code></pre>"},{"location":"tasks/math_extraction/math_latex_extraction.html#common-usage-patterns","title":"Common Usage Patterns","text":""},{"location":"tasks/math_extraction/math_latex_extraction.html#basic-extraction-workflow","title":"Basic Extraction Workflow","text":"<pre><code># Choose your preferred extractor\nfrom omnidocs.tasks.math_expression_extraction.extractors.donut import DonutExtractor\n\n# Initialize\nextractor = DonutExtractor(device='cuda', show_log=True)\n\n# Extract expressions\nresult = extractor.extract(\"equation_image.png\")\n\n# Access results\nfor i, expr in enumerate(result.expressions):\n    print(f\"Expression {i+1}: {expr}\")\n</code></pre>"},{"location":"tasks/math_extraction/math_latex_extraction.html#batch-processing-multiple-images","title":"Batch Processing Multiple Images","text":"<pre><code>import os\nfrom omnidocs.tasks.math_expression_extraction.extractors.donut import DonutExtractor\n\n# Initialize extractor once (choose Donut, Nougat, SuryaMath, or UniMERNet)\nextractor = DonutExtractor(device='cuda', show_log=True)\n\n# Process multiple images in a folder\nimage_folder = \"path/to/math_images/\"\nresults = []\n\nfor filename in os.listdir(image_folder):\n    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n        image_path = os.path.join(image_folder, filename)\n        try:\n            result = extractor.extract(image_path)\n            results.append({\n                'filename': filename,\n                'expressions': result.expressions,\n                'image_size': result.source_img_size\n            })\n        except Exception as e:\n            print(f\"Error processing {filename}: {e}\")\n\n# Print all results\nfor item in results:\n    print(f\"\\n{item['filename']}:\")\n    for expr in item['expressions']:\n        print(f\"  {expr}\")\n</code></pre>"},{"location":"tasks/math_extraction/math_latex_extraction.html#working-with-different-input-types","title":"Working with Different Input Types","text":"<pre><code>from PIL import Image\nimport numpy as np\nfrom omnidocs.tasks.math_expression_extraction.extractors.donut import DonutExtractor\n\nextractor = DonutExtractor(device='cuda', show_log=True)\n\n# From file path\nresult1 = extractor.extract(\"equation.png\")\n\n# From PIL Image\nimg = Image.open(\"equation.png\")\nresult2 = extractor.extract(img)\n\n# From numpy array (if supported)\nimg_array = np.array(img)\nresult3 = extractor.extract(img_array)\n\nprint(\"All methods produce LaTeX:\", result1.expressions[0])\n\n## Extractor Comparison\n\n| Extractor | Accuracy | Speed | Memory Usage | Best Use Case |\n|-----------|----------|-------|--------------|---------------|\n| **DonutExtractor** | High | Medium | Medium | General mathematical expressions |\n| **NougatExtractor** | Very High | Slow | High | Academic papers, complex layouts |\n| **SuryaMathExtractor** | High | Fast | Low | Lightweight, fast math extraction |\n| **UniMERNetExtractor** | High | Fast | Medium | Universal math recognition |\n\n## Configuration Options\n\n### Device Selection\n```python\n# GPU processing (recommended)\nextractor = DonutExtractor(device='cuda', show_log=True)\n\n# CPU processing (slower but works without GPU)\nextractor = DonutExtractor(device='cpu', show_log=True)\n\n# Auto-detect best device\nextractor = DonutExtractor(device=None, show_log=True)  # Auto-selects best available\n</code></pre>"},{"location":"tasks/math_extraction/math_latex_extraction.html#logging-and-debugging","title":"Logging and Debugging","text":"<pre><code># Enable detailed logging\nextractor = DonutExtractor(device='cuda', show_log=True)\n\n# Disable logging for production\nextractor = DonutExtractor(device='cuda', show_log=False)\n</code></pre>"},{"location":"tasks/math_extraction/math_latex_extraction.html#model-specific-parameters","title":"Model-Specific Parameters","text":"<pre><code># Nougat with specific checkpoint\nextractor = NougatExtractor(\n    device='cuda',\n    show_log=True,\n    model_checkpoint='facebook/nougat-base'  # Specific model version\n)\n</code></pre>"},{"location":"tasks/math_extraction/math_latex_extraction.html#advanced-usage-examples","title":"Advanced Usage Examples","text":""},{"location":"tasks/math_extraction/math_latex_extraction.html#error-handling-and-validation","title":"Error Handling and Validation","text":"<pre><code>from omnidocs.tasks.math_expression_extraction.extractors.donut import DonutExtractor\n\ndef safe_extract_latex(image_path):\n    \"\"\"Safely extract LaTeX with error handling.\"\"\"\n    try:\n        extractor = DonutExtractor(device='cuda', show_log=False)\n        result = extractor.extract(image_path)\n\n        if result.expressions:\n            return result.expressions[0]\n        else:\n            print(f\"No expressions found in {image_path}\")\n            return None\n\n    except ImportError as e:\n        print(f\"Model not available: {e}\")\n        return None\n    except Exception as e:\n        print(f\"Error processing {image_path}: {e}\")\n        return None\n\n# Usage\nlatex_code = safe_extract_latex(\"my_equation.png\")\nif latex_code:\n    print(f\"Extracted: {latex_code}\")\n</code></pre>"},{"location":"tasks/math_extraction/math_latex_extraction.html#comparing-multiple-extractors","title":"Comparing Multiple Extractors","text":"<pre><code>from omnidocs.tasks.math_expression_extraction.extractors import (\n    DonutExtractor, NougatExtractor, SuryaMathExtractor, UniMERNetExtractor\n)\n\ndef compare_extractors(image_path):\n    \"\"\"Compare results from different extractors.\"\"\"\n    extractors = {\n        'Donut': DonutExtractor,\n        'Nougat': NougatExtractor,\n        'SuryaMath': SuryaMathExtractor,\n        'UniMERNet': UniMERNetExtractor\n    }\n\n    results = {}\n    for name, ExtractorClass in extractors.items():\n        try:\n            extractor = ExtractorClass(device='cuda', show_log=False)\n            result = extractor.extract(image_path)\n            results[name] = result.expressions[0] if result.expressions else \"No result\"\n        except Exception as e:\n            results[name] = f\"Error: {e}\"\n\n    return results\n\n# Compare results\nimage_path = \"complex_equation.png\"\ncomparison = compare_extractors(image_path)\n\nfor extractor_name, result in comparison.items():\n    print(f\"{extractor_name}: {result}\")\n</code></pre>"},{"location":"tasks/math_extraction/math_latex_extraction.html#processing-pdf-pages-with-math","title":"Processing PDF Pages with Math","text":"<pre><code>import fitz  # PyMuPDF\nfrom PIL import Image\nfrom omnidocs.tasks.math_expression_extraction.extractors.nougat import NougatExtractor\n\ndef extract_math_from_pdf(pdf_path, page_numbers=None):\n    \"\"\"Extract mathematical expressions from PDF pages.\"\"\"\n    extractor = NougatExtractor(device='cuda', show_log=True)\n\n    # Open PDF\n    doc = fitz.open(pdf_path)\n    results = []\n\n    pages_to_process = page_numbers or range(len(doc))\n\n    for page_num in pages_to_process:\n        try:\n            # Get page as image\n            page = doc[page_num]\n            pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x zoom\n            img_data = pix.tobytes(\"png\")\n\n            # Convert to PIL Image\n            img = Image.open(io.BytesIO(img_data))\n\n            # Extract expressions\n            result = extractor.extract(img)\n\n            results.append({\n                'page': page_num + 1,\n                'expressions': result.expressions,\n                'image_size': result.source_img_size\n            })\n\n        except Exception as e:\n            print(f\"Error processing page {page_num + 1}: {e}\")\n\n    doc.close()\n    return results\n\n# Usage\npdf_results = extract_math_from_pdf(\"research_paper.pdf\", [0, 1, 2])  # First 3 pages\nfor page_result in pdf_results:\n    print(f\"\\nPage {page_result['page']}:\")\n    for expr in page_result['expressions']:\n        print(f\"  {expr}\")\n</code></pre>"},{"location":"tasks/math_extraction/math_latex_extraction.html#output-format","title":"Output Format","text":"<p>All extractors return a <code>LatexOutput</code> object with the following structure:</p> <pre><code>class LatexOutput:\n    expressions: List[str]        # List of LaTeX expressions\n    source_img_size: Tuple[int, int]  # Original image dimensions (width, height)\n</code></pre>"},{"location":"tasks/math_extraction/math_latex_extraction.html#accessing-results","title":"Accessing Results","text":"<pre><code>result = extractor.extract(\"equation.png\")\n\n# Get all expressions\nall_expressions = result.expressions\n\n# Get first expression\nfirst_expr = result.expressions[0]\n\n# Get image dimensions\nwidth, height = result.source_img_size\n\n# Check if any expressions were found\nif result.expressions:\n    print(f\"Found {len(result.expressions)} expressions\")\n    for i, expr in enumerate(result.expressions):\n        print(f\"Expression {i+1}: {expr}\")\nelse:\n    print(\"No mathematical expressions detected\")\n</code></pre>"},{"location":"tasks/math_extraction/math_latex_extraction.html#installation-requirements","title":"Installation Requirements","text":"<p>Each extractor has specific dependencies:</p>"},{"location":"tasks/math_extraction/math_latex_extraction.html#donutextractor","title":"DonutExtractor","text":"<pre><code>pip install transformers torch pillow\n</code></pre>"},{"location":"tasks/math_extraction/math_latex_extraction.html#nougatextractor","title":"NougatExtractor","text":"<pre><code>pip install nougat-ocr\n</code></pre>"},{"location":"tasks/math_extraction/math_latex_extraction.html#suryamathextractor","title":"SuryaMathExtractor","text":"<pre><code>pip install surya-ocr torch pillow\n</code></pre>"},{"location":"tasks/math_extraction/math_latex_extraction.html#unimernetextractor","title":"UniMERNetExtractor","text":"<pre><code>pip install unimer-net torch pillow\n</code></pre>"},{"location":"tasks/math_extraction/math_latex_extraction.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"tasks/math_extraction/math_latex_extraction.html#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>1. CUDA Out of Memory <pre><code># Use CPU instead\nextractor = DonutExtractor(device='cpu', show_log=True)\n\n# Or process smaller batches\n</code></pre></p> <p>2. Model Download Issues <pre><code># Ensure internet connection for first-time model download\n# Models are cached locally after first download\nextractor = DonutExtractor(device='cuda', show_log=True)  # Will download if needed\n</code></pre></p> <p>3. Import Errors <pre><code>try:\n    from omnidocs.tasks.math_expression_extraction.extractors.donut import DonutExtractor\n    extractor = DonutExtractor(device='cuda', show_log=True)\nexcept ImportError as e:\n    print(f\"Missing dependencies: {e}\")\n    print(\"Please install required packages\")\n</code></pre></p> <p>4. No Expressions Detected <pre><code>result = extractor.extract(\"image.png\")\nif not result.expressions:\n    print(\"Try:\")\n    print(\"1. Check image quality and resolution\")\n    print(\"2. Ensure mathematical content is clearly visible\")\n    print(\"3. Try a different extractor model\")\n</code></pre></p>"},{"location":"tasks/math_extraction/math_latex_extraction.html#best-practices","title":"Best Practices","text":"<ol> <li>Choose the Right Extractor: </li> <li>Use Donut for general-purpose extraction</li> <li>Use Nougat for academic papers</li> <li>Use SuryaMath for lightweight, fast math extraction</li> <li> <p>Use UniMERNet for universal math recognition</p> </li> <li> <p>Optimize Performance:</p> </li> <li>Use GPU when available (<code>device='cuda'</code>)</li> <li>Initialize extractor once for batch processing</li> <li> <p>Process images in appropriate resolution</p> </li> <li> <p>Handle Errors Gracefully:</p> </li> <li>Always wrap extraction in try-catch blocks</li> <li>Check if expressions list is not empty</li> <li> <p>Validate LaTeX output if needed</p> </li> <li> <p>Image Quality:</p> </li> <li>Use high-resolution images when possible</li> <li>Ensure good contrast between text and background</li> <li>Crop to focus on mathematical content when feasible</li> </ol>"},{"location":"tasks/math_extraction/overview.html","title":"Mathematical LaTeX Expression Extraction with OmniDocs","text":""},{"location":"tasks/math_extraction/overview.html#overview","title":"Overview","text":"<p>OmniDocs provides multiple specialized extractors for LaTeX mathematical expression recognition. Each extractor is implemented as a standalone class that can be imported and used independently, following a consistent interface pattern.</p>"},{"location":"tasks/math_extraction/overview.html#available-latex-extractors","title":"Available LaTeX Extractors","text":""},{"location":"tasks/math_extraction/overview.html#1-donutextractor","title":"1. DonutExtractor","text":"<p>Document Understanding Transformer for mathematical expressions.</p> <pre><code>from omnidocs.tasks.math_expression_extraction.extractors.donut import DonutExtractor\n\n# Initialize extractor\nextractor = DonutExtractor(device='cuda', show_log=True)\n\n# Extract from image\nresult = extractor.extract(\"path/to/math_equation_image.png\")\nprint(result.expressions[0])  # Prints LaTeX code\n</code></pre>"},{"location":"tasks/math_extraction/overview.html#2-nougatextractor","title":"2. NougatExtractor","text":"<p>Neural Optical Understanding for Academic Documents.</p> <pre><code>from omnidocs.tasks.math_expression_extraction.extractors.nougat import NougatExtractor\n\n# Initialize Nougat extractor\nextractor = NougatExtractor(device='cuda', show_log=True)\n\n# Extract from academic paper image\nresult = extractor.extract(\"paper_with_equations.png\")\nprint(result.expressions[0])  # Mathematical expressions in LaTeX\n</code></pre>"},{"location":"tasks/math_extraction/overview.html#common-usage-patterns","title":"Common Usage Patterns","text":""},{"location":"tasks/math_extraction/overview.html#basic-extraction-workflow","title":"Basic Extraction Workflow","text":"<pre><code># Choose your preferred extractor\nfrom omnidocs.tasks.math_expression_extraction.extractors.donut import DonutExtractor\n\n# Initialize\nextractor = DonutExtractor(device='cuda', show_log=True)\n\n# Extract expressions\nresult = extractor.extract(\"equation_image.png\")\n\n# Access results\nfor i, expr in enumerate(result.expressions):\n    print(f\"Expression {i+1}: {expr}\")\n</code></pre>"},{"location":"tasks/math_extraction/overview.html#batch-processing-multiple-images","title":"Batch Processing Multiple Images","text":"<pre><code>import os\nfrom omnidocs.tasks.math_expression_extraction.extractors.donut import DonutExtractor\n\n# Initialize extractor once (choose Donut, Nougat, SuryaMath, or UniMERNet)\nextractor = DonutExtractor(device='cuda', show_log=True)\n\n# Process multiple images in a folder\nimage_folder = \"path/to/math_images/\"\nresults = []\n\nfor filename in os.listdir(image_folder):\n    if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n        image_path = os.path.join(image_folder, filename)\n        try:\n            result = extractor.extract(image_path)\n            results.append({\n                'filename': filename,\n                'expressions': result.expressions,\n                'image_size': result.source_img_size\n            })\n        except Exception as e:\n            print(f\"Error processing {filename}: {e}\")\n\n# Print all results\nfor item in results:\n    print(f\"\\n{item['filename']}:\")\n    for expr in item['expressions']:\n        print(f\"  {expr}\")\n</code></pre>"},{"location":"tasks/math_extraction/overview.html#working-with-different-input-types","title":"Working with Different Input Types","text":"<pre><code>from PIL import Image\nimport numpy as np\nfrom omnidocs.tasks.math_expression_extraction.extractors.donut import DonutExtractor\n\nextractor = DonutExtractor(device='cuda', show_log=True)\n\n# From file path\nresult1 = extractor.extract(\"equation.png\")\n\n# From PIL Image\nimg = Image.open(\"equation.png\")\nresult2 = extractor.extract(img)\n\n# From numpy array (if supported)\nimg_array = np.array(img)\nresult3 = extractor.extract(img_array)\n\nprint(\"All methods produce LaTeX:\", result1.expressions[0])\n</code></pre>"},{"location":"tasks/math_extraction/overview.html#usrbinenv-python3","title":"!/usr/bin/env python3","text":"<pre><code>from omnidocs.tasks.math_expression_extraction.extractors.unimernet_ocr import UniMERNetExtractor\nimport torch\n\nprint('Testing UniMERNet functionality...')\ntry:\n    extractor = UniMERNetExtractor()\n    print('\u2713 UniMERNet extractor initialized successfully')\n    print(f'\u2713 Device: {extractor.device}')\n    print(f'\u2713 Model: {type(extractor.model)}')\n    print('UniMERNet is ready to use!')\nexcept Exception as e:\n    print(f'\u2717 Error: {e}')\n    import traceback\n    traceback.print_exc()\n\n## Extractor Comparison\n\n| Extractor | Accuracy | Speed | Memory Usage | Best Use Case |\n|-----------|----------|-------|--------------|---------------|\n| **DonutExtractor** | High | Medium | Medium | General mathematical expressions |\n| **NougatExtractor** | Very High | Slow | High | Academic papers, complex layouts |\n| **SuryaMathExtractor** | High | Fast | Low | Lightweight, fast math extraction |\n| **UniMERNetExtractor** | High | Fast | Medium | Universal math recognition |\n\n## Configuration Options\n\n### Device Selection\n```python\n# GPU processing (recommended)\nextractor = DonutExtractor(device='cuda', show_log=True)\n\n# CPU processing (slower but works without GPU)\nextractor = DonutExtractor(device='cpu', show_log=True)\n\n# Auto-detect best device\nextractor = DonutExtractor(device=None, show_log=True)  # Auto-selects best available\n</code></pre>"},{"location":"tasks/math_extraction/overview.html#logging-and-debugging","title":"Logging and Debugging","text":"<pre><code># Enable detailed logging\nextractor = DonutExtractor(device='cuda', show_log=True)\n\n# Disable logging for production\nextractor = DonutExtractor(device='cuda', show_log=False)\n</code></pre>"},{"location":"tasks/math_extraction/overview.html#model-specific-parameters","title":"Model-Specific Parameters","text":"<pre><code># Nougat with specific checkpoint\nextractor = NougatExtractor(\n    device='cuda',\n    show_log=True,\n    model_checkpoint='facebook/nougat-base'  # Specific model version\n)\n</code></pre>"},{"location":"tasks/math_extraction/overview.html#advanced-usage-examples","title":"Advanced Usage Examples","text":""},{"location":"tasks/math_extraction/overview.html#error-handling-and-validation","title":"Error Handling and Validation","text":"<pre><code>from omnidocs.tasks.math_expression_extraction.extractors.donut import DonutExtractor\n\ndef safe_extract_latex(image_path):\n    \"\"\"Safely extract LaTeX with error handling.\"\"\"\n    try:\n        extractor = DonutExtractor(device='cuda', show_log=False)\n        result = extractor.extract(image_path)\n\n        if result.expressions:\n            return result.expressions[0]\n        else:\n            print(f\"No expressions found in {image_path}\")\n            return None\n\n    except ImportError as e:\n        print(f\"Model not available: {e}\")\n        return None\n    except Exception as e:\n        print(f\"Error processing {image_path}: {e}\")\n        return None\n\n# Usage\nlatex_code = safe_extract_latex(\"my_equation.png\")\nif latex_code:\n    print(f\"Extracted: {latex_code}\")\n</code></pre>"},{"location":"tasks/math_extraction/overview.html#comparing-multiple-extractors","title":"Comparing Multiple Extractors","text":"<pre><code>from omnidocs.tasks.math_expression_extraction.extractors import (\n    DonutExtractor, NougatExtractor, SuryaMathExtractor, UniMERNetExtractor\n)\n\ndef compare_extractors(image_path):\n    \"\"\"Compare results from different extractors.\"\"\"\n    extractors = {\n        'Donut': DonutExtractor,\n        'Nougat': NougatExtractor,\n        'SuryaMath': SuryaMathExtractor,\n        'UniMERNet': UniMERNetExtractor\n    }\n\n    results = {}\n    for name, ExtractorClass in extractors.items():\n        try:\n            extractor = ExtractorClass(device='cuda', show_log=False)\n            result = extractor.extract(image_path)\n            results[name] = result.expressions[0] if result.expressions else \"No result\"\n        except Exception as e:\n            results[name] = f\"Error: {e}\"\n\n    return results\n\n# Compare results\nimage_path = \"complex_equation.png\"\ncomparison = compare_extractors(image_path)\n\nfor extractor_name, result in comparison.items():\n    print(f\"{extractor_name}: {result}\")\n</code></pre>"},{"location":"tasks/math_extraction/overview.html#processing-pdf-pages-with-math","title":"Processing PDF Pages with Math","text":"<pre><code>import fitz  # PyMuPDF\nfrom PIL import Image\nfrom omnidocs.tasks.math_expression_extraction.extractors.nougat import NougatExtractor\n\ndef extract_math_from_pdf(pdf_path, page_numbers=None):\n    \"\"\"Extract mathematical expressions from PDF pages.\"\"\"\n    extractor = NougatExtractor(device='cuda', show_log=True)\n\n    # Open PDF\n    doc = fitz.open(pdf_path)\n    results = []\n\n    pages_to_process = page_numbers or range(len(doc))\n\n    for page_num in pages_to_process:\n        try:\n            # Get page as image\n            page = doc[page_num]\n            pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))  # 2x zoom\n            img_data = pix.tobytes(\"png\")\n\n            # Convert to PIL Image\n            img = Image.open(io.BytesIO(img_data))\n\n            # Extract expressions\n            result = extractor.extract(img)\n\n            results.append({\n                'page': page_num + 1,\n                'expressions': result.expressions,\n                'image_size': result.source_img_size\n            })\n\n        except Exception as e:\n            print(f\"Error processing page {page_num + 1}: {e}\")\n\n    doc.close()\n    return results\n\n# Usage\npdf_results = extract_math_from_pdf(\"research_paper.pdf\", [0, 1, 2])  # First 3 pages\nfor page_result in pdf_results:\n    print(f\"\\nPage {page_result['page']}:\")\n    for expr in page_result['expressions']:\n        print(f\"  {expr}\")\n</code></pre>"},{"location":"tasks/math_extraction/overview.html#output-format","title":"Output Format","text":"<p>All extractors return a <code>LatexOutput</code> object with the following structure:</p> <pre><code>class LatexOutput:\n    expressions: List[str]        # List of LaTeX expressions\n    source_img_size: Tuple[int, int]  # Original image dimensions (width, height)\n</code></pre>"},{"location":"tasks/math_extraction/overview.html#accessing-results","title":"Accessing Results","text":"<pre><code>result = extractor.extract(\"equation.png\")\n\n# Get all expressions\nall_expressions = result.expressions\n\n# Get first expression\nfirst_expr = result.expressions[0]\n\n# Get image dimensions\nwidth, height = result.source_img_size\n\n# Check if any expressions were found\nif result.expressions:\n    print(f\"Found {len(result.expressions)} expressions\")\n    for i, expr in enumerate(result.expressions):\n        print(f\"Expression {i+1}: {expr}\")\nelse:\n    print(\"No mathematical expressions detected\")\n</code></pre>"},{"location":"tasks/math_extraction/overview.html#installation-requirements","title":"Installation Requirements","text":"<p>Each extractor has specific dependencies:</p>"},{"location":"tasks/math_extraction/overview.html#donutextractor","title":"DonutExtractor","text":"<pre><code>pip install transformers torch pillow\n</code></pre>"},{"location":"tasks/math_extraction/overview.html#nougatextractor","title":"NougatExtractor","text":"<pre><code>pip install nougat-ocr\n</code></pre>"},{"location":"tasks/math_extraction/overview.html#suryamathextractor","title":"SuryaMathExtractor","text":"<pre><code>pip install surya-ocr torch pillow\n</code></pre>"},{"location":"tasks/math_extraction/overview.html#unimernetextractor","title":"UniMERNetExtractor","text":"<pre><code>pip install unimer-net torch pillow\n</code></pre>"},{"location":"tasks/math_extraction/overview.html#troubleshooting","title":"Troubleshooting","text":""},{"location":"tasks/math_extraction/overview.html#common-issues-and-solutions","title":"Common Issues and Solutions","text":"<p>1. CUDA Out of Memory <pre><code># Use CPU instead\nextractor = DonutExtractor(device='cpu', show_log=True)\n\n# Or process smaller batches\n</code></pre></p> <p>2. Model Download Issues <pre><code># Ensure internet connection for first-time model download\n# Models are cached locally after first download\nextractor = DonutExtractor(device='cuda', show_log=True)  # Will download if needed\n</code></pre></p> <p>3. Import Errors <pre><code>try:\n    from omnidocs.tasks.math_expression_extraction.extractors.donut import DonutExtractor\n    extractor = DonutExtractor(device='cuda', show_log=True)\nexcept ImportError as e:\n    print(f\"Missing dependencies: {e}\")\n    print(\"Please install required packages\")\n</code></pre></p> <p>4. No Expressions Detected <pre><code>result = extractor.extract(\"image.png\")\nif not result.expressions:\n    print(\"Try:\")\n    print(\"1. Check image quality and resolution\")\n    print(\"2. Ensure mathematical content is clearly visible\")\n    print(\"3. Try a different extractor model\")\n</code></pre></p>"},{"location":"tasks/math_extraction/overview.html#best-practices","title":"Best Practices","text":"<ol> <li>Choose the Right Extractor: </li> <li>Use Donut for general-purpose extraction</li> <li>Use Nougat for academic papers</li> <li>Use SuryaMath for lightweight, fast math extraction</li> <li> <p>Use UniMERNet for universal math recognition</p> </li> <li> <p>Optimize Performance:</p> </li> <li>Use GPU when available (<code>device='cuda'</code>)</li> <li>Initialize extractor once for batch processing</li> <li> <p>Process images in appropriate resolution</p> </li> <li> <p>Handle Errors Gracefully:</p> </li> <li>Always wrap extraction in try-catch blocks</li> <li>Check if expressions list is not empty</li> <li> <p>Validate LaTeX output if needed</p> </li> <li> <p>Image Quality:</p> </li> <li>Use high-resolution images when possible</li> <li>Ensure good contrast between text and background</li> <li>Crop to focus on mathematical content when feasible</li> </ol>"},{"location":"tasks/math_extraction/tutorials/donut.html","title":"Donut","text":"In\u00a0[1]: Copied! <pre>from omnidocs.tasks.math_expression_extraction.extractors.donut import DonutExtractor\nprint(\"DonutExtractor imported successfully!\")\n</pre> from omnidocs.tasks.math_expression_extraction.extractors.donut import DonutExtractor print(\"DonutExtractor imported successfully!\") <pre>c:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n</pre> <pre>DonutExtractor imported successfully!\n</pre> In\u00a0[2]: Copied! <pre>image_path = \"../../../../tests/math_expression_extraction/assets/math_equation.png\"\nextractor = DonutExtractor(device='cpu', show_log=False)\nresult = extractor.extract(image_path)\n</pre> image_path = \"../../../../tests/math_expression_extraction/assets/math_equation.png\" extractor = DonutExtractor(device='cpu', show_log=False) result = extractor.extract(image_path) <pre>INFO     [timestamp]2025-07-31 12:50:25[/] |                                                                       \n         [logger.name]omnidocs.tasks.math_expression_extraction.extractors.donut[/] | [function]donut.py:153[/] |  \n         [info]Loading Donut model from local path:                                                                \n         C:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\omnidocs\\models\\donut_models\\naver-clova-ix_don\n         ut-base-finetuned-cord-v2[/]                                                                              \n</pre> <pre>INFO     [timestamp]2025-07-31 12:50:25[/] |                                                                       \n         [logger.name]omnidocs.tasks.math_expression_extraction.extractors.donut[/] | [function]donut.py:153[/] |  \n         [info]Loading Donut model from local path:                                                                \n         C:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\omnidocs\\models\\donut_models\\naver-clova-ix_don\n         ut-base-finetuned-cord-v2[/]                                                                              \n</pre> <pre>Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nThe following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nPassing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n</pre> <pre>INFO     [timestamp]2025-07-31 12:50:40[/] |                                                                       \n         [logger.name]omnidocs.tasks.math_expression_extraction.extractors.donut[/] | [function]logging.py:150[/] |\n         [info]extract completed in 10.32s[/]                                                                      \n</pre> <pre>INFO     [timestamp]2025-07-31 12:50:40[/] |                                                                       \n         [logger.name]omnidocs.tasks.math_expression_extraction.extractors.donut[/] | [function]logging.py:150[/] |\n         [info]extract completed in 10.32s[/]                                                                      \n</pre> In\u00a0[3]: Copied! <pre>expr = result.expressions[0]\nprint(f\"LaTeX: {expr[:80]}...\")\n</pre> expr = result.expressions[0] print(f\"LaTeX: {expr[:80]}...\") <pre>LaTeX: &lt;s_menu&gt;&lt;s_nm&gt; lim&lt;/s_nm&gt;&lt;s_unitprice&gt; 9x-6&lt;/s_unitprice&gt;&lt;s_cnt&gt; 9x+24&lt;/s_cnt&gt;&lt;s...\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tasks/math_extraction/tutorials/donut.html#donut-math-extraction","title":"Donut Math Extraction\u00b6","text":""},{"location":"tasks/math_extraction/tutorials/nougat.html","title":"Nougat","text":"In\u00a0[\u00a0]: Copied! <pre>from omnidocs.tasks.math_expression_extraction.extractors.nougat import NougatExtractor\nprint(\"NougatExtractor imported successfully!\")\n</pre> from omnidocs.tasks.math_expression_extraction.extractors.nougat import NougatExtractor print(\"NougatExtractor imported successfully!\") In\u00a0[\u00a0]: Copied! <pre>import re \nimage_path = \"../../../../tests/math_expression_extraction/assets/math_equation.png\"\nextractor = NougatExtractor(device='cuda', show_log=False)\nresult = extractor.extract(image_path)\nprint(f\"Nougat: Found {len(result.expressions)} expressions\")\n</pre> import re  image_path = \"../../../../tests/math_expression_extraction/assets/math_equation.png\" extractor = NougatExtractor(device='cuda', show_log=False) result = extractor.extract(image_path) print(f\"Nougat: Found {len(result.expressions)} expressions\") In\u00a0[\u00a0]: Copied! <pre>expr = result.expressions[0]\nprint(f\"LaTeX: {expr[:80]}...\")\n</pre> expr = result.expressions[0] print(f\"LaTeX: {expr[:80]}...\")"},{"location":"tasks/math_extraction/tutorials/nougat.html#nougat-math-extraction","title":"Nougat Math Extraction\u00b6","text":""},{"location":"tasks/math_extraction/tutorials/suryamath.html","title":"SuryaMath","text":"In\u00a0[1]: Copied! <pre>from omnidocs.tasks.math_expression_extraction.extractors import SuryaMathExtractor\nprint(\"Surya imported successfully!\")\n</pre> from omnidocs.tasks.math_expression_extraction.extractors import SuryaMathExtractor print(\"Surya imported successfully!\") <pre>c:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n</pre> <pre>Surya imported successfully!\n</pre> In\u00a0[2]: Copied! <pre>image_path = \"../../../../tests/math_expression_extraction/assets/math_equation.png\"\nextractor = SuryaMathExtractor(device='cpu', show_log=False)\nresult = extractor.extract(image_path)\n</pre> image_path = \"../../../../tests/math_expression_extraction/assets/math_equation.png\" extractor = SuryaMathExtractor(device='cpu', show_log=False) result = extractor.extract(image_path) <pre>Detecting bboxes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:01&lt;00:00,  1.35s/it]\nRecognizing Text: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:04&lt;00:00,  4.20s/it]\n</pre> <pre>INFO     [timestamp]2025-07-31 12:53:03[/] |                                                                       \n         [logger.name]omnidocs.tasks.math_expression_extraction.extractors.surya_math[/] |                         \n         [function]logging.py:150[/] | [info]extract completed in 5.58s[/]                                         \n</pre> <pre>INFO     [timestamp]2025-07-31 12:53:03[/] |                                                                       \n         [logger.name]omnidocs.tasks.math_expression_extraction.extractors.surya_math[/] |                         \n         [function]logging.py:150[/] | [info]extract completed in 5.58s[/]                                         \n</pre> In\u00a0[3]: Copied! <pre>expr = result.expressions[0]\nprint(f\"   LaTeX: {expr[:80]}...\")\n</pre> expr = result.expressions[0] print(f\"   LaTeX: {expr[:80]}...\") <pre>   LaTeX: &lt;math display=\"block\"&gt;\\lim_{x\\to 2} \\left( \\frac{9x-6}{3x^2-10x+8} - \\frac{9x+24...\n</pre>"},{"location":"tasks/math_extraction/tutorials/suryamath.html#surya-math-extraction","title":"Surya Math Extraction\u00b6","text":""},{"location":"tasks/math_extraction/tutorials/unimernet.html","title":"UniMERNet","text":"In\u00a0[\u00a0]: Copied! <pre>from omnidocs.tasks.math_expression_extraction.extractors.unimernet import UniMERNetExtractor\nprint(\"UnimernetExtractor imported successfully!\")\n</pre> from omnidocs.tasks.math_expression_extraction.extractors.unimernet import UniMERNetExtractor print(\"UnimernetExtractor imported successfully!\") In\u00a0[\u00a0]: Copied! <pre>image_path = \"../../../../tests/math_expression_extraction/assets/math_equation.png\"\nextractor = UniMERNetExtractor(device='cuda', show_log=False)\nresult = extractor.extract(image_path)\nprint(f\"Unimernet: Found {len(result.expressions)} expressions\")\n</pre> image_path = \"../../../../tests/math_expression_extraction/assets/math_equation.png\" extractor = UniMERNetExtractor(device='cuda', show_log=False) result = extractor.extract(image_path) print(f\"Unimernet: Found {len(result.expressions)} expressions\")"},{"location":"tasks/math_extraction/tutorials/unimernet.html#unimernet-math-extractor","title":"Unimernet Math Extractor\u00b6","text":""},{"location":"tasks/ocr/overview.html","title":"Optical Character Recognition (OCR) in OmniDocs","text":"<p>OmniDocs provides a unified, production-ready interface for extracting text from images and documents using multiple OCR engines. Whether you need fast, lightweight extraction or advanced multilingual support, OmniDocs makes it easy to switch between backends and integrate OCR into your document workflows.</p>"},{"location":"tasks/ocr/overview.html#key-features","title":"\ud83d\ude80 Key Features","text":"<ul> <li>Multiple OCR Engines: Seamlessly switch between PaddleOCR, Tesseract, EasyOCR, and Surya OCR.</li> <li>Unified API: Consistent input/output formats across all engines.</li> <li>Multilingual Support: Extract text in dozens of languages, with automatic language mapping.</li> <li>Bounding Boxes &amp; Layout: Get word/line bounding boxes, reading order, and more.</li> <li>Visualization: Easily visualize OCR results on images.</li> <li>Batch Processing: Process single files or entire folders with the same interface.</li> </ul>"},{"location":"tasks/ocr/overview.html#supported-ocr-engines","title":"\ud83e\udde9 Supported OCR Engines","text":"Engine Source &amp; Docs License CPU GPU Highlights PaddleOCR GitHub Apache 2.0 \u2705 \u2705 Fast, accurate, layout-aware, 90+ languages Tesseract GitHub BSD-3-Clause \u2705 \u2705 Classic, robust, many languages EasyOCR GitHub MIT \u2705 \u2705 PyTorch-based, easy to use, many languages Surya OCR GitHub GPL-3.0-or-later \u2705 \u2705 Modern, high-accuracy, Indian languages"},{"location":"tasks/ocr/overview.html#quick-example","title":"\ud83d\udcdd Quick Example","text":"<pre><code>from omnidocs.tasks.ocr_extraction import EasyOCRExtractor\n\nextractor = EasyOCRExtractor(languages=[\"en\"], device=\"cpu\")\nresult = extractor.extract(\"path/to/image.png\")\nprint(result.full_text)\n</code></pre> <p>You can swap <code>EasyOCRExtractor</code> for <code>TesseractOCRExtractor</code>, <code>PaddleOCRExtractor</code>, or <code>SuryaOCRExtractor</code> with no code changes.</p>"},{"location":"tasks/ocr/overview.html#visualization","title":"\ud83c\udfa8 Visualization","text":"<p>OmniDocs can visualize OCR results with bounding boxes and recognized text:</p> <pre><code>extractor.visualize(result, \"path/to/image.png\", output_path=\"ocr_vis.png\", show_text=True)\n</code></pre>"},{"location":"tasks/ocr/overview.html#advanced-usage","title":"\ud83d\udcda Advanced Usage","text":"<ul> <li>Language Mapping: Standardizes language codes across engines.</li> <li>Batch Extraction: Use <code>extract_all</code> for folders or lists of images.</li> <li>Custom Preprocessing: Override or extend input preprocessing as needed.</li> </ul>"},{"location":"tasks/ocr/overview.html#tutorials-further-reading","title":"\ud83d\udcd6 Tutorials &amp; Further Reading","text":"<ul> <li>EasyOCR Tutorial</li> <li>Tesseract Tutorial</li> <li>PaddleOCR Tutorial</li> <li>Surya OCR Tutorial</li> <li>Visual Comparison OCR Test Notebook</li> <li>API Reference</li> </ul> <p>For more, see the README and the main OmniDocs documentation.</p>"},{"location":"tasks/ocr/tutorials/easyocr.html","title":"EasyOCR","text":"In\u00a0[1]: Copied! <pre>from omnidocs.tasks.ocr_extraction.extractors.easy_ocr import EasyOCRExtractor\n</pre> from omnidocs.tasks.ocr_extraction.extractors.easy_ocr import EasyOCRExtractor In\u00a0[\u00a0]: Copied! <pre>image_path = \"../../../../tests/ocr_extraction/assets/invoice.jpg\"\n\nextractor = EasyOCRExtractor()\n\nresult = extractor.extract(image_path)\n\nprint(f\"'{result.full_text[:200]}...'\")\n</pre> image_path = \"../../../../tests/ocr_extraction/assets/invoice.jpg\"  extractor = EasyOCRExtractor()  result = extractor.extract(image_path)  print(f\"'{result.full_text[:200]}...'\") <pre>INFO     [timestamp]2025-07-31 12:37:36[/] |                                   \n         [logger.name]omnidocs.tasks.ocr_extraction.extractors.easy_ocr[/] |   \n         [function]logging.py:150[/] | [info]extract completed in 4.16s[/]     \nINFO     [timestamp]2025-07-31 12:37:36[/] |                                   \n         [logger.name]omnidocs.tasks.ocr_extraction.extractors.easy_ocr[/] |   \n         [function]logging.py:150[/] | [info]extract completed in 4.16s[/]     \n</pre> <pre>[2025-07-31 12:37:36,175] [    INFO] logging.py:150 - extract completed in 4.16s\n</pre> <pre>'Invoice Account number: PATZD 32 Need holp? Normal business hours are: Involce number: 6312 Monday -Friday 8.00 am to 5.00 pm LABOR HOURLY RATE HOURS AMOUNT Jamle M S45.00 53,650 0D Tarric W 536.00 52...'\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tasks/ocr/tutorials/easyocr.html#easy-ocr","title":"Easy OCR\u00b6","text":""},{"location":"tasks/ocr/tutorials/paddle.html","title":"PaddleOCR","text":"In\u00a0[1]: Copied! <pre>from omnidocs.tasks.ocr_extraction.extractors.paddle import PaddleOCRExtractor\n</pre> from omnidocs.tasks.ocr_extraction.extractors.paddle import PaddleOCRExtractor  <pre>c:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n</pre> In\u00a0[\u00a0]: Copied! <pre>image_path = \"../../../../tests/ocr_extraction/assets/invoice.jpg\"\n\nextractor = PaddleOCRExtractor()\n\nresult = extractor.extract(image_path)\n\nprint(f\"'{result.full_text[:200]}...'\")\n</pre> image_path = \"../../../../tests/ocr_extraction/assets/invoice.jpg\"  extractor = PaddleOCRExtractor()  result = extractor.extract(image_path)  print(f\"'{result.full_text[:200]}...'\") <pre>INFO     [timestamp]2025-07-31 12:36:15[/] | [logger.name]omnidocs.tasks.ocr_extraction.extractors.paddle[/] |     \n         [function]logging.py:150[/] | [info]extract completed in 5.90s[/]                                         \n</pre> <pre>INFO     [timestamp]2025-07-31 12:36:15[/] | [logger.name]omnidocs.tasks.ocr_extraction.extractors.paddle[/] |     \n         [function]logging.py:150[/] | [info]extract completed in 5.90s[/]                                         \n</pre> <pre>[2025-07-31 12:36:15,484] [    INFO] logging.py:150 - extract completed in 5.90s\n</pre> <pre>'Invoice Account number:PAT20-32 Need help? Normal business hours are: Invoice number6312 Monday-Friday 8:00 am to 5:00 pm LABOR HOURLY RATE HOURS AMOUNT Jamle M. $45.00 82 $3,690.00 Tarric W. $36.00 6...'\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tasks/ocr/tutorials/paddle.html#paddle-ocr","title":"Paddle OCR\u00b6","text":""},{"location":"tasks/ocr/tutorials/suryaocr.html","title":"SuryaOCR","text":"In\u00a0[3]: Copied! <pre>from omnidocs.tasks.ocr_extraction.extractors.surya_ocr import SuryaOCRExtractor\n</pre> from omnidocs.tasks.ocr_extraction.extractors.surya_ocr import SuryaOCRExtractor  In\u00a0[5]: Copied! <pre>image_path = \"../../../../tests/ocr_extraction/assets/invoice.jpg\"\n\nextractor = SuryaOCRExtractor()\n\nresult = extractor.extract(image_path)\n\nprint(f\"'{result.full_text[:200]}...'\")\n</pre> image_path = \"../../../../tests/ocr_extraction/assets/invoice.jpg\"  extractor = SuryaOCRExtractor()  result = extractor.extract(image_path)  print(f\"'{result.full_text[:200]}...'\") <pre>Detecting bboxes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00&lt;00:00,  1.74it/s]\nRecognizing Text: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 86/86 [00:05&lt;00:00, 14.63it/s]\n</pre> <pre>INFO     [timestamp]2025-07-31 12:49:20[/] | [logger.name]omnidocs.tasks.ocr_extraction.extractors.surya_ocr[/] |  \n         [function]logging.py:150[/] | [info]extract completed in 6.61s[/]                                         \n</pre> <pre>INFO     [timestamp]2025-07-31 12:49:20[/] | [logger.name]omnidocs.tasks.ocr_extraction.extractors.surya_ocr[/] |  \n         [function]logging.py:150[/] | [info]extract completed in 6.61s[/]                                         \n</pre> <pre>[2025-07-31 12:49:20,411] [    INFO] logging.py:150 - extract completed in 6.61s\n</pre> <pre>'Invoice Account number: PAT20-32 Need help? Normal business hours are: Invoice number: 6312 Monday - Friday 8:00 am to 5:00 pm LABOR &lt;b&gt;HOURLY RATE&lt;/b&gt; &lt;b&gt;HOURS&lt;/b&gt; &lt;b&gt;AMOUNT&lt;/b&gt; Jamie M. $45.00 82 $3...'\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tasks/ocr/tutorials/suryaocr.html#suryaocr","title":"SuryaOCR\u00b6","text":""},{"location":"tasks/ocr/tutorials/tesseract.html","title":"Tesseract","text":"In\u00a0[1]: Copied! <pre>from omnidocs.tasks.ocr_extraction.extractors.tesseract_ocr import TesseractOCRExtractor\n</pre> from omnidocs.tasks.ocr_extraction.extractors.tesseract_ocr import TesseractOCRExtractor <pre>c:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n</pre> In\u00a0[2]: Copied! <pre>image_path = \"../../../../tests/ocr_extraction/assets/invoice.jpg\"\n\nextractor = TesseractOCRExtractor()\n\nresult = extractor.extract(image_path)\n\nprint(f\"'{result.full_text[:200]}...'\")\n</pre> image_path = \"../../../../tests/ocr_extraction/assets/invoice.jpg\"  extractor = TesseractOCRExtractor()  result = extractor.extract(image_path)  print(f\"'{result.full_text[:200]}...'\") <pre>INFO     [timestamp]2025-07-31 13:05:01[/] | [logger.name]omnidocs.tasks.ocr_extraction.extractors.tesseract_ocr[/]\n         | [function]logging.py:150[/] | [info]extract completed in 2.11s[/]                                       \n</pre> <pre>INFO     [timestamp]2025-07-31 13:05:01[/] | [logger.name]omnidocs.tasks.ocr_extraction.extractors.tesseract_ocr[/]\n         | [function]logging.py:150[/] | [info]extract completed in 2.11s[/]                                       \n</pre> <pre>[2025-07-31 13:05:01,798] [    INFO] logging.py:150 - extract completed in 2.11s\n</pre> <pre>'| . \u2018Account number PAT20-92 Need help? Normal business hous ore: Invoice number: 6912 i Monday \u2014Friday 8:00 am to 5:00 pm Lasor HOURLY RATE Hours AMOUNT Jamie M. $45.00 2 $3,690 Taro W. $36.00 oe $2,...'\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tasks/ocr/tutorials/tesseract.html#tesseract","title":"Tesseract\u00b6","text":""},{"location":"tasks/table_extraction/overview.html","title":"Table Extraction","text":"<p>OmniDocs provides multiple specialized extractors for table recognition in PDFs and images. Each extractor is implemented as a standalone class with a consistent interface, making it easy to swap between methods for different use cases.</p>"},{"location":"tasks/table_extraction/overview.html#available-table-extractors","title":"Available Table Extractors","text":"<ul> <li>CamelotExtractor: Accurate table extraction from PDFs (lattice and stream modes).</li> <li>PDFPlumberExtractor: Lightweight, fast PDF table extraction.</li> <li>SuryaTableExtractor: Deep learning-based table structure recognition.</li> <li>TabulaExtractor: Java-based PDF table extraction (requires Java runtime).</li> <li>TableTransformerExtractor: Transformer-based table detection and extraction.</li> <li>TableFormerExtractor: Advanced table structure parsing with deep learning.</li> </ul>"},{"location":"tasks/table_extraction/overview.html#tutorials","title":"Tutorials","text":"<ul> <li>Camelot</li> <li>PDFPlumber</li> <li>Surya Table</li> <li>Tabula</li> <li>TableTransformer</li> <li>TableFormer</li> </ul>"},{"location":"tasks/table_extraction/overview.html#basic-usage-example","title":"Basic Usage Example","text":"<pre><code>from omnidocs.tasks.table_extraction.extractors.camelot import CamelotExtractor\n\n# Initialize extractor\nextractor = CamelotExtractor()\n\n# Extract tables from PDF\nresults = extractor.extract(\"path/to/file.pdf\")\n\n# Access tables\nfor i, table in enumerate(results.tables):\n    print(f\"Table {i+1} as DataFrame:\\n\", table.df)\n</code></pre>"},{"location":"tasks/table_extraction/overview.html#batch-processing-multiple-pdfs","title":"Batch Processing Multiple PDFs","text":"<pre><code>import os\nfrom omnidocs.tasks.table_extraction.extractors.pdfplumber import PDFPlumberExtractor\n\nextractor = PDFPlumberExtractor()\npdf_folder = \"path/to/pdf_folder/\"\n\nfor filename in os.listdir(pdf_folder):\n    if filename.lower().endswith(\".pdf\"):\n        pdf_path = os.path.join(pdf_folder, filename)\n        try:\n            results = extractor.extract(pdf_path)\n            print(f\"{filename}: {len(results.tables)} tables found\")\n        except Exception as e:\n            print(f\"Error processing {filename}: {e}\")\n</code></pre>"},{"location":"tasks/table_extraction/overview.html#working-with-different-extractors","title":"Working with Different Extractors","text":"<pre><code>from omnidocs.tasks.table_extraction.extractors import (\n    CamelotExtractor, PDFPlumberExtractor, SuryaTableExtractor, TabulaExtractor\n)\n\nextractors = [CamelotExtractor(), PDFPlumberExtractor(), SuryaTableExtractor(), TabulaExtractor()]\npdf_path = \"sample.pdf\"\n\nfor extractor in extractors:\n    try:\n        results = extractor.extract(pdf_path)\n        print(f\"{extractor.__class__.__name__}: {len(results.tables)} tables\")\n    except Exception as e:\n        print(f\"{extractor.__class__.__name__} error: {e}\")\n</code></pre>"},{"location":"tasks/table_extraction/overview.html#output-format","title":"Output Format","text":"<p>All extractors return a <code>TableOutput</code> object with:</p> <pre><code>class TableOutput:\n    tables: List[TableResult]  # Each table as a DataFrame and metadata\n    source_file: str           # Path to the processed file\n</code></pre>"},{"location":"tasks/table_extraction/overview.html#installation-requirements","title":"Installation Requirements","text":"<p>Each extractor may require specific dependencies:</p>"},{"location":"tasks/table_extraction/overview.html#camelotextractor","title":"CamelotExtractor","text":"<pre><code>pip install camelot-py[cv] pandas\n</code></pre>"},{"location":"tasks/table_extraction/overview.html#pdfplumberextractor","title":"PDFPlumberExtractor","text":"<pre><code>pip install pdfplumber pandas\n</code></pre>"},{"location":"tasks/table_extraction/overview.html#suryatableextractor","title":"SuryaTableExtractor","text":"<pre><code>pip install surya-table torch pandas\n</code></pre>"},{"location":"tasks/table_extraction/overview.html#tabulaextractor","title":"TabulaExtractor","text":"<pre><code>pip install tabula-py pandas\n# Requires Java installed and in PATH\n</code></pre>"},{"location":"tasks/table_extraction/overview.html#tabletransformerextractor","title":"TableTransformerExtractor","text":"<pre><code>pip install table-transformer torch pandas\n</code></pre>"},{"location":"tasks/table_extraction/overview.html#tableformerextractor","title":"TableFormerExtractor","text":"<pre><code>pip install tableformer torch pandas\n</code></pre>"},{"location":"tasks/table_extraction/overview.html#troubleshooting","title":"Troubleshooting","text":"<p>1. Java Not Found (TabulaExtractor): - Ensure Java is installed and added to your system PATH.</p> <p>2. No Tables Detected: - Try a different extractor or adjust parameters (e.g., lattice/stream mode for Camelot). - Check PDF quality and ensure tables are not scanned images (use OCR if needed).</p> <p>3. Import Errors: - Install missing dependencies as shown above.</p> <p>4. Output Not as Expected: - Inspect the DataFrame output and adjust extraction settings.</p>"},{"location":"tasks/table_extraction/overview.html#best-practices","title":"Best Practices","text":"<ol> <li>Choose the Right Extractor:</li> <li>Use Camelot for vector PDFs with clear table lines.</li> <li>PDFPlumber works well for lightweight, fast extraction.</li> <li>SuryaTable, TableTransformer, or TableFormer handle complex or scanned tables effectively.</li> <li> <p>Tabula is ideal when you need Java-based extraction.</p> </li> <li> <p>Optimize Performance:</p> </li> <li>Batch process files and initialize extractors once.</li> <li> <p>Use GPU-enabled extractors for large-scale jobs.</p> </li> <li> <p>Handle Errors Gracefully:</p> </li> <li>Wrap extraction in try-except blocks.</li> <li> <p>Log or print errors for debugging.</p> </li> <li> <p>Validate Output:</p> </li> <li>Always inspect the DataFrame output for correctness.</li> <li>Post-process tables as needed for your workflow.</li> </ol>"},{"location":"tasks/table_extraction/tutorials/camelot.html","title":"Camelot","text":"In\u00a0[2]: Copied! <pre>from omnidocs.tasks.table_extraction.extractors import CamelotExtractor\n</pre> from omnidocs.tasks.table_extraction.extractors import CamelotExtractor <pre>c:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n</pre> In\u00a0[3]: Copied! <pre>pdf_path = \"../../../../tests/table_extraction/assets/table_document.pdf\"\nresult = CamelotExtractor().extract(pdf_path)\n</pre> pdf_path = \"../../../../tests/table_extraction/assets/table_document.pdf\" result = CamelotExtractor().extract(pdf_path) <pre>c:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\pypdf\\_crypt_providers\\_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from cryptography.hazmat.primitives.ciphers.algorithms in 48.0.0.\n  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n</pre> <pre>INFO     [timestamp]2025-07-31 13:10:29[/] | [logger.name]omnidocs.tasks.table_extraction.extractors.camelot[/] |  \n         [function]logging.py:150[/] | [info]extract completed in 1.29s[/]                                         \n</pre> <pre>INFO     [timestamp]2025-07-31 13:10:29[/] | [logger.name]omnidocs.tasks.table_extraction.extractors.camelot[/] |  \n         [function]logging.py:150[/] | [info]extract completed in 1.29s[/]                                         \n</pre> In\u00a0[4]: Copied! <pre>for i, table in enumerate(result.tables):\n                print(f\"\\nTable {i+1}: {table.num_rows} rows x {table.num_cols} columns\")\n                print(f\"Total cells: {len(table.cells)}\")\n\n                if table.cells:\n                    non_empty_cells = [cell for cell in table.cells if cell.text.strip()]\n                    print(f\"Non-empty cells: {len(non_empty_cells)}\")\n\n                    # Show first few cells\n                    for cell in table.cells[:10]:\n                        if cell.text.strip():\n                            text = cell.text.strip()[:30]\n                            print(f\"  [{cell.row},{cell.col}]: '{text}'\")\n</pre> for i, table in enumerate(result.tables):                 print(f\"\\nTable {i+1}: {table.num_rows} rows x {table.num_cols} columns\")                 print(f\"Total cells: {len(table.cells)}\")                  if table.cells:                     non_empty_cells = [cell for cell in table.cells if cell.text.strip()]                     print(f\"Non-empty cells: {len(non_empty_cells)}\")                      # Show first few cells                     for cell in table.cells[:10]:                         if cell.text.strip():                             text = cell.text.strip()[:30]                             print(f\"  [{cell.row},{cell.col}]: '{text}'\") <pre>\nTable 1: 6 rows x 6 columns\nTotal cells: 36\nNon-empty cells: 31\n  [0,0]: 'Disability \nCategory'\n  [0,1]: 'Participants'\n  [0,2]: 'Ballots \nCompleted'\n  [0,3]: 'Ballots \nIncomplete/ \nTerminat'\n  [0,4]: 'Results'\n</pre>"},{"location":"tasks/table_extraction/tutorials/camelot.html#camelot","title":"Camelot\u00b6","text":""},{"location":"tasks/table_extraction/tutorials/pdfplumber.html","title":"PDFPlumber","text":"In\u00a0[1]: Copied! <pre>from omnidocs.tasks.table_extraction.extractors import PDFPlumberExtractor\n</pre> from omnidocs.tasks.table_extraction.extractors import PDFPlumberExtractor In\u00a0[2]: Copied! <pre>pdf_path = \"../../../../tests/table_extraction/assets/table_document.pdf\"\nresult = PDFPlumberExtractor().extract(pdf_path)\n</pre> pdf_path = \"../../../../tests/table_extraction/assets/table_document.pdf\" result = PDFPlumberExtractor().extract(pdf_path) <pre>INFO     [timestamp]2025-08-02 12:41:56[/] |                                   \n         [logger.name]omnidocs.tasks.table_extraction.extractors.pdfplumber[/] \n         | [function]logging.py:150[/] | [info]extract completed in 0.90s[/]   \nINFO     [timestamp]2025-08-02 12:41:56[/] |                                   \n         [logger.name]omnidocs.tasks.table_extraction.extractors.pdfplumber[/] \n         | [function]logging.py:150[/] | [info]extract completed in 0.90s[/]   \n</pre> In\u00a0[5]: Copied! <pre>for i, table in enumerate(result.tables):\n                print(f\"\\nTable {i+1}: {table.num_rows} rows x {table.num_cols} columns\")\n                print(f\"Total cells: {len(table.cells)}\")\n\n                if table.cells:\n                    non_empty_cells = [cell for cell in table.cells if cell.text.strip()]\n                    print(f\"Non-empty cells: {len(non_empty_cells)}\")\n\n                    # Show first few cells\n                    for cell in table.cells[:20]:\n                        if cell.text.strip():\n                            text = cell.text.strip()[:50]\n                            print(f\"  [{cell.row},{cell.col}]: '{text}'\")\n</pre> for i, table in enumerate(result.tables):                 print(f\"\\nTable {i+1}: {table.num_rows} rows x {table.num_cols} columns\")                 print(f\"Total cells: {len(table.cells)}\")                  if table.cells:                     non_empty_cells = [cell for cell in table.cells if cell.text.strip()]                     print(f\"Non-empty cells: {len(non_empty_cells)}\")                      # Show first few cells                     for cell in table.cells[:20]:                         if cell.text.strip():                             text = cell.text.strip()[:50]                             print(f\"  [{cell.row},{cell.col}]: '{text}'\") <pre>\nTable 1: 14 rows x 18 columns\nTotal cells: 59\nNon-empty cells: 36\n  [0,13]: 'Results'\n  [1,10]: 'Ballots'\n  [2,1]: 'Disability'\n  [2,7]: 'Ballots'\n  [3,4]: 'Participants'\n  [3,10]: 'Incomplete/'\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tasks/table_extraction/tutorials/pdfplumber.html#pdf-plumber","title":"PDF plumber\u00b6","text":""},{"location":"tasks/table_extraction/tutorials/surya_table.html","title":"Surya Table","text":"In\u00a0[1]: Copied! <pre>from omnidocs.tasks.table_extraction.extractors import SuryaTableExtractor\n</pre> from omnidocs.tasks.table_extraction.extractors import SuryaTableExtractor <pre>c:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n</pre> In\u00a0[4]: Copied! <pre>image_path = \"../../../../tests/table_extraction/assets/table_image.png\"\nresult = SuryaTableExtractor().extract(image_path)\n</pre> image_path = \"../../../../tests/table_extraction/assets/table_image.png\" result = SuryaTableExtractor().extract(image_path)  <pre>Recognizing layout: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:04&lt;00:00,  4.21s/it]\nDetecting bboxes: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:03&lt;00:00,  3.03s/it]\nRecognizing Text: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 38/38 [00:20&lt;00:00,  1.87it/s]\n</pre> <pre>INFO     [timestamp]2025-07-31 13:25:01[/] | [logger.name]omnidocs.tasks.table_extraction.extractors.surya_table[/]\n         | [function]logging.py:150[/] | [info]extract completed in 27.89s[/]                                      \n</pre> <pre>INFO     [timestamp]2025-07-31 13:25:01[/] | [logger.name]omnidocs.tasks.table_extraction.extractors.surya_table[/]\n         | [function]logging.py:150[/] | [info]extract completed in 27.89s[/]                                      \n</pre> In\u00a0[5]: Copied! <pre>for i, table in enumerate(result.tables):\n                print(f\"\\nTable {i+1}: {table.num_rows} rows x {table.num_cols} columns\")\n                print(f\"Total cells: {len(table.cells)}\")\n\n                if table.cells:\n                    non_empty_cells = [cell for cell in table.cells if cell.text.strip()]\n                    print(f\"Non-empty cells: {len(non_empty_cells)}\")\n\n                    # Show first few cells\n                    for cell in table.cells[:10]:\n                        if cell.text.strip():\n                            text = cell.text.strip()[:30]\n                            print(f\"  [{cell.row},{cell.col}]: '{text}'\")\n</pre> for i, table in enumerate(result.tables):                 print(f\"\\nTable {i+1}: {table.num_rows} rows x {table.num_cols} columns\")                 print(f\"Total cells: {len(table.cells)}\")                  if table.cells:                     non_empty_cells = [cell for cell in table.cells if cell.text.strip()]                     print(f\"Non-empty cells: {len(non_empty_cells)}\")                      # Show first few cells                     for cell in table.cells[:10]:                         if cell.text.strip():                             text = cell.text.strip()[:30]                             print(f\"  [{cell.row},{cell.col}]: '{text}'\") <pre>\nTable 1: 12 rows x 6 columns\nTotal cells: 38\nNon-empty cells: 38\n  [0,0]: '&lt;b&gt;Results&lt;/b&gt;'\n  [1,0]: '&lt;b&gt;Ballots&lt;/b&gt;'\n  [2,0]: '&lt;b&gt;Disability&lt;/b&gt;'\n  [2,1]: '&lt;b&gt;Ballots&lt;/b&gt;'\n  [3,0]: '&lt;b&gt;Participants&lt;/b&gt;'\n  [3,1]: 'Incomplete/'\n  [4,0]: '&lt;b&gt;Category&lt;/b&gt;'\n  [4,1]: '&lt;b&gt;Completed&lt;/b&gt;'\n  [4,2]: '&lt;b&gt;Accuracy&lt;/b&gt;'\n  [4,3]: '&lt;b&gt;Time to&lt;/b&gt;'\n</pre>"},{"location":"tasks/table_extraction/tutorials/table_transformer.html","title":"Table transformer","text":"In\u00a0[1]: Copied! <pre>from omnidocs.tasks.table_extraction.extractors import TableTransformerExtractor\n</pre> from omnidocs.tasks.table_extraction.extractors import TableTransformerExtractor  <pre>c:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n</pre> In\u00a0[3]: Copied! <pre>image_path = \"../../../../tests/table_extraction/assets/table_image.png\"\nresult = TableTransformerExtractor().extract(image_path)\n</pre> image_path = \"../../../../tests/table_extraction/assets/table_image.png\" result = TableTransformerExtractor().extract(image_path) <pre>c:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer1.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer1.0.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer1.0.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer1.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer1.0.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer1.0.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer1.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer1.1.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer1.1.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer1.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer1.1.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer1.1.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer2.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer2.0.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer2.0.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer2.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer2.0.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer2.0.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer2.0.downsample.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer2.0.downsample.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer2.0.downsample.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer2.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer2.1.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer2.1.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer2.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer2.1.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer2.1.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer3.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer3.0.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer3.0.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer3.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer3.0.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer3.0.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer3.0.downsample.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer3.0.downsample.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer3.0.downsample.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer3.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer3.1.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer3.1.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer3.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer3.1.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer3.1.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer4.0.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer4.0.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer4.0.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer4.0.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer4.0.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer4.0.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer4.0.downsample.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer4.0.downsample.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer4.0.downsample.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer4.1.conv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer4.1.bn1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer4.1.bn1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer4.1.conv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer4.1.bn2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nc:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2409: UserWarning: for layer4.1.bn2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(\nSome weights of the model checkpoint at microsoft/table-transformer-detection were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n</pre> <pre>INFO     [timestamp]2025-07-31 13:27:17[/] |                                                                       \n         [logger.name]omnidocs.tasks.table_extraction.extractors.table_transfomer[/] | [function]logging.py:150[/] \n         | [info]extract completed in 1.50s[/]                                                                     \n</pre> <pre>INFO     [timestamp]2025-07-31 13:27:17[/] |                                                                       \n         [logger.name]omnidocs.tasks.table_extraction.extractors.table_transfomer[/] | [function]logging.py:150[/] \n         | [info]extract completed in 1.50s[/]                                                                     \n</pre> In\u00a0[4]: Copied! <pre>for i, table in enumerate(result.tables):\n                print(f\"\\nTable {i+1}: {table.num_rows} rows x {table.num_cols} columns\")\n                print(f\"Total cells: {len(table.cells)}\")\n\n                if table.cells:\n                    non_empty_cells = [cell for cell in table.cells if cell.text.strip()]\n                    print(f\"Non-empty cells: {len(non_empty_cells)}\")\n\n                    # Show first few cells\n                    for cell in table.cells[:10]:\n                        if cell.text.strip():\n                            text = cell.text.strip()[:30]\n                            print(f\"  [{cell.row},{cell.col}]: '{text}'\")\n</pre> for i, table in enumerate(result.tables):                 print(f\"\\nTable {i+1}: {table.num_rows} rows x {table.num_cols} columns\")                 print(f\"Total cells: {len(table.cells)}\")                  if table.cells:                     non_empty_cells = [cell for cell in table.cells if cell.text.strip()]                     print(f\"Non-empty cells: {len(non_empty_cells)}\")                      # Show first few cells                     for cell in table.cells[:10]:                         if cell.text.strip():                             text = cell.text.strip()[:30]                             print(f\"  [{cell.row},{cell.col}]: '{text}'\") <pre>\nTable 1: 4 rows x 7 columns\nTotal cells: 25\nNon-empty cells: 25\n  [0,0]: 'Cell_0_0'\n  [0,1]: 'Cell_0_1'\n  [0,2]: 'Cell_0_2'\n  [0,3]: 'Cell_0_3'\n  [0,4]: 'Cell_0_4'\n  [0,5]: 'Cell_0_5'\n  [0,6]: 'Cell_0_6'\n  [1,1]: 'Cell_1_1'\n  [1,2]: 'Cell_1_2'\n  [1,3]: 'Cell_1_3'\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tasks/table_extraction/tutorials/table_transformer.html#table-transformer-with-json-and-accurate-coordinates","title":"Table Transformer with json and accurate coordinates.\u00b6","text":"<p>refer: docs/getting_started/table_test.ipynb</p>"},{"location":"tasks/table_extraction/tutorials/tableformer.html","title":"TableFormer","text":"In\u00a0[1]: Copied! <pre>from omnidocs.tasks.table_extraction.extractors import TableFormerExtractor\n</pre> from omnidocs.tasks.table_extraction.extractors import TableFormerExtractor <pre>c:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n</pre> In\u00a0[2]: Copied! <pre>image_path = \"../../../../tests/table_extraction/assets/table_image.png\"\nresult = TableFormerExtractor().extract(image_path)\n</pre> image_path = \"../../../../tests/table_extraction/assets/table_image.png\" result = TableFormerExtractor().extract(image_path)  <pre>INFO     [timestamp]2025-07-31 13:29:40[/] | [logger.name]omnidocs.tasks.table_extraction.extractors.tableformer[/]\n         | [function]logging.py:150[/] | [info]extract completed in 1.50s[/]                                       \n</pre> <pre>INFO     [timestamp]2025-07-31 13:29:40[/] | [logger.name]omnidocs.tasks.table_extraction.extractors.tableformer[/]\n         | [function]logging.py:150[/] | [info]extract completed in 1.50s[/]                                       \n</pre> In\u00a0[3]: Copied! <pre>for i, table in enumerate(result.tables):\n                print(f\"\\nTable {i+1}: {table.num_rows} rows x {table.num_cols} columns\")\n                print(f\"Total cells: {len(table.cells)}\")\n\n                if table.cells:\n                    non_empty_cells = [cell for cell in table.cells if cell.text.strip()]\n                    print(f\"Non-empty cells: {len(non_empty_cells)}\")\n\n                    # Show first few cells\n                    for cell in table.cells[:10]:\n                        if cell.text.strip():\n                            text = cell.text.strip()[:30]\n                            print(f\"  [{cell.row},{cell.col}]: '{text}'\")\n</pre> for i, table in enumerate(result.tables):                 print(f\"\\nTable {i+1}: {table.num_rows} rows x {table.num_cols} columns\")                 print(f\"Total cells: {len(table.cells)}\")                  if table.cells:                     non_empty_cells = [cell for cell in table.cells if cell.text.strip()]                     print(f\"Non-empty cells: {len(non_empty_cells)}\")                      # Show first few cells                     for cell in table.cells[:10]:                         if cell.text.strip():                             text = cell.text.strip()[:30]                             print(f\"  [{cell.row},{cell.col}]: '{text}'\") <pre>\nTable 1: 6 rows x 7 columns\nTotal cells: 39\nNon-empty cells: 39\n  [0,0]: 'Cell_0_0'\n  [0,1]: 'Cell_0_1'\n  [0,2]: 'Cell_0_2'\n  [0,3]: 'Cell_0_3'\n  [0,4]: 'Cell_0_4'\n  [0,5]: 'Cell_0_5'\n  [0,6]: 'Cell_0_6'\n  [1,0]: 'Cell_1_0'\n  [1,1]: 'Cell_1_1'\n  [1,2]: 'Cell_1_2'\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tasks/table_extraction/tutorials/tableformer.html#tableformer-with-json-and-accurate-coordinates","title":"TableFormer with json and accurate coordinates.\u00b6","text":"<p>refer: docs/getting_started/table_test.ipynb</p>"},{"location":"tasks/table_extraction/tutorials/tabula.html","title":"Tabula","text":"In\u00a0[7]: Copied! <pre>from omnidocs.tasks.table_extraction.extractors import TabulaExtractor\n</pre> from omnidocs.tasks.table_extraction.extractors import TabulaExtractor  In\u00a0[8]: Copied! <pre>pdf_path = \"../../../../tests/table_extraction/assets/table_document.pdf\"\nresult = TabulaExtractor().extract(pdf_path)\n</pre> pdf_path = \"../../../../tests/table_extraction/assets/table_document.pdf\" result = TabulaExtractor().extract(pdf_path) <pre>INFO     [timestamp]2025-07-31 13:32:36[/] | [logger.name]omnidocs.tasks.table_extraction.extractors.tabula[/] |   \n         [function]logging.py:150[/] | [info]extract completed in 2.19s[/]                                         \n</pre> <pre>INFO     [timestamp]2025-07-31 13:32:36[/] | [logger.name]omnidocs.tasks.table_extraction.extractors.tabula[/] |   \n         [function]logging.py:150[/] | [info]extract completed in 2.19s[/]                                         \n</pre> In\u00a0[9]: Copied! <pre>for i, table in enumerate(result.tables):\n                print(f\"\\nTable {i+1}: {table.num_rows} rows x {table.num_cols} columns\")\n                print(f\"Total cells: {len(table.cells)}\")\n\n                if table.cells:\n                    non_empty_cells = [cell for cell in table.cells if cell.text.strip()]\n                    print(f\"Non-empty cells: {len(non_empty_cells)}\")\n\n                    # Show first few cells\n                    for cell in table.cells[:60]:\n                        if cell.text.strip():\n                            text = cell.text.strip()[:60]\n                            print(f\"  [{cell.row},{cell.col}]: '{text}'\")\n</pre> for i, table in enumerate(result.tables):                 print(f\"\\nTable {i+1}: {table.num_rows} rows x {table.num_cols} columns\")                 print(f\"Total cells: {len(table.cells)}\")                  if table.cells:                     non_empty_cells = [cell for cell in table.cells if cell.text.strip()]                     print(f\"Non-empty cells: {len(non_empty_cells)}\")                      # Show first few cells                     for cell in table.cells[:60]:                         if cell.text.strip():                             text = cell.text.strip()[:60]                             print(f\"  [{cell.row},{cell.col}]: '{text}'\") <pre>\nTable 1: 13 rows x 11 columns\nTotal cells: 143\nNon-empty cells: 36\n  [0,5]: 'Results'\n  [1,4]: 'Ballots'\n  [2,1]: 'Disability'\n  [2,4]: 'Ballots'\n  [3,2]: 'articipants'\n  [3,4]: 'Incomplete/'\n  [3,6]: 'Accuracy'\n  [3,9]: 'Time to'\n  [4,0]: 'Category'\n  [4,1]: 'Completed'\n  [5,3]: 'Terminated'\n</pre>"},{"location":"tasks/table_extraction/tutorials/tabula.html#tabula-table-extractor","title":"Tabula Table Extractor\u00b6","text":""},{"location":"tasks/text_extraction/overview.html","title":"Text Extraction","text":"<p>OmniDocs provides several extractors for extracting raw text from PDFs and images. Each extractor is implemented as a class with a consistent interface, making it easy to switch between methods for different document types.</p>"},{"location":"tasks/text_extraction/overview.html#available-text-extractors","title":"Available Text Extractors","text":"<ul> <li>PyMuPDFTextExtractor: Fast, reliable text extraction from PDFs using PyMuPDF.</li> <li>PdfplumberTextExtractor: Lightweight PDF text extraction with layout awareness.</li> <li>PyPDF2TextExtractor: Simple PDF text extraction using PyPDF2.</li> <li>PdftextTextExtractor: General PDF text extraction backend.</li> <li>SuryaTextExtractor: Deep learning-based text extraction for scanned or complex documents.</li> <li>DoclingTextExtractor: Advanced parsing and text extraction for structured documents.</li> </ul>"},{"location":"tasks/text_extraction/overview.html#tutorials","title":"Tutorials","text":"<ul> <li>Text Extractors</li> </ul>"},{"location":"tasks/text_extraction/overview.html#basic-usage-example","title":"Basic Usage Example","text":"<pre><code>from omnidocs.tasks.text_extraction.extractors.pymupdf import PyMuPDFExtractor\n\n# Initialize extractor\nextractor = PyMuPDFExtractor()\n\n# Extract text from PDF\nresult = extractor.extract(\"path/to/file.pdf\")\n\n# Access text\nprint(result.text)\n</code></pre>"},{"location":"tasks/text_extraction/overview.html#batch-processing-multiple-pdfs","title":"Batch Processing Multiple PDFs","text":"<pre><code>import os\nfrom omnidocs.tasks.text_extraction.extractors.pdfplumber import PDFPlumberExtractor\n\nextractor = PDFPlumberExtractor()\npdf_folder = \"path/to/pdf_folder/\"\n\nfor filename in os.listdir(pdf_folder):\n    if filename.lower().endswith(\".pdf\"):\n        pdf_path = os.path.join(pdf_folder, filename)\n        try:\n            result = extractor.extract(pdf_path)\n            print(f\"{filename}: {len(result.text)} characters extracted\")\n        except Exception as e:\n            print(f\"Error processing {filename}: {e}\")\n</code></pre>"},{"location":"tasks/text_extraction/overview.html#working-with-different-extractors","title":"Working with Different Extractors","text":"<pre><code>from omnidocs.tasks.text_extraction.extractors import (\n    PyMuPDFTextExtractor, PdfplumberTextExtractor, PyPDF2TextExtractor,\n    PdftextTextExtractor, SuryaTextExtractor, DoclingTextExtractor\n)\n\nextractors = [\n    PyMuPDFTextExtractor(),\n    PdfplumberTextExtractor(),\n    PyPDF2TextExtractor(),\n    PdftextTextExtractor(),\n    SuryaTextExtractor(),\n    DoclingTextExtractor()\n]\npdf_path = \"sample.pdf\"\n\nfor extractor in extractors:\n    try:\n        result = extractor.extract(pdf_path)\n        print(f\"{extractor.__class__.__name__}: {len(result.text)} characters\")\n    except Exception as e:\n        print(f\"{extractor.__class__.__name__} error: {e}\")\n</code></pre>"},{"location":"tasks/text_extraction/overview.html#output-format","title":"Output Format","text":"<p>All extractors return a <code>TextOutput</code> object with:</p> <pre><code>class TextOutput:\n    text: str           # Extracted text content\n    source_file: str    # Path to the processed file\n</code></pre>"},{"location":"tasks/text_extraction/overview.html#installation-requirements","title":"Installation Requirements","text":"<p>Each extractor may require specific dependencies:</p>"},{"location":"tasks/text_extraction/overview.html#pymupdftextextractor","title":"PyMuPDFTextExtractor","text":"<pre><code>pip install pymupdf\n</code></pre>"},{"location":"tasks/text_extraction/overview.html#pdfplumbertextextractor","title":"PdfplumberTextExtractor","text":"<pre><code>pip install pdfplumber\n</code></pre>"},{"location":"tasks/text_extraction/overview.html#pypdf2textextractor","title":"PyPDF2TextExtractor","text":"<pre><code>pip install pypdf2\n</code></pre>"},{"location":"tasks/text_extraction/overview.html#pdftexttextextractor","title":"PdftextTextExtractor","text":"<pre><code>pip install pdfminer.six\n</code></pre>"},{"location":"tasks/text_extraction/overview.html#suryatextextractor","title":"SuryaTextExtractor","text":"<pre><code>pip install surya-ocr torch\n</code></pre>"},{"location":"tasks/text_extraction/overview.html#doclingtextextractor","title":"DoclingTextExtractor","text":"<pre><code>pip install docling-tools\n</code></pre>"},{"location":"tasks/text_extraction/overview.html#troubleshooting","title":"Troubleshooting","text":"<p>1. No Text Detected:   - Try a different extractor.   - Check if the PDF is scanned (use OCR extractors if so).</p> <p>2. Import Errors:   - Install missing dependencies as shown above.</p> <p>3. Output Not as Expected:   - Inspect the extracted text and try another extractor for better results.</p>"},{"location":"tasks/text_extraction/overview.html#best-practices","title":"Best Practices","text":"<ol> <li>Choose the Right Extractor:</li> <li>Use PyMuPDF for fast, general-purpose extraction.</li> <li>Use PDFPlumber for layout-aware extraction.</li> <li> <p>Use PyPDF2 for simple, lightweight extraction.</p> </li> <li> <p>Optimize Performance:</p> </li> <li> <p>Batch process files and initialize extractors once.</p> </li> <li> <p>Handle Errors Gracefully:</p> </li> <li>Wrap extraction in try-except blocks.</li> <li> <p>Log or print errors for debugging.</p> </li> <li> <p>Validate Output:</p> </li> <li>Always inspect the extracted text for completeness and accuracy.</li> </ol>"},{"location":"tasks/text_extraction/tutorials/text_extractors.html","title":"Text Extractors","text":"In\u00a0[2]: Copied! <pre>from omnidocs.tasks.text_extraction.extractors import (\n    PyMuPDFExtractor,\n    PyPDF2Extractor,\n    PDFPlumberTextExtractor,\n    PDFTextExtractor,\n    DoclingExtractor,\n    SuryaTextExtractor\n)\n</pre> from omnidocs.tasks.text_extraction.extractors import (     PyMuPDFExtractor,     PyPDF2Extractor,     PDFPlumberTextExtractor,     PDFTextExtractor,     DoclingExtractor,     SuryaTextExtractor ) <pre>c:\\Users\\laxma\\OneDrive\\Desktop\\CogLab\\11-07-2025\\Omnidocs\\new\\Lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n  warnings.warn(\n</pre> In\u00a0[5]: Copied! <pre>pdf_extractors = [\n        PyMuPDFExtractor,\n        PyPDF2Extractor,\n        PDFPlumberTextExtractor,\n        PDFTextExtractor,\n        DoclingExtractor,\n        # SuryaTextExtractor pass image, it only works with images\n        \n]\npdf_path = \"../../../../tests/text_extraction/assets/sample_document.pdf\"\n</pre> pdf_extractors = [         PyMuPDFExtractor,         PyPDF2Extractor,         PDFPlumberTextExtractor,         PDFTextExtractor,         DoclingExtractor,         # SuryaTextExtractor pass image, it only works with images          ] pdf_path = \"../../../../tests/text_extraction/assets/sample_document.pdf\" In\u00a0[7]: Copied! <pre>for extractor_cls in pdf_extractors:\n    result = extractor_cls().extract(pdf_path)\n    full_text_preview = result.full_text.strip()[:200]\n    if len(result.full_text.strip()) &gt; 200:\n                full_text_preview += \"...\"\n    print(f\"\\nFull text preview: '{full_text_preview}'\")\n</pre> for extractor_cls in pdf_extractors:     result = extractor_cls().extract(pdf_path)     full_text_preview = result.full_text.strip()[:200]     if len(result.full_text.strip()) &gt; 200:                 full_text_preview += \"...\"     print(f\"\\nFull text preview: '{full_text_preview}'\") <pre>\nFull text preview: '1\n\nSample PDF\n\nCreated for testing PDFObject\n\nThis PDF is three pages long. Three long pages. Or three short pages if\n\nyou\u2019re optimistic. Is it the same as saying \u201cthree long minutes\u201d, knowing\n\nthat a...'\n\nFull text preview: '1 Sample PDF Created for testing PDFObject This PDF is three pages long. Three long pages. Or three short pages if you\u2019re optimistic. Is it the same as saying \u201cthree long minutes\u201d, knowing that all mi...'\n\nFull text preview: 'Sample\n\nPDF\n\nCreated\n\nfor\n\ntesting\n\nPDFObject\n\nThis\n\nPDF\n\nis\n\nthree\n\npages\n\nlong.\n\nThree\n\nlong\n\npages.\n\nOr\n\nthree\n\nshort\n\npages\n\nif\n\nyou\u2019re\n\noptimistic.\n\nIs\n\nit\n\nthe\n\nsame\n\nas\n\nsaying\n\n\u201cthree\n\nlong\n\nm...'\n\nFull text preview: '1\n\nSample PDF\n\nCreated for testing PDFObject\n\nThis PDF is three pages long. Three long pages. Or three short pages if\nyou\u2019re optimistic. Is it the same as saying \u201cthree long minutes\u201d, knowing\nthat all...'\n\nFull text preview: 'Sample PDF\n\nCreated for testing PDFObject\n\nThis PDF is three pages long. Three long pages. Or three short pages if you're optimistic. Is it the same as saying 'three long minutes', knowing that all mi...'\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tasks/text_extraction/tutorials/text_extractors.html#text-extraction-with-docling-parse-and-other-extractors","title":"Text Extraction with Docling Parse and Other Extractors\u00b6","text":"<p>This notebook demonstrates text extraction from PDF files using Docling Parse and all available extractors in OmniDocs.</p>"},{"location":"workflows/invoice_parser/implementation.html","title":"Implementation","text":""},{"location":"workflows/invoice_parser/implementation.html#parse-invoice","title":"Parse Invoice\u00b6","text":"<pre>from omnidocs.workflows.invoice_parser import InvoiceParser\n</pre>"},{"location":"workflows/pdf_to_md/implementation.html","title":"Implementation","text":""},{"location":"workflows/pdf_to_md/implementation.html#convert-pdf-to-markdown","title":"Convert PDF to Markdown\u00b6","text":"<pre>from omnidocs.workflows.pdf_to_md import PDFToMarkdown\n</pre>"}]}