{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"OmniDocs","text":"<p>Unified Python toolkit for visual document processing - Think Transformers for document AI</p> <p> </p> <p>Status: \ud83d\udea7 v0.2 - Document Loading Complete | Task Extractors In Progress</p>"},{"location":"#overview","title":"Overview","text":"<p>OmniDocs provides a consistent, type-safe API across multiple document processing models and tasks: - \u2705 Document Loading - Lazy-loaded PDFs and images with metadata (v0.2) - \ud83d\udea7 Layout Analysis - Coming soon - \ud83d\udea7 OCR Extraction - Coming soon - \ud83d\udea7 Text Extraction - Coming soon - \ud83d\udea7 Table Extraction - Coming soon</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install omnidocs\n</code></pre> <p>Or for development:</p> <pre><code>git clone https://github.com/adithya-s-k/OmniDocs.git\ncd OmniDocs/Omnidocs\nuv sync\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from omnidocs import Document\n\n# Load PDF with lazy rendering\ndoc = Document.from_pdf(\"paper.pdf\", dpi=150)\n\n# Access pages (rendered on demand)\npage = doc.get_page(0)  # PIL Image\ntext = doc.get_page_text(1)  # 1-indexed\n\n# Memory efficient iteration\nfor page in doc.iter_pages():\n    # Process each page\n    pass\n\n# Full document text (cached)\nfull_text = doc.text\n\n# Metadata\nprint(f\"Pages: {doc.page_count}\")\nprint(f\"Size: {doc.metadata.file_size}\")\n</code></pre>"},{"location":"#features","title":"Features","text":""},{"location":"#document-loading","title":"Document Loading \u2705","text":"<ul> <li>Multiple Sources: PDF files, URLs, bytes, images</li> <li>Lazy Loading: Pages rendered only when accessed</li> <li>MIT/Apache Licensed: pypdfium2 (Apache 2.0) + pdfplumber (MIT)</li> <li>Type-Safe: Pydantic models for configs and outputs</li> <li>Memory Efficient: Page caching with manual control</li> </ul> <pre><code># From file\ndoc = Document.from_pdf(\"file.pdf\", page_range=(0, 9))\n\n# From URL\ndoc = Document.from_url(\"https://arxiv.org/pdf/1706.03762\")\n\n# From bytes\ndoc = Document.from_bytes(pdf_bytes)\n\n# From images\ndoc = Document.from_image(\"page.png\")\ndoc = Document.from_images([\"p1.png\", \"p2.png\"])\n</code></pre>"},{"location":"#architecture","title":"Architecture","text":"<p>OmniDocs follows a clean, stateless design: - Document = Source data only (doesn't store task results) - Tasks = Analysis operations (layout, OCR, text extraction) - Backends = Inference engines (PyTorch, VLLM, MLX, API)</p> <p>See Design Documents for full architecture details.</p>"},{"location":"#development","title":"Development","text":"<p>Run tests:</p> <pre><code>uv run pytest tests/ -v\n</code></pre> <p>Run fast tests only:</p> <pre><code>uv run pytest tests/ -v -m \"not slow\"\n</code></pre> <p>Build docs:</p> <pre><code>uv run mkdocs serve\n</code></pre>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 - 3.11</li> <li>Dependencies managed with uv</li> </ul>"},{"location":"#license","title":"License","text":"<p>Apache 2.0 - See LICENSE for details</p>"},{"location":"#contributing","title":"Contributing","text":"<p>See CONTRIBUTING.md for development guidelines.</p>"},{"location":"#links","title":"Links","text":"<ul> <li>\ud83d\udcda Documentation</li> <li>\ud83d\udc1b Issues</li> <li>\ud83d\udce6 PyPI</li> <li>\ud83d\udcdd Changelog</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing to OmniDocs","text":"<p>Thank you for your interest in contributing to OmniDocs! \ud83c\udf89</p>"},{"location":"CONTRIBUTING/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/adithya-s-k/OmniDocs.git\ncd OmniDocs/Omnidocs\n</code></pre></p> </li> <li> <p>Install dependencies with uv: <pre><code># Install uv if you don't have it\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Sync dependencies\nuv sync\n</code></pre></p> </li> <li> <p>Run tests: <pre><code>uv run pytest tests/ -v\n</code></pre></p> </li> </ol>"},{"location":"CONTRIBUTING/#project-structure","title":"Project Structure","text":"<pre><code>Omnidocs/\n\u251c\u2500\u2500 omnidocs/          # Main package\n\u2502   \u251c\u2500\u2500 document.py    # Document loading (\u2705 complete)\n\u2502   \u251c\u2500\u2500 tasks/         # Task extractors (\ud83d\udea7 in progress)\n\u2502   \u251c\u2500\u2500 inference/     # Backend implementations (planned)\n\u2502   \u2514\u2500\u2500 utils/         # Utilities\n\u251c\u2500\u2500 tests/             # Test suite\n\u2502   \u251c\u2500\u2500 fixtures/      # Test data (PDFs, images)\n\u2502   \u2514\u2500\u2500 tasks/         # Future task tests\n\u2514\u2500\u2500 docs/              # Documentation\n</code></pre>"},{"location":"CONTRIBUTING/#design-documents","title":"Design Documents","text":"<p>\ud83d\udd34 IMPORTANT: Before implementing any new features, read the design documents: - <code>docs/architecture.md</code> - Backend and config system - <code>docs/developer-guide.md</code> - API design and usage patterns</p> <p>These documents define the architecture for v0.2+.</p>"},{"location":"CONTRIBUTING/#development-workflow","title":"Development Workflow","text":""},{"location":"CONTRIBUTING/#1-testing-phase-modal_scripts","title":"1. Testing Phase (modal_scripts/)","text":"<ul> <li>Test models in isolation using Modal scripts</li> <li>Validate inference and outputs</li> <li>Benchmark performance</li> </ul>"},{"location":"CONTRIBUTING/#2-integration-phase-omnidocs","title":"2. Integration Phase (omnidocs/)","text":"<ul> <li>Follow the config pattern (single-backend vs multi-backend)</li> <li>Use Pydantic for all configs and outputs</li> <li>Maintain consistent <code>.extract()</code> API</li> <li>Add comprehensive tests</li> </ul>"},{"location":"CONTRIBUTING/#3-documentation","title":"3. Documentation","text":"<ul> <li>Add docstrings (Google style)</li> <li>Update relevant docs</li> <li>Add usage examples</li> </ul>"},{"location":"CONTRIBUTING/#code-standards","title":"Code Standards","text":""},{"location":"CONTRIBUTING/#required","title":"\u2705 Required","text":"<ul> <li>Type hints for all public APIs</li> <li>Pydantic models for configs (<code>extra=\"forbid\"</code>)</li> <li>Docstrings (Google style) for classes and methods</li> <li>Tests with &gt;80% coverage</li> </ul>"},{"location":"CONTRIBUTING/#avoid","title":"\u274c Avoid","text":"<ul> <li>String-based factories (use class imports)</li> <li>Storing task results in Document</li> <li>Breaking changes without deprecation</li> <li>Adding features beyond requirements</li> <li>Over-engineering</li> </ul>"},{"location":"CONTRIBUTING/#version-management","title":"Version Management","text":"<p>OmniDocs follows Semantic Versioning: - MAJOR (X.0.0): Breaking API changes - MINOR (0.X.0): New features, backward compatible - PATCH (0.0.X): Bug fixes, backward compatible</p>"},{"location":"CONTRIBUTING/#incrementing-version","title":"Incrementing Version","text":"<p>Version is managed in one place: <code>omnidocs/_version.py</code></p> <pre><code># omnidocs/_version.py\n__version__ = \"0.2.0\"\n</code></pre> <p>The <code>pyproject.toml</code> reads from this file dynamically, so you only need to update one file.</p>"},{"location":"CONTRIBUTING/#version-bump-process","title":"Version Bump Process","text":"<ol> <li> <p>Update version: <pre><code># Edit omnidocs/_version.py\n# Change __version__ = \"0.2.0\" to __version__ = \"0.2.1\"\n</code></pre></p> </li> <li> <p>Verify (version should be accessible everywhere): <pre><code>from omnidocs import __version__\nprint(__version__)  # Should show new version\n</code></pre></p> </li> <li> <p>Commit: <pre><code>git add omnidocs/_version.py\ngit commit -m \"chore: bump version to 0.2.1\"\n</code></pre></p> </li> <li> <p>Tag and release (maintainers only): <pre><code>git tag v0.2.1\ngit push origin v0.2.1\n</code></pre></p> </li> </ol> <p>This will trigger the GitHub Actions workflow to: - Build the package - Create a GitHub release - Publish to PyPI</p>"},{"location":"CONTRIBUTING/#when-to-bump","title":"When to Bump","text":"<ul> <li>Patch (0.2.X): Bug fixes, typos, documentation</li> <li>Minor (0.X.0): New features, new task extractors, new models</li> <li>Major (X.0.0): Breaking API changes (rare, requires discussion)</li> </ul>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>OmniDocs uses MkDocs Material for documentation.</p>"},{"location":"CONTRIBUTING/#building-docs-locally","title":"Building Docs Locally","text":"<pre><code># Install docs dependencies\nuv sync --group docs\n\n# Serve docs with live reload\nuv run mkdocs serve\n\n# Open http://127.0.0.1:8000 in your browser\n</code></pre>"},{"location":"CONTRIBUTING/#building-static-site","title":"Building Static Site","text":"<pre><code># Build static site to site/ directory\nuv run mkdocs build\n\n# Build with strict mode (warnings become errors)\nuv run mkdocs build --strict\n</code></pre>"},{"location":"CONTRIBUTING/#documentation-structure","title":"Documentation Structure","text":"<pre><code>docs/\n\u251c\u2500\u2500 README.md              # Homepage (also serves as package README)\n\u251c\u2500\u2500 architecture.md        # Backend and config system design\n\u2514\u2500\u2500 developer-guide.md     # API design and usage patterns\n</code></pre>"},{"location":"CONTRIBUTING/#automatic-deployment","title":"Automatic Deployment","text":"<p>Documentation is automatically built and deployed to GitHub Pages when: - Code is pushed to the <code>master</code> branch - A maintainer manually triggers the workflow</p> <p>The docs are published at: https://adithya-s-k.github.io/OmniDocs/</p>"},{"location":"CONTRIBUTING/#adding-new-documentation","title":"Adding New Documentation","text":"<ol> <li>Create markdown files in <code>docs/</code></li> <li>Update <code>mkdocs.yml</code> nav section to include new pages</li> <li>Use Google-style docstrings in code (automatically extracted)</li> <li>Preview changes locally with <code>uv run mkdocs serve</code></li> </ol>"},{"location":"CONTRIBUTING/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Create a branch: <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make changes:</p> </li> <li>Follow the design patterns in docs/</li> <li>Add tests for new functionality</li> <li> <p>Update relevant documentation</p> </li> <li> <p>Run tests: <pre><code># Run all tests\nuv run pytest tests/ -v\n\n# Run fast tests only\nuv run pytest tests/ -v -m \"not slow\"\n</code></pre></p> </li> <li> <p>Submit PR:</p> </li> <li>Provide clear description</li> <li>Reference any related issues</li> <li>Ensure tests pass</li> </ol>"},{"location":"CONTRIBUTING/#commit-guidelines","title":"Commit Guidelines","text":"<p>Follow conventional commits: <pre><code>feat: add DocLayoutYOLO extractor\nfix: resolve page range validation\ndocs: update architecture guide\ntest: add fixture-based PDF tests\n</code></pre></p>"},{"location":"CONTRIBUTING/#need-help","title":"Need Help?","text":"<ul> <li>\ud83d\udcd6 Read the design documents</li> <li>\ud83d\udc1b Open an issue</li> <li>\ud83d\udcac Ask questions in discussions</li> </ul>"},{"location":"CONTRIBUTING/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the Apache 2.0 License.</p>"},{"location":"LICENSE/","title":"License","text":"<p>Apache License Version 2.0, January 2004 http://www.apache.org/licenses/</p>"},{"location":"LICENSE/#terms-and-conditions-for-use-reproduction-and-distribution","title":"TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION","text":""},{"location":"LICENSE/#1-definitions","title":"1. Definitions","text":"<p>\"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.</p>"},{"location":"LICENSE/#2-grant-of-copyright-license","title":"2. Grant of Copyright License","text":"<p>Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.</p>"},{"location":"LICENSE/#3-grant-of-patent-license","title":"3. Grant of Patent License","text":"<p>Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.</p>"},{"location":"LICENSE/#4-redistribution","title":"4. Redistribution","text":"<p>You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:</p> <ol> <li>You must give any other recipients of the Work or Derivative Works a copy of this License; and</li> <li>You must cause any modified files to carry prominent notices stating that You changed the files; and</li> <li>You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and</li> <li>If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.</li> </ol> <p>You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.</p>"},{"location":"LICENSE/#5-submission-of-contributions","title":"5. Submission of Contributions","text":"<p>Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.</p>"},{"location":"LICENSE/#6-trademarks","title":"6. Trademarks","text":"<p>This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.</p>"},{"location":"LICENSE/#7-disclaimer-of-warranty","title":"7. Disclaimer of Warranty","text":"<p>Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.</p>"},{"location":"LICENSE/#8-limitation-of-liability","title":"8. Limitation of Liability","text":"<p>In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.</p>"},{"location":"LICENSE/#9-accepting-warranty-or-additional-liability","title":"9. Accepting Warranty or Additional Liability","text":"<p>While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.</p>"},{"location":"LICENSE/#end-of-terms-and-conditions","title":"END OF TERMS AND CONDITIONS","text":"<p>Copyright 2025 OmniDocs Contributors</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"ROADMAP/","title":"OmniDocs Development Roadmap","text":""},{"location":"ROADMAP/#target-model-support","title":"\ud83d\udce6 Target Model Support","text":"<p>Research Date: January 2026 Status: Comprehensive model research completed Models Ordered By: Release date (newest first within each size category)</p>"},{"location":"ROADMAP/#quick-reference-model-capabilities-backend-support","title":"\ud83c\udfaf Quick Reference: Model Capabilities &amp; Backend Support","text":""},{"location":"ROADMAP/#comprehensive-model-comparison-table","title":"Comprehensive Model Comparison Table","text":"Model Size PyTorch VLLM MLX OpenAI API Tasks Release Granite-Docling-258M 258M \u2705 \u26a0\ufe0f \u2705 \u274c T, L, Tab, F Dec 2024 GOT-OCR2.0 700M \u2705 \u274c \u274c \u274c T, O, F, Tab Sep 2024 PaddleOCR-VL 900M \u2705 \u26a0\ufe0f \u274c \u274c T, L, O, Tab, F Oct 2025 LightOnOCR-1B 1B \u2705 \u274c \u274c \u274c T, O Oct 2025 LightOnOCR-2-1B 1B \u2705 \u2705 \u274c \u274c T, O Jan 2025 LightOnOCR-2-1B-bbox 1B \u2705 \u2705 \u274c \u274c T, L, O Jan 2025 MinerU2.5-2509-1.2B 1.2B \u2705 \u2705 \u2705 \u274c T, L, Tab, F, O Sep 2024 dots.ocr 1.7B \u2705 \u2705 \u274c \u274c T, L, Tab, F, O Dec 2024 Qwen3-VL-2B 2B \u2705 \u2705 \u2705 \u2705 T, L, S, O, Tab Oct 2025 DeepSeek-OCR 3B \u2705 \u2705 \u2705 \u2705 T, O, Tab Oct 2024 Qwen2.5-VL-3B 3B \u2705 \u2705 \u2705 \u2705 T, L, S, O 2024 Nanonets-OCR2-3B 3B \u2705 \u2705 \u2705 \u274c T, F, O 2024 Qwen3-VL-4B 4B \u2705 \u2705 \u2705 \u274c T, L, S, O, Tab Oct 2025 Gemma-3-4B-IT 4B \u2705 \u274c \u274c \u2705 T, S, O 2025 olmOCR-2-7B 7B \u2705 \u2705 \u274c \u2705 T, O, Tab, F Oct 2025 Qwen2.5-VL-7B 7B \u2705 \u2705 \u2705 \u2705 T, L, S, O, Tab 2024 Qwen3-VL-8B 8B \u2705 \u2705 \u2705 \u2705 T, L, S, O, Tab, F Oct 2025 Chandra 9B \u2705 \u2705 \u274c \u274c T, L, O, Tab, F 2024 Qwen3-VL-32B 32B \u2705 \u2705 \u2705 \u2705 T, L, S, O, Tab, F Oct 2025 Qwen2.5-VL-32B 32B \u2705 \u2705 \u2705 \u2705 T, L, S, O, Tab 2024 <p>Legend: - Tasks: T=Text Extract, L=Layout, O=OCR, S=Structured, Tab=Table, F=Formula - \u2705 = Fully supported | \u26a0\ufe0f = Limited/Partial support | \u274c = Not supported</p>"},{"location":"ROADMAP/#backend-details","title":"Backend Details","text":""},{"location":"ROADMAP/#pytorch-support","title":"PyTorch Support","text":"<ul> <li>All models support PyTorch via HuggingFace Transformers</li> <li>Primary development backend for all models</li> <li>Requirements: <code>transformers&gt;=4.46</code>, <code>torch&gt;=2.0</code></li> </ul>"},{"location":"ROADMAP/#vllm-support-high-throughput-production","title":"VLLM Support (High-Throughput Production)","text":"<p>Fully Supported (\u2705): - Qwen3-VL Series (vllm&gt;=0.11.0) - Qwen2.5-VL Series - DeepSeek-OCR (official upstream) - dots.ocr (recommended, vllm&gt;=0.9.1) - MinerU2.5 - olmOCR-2 (via olmOCR toolkit) - Chandra - LightOnOCR-2-1B (vllm&gt;=0.11.1) - Nanonets-OCR2-3B</p> <p>Limited Support (\u26a0\ufe0f): - Granite-Docling-258M (untied weights required) - PaddleOCR-VL (possible but not officially confirmed)</p> <p>Not Supported (\u274c): - GOT-OCR2.0 - Gemma-3-4B-IT - LightOnOCR-1B (legacy)</p>"},{"location":"ROADMAP/#mlx-support-apple-silicon-m1m2m3","title":"MLX Support (Apple Silicon M1/M2/M3+)","text":"<p>Fully Supported via mlx-community (\u2705): - Qwen3-VL Series - Collection   - 2B, 4B, 8B, 32B (4-bit, 8-bit variants) - Qwen2.5-VL Series - Collection   - 3B, 7B, 32B, 72B (4-bit, 8-bit variants) - DeepSeek-OCR - 4-bit, 8-bit - Granite-Docling-258M - Official MLX - MinerU2.5 - bf16 - Nanonets-OCR2-3B - 4-bit</p> <p>Usage: <pre><code>pip install mlx-vlm\npython -m mlx_vlm.generate --model mlx-community/Qwen3-VL-8B-Instruct-4bit \\\n  --prompt \"Extract text from this document\" --image doc.png\n</code></pre></p>"},{"location":"ROADMAP/#openai-compatible-api-providers","title":"OpenAI-Compatible API Providers","text":"<p>OpenRouter (openrouter.ai): - \u2705 Qwen3-VL-235B-A22B ($0.45/$3.50 per M tokens) - \u2705 Qwen3-VL-30B-A3B - \u2705 Qwen2.5-VL-3B (SOTA visual understanding) - \u2705 Qwen2.5-VL-32B (structured outputs, math) - \u2705 Qwen2.5-VL-72B (best overall)</p> <p>Novita AI (novita.ai): - \u2705 DeepSeek-OCR (Model Page) - \u2705 Qwen2.5-VL-72B (OCR + scientific reasoning) - \u2705 Qwen3-VL-8B ($0.08/$0.50 per M tokens)</p> <p>Together AI (together.ai): - \u2705 Various vision-language models - \u2705 Lightweight models with multilingual support</p> <p>Replicate (replicate.com): - \u2705 Vision models collection - \u2705 Pay-per-use inference</p> <p>Others: - DeepInfra: olmOCR-2-7B - Parasail: olmOCR-2-7B - Cirrascale: olmOCR-2-7B</p> <p>API Integration Example: <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenAPIConfig\n\n# OpenRouter\nextractor = QwenTextExtractor(\n    backend=QwenAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=\"YOUR_OPENROUTER_KEY\",\n        base_url=\"https://openrouter.ai/api/v1\"\n    )\n)\n\n# Novita AI\nextractor = QwenTextExtractor(\n    backend=QwenAPIConfig(\n        model=\"novita/qwen3-vl-8b-instruct\",\n        api_key=\"YOUR_NOVITA_KEY\",\n        base_url=\"https://api.novita.ai/v3/openai\"\n    )\n)\n</code></pre></p>"},{"location":"ROADMAP/#task-capability-matrix","title":"Task Capability Matrix","text":"Task Description Model Count Top Models Text Extract (T) Document \u2192 Markdown/HTML 18 LightOnOCR-2, Chandra, Qwen3-VL-8B Layout (L) Structure detection with bboxes 8 Qwen3-VL-8B, Chandra, MinerU2.5 OCR (O) Text + bbox coordinates 15 LightOnOCR-2, olmOCR-2, Chandra Structured (S) Schema-based extraction 5 Qwen3-VL (all), Qwen2.5-VL (all), Gemma-3 Table (Tab) Table detection/extraction 12 Qwen3-VL-8B, DeepSeek-OCR, olmOCR-2 Formula (F) Math expression recognition 8 Nanonets-OCR2, Qwen3-VL-8B, GOT-OCR2.0"},{"location":"ROADMAP/#model-overview-by-task-capability","title":"Model Overview by Task Capability","text":""},{"location":"ROADMAP/#task-categories","title":"Task Categories","text":"Task Description Model Count text_extract Document to Markdown/HTML conversion 18 layout Document structure detection with bounding boxes 8 ocr Text extraction with bbox coordinates 6 structured Schema-based data extraction 5 table Table detection and extraction 4 formula Mathematical expression recognition 3"},{"location":"ROADMAP/#core-models-by-size-release-date","title":"\ud83c\udfaf Core Models (By Size &amp; Release Date)","text":""},{"location":"ROADMAP/#ultra-compact-models-1b-parameters","title":"Ultra-Compact Models (&lt;1B Parameters)","text":""},{"location":"ROADMAP/#1-ibm-granite-docling-258m","title":"1. IBM Granite-Docling-258M","text":"<p>Released: December 2024 | Parameters: 258M | License: Apache 2.0</p> <p>HuggingFace: ibm-granite/granite-docling-258M</p> <p>Description: Ultra-compact vision-language model (VLM) for converting documents to machine-readable formats while fully preserving layout, tables, equations, and lists. Built on Idefics3 architecture with siglip2-base-patch16-512 vision encoder and Granite 165M LLM.</p> <p>Key Features: - End-to-end document understanding at 258M parameters - Handles inline/floating math, code, table structure - Rivals systems several times its size - Extremely cost-effective</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 MLX (Apple Silicon) - ibm-granite/granite-docling-258M-mlx - \u2705 WebGPU - Demo Space</p> <p>Integration: <pre><code>pip install docling  # Automatically downloads model\n</code></pre></p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>pillow</code>, <code>docling</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>table</code>, <code>formula</code></p> <p>Links: - Model Card - MLX Version - Demo Space - Official Docs - Collection</p>"},{"location":"ROADMAP/#2-stepfun-ai-got-ocr20","title":"2. stepfun-ai GOT-OCR2.0","text":"<p>Released: September 2024 | Parameters: 700M | License: Apache 2.0</p> <p>HuggingFace: stepfun-ai/GOT-OCR2_0</p> <p>Description: General OCR Theory model for multilingual OCR on plain documents, scene text, formatted documents, tables, charts, mathematical formulas, geometric shapes, molecular formulas, and sheet music.</p> <p>Key Features: - Interactive OCR with region-specific recognition (coordinate or color-based) - Plain text OCR + formatted text OCR (markdown, LaTeX) - Multi-page document processing - Wide range of specialized content types</p> <p>Model Variations: - stepfun-ai/GOT-OCR2_0 - Original with custom code - stepfun-ai/GOT-OCR-2.0-hf - HuggingFace-native transformers integration</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 Custom inference pipeline</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>pillow</code></p> <p>Tasks: <code>text_extract</code>, <code>ocr</code>, <code>formula</code>, <code>table</code></p> <p>Links: - Model Card - HF-Native Version</p>"},{"location":"ROADMAP/#compact-models-1-2b-parameters","title":"Compact Models (1-2B Parameters)","text":""},{"location":"ROADMAP/#3-rednote-hilab-dotsocr","title":"3. rednote-hilab dots.ocr","text":"<p>Released: December 2024 | Parameters: 1.7B | License: MIT</p> <p>HuggingFace: rednote-hilab/dots.ocr</p> <p>Description: Multilingual documents parsing model based on 1.7B LLM with SOTA performance. Provides faster inference than many high-performing models based on larger foundations.</p> <p>Key Features: - Task switching via prompt alteration only - Competitive detection vs traditional models (DocLayout-YOLO) - Built-in VLLM support for high throughput - Released with paper arXiv:2512.02498</p> <p>Model Variations: - rednote-hilab/dots.ocr - Full model - rednote-hilab/dots.ocr.base - Base variant</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM (Recommended for production) - vLLM 0.9.1+</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>vllm&gt;=0.9.1</code> (recommended)</p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>table</code>, <code>formula</code>, <code>ocr</code></p> <p>Links: - Model Card - GitHub - Live Demo - Paper - Collection</p>"},{"location":"ROADMAP/#4-paddlepaddle-paddleocr-vl","title":"4. PaddlePaddle PaddleOCR-VL","text":"<p>Released: October 2025 | Parameters: 900M | License: Apache 2.0</p> <p>HuggingFace: PaddlePaddle/PaddleOCR-VL</p> <p>Description: Ultra-compact multilingual documents parsing VLM with SOTA performance. Integrates NaViT-style dynamic resolution visual encoder with ERNIE-4.5-0.3B language model.</p> <p>Key Features: - Supports 109 languages - Excels in recognizing complex elements (text, tables, formulas, charts) - Minimal resource consumption - Fast inference speeds - SOTA in page-level parsing and element-level recognition</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers - officially integrated) - \u2705 PaddlePaddle framework</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>paddlepaddle</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>ocr</code>, <code>table</code>, <code>formula</code></p> <p>Links: - Model Card - Online Demo - Collection - Transformers Docs - GitHub - PaddleOCR</p>"},{"location":"ROADMAP/#5-lighton-ai-lightonocr-series","title":"5. LightOn AI LightOnOCR Series","text":"<p>Released: January 2025 (v2), October 2025 (v1) | Parameters: 1B | License: Apache 2.0</p> <p>HuggingFace Models: - lightonai/LightOnOCR-2-1B - Recommended for OCR - lightonai/LightOnOCR-2-1B-bbox - Best localization - lightonai/LightOnOCR-2-1B-bbox-soup - Balanced OCR + bbox - lightonai/LightOnOCR-1B-1025 - Legacy v1</p> <p>Description: Compact, end-to-end vision-language model for OCR and document understanding. State-of-the-art accuracy in its weight class while being several times faster than larger VLMs.</p> <p>Key Features: - LightOnOCR-2-1B: SOTA on OlmOCR-Bench (83.2 \u00b1 0.9), outperforms Chandra-9B - Performance: 3.3\u00d7 faster than Chandra, 1.7\u00d7 faster than OlmOCR, 5\u00d7 faster than dots.ocr - Variants: OCR-only, bbox-capable (figure/image localization), and balanced checkpoints - Paper: arXiv:2601.14251</p> <p>Model Comparison: | Model | Use Case | Bbox Support | |-------|----------|--------------| | LightOnOCR-2-1B | Default for PDF\u2192Text/Markdown | \u274c | | LightOnOCR-2-1B-bbox | Best localization of figures/images | \u2705 Best | | LightOnOCR-2-1B-bbox-soup | Balanced OCR + localization | \u2705 Balanced |</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers - upstream support) - \u26a0\ufe0f Requires transformers from source for v2 (not yet in stable release)</p> <p>Quantized Versions: - GGUF format</p> <p>Dependencies: <code>transformers&gt;=4.48</code> (from source for v2), <code>torch</code>, <code>pillow</code></p> <p>Tasks: <code>text_extract</code>, <code>ocr</code>, <code>layout</code> (bbox variants only)</p> <p>Links: - LightOnOCR-2 Blog - LightOnOCR-1 Blog - Demo Space - Paper (arXiv) - Organization</p>"},{"location":"ROADMAP/#6-opendatalab-mineru25","title":"6. opendatalab MinerU2.5","text":"<p>Released: September 2024 | Parameters: 1.2B | License: Apache 2.0</p> <p>HuggingFace: opendatalab/MinerU2.5-2509-1.2B</p> <p>Description: Decoupled vision-language model for efficient high-resolution document parsing with state-of-the-art accuracy and low computational overhead.</p> <p>Key Features: - Two-stage parsing: global layout analysis on downsampled images \u2192 fine-grained content recognition on native-resolution crops - Outperforms Gemini-2.5 Pro, Qwen2.5-VL-72B, GPT-4o, MonkeyOCR, dots.ocr, PP-StructureV3 - Large-scale diverse data engine for pretraining/fine-tuning - New performance records in text, formula, table recognition, and reading order</p> <p>Model Variations: - opendatalab/MinerU2.5-2509-1.2B - Official model - mlx-community/MinerU2.5-2509-1.2B-bf16 - MLX for Apple Silicon - Mungert/MinerU2.5-2509-1.2B-GGUF - GGUF quantized</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM (with OpenAI API specs) - \u2705 MLX (Apple Silicon)</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>vllm</code> (optional)</p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>table</code>, <code>formula</code>, <code>ocr</code></p> <p>Links: - Model Card - Paper (arXiv:2509.22186) - MLX Version - GGUF Version</p>"},{"location":"ROADMAP/#small-models-2-4b-parameters","title":"Small Models (2-4B Parameters)","text":""},{"location":"ROADMAP/#7-qwen3-vl-2b-instruct","title":"7. Qwen3-VL-2B-Instruct","text":"<p>Released: October 2025 | Parameters: 2B | License: Apache 2.0</p> <p>HuggingFace: Qwen/Qwen3-VL-2B-Instruct</p> <p>Description: Multimodal LLM from Alibaba Cloud's Qwen team with comprehensive upgrades: superior text understanding/generation, deeper visual perception/reasoning, extended context, and stronger agent interaction.</p> <p>Key Features: - Dense and MoE architectures that scale from edge to cloud - Instruct and reasoning-enhanced \"Thinking\" editions - Enhanced spatial and video dynamics comprehension - Part of Qwen3-VL multimodal retrieval framework (arXiv:2601.04720, 2026)</p> <p>Model Variations: - Qwen/Qwen3-VL-2B-Instruct - Instruction-tuned - Qwen/Qwen3-VL-2B-Thinking - Reasoning-enhanced - Qwen/Qwen3-VL-2B-Instruct-GGUF - Quantized GGUF</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM - \u2705 MLX (via mlx-community) - \u2705 API (via cloud providers)</p> <p>Dependencies: <code>transformers&gt;=4.46</code>, <code>torch</code>, <code>qwen-vl-utils</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>structured</code>, <code>ocr</code>, <code>table</code></p> <p>Links: - Model Card - GitHub - Collection - GGUF Version</p>"},{"location":"ROADMAP/#8-deepseek-ocr","title":"8. DeepSeek-OCR","text":"<p>Released: October 2024 | Parameters: ~3B | License: MIT</p> <p>HuggingFace: deepseek-ai/DeepSeek-OCR</p> <p>Description: High-accuracy OCR model from DeepSeek-AI for extracting text from complex visual inputs (documents, screenshots, receipts, natural scenes).</p> <p>Key Features: - Built for real-world documents: PDFs, forms, tables, handwritten/noisy text - Outputs clean, structured Markdown - VLLM support upstream - ~2500 tokens/s on A100 with vLLM - Paper: arXiv:2510.18234</p> <p>Model Variations: - deepseek-ai/DeepSeek-OCR - Official BF16 (~6.7 GB) - NexaAI/DeepSeek-OCR-GGUF - Quantized GGUF</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM (officially supported)</p> <p>Requirements: - Python 3.12.9 + CUDA 11.8 - <code>torch==2.6.0</code>, <code>transformers==4.46.3</code>, <code>flash-attn==2.7.3</code> - L4 / A100 GPUs (\u226516 GB VRAM)</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>vllm</code>, <code>flash-attn</code>, <code>einops</code></p> <p>Tasks: <code>text_extract</code>, <code>ocr</code>, <code>table</code></p> <p>Links: - Model Card - GitHub - GGUF Version - Demo Space</p>"},{"location":"ROADMAP/#9-nanonets-ocr2-3b","title":"9. Nanonets-OCR2-3B","text":"<p>Released: 2024 | Parameters: 3B | License: Apache 2.0</p> <p>HuggingFace: nanonets/Nanonets-OCR2-3B</p> <p>Description: State-of-the-art image-to-markdown OCR model that transforms documents into structured markdown with intelligent content recognition and semantic tagging, optimized for LLM downstream processing.</p> <p>Key Features: - LaTeX equation recognition (inline $...$ and display $$...$$) - Intelligent image description with structured tags (logos, charts, graphs) - 125K context window - ~7.53 GB model size</p> <p>Model Variations: - nanonets/Nanonets-OCR2-3B - Full BF16 - Mungert/Nanonets-OCR2-3B-GGUF - GGUF quantized - mlx-community/Nanonets-OCR2-3B-4bit - MLX 4-bit - yasserrmd/Nanonets-OCR2-3B - Ollama format</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 MLX (Apple Silicon) - \u2705 Ollama</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>pillow</code></p> <p>Tasks: <code>text_extract</code>, <code>formula</code>, <code>ocr</code></p> <p>Links: - Model Card - GGUF Version - MLX 4-bit - Ollama</p>"},{"location":"ROADMAP/#10-qwen3-vl-4b-instruct","title":"10. Qwen3-VL-4B-Instruct","text":"<p>Released: October 2025 | Parameters: 4B | License: Apache 2.0</p> <p>HuggingFace: Qwen/Qwen3-VL-4B-Instruct</p> <p>Description: Mid-size Qwen3-VL model with balanced performance and efficiency. Part of comprehensive multimodal model series with text understanding, visual reasoning, and agent capabilities.</p> <p>Model Variations: - Qwen/Qwen3-VL-4B-Instruct - Instruction-tuned - Qwen/Qwen3-VL-4B-Thinking - Reasoning-enhanced</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM - \u2705 MLX (via mlx-community) - \u2705 API (via cloud providers)</p> <p>Dependencies: <code>transformers&gt;=4.46</code>, <code>torch</code>, <code>qwen-vl-utils</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>structured</code>, <code>ocr</code>, <code>table</code></p> <p>Links: - Collection - GitHub</p>"},{"location":"ROADMAP/#11-google-gemma-3-4b-it","title":"11. Google Gemma-3-4B-IT","text":"<p>Released: 2025 | Parameters: 4B | License: Gemma License</p> <p>HuggingFace: google/gemma-3-4b-it</p> <p>Description: Lightweight, state-of-the-art multimodal model from Google built from same research/technology as Gemini. Handles text and image input, generates text output.</p> <p>Key Features: - 128K context window - Multilingual support (140+ languages) - SigLIP image encoder (896\u00d7896 square images) - Gemma-3-4B-IT beats Gemma-2-27B-IT on benchmarks</p> <p>Model Variations: - google/gemma-3-4b-it - Instruction-tuned (vision-capable) - google/gemma-3-4b-pt - Pre-trained base - google/gemma-3-4b-it-qat-q4_0-gguf - Quantized GGUF - bartowski/google_gemma-3-4b-it-GGUF - Community GGUF</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 Google AI SDK - \u2705 API (Google AI Studio)</p> <p>Dependencies: <code>transformers&gt;=4.46</code>, <code>torch</code>, <code>pillow</code></p> <p>Tasks: <code>text_extract</code>, <code>structured</code>, <code>ocr</code></p> <p>Links: - Model Card - Blog Post - Transformers Docs - Google Docs - DeepMind Page</p>"},{"location":"ROADMAP/#medium-models-7-9b-parameters","title":"Medium Models (7-9B Parameters)","text":""},{"location":"ROADMAP/#12-allenai-olmocr-2-7b-1025","title":"12. allenai olmOCR-2-7B-1025","text":"<p>Released: October 2025 | Parameters: 7B | License: Apache 2.0</p> <p>HuggingFace: allenai/olmOCR-2-7B-1025</p> <p>Description: State-of-the-art OCR for English-language digitized print documents. Fine-tuned from Qwen2.5-VL-7B-Instruct using olmOCR-mix-1025 dataset + GRPO RL training.</p> <p>Key Features: - 82.4 points on olmOCR-Bench (SOTA for real-world documents) - Substantial improvements where OCR often fails (math equations, tables, tricky cases) - Boosted via reinforcement learning (GRPO)</p> <p>Model Variations: - allenai/olmOCR-2-7B-1025 - Full BF16 version - allenai/olmOCR-2-7B-1025-FP8 - Recommended FP8 quantized (practical use except fine-tuning) - bartowski/allenai_olmOCR-2-7B-1025-GGUF - GGUF quantized - richardyoung/olmOCR-2-7B-1025-GGUF - Alternative GGUF</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM (recommended via olmOCR toolkit) - \u2705 API (DeepInfra, Parasail, Cirrascale)</p> <p>Best Usage: Via olmOCR toolkit with VLLM for efficient inference at scale (millions of documents).</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>vllm</code>, <code>olmocr</code> (toolkit Tasks: <code>text_extract</code>, <code>ocr</code>, <code>table</code>, <code>formula</code></p> <p>Links: - Model Card - FP8 Version - Blog Post - GGUF (bartowski)</p>"},{"location":"ROADMAP/#13-qwen3-vl-8b-instruct","title":"13. Qwen3-VL-8B-Instruct","text":"<p>Released: October 2025 | Parameters: 8B | License: Apache 2.0</p> <p>HuggingFace: Qwen/Qwen3-VL-8B-Instruct</p> <p>Description: Primary model in Qwen3-VL series with optimal balance of performance and efficiency. Enhanced document parsing over Qwen2.5-VL with improved visual perception, text understanding, and advanced reasoning.</p> <p>Key Features: - Custom layout label support (flexible VLM) - Extended context length - Enhanced spatial and video comprehension - Stronger agent interaction capabilities</p> <p>Model Variations: - Qwen/Qwen3-VL-8B-Instruct - Instruction-tuned - Qwen/Qwen3-VL-8B-Thinking - Reasoning-enhanced</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM - \u2705 MLX (Apple Silicon) - mlx-community/Qwen3-VL-8B-Instruct-4bit - \u2705 API (Novita AI, OpenRouter, etc.)</p> <p>API Providers: - Novita AI: Context 131K tokens, Max output 33K tokens   - Pricing: $0.08/M input tokens, $0.50/M output tokens</p> <p>Dependencies: <code>transformers&gt;=4.46</code>, <code>torch</code>, <code>qwen-vl-utils</code>, <code>vllm</code> (optional)</p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>structured</code>, <code>ocr</code>, <code>table</code>, <code>formula</code></p> <p>Links: - Model Card - Collection - GitHub - MLX 4-bit</p>"},{"location":"ROADMAP/#14-datalab-to-chandra","title":"14. datalab-to Chandra","text":"<p>Released: 2024 | Parameters: 9B | License: Apache 2.0</p> <p>HuggingFace: datalab-to/chandra</p> <p>Description: OCR model handling complex tables, forms, and handwriting with full layout preservation. Uses Qwen3VL for document understanding.</p> <p>Key Features: - 83.1 \u00b1 0.9 overall on OlmOCR benchmark (outperforms DeepSeek OCR, dots.ocr, olmOCR) - Strong grounding capabilities - Supports 40+ languages - Layout-aware output with bbox coordinates for every text block, table, and image - Outputs in HTML, Markdown, and JSON with detailed layout</p> <p>Use Cases: - Handwritten forms - Mathematical notation - Multi-column layouts - Complex tables</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM (production throughput)</p> <p>Installation: <pre><code>pip install chandra-ocr\n</code></pre></p> <p>Model Variations: - datalab-to/chandra - Official model - noctrex/Chandra-OCR-GGUF - GGUF quantized</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>vllm</code> (optional), <code>chandra-ocr</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>ocr</code>, <code>table</code>, <code>formula</code></p> <p>Links: - Model Card - GitHub - Blog Post - DeepWiki Docs - GGUF Version</p>"},{"location":"ROADMAP/#large-models-32b-parameters","title":"Large Models (32B+ Parameters)","text":""},{"location":"ROADMAP/#15-qwen3-vl-32b-instruct","title":"15. Qwen3-VL-32B-Instruct","text":"<p>Released: October 2025 | Parameters: 32B | License: Apache 2.0</p> <p>HuggingFace: Qwen/Qwen3-VL-32B-Instruct</p> <p>Description: Largest Qwen3-VL model with maximum performance for complex document understanding and multimodal reasoning tasks.</p> <p>Key Features: - Superior performance on complex documents - Extended context length - Enhanced reasoning capabilities - Production-grade for demanding applications</p> <p>Model Variations: - Qwen/Qwen3-VL-32B-Instruct - Instruction-tuned - Qwen/Qwen3-VL-32B-Thinking - Reasoning-enhanced</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM (recommended for production) - \u2705 API (cloud providers)</p> <p>GPU Requirements: A100 40GB+ or multi-GPU setup</p> <p>Dependencies: <code>transformers&gt;=4.46</code>, <code>torch</code>, <code>qwen-vl-utils</code>, <code>vllm</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>structured</code>, <code>ocr</code>, <code>table</code>, <code>formula</code></p> <p>Links: - Model Card - Collection - GitHub</p>"},{"location":"ROADMAP/#specialized-models","title":"Specialized Models","text":""},{"location":"ROADMAP/#16-docling-projectdocling-models","title":"16. docling-project/docling-models","text":"<p>Released: 2024 | Parameters: Various | License: Apache 2.0</p> <p>HuggingFace: docling-project/docling-models</p> <p>Description: Collection of models powering the Docling PDF document conversion package. Includes layout detection (RT-DETR) and table structure recognition (TableFormer).</p> <p>Models Included: 1. Layout Model: RT-DETR for detecting document components    - Labels: Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title 2. TableFormer Model: Table structure identification from images</p> <p>Note: Superseded by granite-docling-258M for end-to-end document conversion (receives updates and support).</p> <p>Backends Supported: - \u2705 PyTorch (via Docling library)</p> <p>Integration: <pre><code>pip install docling\n</code></pre></p> <p>Dependencies: <code>docling</code>, <code>transformers</code>, <code>torch</code></p> <p>Tasks: <code>layout</code>, <code>table</code></p> <p>Links: - Model Card - Vision Models Docs - SmolDocling (legacy)</p>"},{"location":"ROADMAP/#optional-models-legacyalternative","title":"\ud83d\udce6 Optional Models (Legacy/Alternative)","text":""},{"location":"ROADMAP/#qwen-25-vl-series-previous-generation","title":"Qwen 2.5-VL Series (Previous Generation)","text":""},{"location":"ROADMAP/#qwen25-vl-3b-instruct","title":"Qwen2.5-VL-3B-Instruct","text":"<p>Released: 2024 | Parameters: 3B | License: Apache 2.0</p> <p>HuggingFace: Qwen/Qwen2.5-VL-3B-Instruct</p> <p>Description: Previous generation Qwen VLM with strong visual understanding, agentic capabilities, video understanding (1+ hour), and structured outputs.</p> <p>Key Features: - Analyzes texts, charts, icons, graphics, layouts - Visual agent capabilities (computer use, phone use) - Video comprehension with temporal segment pinpointing - ViT architecture with SwiGLU and RMSNorm - Dynamic resolution + dynamic FPS sampling</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM - \u2705 MLX - \u2705 API</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>qwen-vl-utils</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>structured</code>, <code>ocr</code></p> <p>Links: - Model Card - Collection</p>"},{"location":"ROADMAP/#qwen25-vl-7b-instruct","title":"Qwen2.5-VL-7B-Instruct","text":"<p>Released: 2024 | Parameters: 7B | License: Apache 2.0</p> <p>HuggingFace: Qwen/Qwen2.5-VL-7B-Instruct</p> <p>Description: Mid-size Qwen2.5-VL model with same capabilities as 3B variant but enhanced performance.</p> <p>Model Variations: - Qwen/Qwen2.5-VL-7B-Instruct - Official - unsloth/Qwen2.5-VL-7B-Instruct-GGUF - GGUF quantized - nvidia/Qwen2.5-VL-7B-Instruct-NVFP4 - NVIDIA FP4 optimized</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM - \u2705 MLX - \u2705 API</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>qwen-vl-utils</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>structured</code>, <code>ocr</code>, <code>table</code></p> <p>Links: - Model Card - Collection - GGUF Version</p>"},{"location":"ROADMAP/#model-comparison-summary","title":"\ud83d\udcca Model Comparison Summary","text":""},{"location":"ROADMAP/#by-release-date-2024-2026","title":"By Release Date (2024-2026)","text":"Model Release Params Benchmark Score LightOnOCR-2-1B Jan 2025 1B 83.2 (OlmOCR) dots.ocr Dec 2024 1.7B 79.1 (OlmOCR) Granite-Docling-258M Dec 2024 258M N/A Chandra 2024 9B 83.1 (OlmOCR) Qwen3-VL Series Oct 2025 2-32B SOTA PaddleOCR-VL Oct 2025 900M SOTA olmOCR-2-7B Oct 2025 7B 82.4 (OlmOCR) DeepSeek-OCR Oct 2024 3B 75.4 (OlmOCR) GOT-OCR2.0 Sep 2024 700M N/A MinerU2.5 Sep 2024 1.2B SOTA"},{"location":"ROADMAP/#by-performance-olmocr-bench","title":"By Performance (OlmOCR-Bench)","text":"Rank Model Score Params 1 LightOnOCR-2-1B 83.2 \u00b1 0.9 1B 2 Chandra 83.1 \u00b1 0.9 9B 3 olmOCR-2-7B 82.4 7B 4 dots.ocr 79.1 1.7B 5 olmOCR (v1) 78.5 7B 6 DeepSeek-OCR 75.4 \u00b1 1.0 3B"},{"location":"ROADMAP/#by-speed-relative-performance","title":"By Speed (Relative Performance)","text":"Model Speed Multiplier Params LightOnOCR-2-1B Fastest baseline 1B PaddleOCR-VL 1.73\u00d7 slower 900M DeepSeek-OCR (vLLM) 1.73\u00d7 slower 3B olmOCR-2 1.7\u00d7 slower 7B Chandra 3.3\u00d7 slower 9B dots.ocr 5\u00d7 slower 1.7B"},{"location":"ROADMAP/#backend-support-matrix","title":"\ud83d\udd27 Backend Support Matrix","text":"Model PyTorch VLLM MLX API GGUF Granite-Docling-258M \u2705 \u274c \u2705 \u274c \u274c dots.ocr \u2705 \u2705 \u274c \u274c \u274c GOT-OCR2.0 \u2705 \u274c \u274c \u274c \u274c PaddleOCR-VL \u2705 \u274c \u274c \u274c \u274c MinerU2.5 \u2705 \u2705 \u2705 \u274c \u2705 LightOnOCR-2-1B \u2705 \u274c \u274c \u274c \u2705 Qwen3-VL (all) \u2705 \u2705 \u2705 \u2705 \u2705 DeepSeek-OCR \u2705 \u2705 \u274c \u274c \u2705 Nanonets-OCR2-3B \u2705 \u274c \u2705 \u274c \u2705 Gemma-3-4B-IT \u2705 \u274c \u274c \u2705 \u2705 olmOCR-2-7B \u2705 \u2705 \u274c \u2705 \u2705 Chandra \u2705 \u2705 \u274c \u274c \u2705 Qwen2.5-VL (all) \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"ROADMAP/#recommended-model-selection-guide","title":"\ud83d\udcda Recommended Model Selection Guide","text":""},{"location":"ROADMAP/#by-use-case","title":"By Use Case","text":"Use Case Recommended Model Why Edge/Mobile Deployment Granite-Docling-258M Ultra-compact (258M), MLX support Fast OCR (CPU) LightOnOCR-2-1B Fastest in class, SOTA accuracy Multilingual Documents PaddleOCR-VL 109 languages, minimal resources High-Throughput Serving dots.ocr + VLLM Built for VLLM, fast inference Best Accuracy (English) LightOnOCR-2-1B or Chandra SOTA on OlmOCR-Bench Custom Layout Detection Qwen3-VL-8B Flexible VLM with prompt-based labels Production Balanced Qwen3-VL-8B or olmOCR-2-7B Performance + reliability Complex Documents Chandra or Qwen3-VL-32B Handles tables, forms, handwriting Apple Silicon (M1/M2/M3) Granite-Docling-258M (MLX) Native MLX optimization Cost-Effective API Qwen3-VL-8B (Novita) $0.08/M tokens input"},{"location":"ROADMAP/#quick-start-examples","title":"\ud83d\ude80 Quick Start Examples","text":""},{"location":"ROADMAP/#ultra-compact-258m-granite-docling","title":"Ultra-Compact (258M) - Granite-Docling","text":"<pre><code>from omnidocs.tasks.text_extraction import GraniteDoclingOCR, GraniteDoclingConfig\n\nextractor = GraniteDoclingOCR(\n    config=GraniteDoclingConfig(device=\"cuda\")\n)\nresult = extractor.extract(image, output_format=\"markdown\")\n</code></pre>"},{"location":"ROADMAP/#fastest-ocr-1b-lightonocr-2","title":"Fastest OCR (1B) - LightOnOCR-2","text":"<pre><code>from omnidocs.tasks.text_extraction import LightOnOCRExtractor, LightOnOCRConfig\n\nextractor = LightOnOCRExtractor(\n    config=LightOnOCRConfig(\n        model=\"lightonai/LightOnOCR-2-1B\",\n        device=\"cuda\"\n    )\n)\nresult = extractor.extract(image, output_format=\"markdown\")\n</code></pre>"},{"location":"ROADMAP/#high-throughput-17b-dotsocr-vllm","title":"High-Throughput (1.7B) - dots.ocr + VLLM","text":"<pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nextractor = DotsOCRTextExtractor(\n    backend=DotsOCRVLLMConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.9\n    )\n)\nresult = extractor.extract(image, output_format=\"markdown\")\n</code></pre>"},{"location":"ROADMAP/#best-accuracy-7-9b-olmocr-2-or-chandra","title":"Best Accuracy (7-9B) - olmOCR-2 or Chandra","text":"<pre><code>from omnidocs.tasks.text_extraction import OlmOCRExtractor, ChandraTextExtractor\nfrom omnidocs.tasks.text_extraction.olm import OlmOCRVLLMConfig\nfrom omnidocs.tasks.text_extraction.chandra import ChandraPyTorchConfig\n\n# Option 1: olmOCR-2-7B with VLLM\nextractor = OlmOCRExtractor(\n    backend=OlmOCRVLLMConfig(\n        model=\"allenai/olmOCR-2-7B-1025-FP8\",\n        tensor_parallel_size=1\n    )\n)\n\n# Option 2: Chandra-9B\nextractor = ChandraTextExtractor(\n    backend=ChandraPyTorchConfig(\n        model=\"datalab-to/chandra\",\n        device=\"cuda\"\n    )\n)\n</code></pre>"},{"location":"ROADMAP/#flexible-custom-layouts-8b-qwen3-vl","title":"Flexible Custom Layouts (8B) - Qwen3-VL","text":"<pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenPyTorchConfig\n\nlayout = QwenLayoutDetector(\n    backend=QwenPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\"\n    )\n)\n\nresult = layout.extract(\n    image,\n    custom_labels=[\"code_block\", \"sidebar\", \"diagram\"]\n)\n</code></pre>"},{"location":"ROADMAP/#current-focus-layout-analysis-models","title":"\ud83c\udfaf Current Focus: Layout Analysis Models","text":""},{"location":"ROADMAP/#phase-1-multi-backend-vlm-integration","title":"Phase 1: Multi-Backend VLM Integration","text":""},{"location":"ROADMAP/#1-qwen3-vl-8b-instruct-integration","title":"1. Qwen3-VL-8B-Instruct Integration","text":"<p>Status: \ud83d\udfe1 In Progress</p> <p>Integrate Qwen3-VL-8B-Instruct for flexible layout detection with custom label support across all backends.</p> <p>Key Features: - Enhanced document parsing over Qwen2.5-VL - Improved visual perception and text understanding - Advanced reasoning capabilities - Custom layout label support</p>"},{"location":"ROADMAP/#implementation-checklist","title":"Implementation Checklist:","text":"<ul> <li>[ ] HuggingFace/PyTorch Backend (<code>QwenLayoutDetector</code> + <code>QwenPyTorchConfig</code>)</li> </ul> <p>Model: <code>Qwen/Qwen3-VL-8B-Instruct</code></p> <p>Config Class: <code>omnidocs/tasks/layout_analysis/qwen/pytorch.py</code> <pre><code>class QwenPyTorchConfig(BaseModel):\n    model: str = \"Qwen/Qwen3-VL-8B-Instruct\"\n    device: str = \"cuda\"\n    torch_dtype: Literal[\"auto\", \"float16\", \"bfloat16\"] = \"auto\"\n    attn_implementation: Optional[str] = None  # \"flash_attention_2\" if available\n    cache_dir: Optional[str] = None\n</code></pre></p> <p>Dependencies:   - <code>torch</code>, <code>transformers</code>   - <code>qwen-vl-utils</code> (model-specific utility)</p> <p>Reference Implementation: See <code>scripts/layout/modal_qwen3_vl_layout.py</code> in the repository</p> <p>Testing:   - Validate on synthetic document images   - Compare detection accuracy with ground truth   - Test custom label support</p> <ul> <li>[ ] VLLM Backend (<code>QwenVLLMConfig</code>)</li> </ul> <p>Model: <code>Qwen/Qwen3-VL-8B-Instruct</code></p> <p>Config Class: <code>omnidocs/tasks/layout_analysis/qwen/vllm.py</code> <pre><code>class QwenVLLMConfig(BaseModel):\n    model: str = \"Qwen/Qwen3-VL-8B-Instruct\"\n    tensor_parallel_size: int = 1\n    gpu_memory_utilization: float = 0.9\n    max_model_len: Optional[int] = None\n    trust_remote_code: bool = True\n</code></pre></p> <p>Dependencies:   - <code>vllm&gt;=0.4.0</code>   - <code>torch&gt;=2.0</code></p> <p>Use Case: High-throughput batch processing (10+ documents/second)</p> <p>Modal Config:   - GPU: <code>A10G:1</code> (minimum), <code>A100:1</code> (recommended for production)   - Image: VLLM GPU Image with flash-attn</p> <p>Testing:   - Benchmark throughput vs PyTorch   - Validate output consistency   - Test batch processing</p> <ul> <li>[ ] MLX Backend (<code>QwenMLXConfig</code>)</li> </ul> <p>Model: <code>mlx-community/Qwen3-VL-8B-Instruct-4bit</code></p> <p>Config Class: <code>omnidocs/tasks/layout_analysis/qwen/mlx.py</code> <pre><code>class QwenMLXConfig(BaseModel):\n    model: str = \"mlx-community/Qwen3-VL-8B-Instruct-4bit\"\n    quantization: Literal[\"4bit\", \"8bit\"] = \"4bit\"\n    max_tokens: int = 4096\n</code></pre></p> <p>Dependencies:   - <code>mlx&gt;=0.10</code>   - <code>mlx-lm&gt;=0.10</code></p> <p>Platform: Apple Silicon only (M1/M2/M3+)</p> <p>Use Case: Local development and testing on macOS</p> <p>Note: \u26a0\ufe0f DO NOT deploy MLX to Modal - local development only</p> <ul> <li>[ ] API Backend (<code>QwenAPIConfig</code>)</li> </ul> <p>Model: <code>qwen3-vl-8b-instruct</code></p> <p>Config Class: <code>omnidocs/tasks/layout_analysis/qwen/api.py</code> <pre><code>class QwenAPIConfig(BaseModel):\n    model: str = \"novita/qwen3-vl-8b-instruct\"\n    api_key: str\n    base_url: Optional[str] = None\n    max_tokens: int = 4096\n    temperature: float = 0.1\n</code></pre></p> <p>Provider: Novita AI   - Context Length: 131K tokens   - Max Output: 33K tokens   - Pricing:     - Input: $0.08/M tokens     - Output: $0.50/M tokens</p> <p>Dependencies:   - <code>litellm&gt;=1.30</code>   - <code>openai&gt;=1.0</code></p> <p>Use Case:   - Serverless deployments   - No GPU infrastructure required   - Cost-effective for low-volume processing</p> <ul> <li>[ ] Main Extractor Class (<code>omnidocs/tasks/layout_analysis/qwen.py</code>)</li> </ul> <p>Implement unified <code>QwenLayoutDetector</code> class:   <pre><code>from typing import Union, List, Optional\nfrom PIL import Image\nfrom .base import BaseLayoutExtractor\nfrom .models import LayoutOutput\nfrom .qwen import (\n    QwenPyTorchConfig,\n    QwenVLLMConfig,\n    QwenMLXConfig,\n    QwenAPIConfig,\n)\n\nQwenBackendConfig = Union[\n    QwenPyTorchConfig,\n    QwenVLLMConfig,\n    QwenMLXConfig,\n    QwenAPIConfig,\n]\n\nclass QwenLayoutDetector(BaseLayoutExtractor):\n    \"\"\"Flexible VLM-based layout detector with custom label support.\"\"\"\n\n    def __init__(self, backend: QwenBackendConfig):\n        self.backend_config = backend\n        self._backend = self._create_backend()\n\n    def extract(\n        self,\n        image: Image.Image,\n        custom_labels: Optional[List[str]] = None,\n    ) -&gt; LayoutOutput:\n        \"\"\"\n        Detect layout elements with optional custom labels.\n\n        Args:\n            image: PIL Image\n            custom_labels: Optional custom layout categories\n                Default: [\"title\", \"paragraph\", \"table\", \"figure\",\n                         \"caption\", \"formula\", \"list\"]\n\n        Returns:\n            LayoutOutput with detected bounding boxes\n        \"\"\"\n        # Implementation...\n</code></pre></p> <ul> <li>[ ] Integration Tests</li> </ul> <p>Test suite covering:   - All backend configurations   - Custom label functionality   - Cross-backend output consistency   - Edge cases (empty images, single elements, complex layouts)</p> <ul> <li> <p>[ ] Documentation</p> </li> <li> <p>API reference with examples for each backend</p> </li> <li>Performance comparison table (PyTorch vs VLLM vs MLX vs API)</li> <li>Migration guide from Qwen2.5-VL</li> <li> <p>Custom label usage examples</p> </li> <li> <p>[ ] Modal Deployment Script</p> </li> </ul> <p>Create production-ready deployment:   - <code>scripts/layout_omnidocs/modal_qwen_layout_vllm_online.py</code>   - Web endpoint for layout detection API   - Batch processing support   - Monitoring and logging</p>"},{"location":"ROADMAP/#phase-2-additional-layout-models","title":"Phase 2: Additional Layout Models","text":""},{"location":"ROADMAP/#2-rt-detr-layout-detector","title":"2. RT-DETR Layout Detector","text":"<ul> <li>[ ] Single-Backend Implementation (PyTorch only)</li> <li>Model: <code>RT-DETR</code> (Facebook AI)</li> <li>Fixed label support (COCO-based)</li> <li>Real-time detection optimization</li> </ul>"},{"location":"ROADMAP/#3-surya-layout-detector","title":"3. Surya Layout Detector","text":"<ul> <li>[ ] Single-Backend Implementation (PyTorch only)</li> <li>Model: <code>vikp/surya_layout</code></li> <li>Multi-language document support</li> <li>Optimized for speed</li> </ul>"},{"location":"ROADMAP/#4-florence-2-layout-detector","title":"4. Florence-2 Layout Detector","text":"<ul> <li>[ ] Multi-Backend Implementation</li> <li>HuggingFace/PyTorch backend</li> <li>API backend (Microsoft Azure)</li> <li>Object detection + dense captioning</li> </ul>"},{"location":"ROADMAP/#future-phases","title":"\ud83d\udd2e Future Phases","text":"<p>Additional task categories will be added after layout analysis is complete:</p> <ul> <li>OCR Extraction: Surya-OCR, PaddleOCR, Qwen-OCR</li> <li>Text Extraction: VLM-based Markdown/HTML extraction</li> <li>Table Extraction: Table Transformer, Surya-Table</li> <li>Math Expression Extraction: UniMERNet, Surya-Math</li> <li>Advanced Features: Reading order, image captioning, chart understanding</li> <li>Package &amp; Distribution: PyPI publishing, comprehensive documentation</li> </ul>"},{"location":"ROADMAP/#success-metrics-layout-analysis","title":"\ud83c\udfaf Success Metrics (Layout Analysis)","text":""},{"location":"ROADMAP/#performance-targets","title":"Performance Targets","text":"Metric Target Current Layout Detection Accuracy (mAP) &gt;90% TBD Inference Speed (PyTorch) &lt;2s per page TBD Inference Speed (VLLM) &lt;0.5s per page TBD Custom Label Support 100% functional TBD"},{"location":"ROADMAP/#quality-targets","title":"Quality Targets","text":"<ul> <li>[ ] Type hints coverage: 100%</li> <li>[ ] Docstring coverage: 100%</li> <li>[ ] Test coverage: &gt;80%</li> <li>[ ] All backends tested on production data</li> <li>[ ] Cross-backend output consistency validated</li> </ul>"},{"location":"ROADMAP/#infrastructure","title":"\ud83d\udd27 Infrastructure","text":""},{"location":"ROADMAP/#modal-deployment-standards","title":"Modal Deployment Standards","text":"<p>Consistency Requirements (as per CLAUDE.md):</p> <ul> <li>Volume Name: <code>omnidocs</code></li> <li>Secret Name: <code>adithya-hf-wandb</code></li> <li>CUDA Version: <code>12.4.0-devel-ubuntu22.04</code></li> <li>Python Version: <code>3.11</code> (3.12 for Qwen3-VL)</li> <li>Cache Directory: <code>/data/.cache</code> (HuggingFace)</li> <li>Model Cache: <code>/data/omnidocs_models</code></li> <li>Dependency Management: <code>.uv_pip_install()</code> (NO version pinning)</li> </ul>"},{"location":"ROADMAP/#gpu-configurations","title":"GPU Configurations","text":"GPU Use Case Cost (est.) <code>A10G:1</code> Development &amp; Testing $0.60/hr <code>A100:1</code> Production Inference $3.00/hr <code>A100:2</code> High-Throughput VLLM $6.00/hr"},{"location":"ROADMAP/#references","title":"\ud83d\udcda References","text":""},{"location":"ROADMAP/#design-documents","title":"Design Documents","text":"<ul> <li>Backend Architecture - Core design principles (see <code>IMPLEMENTATION_PLAN/BACKEND_ARCHITECTURE.md</code>)</li> <li>Developer Experience (DevEx) - API design and patterns (see <code>IMPLEMENTATION_PLAN/DEVEX.md</code>)</li> <li>Claude Development Guide - Implementation standards (see <code>CLAUDE.md</code> in repo root)</li> </ul>"},{"location":"ROADMAP/#external-resources","title":"External Resources","text":"<ul> <li>Qwen3-VL Model Card</li> <li>Qwen3-VL MLX (4bit)</li> <li>Modal Documentation</li> <li>UV Package Manager</li> </ul>"},{"location":"ROADMAP/#notes","title":"\ud83d\udcdd Notes","text":""},{"location":"ROADMAP/#implementation-order-rationale","title":"Implementation Order Rationale","text":"<ol> <li>Qwen3-VL Priority: Multi-backend support demonstrates v2.0 architecture</li> <li>RT-DETR: Fast fixed-label detection for production use</li> <li>Surya: Multi-language support and speed optimization</li> <li>Florence-2: Microsoft's advanced VLM capabilities</li> </ol>"},{"location":"ROADMAP/#breaking-changes-from-v10","title":"Breaking Changes from v1.0","text":"<ul> <li>String-based factory pattern removed (use class imports)</li> <li>Document class is now stateless (doesn't store results)</li> <li>Config classes are model-specific (not generic)</li> <li>Backend selection via config type (not string parameter)</li> </ul> <p>Last Updated: January 21, 2026 Maintainer: Adithya S Kolavi Version: 2.0.0-dev</p>"},{"location":"architecture/","title":"OmniDocs - Backend Architecture","text":"<p>Status: \u2705 Design Complete Last Updated: January 20, 2026 Version: 2.0.0</p>"},{"location":"architecture/#overview","title":"Overview","text":"<p>OmniDocs supports 4 inference backends:</p> Backend Use Case Platform Key Dependencies PyTorch Default local inference CPU/GPU torch, transformers VLLM High-throughput serving GPU only vllm MLX Apple Silicon optimization macOS M1/M2/M3+ mlx, mlx-lm API Hosted models Cloud litellm"},{"location":"architecture/#core-architecture-principles","title":"Core Architecture Principles","text":""},{"location":"architecture/#separation-of-concerns-__init__-vs-extract","title":"Separation of Concerns: <code>__init__</code> vs <code>.extract()</code>","text":"<p>OmniDocs maintains a clear separation between model initialization and runtime parameters:</p> <p><code>__init__</code> (via config) - Model Setup &amp; Verification - Which model to use - Which backend (PyTorch/VLLM/MLX/API) - Model loading settings (device, dtype, quantization) - Download and cache paths - Model verification and validation</p> <p><code>.extract()</code> - Runtime Task Parameters - Output format (markdown/html) - Custom prompts - Task-specific options (include_layout, custom_labels) - Per-call inference settings</p> <p>Example: <pre><code># Init: Model setup (happens once)\nextractor = QwenTextExtractor(\n    backend=QwenPyTorchConfig(\n        model=\"Qwen/Qwen2-VL-7B\",  # Which model\n        device=\"cuda\",              # Where to run\n        torch_dtype=\"bfloat16\",     # How to load\n    )\n)\n\n# Extract: Runtime params (can vary per call)\nresult1 = extractor.extract(image1, output_format=\"markdown\")\nresult2 = extractor.extract(image2, output_format=\"html\", custom_prompt=\"...\")\n</code></pre></p>"},{"location":"architecture/#design-principle-model-specific-configs","title":"Design Principle: Model-Specific Configs","text":"<p>Each model defines its own config classes for supported backends. This provides:</p> <ol> <li>IDE Autocomplete - Only relevant parameters shown</li> <li>Type Safety - Pydantic validation at config creation</li> <li>Clear Discoverability - Config exists = backend supported</li> <li>No Abstraction Leakage - Each backend can have unique parameters</li> </ol>"},{"location":"architecture/#config-class-structure","title":"Config Class Structure","text":""},{"location":"architecture/#single-backend-model","title":"Single-Backend Model","text":"<p>Models with only one backend (e.g., DocLayoutYOLO = PyTorch only):</p> <pre><code># omnidocs/tasks/layout_analysis/doc_layout_yolo.py\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\nfrom PIL import Image\n\nclass DocLayoutYOLOConfig(BaseModel):\n    \"\"\"Configuration for DocLayoutYOLO model.\"\"\"\n\n    device: str = Field(default=\"cuda\", description=\"Device to run on\")\n    model_path: Optional[str] = Field(default=None, description=\"Custom model weights\")\n    img_size: int = Field(default=1024, description=\"Input image size\")\n    confidence: float = Field(default=0.25, ge=0.0, le=1.0)\n\n    class Config:\n        extra = \"forbid\"  # Raise error on unknown params\n\n\nclass DocLayoutYOLO:\n    \"\"\"DocLayout-YOLO layout detector. PyTorch only.\"\"\"\n\n    def __init__(self, config: DocLayoutYOLOConfig):\n        self.config = config\n        self._load_model()\n\n    def _load_model(self):\n        \"\"\"Load model with PyTorch.\"\"\"\n        import torch\n        # Load model...\n\n    def extract(self, image: Image.Image) -&gt; LayoutOutput:\n        \"\"\"Run layout detection.\"\"\"\n        # Inference...\n        pass\n</code></pre>"},{"location":"architecture/#multi-backend-model","title":"Multi-Backend Model","text":"<p>Models with multiple backends (e.g., Qwen = PyTorch, VLLM, MLX, API):</p> <pre><code># omnidocs/tasks/text_extraction/qwen.py\n\nfrom typing import Union\nfrom PIL import Image\n\n# Import all backend configs\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenPyTorchConfig,\n    QwenVLLMConfig,\n    QwenMLXConfig,\n    QwenAPIConfig,\n)\n\n# Union type for all supported backends\nQwenBackendConfig = Union[\n    QwenPyTorchConfig,\n    QwenVLLMConfig,\n    QwenMLXConfig,\n    QwenAPIConfig,\n]\n\n\nclass QwenTextExtractor:\n    \"\"\"Qwen VLM text extractor. Supports PyTorch, VLLM, MLX, API backends.\"\"\"\n\n    def __init__(self, backend: QwenBackendConfig):\n        self.backend_config = backend\n        self._backend = self._create_backend()\n\n    def _create_backend(self):\n        \"\"\"Create appropriate backend based on config type.\"\"\"\n        if isinstance(self.backend_config, QwenPyTorchConfig):\n            from omnidocs.inference.pytorch import PyTorchInference\n            return PyTorchInference(self.backend_config)\n\n        elif isinstance(self.backend_config, QwenVLLMConfig):\n            from omnidocs.inference.vllm import VLLMInference\n            return VLLMInference(self.backend_config)\n\n        elif isinstance(self.backend_config, QwenMLXConfig):\n            from omnidocs.inference.mlx import MLXInference\n            return MLXInference(self.backend_config)\n\n        elif isinstance(self.backend_config, QwenAPIConfig):\n            from omnidocs.inference.api import APIInference\n            return APIInference(self.backend_config)\n\n        else:\n            raise TypeError(f\"Unknown backend config: {type(self.backend_config)}\")\n\n    def extract(\n        self,\n        image: Image.Image,\n        output_format: str = \"markdown\",\n        include_layout: bool = False,\n        custom_prompt: Optional[str] = None,\n    ) -&gt; TextOutput:\n        \"\"\"\n        Extract text from image.\n\n        Args:\n            image: PIL Image\n            output_format: \"markdown\" or \"html\"\n            include_layout: Include layout information\n            custom_prompt: Override default prompt\n\n        Returns:\n            TextOutput with extracted content\n        \"\"\"\n        prompt = custom_prompt or self._get_default_prompt(output_format, include_layout)\n        raw_output = self._backend.infer(image, prompt)\n        return self._postprocess(raw_output, output_format)\n</code></pre>"},{"location":"architecture/#backend-config-definitions","title":"Backend Config Definitions","text":""},{"location":"architecture/#pytorch-config","title":"PyTorch Config","text":"<pre><code># omnidocs/tasks/text_extraction/qwen/pytorch.py\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, Literal\n\nclass QwenPyTorchConfig(BaseModel):\n    \"\"\"PyTorch/HuggingFace backend configuration for Qwen.\"\"\"\n\n    model: str = Field(..., description=\"HuggingFace model ID\")\n    device: str = Field(default=\"cuda\", description=\"Device (cuda/cpu)\")\n    torch_dtype: Literal[\"float16\", \"bfloat16\", \"float32\"] = Field(\n        default=\"bfloat16\",\n        description=\"Torch dtype for model\"\n    )\n    trust_remote_code: bool = Field(default=True)\n    device_map: Optional[str] = Field(default=\"auto\")\n    max_memory: Optional[dict] = Field(default=None)\n    quantization: Optional[Literal[\"4bit\", \"8bit\"]] = Field(default=None)\n\n    class Config:\n        extra = \"forbid\"\n</code></pre>"},{"location":"architecture/#vllm-config","title":"VLLM Config","text":"<pre><code># omnidocs/tasks/text_extraction/qwen/vllm.py\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass QwenVLLMConfig(BaseModel):\n    \"\"\"VLLM backend configuration for Qwen.\"\"\"\n\n    model: str = Field(..., description=\"HuggingFace model ID\")\n    tensor_parallel_size: int = Field(default=1, ge=1)\n    gpu_memory_utilization: float = Field(default=0.9, ge=0.1, le=1.0)\n    max_model_len: Optional[int] = Field(default=None)\n    enforce_eager: bool = Field(default=False)\n    trust_remote_code: bool = Field(default=True)\n    dtype: str = Field(default=\"bfloat16\")\n\n    # VLLM-specific features\n    enable_prefix_caching: bool = Field(default=False)\n    enable_chunked_prefill: bool = Field(default=False)\n\n    class Config:\n        extra = \"forbid\"\n</code></pre>"},{"location":"architecture/#mlx-config","title":"MLX Config","text":"<pre><code># omnidocs/tasks/text_extraction/qwen/mlx.py\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, Literal\n\nclass QwenMLXConfig(BaseModel):\n    \"\"\"MLX backend configuration for Qwen (Apple Silicon).\"\"\"\n\n    model: str = Field(..., description=\"MLX model path or HuggingFace ID\")\n    quantization: Optional[Literal[\"4bit\", \"8bit\"]] = Field(default=None)\n    max_tokens: int = Field(default=4096)\n\n    class Config:\n        extra = \"forbid\"\n</code></pre>"},{"location":"architecture/#api-config","title":"API Config","text":"<pre><code># omnidocs/tasks/text_extraction/qwen/api.py\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, Dict\n\nclass QwenAPIConfig(BaseModel):\n    \"\"\"API backend configuration for Qwen (hosted or proxy).\"\"\"\n\n    model: str = Field(..., description=\"API model identifier\")\n    api_key: str = Field(..., description=\"API key\")\n    base_url: Optional[str] = Field(\n        default=None,\n        description=\"Custom API endpoint (for proxies)\"\n    )\n    rate_limit: int = Field(default=10, ge=1, description=\"Requests per minute\")\n    timeout: int = Field(default=30, ge=1, description=\"Request timeout in seconds\")\n    max_retries: int = Field(default=3, ge=0)\n    custom_headers: Optional[Dict[str, str]] = Field(default=None)\n\n    class Config:\n        extra = \"forbid\"\n</code></pre>"},{"location":"architecture/#inference-utilities","title":"Inference Utilities","text":"<p>The <code>omnidocs/inference/</code> module contains shared utilities for each backend:</p> <pre><code>omnidocs/inference/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 base.py          # Base inference class\n\u251c\u2500\u2500 pytorch.py       # PyTorch utilities\n\u251c\u2500\u2500 vllm.py          # VLLM utilities\n\u251c\u2500\u2500 mlx.py           # MLX utilities\n\u2514\u2500\u2500 api.py           # LiteLLM/API utilities\n</code></pre>"},{"location":"architecture/#base-inference-class","title":"Base Inference Class","text":"<pre><code># omnidocs/inference/base.py\n\nfrom abc import ABC, abstractmethod\nfrom typing import Any\nfrom PIL import Image\n\nclass BaseInference(ABC):\n    \"\"\"Base class for inference backends.\"\"\"\n\n    @abstractmethod\n    def load_model(self) -&gt; None:\n        \"\"\"Load model into memory.\"\"\"\n        pass\n\n    @abstractmethod\n    def infer(self, image: Image.Image, prompt: str) -&gt; Any:\n        \"\"\"Run inference.\"\"\"\n        pass\n\n    @abstractmethod\n    def unload(self) -&gt; None:\n        \"\"\"Free resources.\"\"\"\n        pass\n</code></pre>"},{"location":"architecture/#pytorch-inference","title":"PyTorch Inference","text":"<pre><code># omnidocs/inference/pytorch.py\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoProcessor\nfrom PIL import Image\nfrom .base import BaseInference\n\nclass PyTorchInference(BaseInference):\n    \"\"\"PyTorch/HuggingFace inference backend.\"\"\"\n\n    def __init__(self, config):\n        self.config = config\n        self.model = None\n        self.processor = None\n        self.load_model()\n\n    def load_model(self):\n        dtype_map = {\n            \"float16\": torch.float16,\n            \"bfloat16\": torch.bfloat16,\n            \"float32\": torch.float32,\n        }\n\n        self.processor = AutoProcessor.from_pretrained(\n            self.config.model,\n            trust_remote_code=self.config.trust_remote_code,\n        )\n\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.config.model,\n            torch_dtype=dtype_map[self.config.torch_dtype],\n            device_map=self.config.device_map,\n            trust_remote_code=self.config.trust_remote_code,\n        )\n\n        self.model.eval()\n\n    def infer(self, image: Image.Image, prompt: str):\n        inputs = self.processor(\n            text=prompt,\n            images=image,\n            return_tensors=\"pt\",\n        ).to(self.model.device)\n\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=4096,\n            )\n\n        return self.processor.decode(outputs[0], skip_special_tokens=True)\n\n    def unload(self):\n        if self.model:\n            del self.model\n            del self.processor\n            torch.cuda.empty_cache()\n</code></pre>"},{"location":"architecture/#vllm-inference","title":"VLLM Inference","text":"<pre><code># omnidocs/inference/vllm.py\n\nfrom PIL import Image\nfrom .base import BaseInference\n\nclass VLLMInference(BaseInference):\n    \"\"\"VLLM inference backend.\"\"\"\n\n    def __init__(self, config):\n        self.config = config\n        self.llm = None\n        self.load_model()\n\n    def load_model(self):\n        from vllm import LLM\n\n        self.llm = LLM(\n            model=self.config.model,\n            tensor_parallel_size=self.config.tensor_parallel_size,\n            gpu_memory_utilization=self.config.gpu_memory_utilization,\n            max_model_len=self.config.max_model_len,\n            enforce_eager=self.config.enforce_eager,\n            trust_remote_code=self.config.trust_remote_code,\n            dtype=self.config.dtype,\n        )\n\n    def infer(self, image: Image.Image, prompt: str):\n        from vllm import SamplingParams\n\n        sampling_params = SamplingParams(\n            max_tokens=4096,\n            temperature=0.0,\n        )\n\n        outputs = self.llm.generate(\n            {\n                \"prompt\": prompt,\n                \"multi_modal_data\": {\"image\": image},\n            },\n            sampling_params=sampling_params,\n        )\n\n        return outputs[0].outputs[0].text\n\n    def unload(self):\n        if self.llm:\n            del self.llm\n</code></pre>"},{"location":"architecture/#api-inference","title":"API Inference","text":"<pre><code># omnidocs/inference/api.py\n\nimport base64\nfrom io import BytesIO\nfrom PIL import Image\nfrom .base import BaseInference\n\nclass APIInference(BaseInference):\n    \"\"\"LiteLLM/API inference backend.\"\"\"\n\n    def __init__(self, config):\n        self.config = config\n        self.load_model()\n\n    def load_model(self):\n        \"\"\"Validate API configuration.\"\"\"\n        import litellm\n\n        # Configure LiteLLM\n        if self.config.base_url:\n            litellm.api_base = self.config.base_url\n\n    def infer(self, image: Image.Image, prompt: str):\n        import litellm\n\n        # Convert image to base64\n        buffered = BytesIO()\n        image.save(buffered, format=\"PNG\")\n        img_base64 = base64.b64encode(buffered.getvalue()).decode()\n\n        response = litellm.completion(\n            model=self.config.model,\n            api_key=self.config.api_key,\n            base_url=self.config.base_url,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": prompt},\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": f\"data:image/png;base64,{img_base64}\"\n                        }\n                    ]\n                }\n            ],\n            timeout=self.config.timeout,\n            num_retries=self.config.max_retries,\n        )\n\n        return response.choices[0].message.content\n\n    def unload(self):\n        pass  # Nothing to unload\n</code></pre>"},{"location":"architecture/#dependency-management","title":"Dependency Management","text":""},{"location":"architecture/#pyprojecttoml-structure","title":"pyproject.toml Structure","text":"<pre><code>[project]\nname = \"omnidocs\"\ndependencies = [\n    \"pydantic&gt;=2.0\",\n    \"pillow&gt;=10.0\",\n    \"numpy&gt;=1.24\",\n]\n\n[project.optional-dependencies]\n# Individual backends\npytorch = [\n    \"torch&gt;=2.0\",\n    \"torchvision&gt;=0.15\",\n    \"transformers&gt;=4.40\",\n]\n\nvllm = [\n    \"vllm&gt;=0.4.0\",\n    \"torch&gt;=2.0\",\n]\n\nmlx = [\n    \"mlx&gt;=0.10\",\n    \"mlx-lm&gt;=0.10\",\n]\n\napi = [\n    \"litellm&gt;=1.30\",\n    \"openai&gt;=1.0\",\n]\n\n# Convenience groups\nlocal = [\"omnidocs[pytorch]\"]\nall-local = [\"omnidocs[pytorch,vllm,mlx]\"]\nall = [\"omnidocs[pytorch,vllm,mlx,api]\"]\n\n# Development\ndev = [\"omnidocs[all]\", \"pytest\", \"black\", \"mypy\"]\n</code></pre>"},{"location":"architecture/#installation-examples","title":"Installation Examples","text":"<pre><code># Minimal (no inference)\npip install omnidocs\n\n# PyTorch only (most common)\npip install omnidocs[pytorch]\n\n# High-throughput serving\npip install omnidocs[vllm]\n\n# Apple Silicon\npip install omnidocs[mlx]\n\n# API only (no local inference)\npip install omnidocs[api]\n\n# Everything\npip install omnidocs[all]\n</code></pre>"},{"location":"architecture/#lazy-import-pattern","title":"Lazy Import Pattern","text":"<p>To avoid import errors when backends aren't installed:</p> <pre><code># omnidocs/tasks/text_extraction/qwen.py\n\nfrom typing import Union, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from omnidocs.tasks.text_extraction.qwen import (\n        QwenPyTorchConfig,\n        QwenVLLMConfig,\n        QwenMLXConfig,\n        QwenAPIConfig,\n    )\n\n\nclass QwenTextExtractor:\n    def __init__(self, backend):\n        self.backend_config = backend\n        self._backend = None\n        self._load_backend()\n\n    def _load_backend(self):\n        \"\"\"Lazy load backend based on config type.\"\"\"\n        config = self.backend_config\n        config_type = type(config).__name__\n\n        if config_type == \"QwenPyTorchConfig\":\n            try:\n                from omnidocs.inference.pytorch import PyTorchInference\n            except ImportError:\n                raise ImportError(\n                    \"PyTorch backend requires torch and transformers. \"\n                    \"Install with: pip install omnidocs[pytorch]\"\n                )\n            self._backend = PyTorchInference(config)\n\n        elif config_type == \"QwenVLLMConfig\":\n            try:\n                from omnidocs.inference.vllm import VLLMInference\n            except ImportError:\n                raise ImportError(\n                    \"VLLM backend requires vllm. \"\n                    \"Install with: pip install omnidocs[vllm]\"\n                )\n            self._backend = VLLMInference(config)\n\n        # ... etc\n</code></pre>"},{"location":"architecture/#error-handling","title":"Error Handling","text":""},{"location":"architecture/#config-validation","title":"Config Validation","text":"<pre><code>from pydantic import ValidationError\n\ntry:\n    config = QwenVLLMConfig(\n        model=\"Qwen/Qwen2-VL-7B\",\n        tensor_parallel_size=-1,  # Invalid!\n    )\nexcept ValidationError as e:\n    print(e)\n    # tensor_parallel_size: Input should be greater than or equal to 1\n</code></pre>"},{"location":"architecture/#backend-not-installed","title":"Backend Not Installed","text":"<pre><code>try:\n    extractor = QwenTextExtractor(\n        backend=QwenVLLMConfig(model=\"Qwen/Qwen2-VL-7B\")\n    )\nexcept ImportError as e:\n    print(e)\n    # VLLM backend requires vllm. Install with: pip install omnidocs[vllm]\n</code></pre>"},{"location":"architecture/#invalid-backend-for-model","title":"Invalid Backend for Model","text":"<pre><code># DotsOCR doesn't support API\nfrom omnidocs.tasks.text_extraction import DotsOCRTextExtractor\n\n# This import would fail because DotsOCRAPIConfig doesn't exist\n# from omnidocs.tasks.text_extraction.dotsocr import DotsOCRAPIConfig\n\n# User naturally discovers DotsOCR doesn't support API\n# because there's no config class to import\n</code></pre>"},{"location":"architecture/#layout-detection-fixed-vs-flexible-models","title":"Layout Detection: Fixed vs Flexible Models","text":""},{"location":"architecture/#overview_1","title":"Overview","text":"<p>Layout detection models in OmniDocs fall into two categories based on label flexibility:</p> Category Examples Label Support Implementation Fixed Labels DocLayoutYOLO, RT-DETR Predefined only Trained model classes Flexible VLM Qwen, Florence-2 Custom via prompting Vision-language models"},{"location":"architecture/#fixed-label-models","title":"Fixed Label Models","text":"<p>Models: DocLayoutYOLO, RTDETRLayoutDetector, SuryaLayoutDetector</p> <p>These models are trained on specific label sets (title, text, table, figure, etc.) and cannot detect custom elements.</p> <pre><code># omnidocs/tasks/layout_analysis/doc_layout_yolo.py\n\nclass DocLayoutYOLO:\n    \"\"\"Fixed label layout detector. PyTorch only.\"\"\"\n\n    FIXED_LABELS = [\"title\", \"text\", \"list\", \"table\", \"figure\", \"caption\", \"formula\"]\n\n    def __init__(self, config: DocLayoutYOLOConfig):\n        self.config = config\n        self._load_model()\n\n    def extract(self, image: Image.Image) -&gt; LayoutOutput:\n        \"\"\"\n        Extract layout with predefined labels only.\n\n        Args:\n            image: PIL Image\n\n        Returns:\n            LayoutOutput with bboxes using FIXED_LABELS\n        \"\"\"\n        # Run YOLO detection\n        detections = self.model(image)\n\n        # Map to fixed labels\n        bboxes = []\n        for det in detections:\n            label = self.FIXED_LABELS[det.class_id]\n            bboxes.append(LayoutBox(label=label, bbox=det.bbox, confidence=det.conf))\n\n        return LayoutOutput(bboxes=bboxes)\n</code></pre>"},{"location":"architecture/#flexible-vlm-models","title":"Flexible VLM Models","text":"<p>Models: QwenLayoutDetector, Florence2LayoutDetector, VLMLayoutDetector</p> <p>These models use vision-language prompting and can detect ANY custom layout elements.</p> <pre><code># omnidocs/tasks/layout_analysis/qwen.py\n\nfrom typing import Union, List, Optional\nfrom omnidocs.tasks.layout_analysis.models import CustomLabel, LayoutOutput\n\nclass QwenLayoutDetector:\n    \"\"\"Flexible VLM layout detector. Supports custom labels.\"\"\"\n\n    DEFAULT_LABELS = [\"title\", \"text\", \"list\", \"table\", \"figure\", \"caption\", \"formula\"]\n\n    def __init__(self, backend: QwenBackendConfig):\n        self.backend_config = backend\n        self._backend = self._create_backend()\n\n    def extract(\n        self,\n        image: Image.Image,\n        custom_labels: Optional[Union[List[str], List[CustomLabel]]] = None,\n    ) -&gt; LayoutOutput:\n        \"\"\"\n        Extract layout with flexible label support.\n\n        Args:\n            image: PIL Image\n            custom_labels:\n                - None: Use DEFAULT_LABELS\n                - List[str]: Simple custom label names\n                - List[CustomLabel]: Structured labels with metadata\n\n        Returns:\n            LayoutOutput with detected elements\n        \"\"\"\n        # Normalize labels\n        if custom_labels is None:\n            labels = [CustomLabel(name=name) for name in self.DEFAULT_LABELS]\n        else:\n            labels = self._normalize_labels(custom_labels)\n\n        # Build detection prompt\n        prompt = self._build_prompt(labels)\n\n        # Run VLM inference\n        raw_output = self._backend.infer(image, prompt)\n\n        # Parse results\n        return self._parse_detections(raw_output, labels)\n\n    def _normalize_labels(\n        self,\n        labels: Union[List[str], List[CustomLabel]]\n    ) -&gt; List[CustomLabel]:\n        \"\"\"Convert string labels to CustomLabel objects.\"\"\"\n        normalized = []\n        for label in labels:\n            if isinstance(label, str):\n                normalized.append(CustomLabel(name=label))\n            elif isinstance(label, CustomLabel):\n                normalized.append(label)\n        return normalized\n\n    def _build_prompt(self, labels: List[CustomLabel]) -&gt; str:\n        \"\"\"Build detection prompt from labels.\"\"\"\n        label_descriptions = []\n\n        for label in labels:\n            if label.detection_prompt:\n                # Use custom detection prompt\n                label_descriptions.append(\n                    f\"- {label.name}: {label.detection_prompt}\"\n                )\n            else:\n                # Use label name only\n                label_descriptions.append(f\"- {label.name}\")\n\n        prompt = f\"\"\"Detect the following layout elements in this document image:\n\n{chr(10).join(label_descriptions)}\n\nReturn bounding boxes [x1, y1, x2, y2] for each detected element.\"\"\"\n\n        return prompt\n</code></pre>"},{"location":"architecture/#customlabel-definition","title":"CustomLabel Definition","text":"<pre><code># omnidocs/tasks/layout_analysis/models.py\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass CustomLabel(BaseModel):\n    \"\"\"Custom layout label definition for flexible VLM models.\"\"\"\n\n    name: str = Field(..., description=\"Label identifier (e.g., 'code_block')\")\n\n    description: Optional[str] = Field(\n        default=None,\n        description=\"Human-readable description\"\n    )\n\n    detection_prompt: Optional[str] = Field(\n        default=None,\n        description=\"Custom prompt hint for model to use during detection\"\n    )\n\n    color: Optional[str] = Field(\n        default=None,\n        description=\"Visualization color (hex or name)\"\n    )\n\n    class Config:\n        extra = \"allow\"  # Users can add custom fields\n</code></pre>"},{"location":"architecture/#usage-examples","title":"Usage Examples","text":"<p>Fixed Model (Simple, Fast): <pre><code>from omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\n\nlayout = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = layout.extract(image)\n# Returns: title, text, table, figure (fixed set)\n</code></pre></p> <p>Flexible VLM (Simple Strings): <pre><code>from omnidocs.tasks.layout_analysis import QwenLayoutDetector\nfrom omnidocs.tasks.layout_analysis.qwen import QwenPyTorchConfig\n\nlayout = QwenLayoutDetector(\n    backend=QwenPyTorchConfig(model=\"Qwen/Qwen2-VL-7B\")\n)\n\n# Detect custom elements\nresult = layout.extract(\n    image,\n    custom_labels=[\"code_block\", \"sidebar\", \"pull_quote\"]\n)\n</code></pre></p> <p>Flexible VLM (Structured Labels): <pre><code>from omnidocs.tasks.layout_analysis import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_analysis.qwen import QwenPyTorchConfig\n\nlayout = QwenLayoutDetector(\n    backend=QwenPyTorchConfig(model=\"Qwen/Qwen2-VL-7B\")\n)\n\nresult = layout.extract(\n    image,\n    custom_labels=[\n        CustomLabel(\n            name=\"code_block\",\n            description=\"Source code listings\",\n            detection_prompt=\"Regions with monospace text and syntax highlighting\",\n            color=\"#2ecc71\",\n        ),\n        CustomLabel(\n            name=\"sidebar\",\n            description=\"Supplementary content boxes\",\n            detection_prompt=\"Boxed regions with background color or borders\",\n            color=\"#3498db\",\n        ),\n    ]\n)\n\n# Access metadata\nfor box in result.bboxes:\n    print(f\"{box.label.name}: {box.label.description}\")\n</code></pre></p>"},{"location":"architecture/#benefits","title":"Benefits","text":"Feature Fixed Models Flexible VLMs Speed \u26a1 Fast \ud83d\udc22 Slower (VLM inference) Accuracy \u2b50\u2b50\u2b50 High (trained) \u2b50\u2b50 Good (prompted) Custom Labels \u274c No \u2705 Yes Label Metadata \u274c No \u2705 Yes (CustomLabel) Detection Prompts \u274c No \u2705 Yes Extensibility \u274c No \u2705 Yes (extra fields) Use Case Standard documents Any document type"},{"location":"architecture/#summary","title":"Summary","text":""},{"location":"architecture/#key-design-decisions","title":"Key Design Decisions","text":"Decision Choice Rationale Config Pattern Model-specific classes IDE support, type safety Backend Discovery Import exists = supported Obvious, no guessing Lazy Imports Load on use Avoid dependency errors Validation Pydantic Early error detection Error Messages Clear install instructions Good UX"},{"location":"architecture/#config-naming-convention","title":"Config Naming Convention","text":"Model Type Config Location Naming Single-backend Same file as model <code>{Model}Config</code> Multi-backend Subfolder <code>{Model}{Backend}Config</code>"},{"location":"architecture/#parameter-naming","title":"Parameter Naming","text":"Model Type Parameter Single-backend <code>config=</code> Multi-backend <code>backend=</code> <p>Last Updated: January 20, 2026 Status: \u2705 Design Complete</p>"},{"location":"developer-guide/","title":"OmniDocs - Final Developer Experience Design","text":"<p>Status: \u2705 Design Complete - Ready for Implementation Last Updated: January 20, 2026 Version: 2.0.0</p>"},{"location":"developer-guide/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Overview</li> <li>Core Design Principles</li> <li>Backend Configuration</li> <li>Architecture</li> <li>Task Distinctions</li> <li>Layout Detection: Fixed vs Flexible Models</li> <li>Document Loading</li> <li>Usage Patterns</li> <li>Complete Examples</li> <li>Import Reference</li> <li>Implementation Roadmap</li> </ol>"},{"location":"developer-guide/#overview","title":"Overview","text":"<p>OmniDocs is a unified Python toolkit for visual document processing that provides a consistent API across multiple models and tasks.</p>"},{"location":"developer-guide/#core-philosophy","title":"Core Philosophy","text":"<p>Input Standardization: <code>Image \u2192 Model \u2192 Pydantic Output</code></p> <p>All tasks follow this pattern regardless of: - Which model is used (specialized vs VLM) - Which backend runs inference (PyTorch, VLLM, MLX, API) - Task complexity</p>"},{"location":"developer-guide/#supported-tasks","title":"Supported Tasks","text":"<ol> <li>Layout Analysis - Detect document structure (headings, paragraphs, figures, tables)</li> <li>OCR Extraction - Extract text with bounding boxes from images</li> <li>Text Extraction - Export document to Markdown/HTML (10+ specialized VLM models)</li> <li>Table Extraction - Extract tables and convert to structured formats</li> <li>Math Expression Recognition - Convert math to LaTeX</li> <li>Reading Order Detection - Order layout elements in reading sequence</li> <li>Image Captioning - Caption figures and images</li> <li>Chart Understanding - Convert charts to data + metadata</li> <li>Structured Output Extraction - Extract structured data with schemas</li> </ol>"},{"location":"developer-guide/#core-design-principles","title":"Core Design Principles","text":""},{"location":"developer-guide/#final-decisions","title":"\u2705 Final Decisions","text":"<ol> <li> <p>Class-Based Imports - No string-based factory pattern    <pre><code>from omnidocs.tasks.layout_analysis import DocLayoutYOLO  # \u2705 YES\nlayout = LayoutAnalysis(model=\"doclayout-yolo\")           # \u274c NO\n</code></pre></p> </li> <li> <p>Unified Method Name - <code>.extract()</code> for ALL tasks (including layout)    <pre><code>layout_result = layout.extract(image)\nocr_result = ocr.extract(image)\ntext_result = text.extract(image, output_format=\"markdown\")\n</code></pre></p> </li> <li> <p>Model-Specific Configs - Each model defines its own config classes    <pre><code># Single-backend model\nfrom omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\n\nlayout = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\n\n# Multi-backend model - import config for desired backend\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig, QwenAPIConfig\n\nextractor = QwenTextExtractor(backend=QwenPyTorchConfig(model=\"Qwen/Qwen2-VL-7B\"))\n</code></pre></p> </li> <li> <p>Separation of Init vs Extract:</p> </li> <li><code>__init__</code> (via config) = Model initialization, download, verification<ul> <li>Which model to use</li> <li>Which backend (PyTorch/VLLM/MLX/API)</li> <li>Model loading settings (device, dtype, quantization)</li> <li>Download and cache paths</li> </ul> </li> <li> <p><code>.extract()</code> = Runtime task parameters</p> <ul> <li>Output format (markdown/html)</li> <li>Custom prompts</li> <li>Task-specific options (include_layout, custom_labels)</li> <li>Per-call inference settings</li> </ul> </li> <li> <p>Stateless Document - Document is source data only, does NOT store task results    <pre><code>doc = Document.from_pdf(\"file.pdf\")  # Just loads the data\nresult = layout.extract(doc.get_page(0))  # User manages results\n</code></pre></p> </li> <li> <p>Discoverability - Available backends = available config classes    <pre><code># Multi-backend model - see what configs exist\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenPyTorchConfig,  # \u2713 PyTorch supported\n    QwenVLLMConfig,     # \u2713 VLLM supported\n    QwenMLXConfig,      # \u2713 MLX supported\n    QwenAPIConfig,      # \u2713 API supported\n)\n\n# Single-backend model - only one config\nfrom omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\n</code></pre></p> </li> <li> <p>Separation of Concerns:</p> </li> <li>Document Loading = Internal (pypdfium2, PyMuPDF) - NOT separate extractors</li> <li>OCR Extraction = Text + bounding boxes from images</li> <li>Text Extraction = Markdown/HTML export (specialized VLMs)</li> </ol>"},{"location":"developer-guide/#backend-configuration","title":"Backend Configuration","text":""},{"location":"developer-guide/#design-model-specific-config-classes","title":"Design: Model-Specific Config Classes","text":"<p>Each model has config classes specific to its supported backends. This provides: - IDE autocomplete with only relevant parameters - Type safety with Pydantic validation - Clear discoverability of supported backends</p>"},{"location":"developer-guide/#single-backend-models","title":"Single-Backend Models","text":"<p>Models that only support one backend (e.g., DocLayoutYOLO = PyTorch only):</p> <pre><code>from omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\n\n# Config has model-specific parameters\nlayout = DocLayoutYOLO(\n    config=DocLayoutYOLOConfig(\n        device=\"cuda\",\n        model_path=None,        # Optional custom weights\n        img_size=1024,          # Model-specific\n    )\n)\n\nresult = layout.extract(image)\n</code></pre>"},{"location":"developer-guide/#multi-backend-models","title":"Multi-Backend Models","text":"<p>Models that support multiple backends (e.g., Qwen = PyTorch, VLLM, MLX, API):</p> <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenPyTorchConfig,\n    QwenVLLMConfig,\n    QwenMLXConfig,\n    QwenAPIConfig,\n)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Option 1: PyTorch (local HuggingFace)\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nextractor = QwenTextExtractor(\n    backend=QwenPyTorchConfig(\n        model=\"Qwen/Qwen2-VL-7B-Instruct\",\n        device=\"cuda\",\n        trust_remote_code=True,\n        torch_dtype=\"bfloat16\",\n    )\n)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Option 2: VLLM (high-throughput)\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nextractor = QwenTextExtractor(\n    backend=QwenVLLMConfig(\n        model=\"Qwen/Qwen2-VL-7B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n        max_model_len=8192,\n        enforce_eager=False,\n    )\n)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Option 3: MLX (Apple Silicon)\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nextractor = QwenTextExtractor(\n    backend=QwenMLXConfig(\n        model=\"Qwen/Qwen2-VL-7B-Instruct-MLX\",\n        quantization=\"4bit\",\n    )\n)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Option 4: API (hosted or proxy)\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nextractor = QwenTextExtractor(\n    backend=QwenAPIConfig(\n        model=\"qwen2-vl-7b\",\n        api_key=\"YOUR_API_KEY\",\n        base_url=\"https://api.provider.com/v1\",  # Custom endpoint\n        rate_limit=20,\n        timeout=30,\n    )\n)\n\n# Task parameters in .extract()\nresult = extractor.extract(\n    image,\n    output_format=\"markdown\",\n    include_layout=True,\n    custom_prompt=None,\n)\n</code></pre>"},{"location":"developer-guide/#config-class-naming-convention","title":"Config Class Naming Convention","text":"Model Type Config Naming Example Single-backend <code>{Model}Config</code> <code>DocLayoutYOLOConfig</code> Multi-backend PyTorch <code>{Model}PyTorchConfig</code> <code>QwenPyTorchConfig</code> Multi-backend VLLM <code>{Model}VLLMConfig</code> <code>QwenVLLMConfig</code> Multi-backend MLX <code>{Model}MLXConfig</code> <code>QwenMLXConfig</code> Multi-backend API <code>{Model}APIConfig</code> <code>QwenAPIConfig</code>"},{"location":"developer-guide/#model-backend-support-matrix","title":"Model-Backend Support Matrix","text":"Model PyTorch VLLM MLX API Layout Analysis DocLayoutYOLO \u2705 \u274c \u274c \u274c RTDETRLayoutDetector \u2705 \u274c \u274c \u274c SuryaLayoutDetector \u2705 \u274c \u274c \u274c QwenLayoutDetector \u2705 \u2705 \u2705 \u2705 VLMLayoutDetector \u274c \u274c \u274c \u2705 Text Extraction QwenTextExtractor \u2705 \u2705 \u2705 \u2705 DotsOCRTextExtractor \u2705 \u2705 \u2705 \u274c ChandraTextExtractor \u2705 \u2705 \u2705 \u274c GemmaTextExtractor \u2705 \u2705 \u2705 \u2705 VLMTextExtractor \u274c \u274c \u274c \u2705 OCR Extraction TesseractOCR \u2705 \u274c \u274c \u274c SuryaOCR \u2705 \u274c \u274c \u274c QwenOCR \u2705 \u2705 \u2705 \u2705"},{"location":"developer-guide/#architecture","title":"Architecture","text":""},{"location":"developer-guide/#system-overview","title":"System Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Document Loading (Internal)      \u2502\n\u2502  pypdfium2, PyMuPDF, pdfplumber     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Task Layer                  \u2502\n\u2502  Layout, OCR, Text, Table, Math...  \u2502\n\u2502  (Each model has its own configs)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      Inference Layer                \u2502\n\u2502  PyTorch, VLLM, MLX, LiteLLM        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"developer-guide/#directory-structure","title":"Directory Structure","text":"<pre><code>omnidocs/\n\u251c\u2500\u2500 __init__.py                 # Export Document\n\u251c\u2500\u2500 document.py                 # Document class (stateless)\n\u2502\n\u251c\u2500\u2500 tasks/\n\u2502   \u251c\u2500\u2500 layout_analysis/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py         # Export models + configs\n\u2502   \u2502   \u251c\u2500\u2500 base.py             # BaseLayoutExtractor\n\u2502   \u2502   \u251c\u2500\u2500 models.py           # LayoutBox, LayoutOutput (Pydantic)\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 doc_layout_yolo.py  # DocLayoutYOLO + DocLayoutYOLOConfig\n\u2502   \u2502   \u251c\u2500\u2500 rtdetr.py           # RTDETRLayoutDetector + RTDETRConfig\n\u2502   \u2502   \u251c\u2500\u2500 surya.py            # SuryaLayoutDetector + SuryaLayoutConfig\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 qwen.py             # QwenLayoutDetector\n\u2502   \u2502   \u2514\u2500\u2500 qwen/               # Qwen backend configs\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u251c\u2500\u2500 pytorch.py      # QwenPyTorchConfig\n\u2502   \u2502       \u251c\u2500\u2500 vllm.py         # QwenVLLMConfig\n\u2502   \u2502       \u251c\u2500\u2500 mlx.py          # QwenMLXConfig\n\u2502   \u2502       \u2514\u2500\u2500 api.py          # QwenAPIConfig\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 ocr_extraction/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 base.py\n\u2502   \u2502   \u251c\u2500\u2500 models.py           # OCROutput, TextBlock (Pydantic)\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 tesseract.py        # TesseractOCR + TesseractConfig\n\u2502   \u2502   \u251c\u2500\u2500 paddle.py           # PaddleOCR + PaddleOCRConfig\n\u2502   \u2502   \u251c\u2500\u2500 easyocr.py          # EasyOCR + EasyOCRConfig\n\u2502   \u2502   \u251c\u2500\u2500 surya.py            # SuryaOCR + SuryaOCRConfig\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 qwen.py             # QwenOCR\n\u2502   \u2502   \u2514\u2500\u2500 qwen/               # Qwen backend configs\n\u2502   \u2502       \u2514\u2500\u2500 ...\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 text_extraction/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 base.py\n\u2502   \u2502   \u251c\u2500\u2500 models.py           # TextOutput (Pydantic)\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 vlm_extractor.py    # VLMTextExtractor + VLMTextConfig (API-only)\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 qwen.py             # QwenTextExtractor\n\u2502   \u2502   \u251c\u2500\u2500 qwen/               # Qwen backend configs\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 pytorch.py      # QwenPyTorchConfig\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 vllm.py         # QwenVLLMConfig\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 mlx.py          # QwenMLXConfig\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 api.py          # QwenAPIConfig\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 dotsocr.py          # DotsOCRTextExtractor\n\u2502   \u2502   \u251c\u2500\u2500 dotsocr/            # DotsOCR backend configs\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 pytorch.py      # DotsOCRPyTorchConfig\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 vllm.py         # DotsOCRVLLMConfig\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 mlx.py          # DotsOCRMLXConfig (no API)\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 chandra.py          # ChandraTextExtractor\n\u2502   \u2502   \u251c\u2500\u2500 gemma.py            # GemmaTextExtractor\n\u2502   \u2502   \u251c\u2500\u2500 granite.py          # GraniteDoclingOCR\n\u2502   \u2502   \u251c\u2500\u2500 hunyuan.py          # HunyuanTextExtractor\n\u2502   \u2502   \u251c\u2500\u2500 lighton.py          # LightOnOCRExtractor\n\u2502   \u2502   \u251c\u2500\u2500 mineru.py           # MinerUOCRExtractor\n\u2502   \u2502   \u251c\u2500\u2500 nanonuts.py         # NanonutsOCRExtractor\n\u2502   \u2502   \u251c\u2500\u2500 olmo.py             # OlmOCRExtractor\n\u2502   \u2502   \u2514\u2500\u2500 paddle.py           # PaddleTextExtractor\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 table_extraction/\n\u2502   \u2502   \u251c\u2500\u2500 table_transformer.py\n\u2502   \u2502   \u251c\u2500\u2500 surya_table.py\n\u2502   \u2502   \u251c\u2500\u2500 qwen.py\n\u2502   \u2502   \u2514\u2500\u2500 vlm_extractor.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 math_expression_extraction/\n\u2502   \u2502   \u251c\u2500\u2500 unimernet.py\n\u2502   \u2502   \u251c\u2500\u2500 qwen.py\n\u2502   \u2502   \u2514\u2500\u2500 vlm_extractor.py\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 structured_output_extraction/\n\u2502       \u2514\u2500\u2500 vlm_extractor.py\n\u2502\n\u251c\u2500\u2500 inference/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 base.py                 # Base backend classes\n\u2502   \u251c\u2500\u2500 pytorch.py              # PyTorch inference utilities\n\u2502   \u251c\u2500\u2500 vllm.py                 # VLLM inference utilities\n\u2502   \u251c\u2500\u2500 mlx.py                  # MLX inference utilities\n\u2502   \u2514\u2500\u2500 api.py                  # LiteLLM/API utilities\n\u2502\n\u251c\u2500\u2500 workflows/\n\u2502   \u2514\u2500\u2500 document_workflow.py\n\u2502\n\u2514\u2500\u2500 utils/\n    \u251c\u2500\u2500 visualization.py\n    \u2514\u2500\u2500 export.py\n</code></pre>"},{"location":"developer-guide/#task-distinctions","title":"Task Distinctions","text":""},{"location":"developer-guide/#critical-clarifications","title":"\u26a0\ufe0f Critical Clarifications","text":"Component Role Output Examples Document Loading Load PDFs/images PIL Images + Metadata <code>Document.from_pdf()</code> OCR Extraction Text + bounding boxes <code>OCROutput(text_blocks=[...])</code> TesseractOCR, SuryaOCR, QwenOCR Text Extraction Markdown/HTML export <code>TextOutput(content, format)</code> QwenTextExtractor, DotsOCRTextExtractor Layout Analysis Detect structure <code>LayoutOutput(bboxes=[...])</code> DocLayoutYOLO, QwenLayoutDetector <p>Important: - PyMuPDF, PDFPlumber, pypdfium2 are internal to Document - NOT separate extractors - OCR returns text WITH bounding boxes - Text Extraction returns formatted text (MD/HTML) WITHOUT bboxes</p>"},{"location":"developer-guide/#ocr-vs-text-extraction","title":"OCR vs Text Extraction","text":"<pre><code># \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# OCR Extraction - Text + Bounding Boxes\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfrom omnidocs.tasks.ocr_extraction import SuryaOCR, SuryaOCRConfig\n\nocr = SuryaOCR(config=SuryaOCRConfig(device=\"cuda\"))\nresult = ocr.extract(image)\n\n# Output: OCROutput\nfor text_block in result.text_blocks:\n    print(f\"Text: {text_block.text}\")\n    print(f\"BBox: {text_block.bbox}\")\n    print(f\"Confidence: {text_block.confidence}\")\n\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Text Extraction - Markdown/HTML Export\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig, QwenAPIConfig\n\n# Local inference with PyTorch\nextractor = QwenTextExtractor(\n    backend=QwenPyTorchConfig(\n        model=\"Qwen/Qwen2-VL-7B-Instruct\",\n        device=\"cuda\",\n    )\n)\n\n# OR API inference\nextractor = QwenTextExtractor(\n    backend=QwenAPIConfig(\n        model=\"qwen2-vl-7b\",\n        api_key=\"YOUR_API_KEY\",\n        base_url=\"https://api.provider.com/v1\",\n    )\n)\n\n# Task parameters in .extract()\nresult = extractor.extract(\n    image,\n    output_format=\"markdown\",      # \"markdown\" or \"html\"\n    include_layout=True,           # Include layout information\n    custom_prompt=None,            # Override default prompt\n)\n\n# Output: TextOutput\nprint(result.content)      # Full markdown/html\nprint(result.format)       # \"markdown\" or \"html\"\n</code></pre>"},{"location":"developer-guide/#layout-detection-fixed-vs-flexible-models","title":"Layout Detection: Fixed vs Flexible Models","text":""},{"location":"developer-guide/#model-categories","title":"Model Categories","text":"<p>OmniDocs layout detectors fall into two categories:</p> Category Models Label Support Use Case Fixed Labels DocLayoutYOLO, RT-DETR, Surya Predefined only Fast, specialized detection Flexible VLM Qwen, Florence-2, VLMLayoutDetector Custom labels via prompting Adaptable to any document type"},{"location":"developer-guide/#fixed-label-models","title":"Fixed Label Models","text":"<p>Examples: DocLayoutYOLO, RTDETRLayoutDetector, SuryaLayoutDetector</p> <p>These models are trained on specific label sets and cannot detect custom elements.</p> <pre><code>from omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\n\nlayout = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = layout.extract(image)\n\n# Fixed labels only:\n# - title\n# - text\n# - list\n# - table\n# - figure\n# - caption\n# - formula\n</code></pre> <p>Characteristics: - \u2705 Fast inference - \u2705 Highly accurate on standard elements - \u274c Cannot detect custom elements (code blocks, sidebars, etc.) - \u274c Fixed label set (no flexibility)</p>"},{"location":"developer-guide/#flexible-vlm-models","title":"Flexible VLM Models","text":"<p>Examples: QwenLayoutDetector, Florence2LayoutDetector, VLMLayoutDetector</p> <p>These models use vision-language prompting and can detect ANY custom layout elements.</p>"},{"location":"developer-guide/#basic-usage-default-labels","title":"Basic Usage (Default Labels)","text":"<pre><code>from omnidocs.tasks.layout_analysis import QwenLayoutDetector\nfrom omnidocs.tasks.layout_analysis.qwen import QwenPyTorchConfig\n\nlayout = QwenLayoutDetector(\n    backend=QwenPyTorchConfig(model=\"Qwen/Qwen2-VL-7B\")\n)\n\n# Standard labels (same as fixed models)\nresult = layout.extract(image)\n# Returns: title, text, table, figure, etc.\n</code></pre>"},{"location":"developer-guide/#custom-labels-simple-strings","title":"Custom Labels (Simple Strings)","text":"<pre><code># Detect custom elements via simple strings\nresult = layout.extract(\n    image,\n    custom_labels=[\"code_block\", \"sidebar\", \"pull_quote\", \"diagram\"]\n)\n\nfor box in result.bboxes:\n    print(f\"{box.label}: {box.bbox}\")\n    # code_block: [x1, y1, x2, y2]\n    # sidebar: [x1, y1, x2, y2]\n</code></pre>"},{"location":"developer-guide/#custom-labels-structured","title":"Custom Labels (Structured)","text":"<p>For advanced use cases, use <code>CustomLabel</code> with metadata:</p> <pre><code>from omnidocs.tasks.layout_analysis import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_analysis.qwen import QwenPyTorchConfig\n\nlayout = QwenLayoutDetector(\n    backend=QwenPyTorchConfig(model=\"Qwen/Qwen2-VL-7B\")\n)\n\n# Structured labels with metadata\nresult = layout.extract(\n    image,\n    custom_labels=[\n        CustomLabel(\n            name=\"code_block\",\n            description=\"Programming source code areas\",\n            detection_prompt=\"Regions with monospace text and syntax highlighting\",\n            color=\"#2ecc71\",\n        ),\n        CustomLabel(\n            name=\"sidebar\",\n            description=\"Sidebar or callout content\",\n            detection_prompt=\"Boxed regions with supplementary information\",\n            color=\"#3498db\",\n        ),\n        CustomLabel(\n            name=\"pull_quote\",\n            description=\"Highlighted quotations\",\n            detection_prompt=\"Large formatted quotes in different font/color\",\n            color=\"#e74c3c\",\n        ),\n    ]\n)\n\n# Access metadata\nfor box in result.bboxes:\n    print(f\"Label: {box.label.name}\")\n    print(f\"Description: {box.label.description}\")\n    print(f\"Color: {box.label.color}\")\n</code></pre>"},{"location":"developer-guide/#customlabel-type-definition","title":"CustomLabel Type Definition","text":"<pre><code>from pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass CustomLabel(BaseModel):\n    \"\"\"Custom layout label definition for flexible VLM models.\"\"\"\n\n    name: str = Field(..., description=\"Label identifier (e.g., 'code_block')\")\n\n    description: Optional[str] = Field(\n        default=None,\n        description=\"Human-readable description\"\n    )\n\n    detection_prompt: Optional[str] = Field(\n        default=None,\n        description=\"Custom prompt hint for detection\"\n    )\n\n    color: Optional[str] = Field(\n        default=None,\n        description=\"Visualization color (hex or name)\"\n    )\n\n    class Config:\n        extra = \"allow\"  # Users can add custom fields\n</code></pre>"},{"location":"developer-guide/#reusable-label-sets","title":"Reusable Label Sets","text":"<pre><code>from omnidocs.tasks.layout_analysis import CustomLabel\n\nclass TechnicalDocLabels:\n    \"\"\"Reusable labels for technical documentation.\"\"\"\n\n    CODE_BLOCK = CustomLabel(\n        name=\"code_block\",\n        description=\"Source code listings\",\n        color=\"#2ecc71\"\n    )\n\n    API_REFERENCE = CustomLabel(\n        name=\"api_reference\",\n        description=\"API documentation tables\",\n        color=\"#3498db\"\n    )\n\n    DIAGRAM = CustomLabel(\n        name=\"diagram\",\n        description=\"Architecture diagrams\",\n        color=\"#9b59b6\"\n    )\n\n    @classmethod\n    def all(cls):\n        return [cls.CODE_BLOCK, cls.API_REFERENCE, cls.DIAGRAM]\n\n# Use across projects\nresult = layout.extract(image, custom_labels=TechnicalDocLabels.all())\n</code></pre>"},{"location":"developer-guide/#user-extensions","title":"User Extensions","text":"<p>Users can extend <code>CustomLabel</code> with custom fields:</p> <pre><code>from omnidocs.tasks.layout_analysis import CustomLabel\n\nclass MyLabel(CustomLabel):\n    priority: int = 1          # Custom field\n    requires_ocr: bool = True  # Custom field\n\nresult = layout.extract(\n    image,\n    custom_labels=[\n        MyLabel(\n            name=\"important_section\",\n            description=\"High-priority content\",\n            priority=10,\n            requires_ocr=True,\n        )\n    ]\n)\n\n# Access custom fields\nfor box in result.bboxes:\n    print(f\"Priority: {box.label.priority}\")\n    print(f\"Requires OCR: {box.label.requires_ocr}\")\n</code></pre>"},{"location":"developer-guide/#comparison","title":"Comparison","text":"Feature Fixed Models Flexible VLMs Speed \u26a1 Fast \ud83d\udc22 Slower Accuracy (standard) \u2b50\u2b50\u2b50 High \u2b50\u2b50 Good Custom labels \u274c No \u2705 Yes String labels \u274c No \u2705 Yes Structured labels \u274c No \u2705 Yes (CustomLabel) Label metadata \u274c No \u2705 Yes Detection prompts \u274c No \u2705 Yes Use case Standard docs Any document type"},{"location":"developer-guide/#document-loading","title":"Document Loading","text":""},{"location":"developer-guide/#design-decision-stateless-document","title":"Design Decision: Stateless Document","text":"<p>Document is SOURCE DATA only - it does NOT store task results.</p> <p>Rationale: - Clean separation: Document = loaded PDF/images, Tasks = analysis results - Memory efficient: Document doesn't grow with analysis - User control: Users decide what to cache and how - Flexibility: Works with any caching strategy</p>"},{"location":"developer-guide/#document-api","title":"Document API","text":"<pre><code>from omnidocs import Document\n\n# Load from various sources\ndoc = Document.from_pdf(\"file.pdf\", dpi=150, page_range=(0, 4))\ndoc = Document.from_url(\"https://example.com/doc.pdf\")\ndoc = Document.from_bytes(pdf_bytes, filename=\"doc.pdf\")\ndoc = Document.from_image(\"page.png\")\ndoc = Document.from_images([\"page1.png\", \"page2.png\"])\n\n# Properties (metadata only)\ndoc.page_count          # Number of pages\ndoc.metadata            # DocumentMetadata object\ndoc.pages               # List[Image.Image] - all pages\ndoc.text                # Full text (lazy extraction, cached)\n\n# Access specific pages\npage_img = doc.get_page(0)              # 0-indexed\npage_text = doc.get_page_text(1)        # 1-indexed\npage_size = doc.get_page_size(0)        # Dimensions\n\n# Iterate (memory efficient)\nfor page_img in doc.iter_pages():\n    process(page_img)\n\n# Utilities\ndoc.save_images(\"output/\", prefix=\"page\", format=\"PNG\")\ndoc.to_dict()\ndoc.clear_cache()       # Free cached page images\n</code></pre>"},{"location":"developer-guide/#usage-patterns","title":"Usage Patterns","text":""},{"location":"developer-guide/#pattern-1-single-backend-model-simple","title":"Pattern 1: Single-Backend Model (Simple)","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\n\n# Load document\ndoc = Document.from_pdf(\"paper.pdf\")\n\n# Single-backend model - just use config=\nlayout = DocLayoutYOLO(\n    config=DocLayoutYOLOConfig(\n        device=\"cuda\",\n        img_size=1024,\n    )\n)\n\n# Process\nfor i in range(doc.page_count):\n    page = doc.get_page(i)\n    result = layout.extract(page)\n\n    for box in result.bboxes:\n        print(f\"{box.label}: {box.bbox}\")\n</code></pre>"},{"location":"developer-guide/#pattern-2-multi-backend-model-flexible","title":"Pattern 2: Multi-Backend Model (Flexible)","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenPyTorchConfig,\n    QwenVLLMConfig,\n    QwenAPIConfig,\n)\n\ndoc = Document.from_pdf(\"paper.pdf\")\n\n# Choose backend based on environment\nimport os\n\nif os.getenv(\"USE_VLLM\"):\n    backend = QwenVLLMConfig(\n        model=\"Qwen/Qwen2-VL-7B-Instruct\",\n        tensor_parallel_size=2,\n    )\nelif os.getenv(\"USE_API\"):\n    backend = QwenAPIConfig(\n        model=\"qwen2-vl-7b\",\n        api_key=os.getenv(\"API_KEY\"),\n        base_url=os.getenv(\"API_BASE_URL\"),\n    )\nelse:\n    backend = QwenPyTorchConfig(\n        model=\"Qwen/Qwen2-VL-7B-Instruct\",\n        device=\"cuda\",\n    )\n\nextractor = QwenTextExtractor(backend=backend)\n\n# Process with task params in extract()\nfor i in range(doc.page_count):\n    page = doc.get_page(i)\n    result = extractor.extract(\n        page,\n        output_format=\"markdown\",\n        include_layout=True,\n    )\n    print(result.content)\n</code></pre>"},{"location":"developer-guide/#pattern-3-api-only-models-vlmtextextractor","title":"Pattern 3: API-Only Models (VLMTextExtractor)","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import VLMTextExtractor, VLMTextConfig\n\ndoc = Document.from_pdf(\"file.pdf\")\n\n# Generic VLM extractor for API-only models (Gemini, GPT-4, Claude)\nextractor = VLMTextExtractor(\n    config=VLMTextConfig(\n        model=\"gemini-1.5-flash\",      # or \"gpt-4o\", \"claude-3-sonnet\"\n        api_key=\"YOUR_API_KEY\",\n        base_url=None,                  # Optional custom endpoint\n        rate_limit=20,\n    )\n)\n\nresult = extractor.extract(\n    doc.get_page(0),\n    output_format=\"markdown\",\n    custom_prompt=\"Extract all text preserving structure.\",\n)\n</code></pre>"},{"location":"developer-guide/#pattern-4-mixed-pipeline","title":"Pattern 4: Mixed Pipeline","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\nfrom omnidocs.tasks.table_extraction import TableTransformer, TableTransformerConfig\n\ndoc = Document.from_pdf(\"research_paper.pdf\")\n\n# Different models for different tasks\nlayout = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\n\ntext = QwenTextExtractor(\n    backend=QwenPyTorchConfig(\n        model=\"Qwen/Qwen2-VL-7B-Instruct\",\n        device=\"cuda\",\n    )\n)\n\ntable = TableTransformer(config=TableTransformerConfig(device=\"cuda\"))\n\n# Process based on detected layout\npage = doc.get_page(0)\nlayout_result = layout.extract(page)\n\nfor box in layout_result.bboxes:\n    region = page.crop(box.bbox)\n\n    if box.label == \"text\":\n        result = text.extract(region, output_format=\"markdown\")\n    elif box.label == \"table\":\n        result = table.extract(region)\n\n    print(f\"{box.label}: {result}\")\n</code></pre>"},{"location":"developer-guide/#complete-examples","title":"Complete Examples","text":""},{"location":"developer-guide/#example-1-sanskrit-document-processing","title":"Example 1: Sanskrit Document Processing","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenAPIConfig\n\n# Load document\ndoc = Document.from_pdf(\n    \"Mayavada_khandanam.pdf\",\n    dpi=150,\n    page_range=(0, 4)\n)\n\n# Setup extractors\nlayout = DocLayoutYOLO(\n    config=DocLayoutYOLOConfig(device=\"cuda\", confidence=0.25)\n)\n\ntext_extractor = QwenTextExtractor(\n    backend=QwenAPIConfig(\n        model=\"qwen2-vl-72b\",\n        api_key=\"YOUR_API_KEY\",\n        rate_limit=10,\n    )\n)\n\n# Process each page\nall_results = {}\n\nfor page_num in range(doc.page_count):\n    page = doc.get_page(page_num)\n\n    # Detect layout\n    layout_result = layout.extract(page)\n\n    # Extract text from text regions\n    page_results = []\n    for box in layout_result.bboxes:\n        if box.label == \"text\":\n            region = page.crop(box.bbox)\n            text_result = text_extractor.extract(\n                region,\n                output_format=\"markdown\",\n                custom_prompt=\"Extract Sanskrit/Hindi text accurately.\",\n            )\n            page_results.append({\n                \"bbox\": box.bbox,\n                \"text\": text_result.content,\n            })\n\n    all_results[f\"page_{page_num}\"] = page_results\n</code></pre>"},{"location":"developer-guide/#example-2-high-throughput-with-vllm","title":"Example 2: High-Throughput with VLLM","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenVLLMConfig\n\n# VLLM for batch processing\nextractor = QwenTextExtractor(\n    backend=QwenVLLMConfig(\n        model=\"Qwen/Qwen2-VL-7B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n        max_model_len=8192,\n    )\n)\n\n# Process many documents efficiently\ndocuments = [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]\n\nfor doc_path in documents:\n    doc = Document.from_pdf(doc_path)\n\n    for i in range(doc.page_count):\n        result = extractor.extract(\n            doc.get_page(i),\n            output_format=\"markdown\",\n        )\n        # Save result...\n</code></pre>"},{"location":"developer-guide/#example-3-apple-silicon-with-mlx","title":"Example 3: Apple Silicon with MLX","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenMLXConfig\n\n# MLX for Apple Silicon\nextractor = QwenTextExtractor(\n    backend=QwenMLXConfig(\n        model=\"Qwen/Qwen2-VL-7B-Instruct-MLX\",\n        quantization=\"4bit\",\n    )\n)\n\ndoc = Document.from_pdf(\"document.pdf\")\n\nresult = extractor.extract(\n    doc.get_page(0),\n    output_format=\"markdown\",\n)\n</code></pre>"},{"location":"developer-guide/#example-4-structured-output-extraction","title":"Example 4: Structured Output Extraction","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.structured_output_extraction import VLMStructuredExtractor, VLMStructuredConfig\nfrom pydantic import BaseModel\nfrom typing import List\n\n# Define schema\nclass Invoice(BaseModel):\n    vendor: str\n    invoice_number: str\n    date: str\n    total_amount: float\n    line_items: List[dict]\n\n# Setup extractor\nextractor = VLMStructuredExtractor(\n    config=VLMStructuredConfig(\n        model=\"gpt-4o\",\n        api_key=\"YOUR_API_KEY\",\n    )\n)\n\ndoc = Document.from_pdf(\"invoice.pdf\")\n\n# Extract with schema\nresult = extractor.extract(\n    doc.get_page(0),\n    output_model=Invoice,\n)\n\n# Typed, validated output\nprint(f\"Vendor: {result.data.vendor}\")\nprint(f\"Total: ${result.data.total_amount}\")\n</code></pre>"},{"location":"developer-guide/#import-reference","title":"Import Reference","text":""},{"location":"developer-guide/#complete-import-guide","title":"Complete Import Guide","text":"<pre><code># \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Document Loading\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfrom omnidocs import Document\n\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Layout Analysis\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfrom omnidocs.tasks.layout_analysis import (\n    # Single-backend models (config included)\n    DocLayoutYOLO, DocLayoutYOLOConfig,\n    RTDETRLayoutDetector, RTDETRConfig,\n    SuryaLayoutDetector, SuryaLayoutConfig,\n\n    # Multi-backend model\n    QwenLayoutDetector,\n\n    # API-only\n    VLMLayoutDetector, VLMLayoutConfig,\n\n    # Custom label support\n    CustomLabel,\n)\n\n# Qwen layout backend configs\nfrom omnidocs.tasks.layout_analysis.qwen import (\n    QwenPyTorchConfig,\n    QwenVLLMConfig,\n    QwenMLXConfig,\n    QwenAPIConfig,\n)\n\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# OCR Extraction (text + bboxes)\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfrom omnidocs.tasks.ocr_extraction import (\n    # Single-backend models\n    TesseractOCR, TesseractConfig,\n    PaddleOCR, PaddleOCRConfig,\n    EasyOCR, EasyOCRConfig,\n    SuryaOCR, SuryaOCRConfig,\n\n    # Multi-backend model\n    QwenOCR,\n\n    # API-only\n    VLMOCRExtractor, VLMOCRConfig,\n)\n\n# Qwen OCR backend configs\nfrom omnidocs.tasks.ocr_extraction.qwen import (\n    QwenPyTorchConfig,\n    QwenVLLMConfig,\n    QwenMLXConfig,\n    QwenAPIConfig,\n)\n\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Text Extraction (MD/HTML)\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfrom omnidocs.tasks.text_extraction import (\n    # Multi-backend models\n    QwenTextExtractor,\n    DotsOCRTextExtractor,\n    ChandraTextExtractor,\n    GemmaTextExtractor,\n    GraniteDoclingOCR,\n    HunyuanTextExtractor,\n    LightOnOCRExtractor,\n    MinerUOCRExtractor,\n    NanonutsOCRExtractor,\n    OlmOCRExtractor,\n    PaddleTextExtractor,\n\n    # API-only\n    VLMTextExtractor, VLMTextConfig,\n)\n\n# Qwen text extraction backend configs\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenPyTorchConfig,\n    QwenVLLMConfig,\n    QwenMLXConfig,\n    QwenAPIConfig,\n)\n\n# DotsOCR backend configs (no API)\nfrom omnidocs.tasks.text_extraction.dotsocr import (\n    DotsOCRPyTorchConfig,\n    DotsOCRVLLMConfig,\n    DotsOCRMLXConfig,\n)\n\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Table Extraction\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfrom omnidocs.tasks.table_extraction import (\n    TableTransformer, TableTransformerConfig,\n    SuryaTable, SuryaTableConfig,\n    QwenTableExtractor,\n    VLMTableExtractor, VLMTableConfig,\n)\n\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Math Expression Extraction\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfrom omnidocs.tasks.math_expression_extraction import (\n    UniMERNet, UniMERNetConfig,\n    QwenMathExtractor,\n    VLMMathExtractor, VLMMathConfig,\n)\n\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Structured Output Extraction\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfrom omnidocs.tasks.structured_output_extraction import (\n    VLMStructuredExtractor, VLMStructuredConfig,\n)\n\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Workflows (Optional)\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfrom omnidocs.workflows import DocumentWorkflow\n</code></pre>"},{"location":"developer-guide/#implementation-roadmap","title":"Implementation Roadmap","text":""},{"location":"developer-guide/#phase-1-core-infrastructure","title":"Phase 1: Core Infrastructure","text":"<p>Goals: Base classes and config system</p> <ul> <li>[ ] Base extractor classes with <code>.extract()</code> method</li> <li>[ ] Pydantic config classes pattern</li> <li>[ ] Pydantic output models (LayoutOutput, OCROutput, TextOutput)</li> <li>[ ] Document class (stateless)</li> </ul> <p>Deliverables: - <code>omnidocs/document.py</code> - <code>omnidocs/tasks/*/base.py</code> - <code>omnidocs/tasks/*/models.py</code></p>"},{"location":"developer-guide/#phase-2-single-backend-models","title":"Phase 2: Single-Backend Models","text":"<p>Goals: Implement models with single backend</p> <ul> <li>[ ] DocLayoutYOLO + DocLayoutYOLOConfig</li> <li>[ ] SuryaOCR + SuryaOCRConfig</li> <li>[ ] UniMERNet + UniMERNetConfig</li> <li>[ ] TableTransformer + TableTransformerConfig</li> </ul>"},{"location":"developer-guide/#phase-3-multi-backend-models","title":"Phase 3: Multi-Backend Models","text":"<p>Goals: Implement models with multiple backends</p> <ul> <li>[ ] QwenTextExtractor + all backend configs</li> <li>[ ] DotsOCRTextExtractor + backend configs</li> <li>[ ] Backend-specific inference utilities</li> </ul>"},{"location":"developer-guide/#phase-4-api-only-models","title":"Phase 4: API-Only Models","text":"<p>Goals: Generic VLM wrappers</p> <ul> <li>[ ] VLMTextExtractor for Gemini, GPT-4, Claude</li> <li>[ ] VLMStructuredExtractor with schema support</li> <li>[ ] LiteLLM integration</li> </ul>"},{"location":"developer-guide/#phase-5-testing-documentation","title":"Phase 5: Testing &amp; Documentation","text":"<p>Goals: Comprehensive testing and docs</p> <ul> <li>[ ] Unit tests for all extractors</li> <li>[ ] Integration tests</li> <li>[ ] API documentation</li> <li>[ ] Tutorial notebooks</li> </ul>"},{"location":"developer-guide/#summary","title":"Summary","text":""},{"location":"developer-guide/#key-design-decisions","title":"\u2705 Key Design Decisions","text":"Decision Choice Rationale Import Pattern Class-based Direct, explicit, type-safe Method Name <code>.extract()</code> for all Consistent, predictable Config Style Model-specific IDE autocomplete, clear discoverability Init vs Extract Config at init, task params at extract Clear separation Document Design Stateless Separation of concerns Backend Discovery Config classes exist = supported Obvious, no guessing"},{"location":"developer-guide/#config-parameter-naming","title":"Config Parameter Naming","text":"Model Type Parameter Example Single-backend <code>config=</code> <code>DocLayoutYOLO(config=...)</code> Multi-backend <code>backend=</code> <code>QwenTextExtractor(backend=...)</code> API-only <code>config=</code> <code>VLMTextExtractor(config=...)</code> <p>Last Updated: January 20, 2026 Status: \u2705 Design Complete - Ready for Implementation Maintainer: Adithya S Kolavi</p>"}]}