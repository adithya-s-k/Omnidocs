{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"OmniDocs","text":"<p>Unified Python toolkit for visual document processing - Think Transformers for document AI</p> <p> </p> <p>Status: \ud83d\udea7 v0.2 - Document Loading Complete | Task Extractors In Progress</p>"},{"location":"#overview","title":"Overview","text":"<p>OmniDocs provides a consistent, type-safe API across multiple document processing models and tasks: - \u2705 Document Loading - Lazy-loaded PDFs and images with metadata (v0.2) - \ud83d\udea7 Layout Analysis - Coming soon - \ud83d\udea7 OCR Extraction - Coming soon - \ud83d\udea7 Text Extraction - Coming soon - \ud83d\udea7 Table Extraction - Coming soon</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install omnidocs\n</code></pre> <p>Or for development:</p> <pre><code>git clone https://github.com/adithya-s-k/OmniDocs.git\ncd OmniDocs/Omnidocs\nuv sync\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from omnidocs import Document\n\n# Load PDF with lazy rendering\ndoc = Document.from_pdf(\"paper.pdf\", dpi=150)\n\n# Access pages (rendered on demand)\npage = doc.get_page(0)  # PIL Image\ntext = doc.get_page_text(1)  # 1-indexed\n\n# Memory efficient iteration\nfor page in doc.iter_pages():\n    # Process each page\n    pass\n\n# Full document text (cached)\nfull_text = doc.text\n\n# Metadata\nprint(f\"Pages: {doc.page_count}\")\nprint(f\"Size: {doc.metadata.file_size}\")\n</code></pre>"},{"location":"#features","title":"Features","text":""},{"location":"#document-loading","title":"Document Loading \u2705","text":"<ul> <li>Multiple Sources: PDF files, URLs, bytes, images</li> <li>Lazy Loading: Pages rendered only when accessed</li> <li>MIT/Apache Licensed: pypdfium2 (Apache 2.0) + pdfplumber (MIT)</li> <li>Type-Safe: Pydantic models for configs and outputs</li> <li>Memory Efficient: Page caching with manual control</li> </ul> <pre><code># From file\ndoc = Document.from_pdf(\"file.pdf\", page_range=(0, 9))\n\n# From URL\ndoc = Document.from_url(\"https://arxiv.org/pdf/1706.03762\")\n\n# From bytes\ndoc = Document.from_bytes(pdf_bytes)\n\n# From images\ndoc = Document.from_image(\"page.png\")\ndoc = Document.from_images([\"p1.png\", \"p2.png\"])\n</code></pre>"},{"location":"#architecture","title":"Architecture","text":"<p>OmniDocs follows a clean, stateless design: - Document = Source data only (doesn't store task results) - Tasks = Analysis operations (layout, OCR, text extraction) - Backends = Inference engines (PyTorch, VLLM, MLX, API)</p> <p>See Design Documents for full architecture details.</p>"},{"location":"#development","title":"Development","text":"<p>Run tests:</p> <pre><code>uv run pytest tests/ -v\n</code></pre> <p>Run fast tests only:</p> <pre><code>uv run pytest tests/ -v -m \"not slow\"\n</code></pre> <p>Build docs:</p> <pre><code>uv run mkdocs serve\n</code></pre>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python 3.10 - 3.11</li> <li>Dependencies managed with uv</li> </ul>"},{"location":"#license","title":"License","text":"<p>Apache 2.0 - See LICENSE for details</p>"},{"location":"#contributing","title":"Contributing","text":"<p>See CONTRIBUTING.md for development guidelines.</p>"},{"location":"#links","title":"Links","text":"<ul> <li>\ud83d\udcda Documentation</li> <li>\ud83d\udc1b Issues</li> <li>\ud83d\udce6 PyPI</li> <li>\ud83d\udcdd Changelog</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing to OmniDocs","text":"<p>Thank you for your interest in contributing to OmniDocs! \ud83c\udf89</p>"},{"location":"CONTRIBUTING/#development-setup","title":"Development Setup","text":"<ol> <li> <p>Clone the repository: <pre><code>git clone https://github.com/adithya-s-k/OmniDocs.git\ncd OmniDocs/Omnidocs\n</code></pre></p> </li> <li> <p>Install dependencies with uv: <pre><code># Install uv if you don't have it\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Sync dependencies\nuv sync\n</code></pre></p> </li> <li> <p>Run tests: <pre><code>uv run pytest tests/ -v\n</code></pre></p> </li> </ol>"},{"location":"CONTRIBUTING/#project-structure","title":"Project Structure","text":"<pre><code>Omnidocs/\n\u251c\u2500\u2500 omnidocs/          # Main package\n\u2502   \u251c\u2500\u2500 document.py    # Document loading (\u2705 complete)\n\u2502   \u251c\u2500\u2500 tasks/         # Task extractors (\ud83d\udea7 in progress)\n\u2502   \u251c\u2500\u2500 inference/     # Backend implementations (planned)\n\u2502   \u2514\u2500\u2500 utils/         # Utilities\n\u251c\u2500\u2500 tests/             # Test suite\n\u2502   \u251c\u2500\u2500 fixtures/      # Test data (PDFs, images)\n\u2502   \u2514\u2500\u2500 tasks/         # Future task tests\n\u2514\u2500\u2500 docs/              # Documentation\n</code></pre>"},{"location":"CONTRIBUTING/#design-documents","title":"Design Documents","text":"<p>\ud83d\udd34 IMPORTANT: Before implementing any new features, read the design documents: - <code>docs/architecture.md</code> - Backend and config system - <code>docs/developer-guide.md</code> - API design and usage patterns</p> <p>These documents define the architecture for v0.2+.</p>"},{"location":"CONTRIBUTING/#development-workflow","title":"Development Workflow","text":""},{"location":"CONTRIBUTING/#1-testing-phase-modal_scripts","title":"1. Testing Phase (modal_scripts/)","text":"<ul> <li>Test models in isolation using Modal scripts</li> <li>Validate inference and outputs</li> <li>Benchmark performance</li> </ul>"},{"location":"CONTRIBUTING/#2-integration-phase-omnidocs","title":"2. Integration Phase (omnidocs/)","text":"<ul> <li>Follow the config pattern (single-backend vs multi-backend)</li> <li>Use Pydantic for all configs and outputs</li> <li>Maintain consistent <code>.extract()</code> API</li> <li>Add comprehensive tests</li> </ul>"},{"location":"CONTRIBUTING/#3-documentation","title":"3. Documentation","text":"<ul> <li>Add docstrings (Google style)</li> <li>Update relevant docs</li> <li>Add usage examples</li> </ul>"},{"location":"CONTRIBUTING/#code-standards","title":"Code Standards","text":""},{"location":"CONTRIBUTING/#required","title":"\u2705 Required","text":"<ul> <li>Type hints for all public APIs</li> <li>Pydantic models for configs (<code>extra=\"forbid\"</code>)</li> <li>Docstrings (Google style) for classes and methods</li> <li>Tests with &gt;80% coverage</li> </ul>"},{"location":"CONTRIBUTING/#avoid","title":"\u274c Avoid","text":"<ul> <li>String-based factories (use class imports)</li> <li>Storing task results in Document</li> <li>Breaking changes without deprecation</li> <li>Adding features beyond requirements</li> <li>Over-engineering</li> </ul>"},{"location":"CONTRIBUTING/#version-management","title":"Version Management","text":"<p>OmniDocs follows Semantic Versioning: - MAJOR (X.0.0): Breaking API changes - MINOR (0.X.0): New features, backward compatible - PATCH (0.0.X): Bug fixes, backward compatible</p>"},{"location":"CONTRIBUTING/#incrementing-version","title":"Incrementing Version","text":"<p>Version is managed in one place: <code>omnidocs/_version.py</code></p> <pre><code># omnidocs/_version.py\n__version__ = \"0.2.0\"\n</code></pre> <p>The <code>pyproject.toml</code> reads from this file dynamically, so you only need to update one file.</p>"},{"location":"CONTRIBUTING/#version-bump-process","title":"Version Bump Process","text":"<ol> <li> <p>Update version: <pre><code># Edit omnidocs/_version.py\n# Change __version__ = \"0.2.0\" to __version__ = \"0.2.1\"\n</code></pre></p> </li> <li> <p>Verify (version should be accessible everywhere): <pre><code>from omnidocs import __version__\nprint(__version__)  # Should show new version\n</code></pre></p> </li> <li> <p>Commit: <pre><code>git add omnidocs/_version.py\ngit commit -m \"chore: bump version to 0.2.1\"\n</code></pre></p> </li> <li> <p>Tag and release (maintainers only): <pre><code>git tag v0.2.1\ngit push origin v0.2.1\n</code></pre></p> </li> </ol> <p>This will trigger the GitHub Actions workflow to: - Build the package - Create a GitHub release - Publish to PyPI</p>"},{"location":"CONTRIBUTING/#when-to-bump","title":"When to Bump","text":"<ul> <li>Patch (0.2.X): Bug fixes, typos, documentation</li> <li>Minor (0.X.0): New features, new task extractors, new models</li> <li>Major (X.0.0): Breaking API changes (rare, requires discussion)</li> </ul>"},{"location":"CONTRIBUTING/#documentation","title":"Documentation","text":"<p>OmniDocs uses MkDocs Material for documentation.</p>"},{"location":"CONTRIBUTING/#building-docs-locally","title":"Building Docs Locally","text":"<pre><code># Install docs dependencies\nuv sync --group docs\n\n# Serve docs with live reload\nuv run mkdocs serve\n\n# Open http://127.0.0.1:8000 in your browser\n</code></pre>"},{"location":"CONTRIBUTING/#building-static-site","title":"Building Static Site","text":"<pre><code># Build static site to site/ directory\nuv run mkdocs build\n\n# Build with strict mode (warnings become errors)\nuv run mkdocs build --strict\n</code></pre>"},{"location":"CONTRIBUTING/#documentation-structure","title":"Documentation Structure","text":"<pre><code>docs/\n\u251c\u2500\u2500 README.md              # Homepage (also serves as package README)\n\u251c\u2500\u2500 architecture.md        # Backend and config system design\n\u2514\u2500\u2500 developer-guide.md     # API design and usage patterns\n</code></pre>"},{"location":"CONTRIBUTING/#automatic-deployment","title":"Automatic Deployment","text":"<p>Documentation is automatically built and deployed to GitHub Pages when: - Code is pushed to the <code>master</code> branch - A maintainer manually triggers the workflow</p> <p>The docs are published at: https://adithya-s-k.github.io/OmniDocs/</p>"},{"location":"CONTRIBUTING/#adding-new-documentation","title":"Adding New Documentation","text":"<ol> <li>Create markdown files in <code>docs/</code></li> <li>Update <code>mkdocs.yml</code> nav section to include new pages</li> <li>Use Google-style docstrings in code (automatically extracted)</li> <li>Preview changes locally with <code>uv run mkdocs serve</code></li> </ol>"},{"location":"CONTRIBUTING/#pull-request-process","title":"Pull Request Process","text":"<ol> <li> <p>Create a branch: <pre><code>git checkout -b feature/your-feature-name\n</code></pre></p> </li> <li> <p>Make changes:</p> </li> <li>Follow the design patterns in docs/</li> <li>Add tests for new functionality</li> <li> <p>Update relevant documentation</p> </li> <li> <p>Run tests: <pre><code># Run all tests\nuv run pytest tests/ -v\n\n# Run fast tests only\nuv run pytest tests/ -v -m \"not slow\"\n</code></pre></p> </li> <li> <p>Submit PR:</p> </li> <li>Provide clear description</li> <li>Reference any related issues</li> <li>Ensure tests pass</li> </ol>"},{"location":"CONTRIBUTING/#commit-guidelines","title":"Commit Guidelines","text":"<p>Follow conventional commits: <pre><code>feat: add DocLayoutYOLO extractor\nfix: resolve page range validation\ndocs: update architecture guide\ntest: add fixture-based PDF tests\n</code></pre></p>"},{"location":"CONTRIBUTING/#need-help","title":"Need Help?","text":"<ul> <li>\ud83d\udcd6 Read the design documents</li> <li>\ud83d\udc1b Open an issue</li> <li>\ud83d\udcac Ask questions in discussions</li> </ul>"},{"location":"CONTRIBUTING/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the Apache 2.0 License.</p>"},{"location":"LICENSE/","title":"License","text":"<p>Apache License Version 2.0, January 2004 http://www.apache.org/licenses/</p>"},{"location":"LICENSE/#terms-and-conditions-for-use-reproduction-and-distribution","title":"TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION","text":""},{"location":"LICENSE/#1-definitions","title":"1. Definitions","text":"<p>\"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.</p>"},{"location":"LICENSE/#2-grant-of-copyright-license","title":"2. Grant of Copyright License","text":"<p>Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.</p>"},{"location":"LICENSE/#3-grant-of-patent-license","title":"3. Grant of Patent License","text":"<p>Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.</p>"},{"location":"LICENSE/#4-redistribution","title":"4. Redistribution","text":"<p>You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:</p> <ol> <li>You must give any other recipients of the Work or Derivative Works a copy of this License; and</li> <li>You must cause any modified files to carry prominent notices stating that You changed the files; and</li> <li>You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and</li> <li>If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.</li> </ol> <p>You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.</p>"},{"location":"LICENSE/#5-submission-of-contributions","title":"5. Submission of Contributions","text":"<p>Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.</p>"},{"location":"LICENSE/#6-trademarks","title":"6. Trademarks","text":"<p>This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.</p>"},{"location":"LICENSE/#7-disclaimer-of-warranty","title":"7. Disclaimer of Warranty","text":"<p>Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.</p>"},{"location":"LICENSE/#8-limitation-of-liability","title":"8. Limitation of Liability","text":"<p>In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.</p>"},{"location":"LICENSE/#9-accepting-warranty-or-additional-liability","title":"9. Accepting Warranty or Additional Liability","text":"<p>While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.</p>"},{"location":"LICENSE/#end-of-terms-and-conditions","title":"END OF TERMS AND CONDITIONS","text":"<p>Copyright 2025 OmniDocs Contributors</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"architecture/","title":"OmniDocs - Backend Architecture","text":"<p>Status: \u2705 Design Complete Last Updated: January 20, 2026 Version: 2.0.0</p>"},{"location":"architecture/#overview","title":"Overview","text":"<p>OmniDocs supports 4 inference backends:</p> Backend Use Case Platform Key Dependencies PyTorch Default local inference CPU/GPU torch, transformers VLLM High-throughput serving GPU only vllm MLX Apple Silicon optimization macOS M1/M2/M3+ mlx, mlx-lm API Hosted models Cloud litellm"},{"location":"architecture/#core-architecture-principles","title":"Core Architecture Principles","text":""},{"location":"architecture/#separation-of-concerns-__init__-vs-extract","title":"Separation of Concerns: <code>__init__</code> vs <code>.extract()</code>","text":"<p>OmniDocs maintains a clear separation between model initialization and runtime parameters:</p> <p><code>__init__</code> (via config) - Model Setup &amp; Verification - Which model to use - Which backend (PyTorch/VLLM/MLX/API) - Model loading settings (device, dtype, quantization) - Download and cache paths - Model verification and validation</p> <p><code>.extract()</code> - Runtime Task Parameters - Output format (markdown/html) - Custom prompts - Task-specific options (include_layout, custom_labels) - Per-call inference settings</p> <p>Example: <pre><code># Init: Model setup (happens once)\nextractor = QwenTextExtractor(\n    backend=QwenPyTorchConfig(\n        model=\"Qwen/Qwen2-VL-7B\",  # Which model\n        device=\"cuda\",              # Where to run\n        torch_dtype=\"bfloat16\",     # How to load\n    )\n)\n\n# Extract: Runtime params (can vary per call)\nresult1 = extractor.extract(image1, output_format=\"markdown\")\nresult2 = extractor.extract(image2, output_format=\"html\", custom_prompt=\"...\")\n</code></pre></p>"},{"location":"architecture/#design-principle-model-specific-configs","title":"Design Principle: Model-Specific Configs","text":"<p>Each model defines its own config classes for supported backends. This provides:</p> <ol> <li>IDE Autocomplete - Only relevant parameters shown</li> <li>Type Safety - Pydantic validation at config creation</li> <li>Clear Discoverability - Config exists = backend supported</li> <li>No Abstraction Leakage - Each backend can have unique parameters</li> </ol>"},{"location":"architecture/#config-class-structure","title":"Config Class Structure","text":""},{"location":"architecture/#single-backend-model","title":"Single-Backend Model","text":"<p>Models with only one backend (e.g., DocLayoutYOLO = PyTorch only):</p> <pre><code># omnidocs/tasks/layout_analysis/doc_layout_yolo.py\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\nfrom PIL import Image\n\nclass DocLayoutYOLOConfig(BaseModel):\n    \"\"\"Configuration for DocLayoutYOLO model.\"\"\"\n\n    device: str = Field(default=\"cuda\", description=\"Device to run on\")\n    model_path: Optional[str] = Field(default=None, description=\"Custom model weights\")\n    img_size: int = Field(default=1024, description=\"Input image size\")\n    confidence: float = Field(default=0.25, ge=0.0, le=1.0)\n\n    class Config:\n        extra = \"forbid\"  # Raise error on unknown params\n\n\nclass DocLayoutYOLO:\n    \"\"\"DocLayout-YOLO layout detector. PyTorch only.\"\"\"\n\n    def __init__(self, config: DocLayoutYOLOConfig):\n        self.config = config\n        self._load_model()\n\n    def _load_model(self):\n        \"\"\"Load model with PyTorch.\"\"\"\n        import torch\n        # Load model...\n\n    def extract(self, image: Image.Image) -&gt; LayoutOutput:\n        \"\"\"Run layout detection.\"\"\"\n        # Inference...\n        pass\n</code></pre>"},{"location":"architecture/#multi-backend-model","title":"Multi-Backend Model","text":"<p>Models with multiple backends (e.g., Qwen = PyTorch, VLLM, MLX, API):</p> <pre><code># omnidocs/tasks/text_extraction/qwen.py\n\nfrom typing import Union\nfrom PIL import Image\n\n# Import all backend configs\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenPyTorchConfig,\n    QwenVLLMConfig,\n    QwenMLXConfig,\n    QwenAPIConfig,\n)\n\n# Union type for all supported backends\nQwenBackendConfig = Union[\n    QwenPyTorchConfig,\n    QwenVLLMConfig,\n    QwenMLXConfig,\n    QwenAPIConfig,\n]\n\n\nclass QwenTextExtractor:\n    \"\"\"Qwen VLM text extractor. Supports PyTorch, VLLM, MLX, API backends.\"\"\"\n\n    def __init__(self, backend: QwenBackendConfig):\n        self.backend_config = backend\n        self._backend = self._create_backend()\n\n    def _create_backend(self):\n        \"\"\"Create appropriate backend based on config type.\"\"\"\n        if isinstance(self.backend_config, QwenPyTorchConfig):\n            from omnidocs.inference.pytorch import PyTorchInference\n            return PyTorchInference(self.backend_config)\n\n        elif isinstance(self.backend_config, QwenVLLMConfig):\n            from omnidocs.inference.vllm import VLLMInference\n            return VLLMInference(self.backend_config)\n\n        elif isinstance(self.backend_config, QwenMLXConfig):\n            from omnidocs.inference.mlx import MLXInference\n            return MLXInference(self.backend_config)\n\n        elif isinstance(self.backend_config, QwenAPIConfig):\n            from omnidocs.inference.api import APIInference\n            return APIInference(self.backend_config)\n\n        else:\n            raise TypeError(f\"Unknown backend config: {type(self.backend_config)}\")\n\n    def extract(\n        self,\n        image: Image.Image,\n        output_format: str = \"markdown\",\n        include_layout: bool = False,\n        custom_prompt: Optional[str] = None,\n    ) -&gt; TextOutput:\n        \"\"\"\n        Extract text from image.\n\n        Args:\n            image: PIL Image\n            output_format: \"markdown\" or \"html\"\n            include_layout: Include layout information\n            custom_prompt: Override default prompt\n\n        Returns:\n            TextOutput with extracted content\n        \"\"\"\n        prompt = custom_prompt or self._get_default_prompt(output_format, include_layout)\n        raw_output = self._backend.infer(image, prompt)\n        return self._postprocess(raw_output, output_format)\n</code></pre>"},{"location":"architecture/#backend-config-definitions","title":"Backend Config Definitions","text":""},{"location":"architecture/#pytorch-config","title":"PyTorch Config","text":"<pre><code># omnidocs/tasks/text_extraction/qwen/pytorch.py\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, Literal\n\nclass QwenPyTorchConfig(BaseModel):\n    \"\"\"PyTorch/HuggingFace backend configuration for Qwen.\"\"\"\n\n    model: str = Field(..., description=\"HuggingFace model ID\")\n    device: str = Field(default=\"cuda\", description=\"Device (cuda/cpu)\")\n    torch_dtype: Literal[\"float16\", \"bfloat16\", \"float32\"] = Field(\n        default=\"bfloat16\",\n        description=\"Torch dtype for model\"\n    )\n    trust_remote_code: bool = Field(default=True)\n    device_map: Optional[str] = Field(default=\"auto\")\n    max_memory: Optional[dict] = Field(default=None)\n    quantization: Optional[Literal[\"4bit\", \"8bit\"]] = Field(default=None)\n\n    class Config:\n        extra = \"forbid\"\n</code></pre>"},{"location":"architecture/#vllm-config","title":"VLLM Config","text":"<pre><code># omnidocs/tasks/text_extraction/qwen/vllm.py\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass QwenVLLMConfig(BaseModel):\n    \"\"\"VLLM backend configuration for Qwen.\"\"\"\n\n    model: str = Field(..., description=\"HuggingFace model ID\")\n    tensor_parallel_size: int = Field(default=1, ge=1)\n    gpu_memory_utilization: float = Field(default=0.9, ge=0.1, le=1.0)\n    max_model_len: Optional[int] = Field(default=None)\n    enforce_eager: bool = Field(default=False)\n    trust_remote_code: bool = Field(default=True)\n    dtype: str = Field(default=\"bfloat16\")\n\n    # VLLM-specific features\n    enable_prefix_caching: bool = Field(default=False)\n    enable_chunked_prefill: bool = Field(default=False)\n\n    class Config:\n        extra = \"forbid\"\n</code></pre>"},{"location":"architecture/#mlx-config","title":"MLX Config","text":"<pre><code># omnidocs/tasks/text_extraction/qwen/mlx.py\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, Literal\n\nclass QwenMLXConfig(BaseModel):\n    \"\"\"MLX backend configuration for Qwen (Apple Silicon).\"\"\"\n\n    model: str = Field(..., description=\"MLX model path or HuggingFace ID\")\n    quantization: Optional[Literal[\"4bit\", \"8bit\"]] = Field(default=None)\n    max_tokens: int = Field(default=4096)\n\n    class Config:\n        extra = \"forbid\"\n</code></pre>"},{"location":"architecture/#api-config","title":"API Config","text":"<pre><code># omnidocs/tasks/text_extraction/qwen/api.py\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, Dict\n\nclass QwenAPIConfig(BaseModel):\n    \"\"\"API backend configuration for Qwen (hosted or proxy).\"\"\"\n\n    model: str = Field(..., description=\"API model identifier\")\n    api_key: str = Field(..., description=\"API key\")\n    base_url: Optional[str] = Field(\n        default=None,\n        description=\"Custom API endpoint (for proxies)\"\n    )\n    rate_limit: int = Field(default=10, ge=1, description=\"Requests per minute\")\n    timeout: int = Field(default=30, ge=1, description=\"Request timeout in seconds\")\n    max_retries: int = Field(default=3, ge=0)\n    custom_headers: Optional[Dict[str, str]] = Field(default=None)\n\n    class Config:\n        extra = \"forbid\"\n</code></pre>"},{"location":"architecture/#inference-utilities","title":"Inference Utilities","text":"<p>The <code>omnidocs/inference/</code> module contains shared utilities for each backend:</p> <pre><code>omnidocs/inference/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 base.py          # Base inference class\n\u251c\u2500\u2500 pytorch.py       # PyTorch utilities\n\u251c\u2500\u2500 vllm.py          # VLLM utilities\n\u251c\u2500\u2500 mlx.py           # MLX utilities\n\u2514\u2500\u2500 api.py           # LiteLLM/API utilities\n</code></pre>"},{"location":"architecture/#base-inference-class","title":"Base Inference Class","text":"<pre><code># omnidocs/inference/base.py\n\nfrom abc import ABC, abstractmethod\nfrom typing import Any\nfrom PIL import Image\n\nclass BaseInference(ABC):\n    \"\"\"Base class for inference backends.\"\"\"\n\n    @abstractmethod\n    def load_model(self) -&gt; None:\n        \"\"\"Load model into memory.\"\"\"\n        pass\n\n    @abstractmethod\n    def infer(self, image: Image.Image, prompt: str) -&gt; Any:\n        \"\"\"Run inference.\"\"\"\n        pass\n\n    @abstractmethod\n    def unload(self) -&gt; None:\n        \"\"\"Free resources.\"\"\"\n        pass\n</code></pre>"},{"location":"architecture/#pytorch-inference","title":"PyTorch Inference","text":"<pre><code># omnidocs/inference/pytorch.py\n\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoProcessor\nfrom PIL import Image\nfrom .base import BaseInference\n\nclass PyTorchInference(BaseInference):\n    \"\"\"PyTorch/HuggingFace inference backend.\"\"\"\n\n    def __init__(self, config):\n        self.config = config\n        self.model = None\n        self.processor = None\n        self.load_model()\n\n    def load_model(self):\n        dtype_map = {\n            \"float16\": torch.float16,\n            \"bfloat16\": torch.bfloat16,\n            \"float32\": torch.float32,\n        }\n\n        self.processor = AutoProcessor.from_pretrained(\n            self.config.model,\n            trust_remote_code=self.config.trust_remote_code,\n        )\n\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.config.model,\n            torch_dtype=dtype_map[self.config.torch_dtype],\n            device_map=self.config.device_map,\n            trust_remote_code=self.config.trust_remote_code,\n        )\n\n        self.model.eval()\n\n    def infer(self, image: Image.Image, prompt: str):\n        inputs = self.processor(\n            text=prompt,\n            images=image,\n            return_tensors=\"pt\",\n        ).to(self.model.device)\n\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=4096,\n            )\n\n        return self.processor.decode(outputs[0], skip_special_tokens=True)\n\n    def unload(self):\n        if self.model:\n            del self.model\n            del self.processor\n            torch.cuda.empty_cache()\n</code></pre>"},{"location":"architecture/#vllm-inference","title":"VLLM Inference","text":"<pre><code># omnidocs/inference/vllm.py\n\nfrom PIL import Image\nfrom .base import BaseInference\n\nclass VLLMInference(BaseInference):\n    \"\"\"VLLM inference backend.\"\"\"\n\n    def __init__(self, config):\n        self.config = config\n        self.llm = None\n        self.load_model()\n\n    def load_model(self):\n        from vllm import LLM\n\n        self.llm = LLM(\n            model=self.config.model,\n            tensor_parallel_size=self.config.tensor_parallel_size,\n            gpu_memory_utilization=self.config.gpu_memory_utilization,\n            max_model_len=self.config.max_model_len,\n            enforce_eager=self.config.enforce_eager,\n            trust_remote_code=self.config.trust_remote_code,\n            dtype=self.config.dtype,\n        )\n\n    def infer(self, image: Image.Image, prompt: str):\n        from vllm import SamplingParams\n\n        sampling_params = SamplingParams(\n            max_tokens=4096,\n            temperature=0.0,\n        )\n\n        outputs = self.llm.generate(\n            {\n                \"prompt\": prompt,\n                \"multi_modal_data\": {\"image\": image},\n            },\n            sampling_params=sampling_params,\n        )\n\n        return outputs[0].outputs[0].text\n\n    def unload(self):\n        if self.llm:\n            del self.llm\n</code></pre>"},{"location":"architecture/#api-inference","title":"API Inference","text":"<pre><code># omnidocs/inference/api.py\n\nimport base64\nfrom io import BytesIO\nfrom PIL import Image\nfrom .base import BaseInference\n\nclass APIInference(BaseInference):\n    \"\"\"LiteLLM/API inference backend.\"\"\"\n\n    def __init__(self, config):\n        self.config = config\n        self.load_model()\n\n    def load_model(self):\n        \"\"\"Validate API configuration.\"\"\"\n        import litellm\n\n        # Configure LiteLLM\n        if self.config.base_url:\n            litellm.api_base = self.config.base_url\n\n    def infer(self, image: Image.Image, prompt: str):\n        import litellm\n\n        # Convert image to base64\n        buffered = BytesIO()\n        image.save(buffered, format=\"PNG\")\n        img_base64 = base64.b64encode(buffered.getvalue()).decode()\n\n        response = litellm.completion(\n            model=self.config.model,\n            api_key=self.config.api_key,\n            base_url=self.config.base_url,\n            messages=[\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": prompt},\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": f\"data:image/png;base64,{img_base64}\"\n                        }\n                    ]\n                }\n            ],\n            timeout=self.config.timeout,\n            num_retries=self.config.max_retries,\n        )\n\n        return response.choices[0].message.content\n\n    def unload(self):\n        pass  # Nothing to unload\n</code></pre>"},{"location":"architecture/#dependency-management","title":"Dependency Management","text":""},{"location":"architecture/#pyprojecttoml-structure","title":"pyproject.toml Structure","text":"<pre><code>[project]\nname = \"omnidocs\"\ndependencies = [\n    \"pydantic&gt;=2.0\",\n    \"pillow&gt;=10.0\",\n    \"numpy&gt;=1.24\",\n]\n\n[project.optional-dependencies]\n# Individual backends\npytorch = [\n    \"torch&gt;=2.0\",\n    \"torchvision&gt;=0.15\",\n    \"transformers&gt;=4.40\",\n]\n\nvllm = [\n    \"vllm&gt;=0.4.0\",\n    \"torch&gt;=2.0\",\n]\n\nmlx = [\n    \"mlx&gt;=0.10\",\n    \"mlx-lm&gt;=0.10\",\n]\n\napi = [\n    \"litellm&gt;=1.30\",\n    \"openai&gt;=1.0\",\n]\n\n# Convenience groups\nlocal = [\"omnidocs[pytorch]\"]\nall-local = [\"omnidocs[pytorch,vllm,mlx]\"]\nall = [\"omnidocs[pytorch,vllm,mlx,api]\"]\n\n# Development\ndev = [\"omnidocs[all]\", \"pytest\", \"black\", \"mypy\"]\n</code></pre>"},{"location":"architecture/#installation-examples","title":"Installation Examples","text":"<pre><code># Minimal (no inference)\npip install omnidocs\n\n# PyTorch only (most common)\npip install omnidocs[pytorch]\n\n# High-throughput serving\npip install omnidocs[vllm]\n\n# Apple Silicon\npip install omnidocs[mlx]\n\n# API only (no local inference)\npip install omnidocs[api]\n\n# Everything\npip install omnidocs[all]\n</code></pre>"},{"location":"architecture/#lazy-import-pattern","title":"Lazy Import Pattern","text":"<p>To avoid import errors when backends aren't installed:</p> <pre><code># omnidocs/tasks/text_extraction/qwen.py\n\nfrom typing import Union, TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from omnidocs.tasks.text_extraction.qwen import (\n        QwenPyTorchConfig,\n        QwenVLLMConfig,\n        QwenMLXConfig,\n        QwenAPIConfig,\n    )\n\n\nclass QwenTextExtractor:\n    def __init__(self, backend):\n        self.backend_config = backend\n        self._backend = None\n        self._load_backend()\n\n    def _load_backend(self):\n        \"\"\"Lazy load backend based on config type.\"\"\"\n        config = self.backend_config\n        config_type = type(config).__name__\n\n        if config_type == \"QwenPyTorchConfig\":\n            try:\n                from omnidocs.inference.pytorch import PyTorchInference\n            except ImportError:\n                raise ImportError(\n                    \"PyTorch backend requires torch and transformers. \"\n                    \"Install with: pip install omnidocs[pytorch]\"\n                )\n            self._backend = PyTorchInference(config)\n\n        elif config_type == \"QwenVLLMConfig\":\n            try:\n                from omnidocs.inference.vllm import VLLMInference\n            except ImportError:\n                raise ImportError(\n                    \"VLLM backend requires vllm. \"\n                    \"Install with: pip install omnidocs[vllm]\"\n                )\n            self._backend = VLLMInference(config)\n\n        # ... etc\n</code></pre>"},{"location":"architecture/#error-handling","title":"Error Handling","text":""},{"location":"architecture/#config-validation","title":"Config Validation","text":"<pre><code>from pydantic import ValidationError\n\ntry:\n    config = QwenVLLMConfig(\n        model=\"Qwen/Qwen2-VL-7B\",\n        tensor_parallel_size=-1,  # Invalid!\n    )\nexcept ValidationError as e:\n    print(e)\n    # tensor_parallel_size: Input should be greater than or equal to 1\n</code></pre>"},{"location":"architecture/#backend-not-installed","title":"Backend Not Installed","text":"<pre><code>try:\n    extractor = QwenTextExtractor(\n        backend=QwenVLLMConfig(model=\"Qwen/Qwen2-VL-7B\")\n    )\nexcept ImportError as e:\n    print(e)\n    # VLLM backend requires vllm. Install with: pip install omnidocs[vllm]\n</code></pre>"},{"location":"architecture/#invalid-backend-for-model","title":"Invalid Backend for Model","text":"<pre><code># DotsOCR doesn't support API\nfrom omnidocs.tasks.text_extraction import DotsOCRTextExtractor\n\n# This import would fail because DotsOCRAPIConfig doesn't exist\n# from omnidocs.tasks.text_extraction.dotsocr import DotsOCRAPIConfig\n\n# User naturally discovers DotsOCR doesn't support API\n# because there's no config class to import\n</code></pre>"},{"location":"architecture/#layout-detection-fixed-vs-flexible-models","title":"Layout Detection: Fixed vs Flexible Models","text":""},{"location":"architecture/#overview_1","title":"Overview","text":"<p>Layout detection models in OmniDocs fall into two categories based on label flexibility:</p> Category Examples Label Support Implementation Fixed Labels DocLayoutYOLO, RT-DETR Predefined only Trained model classes Flexible VLM Qwen, Florence-2 Custom via prompting Vision-language models"},{"location":"architecture/#fixed-label-models","title":"Fixed Label Models","text":"<p>Models: DocLayoutYOLO, RTDETRLayoutDetector, SuryaLayoutDetector</p> <p>These models are trained on specific label sets (title, text, table, figure, etc.) and cannot detect custom elements.</p> <pre><code># omnidocs/tasks/layout_analysis/doc_layout_yolo.py\n\nclass DocLayoutYOLO:\n    \"\"\"Fixed label layout detector. PyTorch only.\"\"\"\n\n    FIXED_LABELS = [\"title\", \"text\", \"list\", \"table\", \"figure\", \"caption\", \"formula\"]\n\n    def __init__(self, config: DocLayoutYOLOConfig):\n        self.config = config\n        self._load_model()\n\n    def extract(self, image: Image.Image) -&gt; LayoutOutput:\n        \"\"\"\n        Extract layout with predefined labels only.\n\n        Args:\n            image: PIL Image\n\n        Returns:\n            LayoutOutput with bboxes using FIXED_LABELS\n        \"\"\"\n        # Run YOLO detection\n        detections = self.model(image)\n\n        # Map to fixed labels\n        bboxes = []\n        for det in detections:\n            label = self.FIXED_LABELS[det.class_id]\n            bboxes.append(LayoutBox(label=label, bbox=det.bbox, confidence=det.conf))\n\n        return LayoutOutput(bboxes=bboxes)\n</code></pre>"},{"location":"architecture/#flexible-vlm-models","title":"Flexible VLM Models","text":"<p>Models: QwenLayoutDetector, Florence2LayoutDetector, VLMLayoutDetector</p> <p>These models use vision-language prompting and can detect ANY custom layout elements.</p> <pre><code># omnidocs/tasks/layout_analysis/qwen.py\n\nfrom typing import Union, List, Optional\nfrom omnidocs.tasks.layout_analysis.models import CustomLabel, LayoutOutput\n\nclass QwenLayoutDetector:\n    \"\"\"Flexible VLM layout detector. Supports custom labels.\"\"\"\n\n    DEFAULT_LABELS = [\"title\", \"text\", \"list\", \"table\", \"figure\", \"caption\", \"formula\"]\n\n    def __init__(self, backend: QwenBackendConfig):\n        self.backend_config = backend\n        self._backend = self._create_backend()\n\n    def extract(\n        self,\n        image: Image.Image,\n        custom_labels: Optional[Union[List[str], List[CustomLabel]]] = None,\n    ) -&gt; LayoutOutput:\n        \"\"\"\n        Extract layout with flexible label support.\n\n        Args:\n            image: PIL Image\n            custom_labels:\n                - None: Use DEFAULT_LABELS\n                - List[str]: Simple custom label names\n                - List[CustomLabel]: Structured labels with metadata\n\n        Returns:\n            LayoutOutput with detected elements\n        \"\"\"\n        # Normalize labels\n        if custom_labels is None:\n            labels = [CustomLabel(name=name) for name in self.DEFAULT_LABELS]\n        else:\n            labels = self._normalize_labels(custom_labels)\n\n        # Build detection prompt\n        prompt = self._build_prompt(labels)\n\n        # Run VLM inference\n        raw_output = self._backend.infer(image, prompt)\n\n        # Parse results\n        return self._parse_detections(raw_output, labels)\n\n    def _normalize_labels(\n        self,\n        labels: Union[List[str], List[CustomLabel]]\n    ) -&gt; List[CustomLabel]:\n        \"\"\"Convert string labels to CustomLabel objects.\"\"\"\n        normalized = []\n        for label in labels:\n            if isinstance(label, str):\n                normalized.append(CustomLabel(name=label))\n            elif isinstance(label, CustomLabel):\n                normalized.append(label)\n        return normalized\n\n    def _build_prompt(self, labels: List[CustomLabel]) -&gt; str:\n        \"\"\"Build detection prompt from labels.\"\"\"\n        label_descriptions = []\n\n        for label in labels:\n            if label.detection_prompt:\n                # Use custom detection prompt\n                label_descriptions.append(\n                    f\"- {label.name}: {label.detection_prompt}\"\n                )\n            else:\n                # Use label name only\n                label_descriptions.append(f\"- {label.name}\")\n\n        prompt = f\"\"\"Detect the following layout elements in this document image:\n\n{chr(10).join(label_descriptions)}\n\nReturn bounding boxes [x1, y1, x2, y2] for each detected element.\"\"\"\n\n        return prompt\n</code></pre>"},{"location":"architecture/#customlabel-definition","title":"CustomLabel Definition","text":"<pre><code># omnidocs/tasks/layout_analysis/models.py\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass CustomLabel(BaseModel):\n    \"\"\"Custom layout label definition for flexible VLM models.\"\"\"\n\n    name: str = Field(..., description=\"Label identifier (e.g., 'code_block')\")\n\n    description: Optional[str] = Field(\n        default=None,\n        description=\"Human-readable description\"\n    )\n\n    detection_prompt: Optional[str] = Field(\n        default=None,\n        description=\"Custom prompt hint for model to use during detection\"\n    )\n\n    color: Optional[str] = Field(\n        default=None,\n        description=\"Visualization color (hex or name)\"\n    )\n\n    class Config:\n        extra = \"allow\"  # Users can add custom fields\n</code></pre>"},{"location":"architecture/#usage-examples","title":"Usage Examples","text":"<p>Fixed Model (Simple, Fast): <pre><code>from omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\n\nlayout = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = layout.extract(image)\n# Returns: title, text, table, figure (fixed set)\n</code></pre></p> <p>Flexible VLM (Simple Strings): <pre><code>from omnidocs.tasks.layout_analysis import QwenLayoutDetector\nfrom omnidocs.tasks.layout_analysis.qwen import QwenPyTorchConfig\n\nlayout = QwenLayoutDetector(\n    backend=QwenPyTorchConfig(model=\"Qwen/Qwen2-VL-7B\")\n)\n\n# Detect custom elements\nresult = layout.extract(\n    image,\n    custom_labels=[\"code_block\", \"sidebar\", \"pull_quote\"]\n)\n</code></pre></p> <p>Flexible VLM (Structured Labels): <pre><code>from omnidocs.tasks.layout_analysis import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_analysis.qwen import QwenPyTorchConfig\n\nlayout = QwenLayoutDetector(\n    backend=QwenPyTorchConfig(model=\"Qwen/Qwen2-VL-7B\")\n)\n\nresult = layout.extract(\n    image,\n    custom_labels=[\n        CustomLabel(\n            name=\"code_block\",\n            description=\"Source code listings\",\n            detection_prompt=\"Regions with monospace text and syntax highlighting\",\n            color=\"#2ecc71\",\n        ),\n        CustomLabel(\n            name=\"sidebar\",\n            description=\"Supplementary content boxes\",\n            detection_prompt=\"Boxed regions with background color or borders\",\n            color=\"#3498db\",\n        ),\n    ]\n)\n\n# Access metadata\nfor box in result.bboxes:\n    print(f\"{box.label.name}: {box.label.description}\")\n</code></pre></p>"},{"location":"architecture/#benefits","title":"Benefits","text":"Feature Fixed Models Flexible VLMs Speed \u26a1 Fast \ud83d\udc22 Slower (VLM inference) Accuracy \u2b50\u2b50\u2b50 High (trained) \u2b50\u2b50 Good (prompted) Custom Labels \u274c No \u2705 Yes Label Metadata \u274c No \u2705 Yes (CustomLabel) Detection Prompts \u274c No \u2705 Yes Extensibility \u274c No \u2705 Yes (extra fields) Use Case Standard documents Any document type"},{"location":"architecture/#summary","title":"Summary","text":""},{"location":"architecture/#key-design-decisions","title":"Key Design Decisions","text":"Decision Choice Rationale Config Pattern Model-specific classes IDE support, type safety Backend Discovery Import exists = supported Obvious, no guessing Lazy Imports Load on use Avoid dependency errors Validation Pydantic Early error detection Error Messages Clear install instructions Good UX"},{"location":"architecture/#config-naming-convention","title":"Config Naming Convention","text":"Model Type Config Location Naming Single-backend Same file as model <code>{Model}Config</code> Multi-backend Subfolder <code>{Model}{Backend}Config</code>"},{"location":"architecture/#parameter-naming","title":"Parameter Naming","text":"Model Type Parameter Single-backend <code>config=</code> Multi-backend <code>backend=</code> <p>Last Updated: January 20, 2026 Status: \u2705 Design Complete</p>"},{"location":"developer-guide/","title":"OmniDocs - Final Developer Experience Design","text":"<p>Status: \u2705 Design Complete - Ready for Implementation Last Updated: January 20, 2026 Version: 2.0.0</p>"},{"location":"developer-guide/#table-of-contents","title":"\ud83d\udccb Table of Contents","text":"<ol> <li>Overview</li> <li>Core Design Principles</li> <li>Backend Configuration</li> <li>Architecture</li> <li>Task Distinctions</li> <li>Layout Detection: Fixed vs Flexible Models</li> <li>Document Loading</li> <li>Usage Patterns</li> <li>Complete Examples</li> <li>Import Reference</li> <li>Implementation Roadmap</li> </ol>"},{"location":"developer-guide/#overview","title":"Overview","text":"<p>OmniDocs is a unified Python toolkit for visual document processing that provides a consistent API across multiple models and tasks.</p>"},{"location":"developer-guide/#core-philosophy","title":"Core Philosophy","text":"<p>Input Standardization: <code>Image \u2192 Model \u2192 Pydantic Output</code></p> <p>All tasks follow this pattern regardless of: - Which model is used (specialized vs VLM) - Which backend runs inference (PyTorch, VLLM, MLX, API) - Task complexity</p>"},{"location":"developer-guide/#supported-tasks","title":"Supported Tasks","text":"<ol> <li>Layout Analysis - Detect document structure (headings, paragraphs, figures, tables)</li> <li>OCR Extraction - Extract text with bounding boxes from images</li> <li>Text Extraction - Export document to Markdown/HTML (10+ specialized VLM models)</li> <li>Table Extraction - Extract tables and convert to structured formats</li> <li>Math Expression Recognition - Convert math to LaTeX</li> <li>Reading Order Detection - Order layout elements in reading sequence</li> <li>Image Captioning - Caption figures and images</li> <li>Chart Understanding - Convert charts to data + metadata</li> <li>Structured Output Extraction - Extract structured data with schemas</li> </ol>"},{"location":"developer-guide/#core-design-principles","title":"Core Design Principles","text":""},{"location":"developer-guide/#final-decisions","title":"\u2705 Final Decisions","text":"<ol> <li> <p>Class-Based Imports - No string-based factory pattern    <pre><code>from omnidocs.tasks.layout_analysis import DocLayoutYOLO  # \u2705 YES\nlayout = LayoutAnalysis(model=\"doclayout-yolo\")           # \u274c NO\n</code></pre></p> </li> <li> <p>Unified Method Name - <code>.extract()</code> for ALL tasks (including layout)    <pre><code>layout_result = layout.extract(image)\nocr_result = ocr.extract(image)\ntext_result = text.extract(image, output_format=\"markdown\")\n</code></pre></p> </li> <li> <p>Model-Specific Configs - Each model defines its own config classes    <pre><code># Single-backend model\nfrom omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\n\nlayout = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\n\n# Multi-backend model - import config for desired backend\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig, QwenAPIConfig\n\nextractor = QwenTextExtractor(backend=QwenPyTorchConfig(model=\"Qwen/Qwen2-VL-7B\"))\n</code></pre></p> </li> <li> <p>Separation of Init vs Extract:</p> </li> <li><code>__init__</code> (via config) = Model initialization, download, verification<ul> <li>Which model to use</li> <li>Which backend (PyTorch/VLLM/MLX/API)</li> <li>Model loading settings (device, dtype, quantization)</li> <li>Download and cache paths</li> </ul> </li> <li> <p><code>.extract()</code> = Runtime task parameters</p> <ul> <li>Output format (markdown/html)</li> <li>Custom prompts</li> <li>Task-specific options (include_layout, custom_labels)</li> <li>Per-call inference settings</li> </ul> </li> <li> <p>Stateless Document - Document is source data only, does NOT store task results    <pre><code>doc = Document.from_pdf(\"file.pdf\")  # Just loads the data\nresult = layout.extract(doc.get_page(0))  # User manages results\n</code></pre></p> </li> <li> <p>Discoverability - Available backends = available config classes    <pre><code># Multi-backend model - see what configs exist\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenPyTorchConfig,  # \u2713 PyTorch supported\n    QwenVLLMConfig,     # \u2713 VLLM supported\n    QwenMLXConfig,      # \u2713 MLX supported\n    QwenAPIConfig,      # \u2713 API supported\n)\n\n# Single-backend model - only one config\nfrom omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\n</code></pre></p> </li> <li> <p>Separation of Concerns:</p> </li> <li>Document Loading = Internal (pypdfium2, PyMuPDF) - NOT separate extractors</li> <li>OCR Extraction = Text + bounding boxes from images</li> <li>Text Extraction = Markdown/HTML export (specialized VLMs)</li> </ol>"},{"location":"developer-guide/#backend-configuration","title":"Backend Configuration","text":""},{"location":"developer-guide/#design-model-specific-config-classes","title":"Design: Model-Specific Config Classes","text":"<p>Each model has config classes specific to its supported backends. This provides: - IDE autocomplete with only relevant parameters - Type safety with Pydantic validation - Clear discoverability of supported backends</p>"},{"location":"developer-guide/#single-backend-models","title":"Single-Backend Models","text":"<p>Models that only support one backend (e.g., DocLayoutYOLO = PyTorch only):</p> <pre><code>from omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\n\n# Config has model-specific parameters\nlayout = DocLayoutYOLO(\n    config=DocLayoutYOLOConfig(\n        device=\"cuda\",\n        model_path=None,        # Optional custom weights\n        img_size=1024,          # Model-specific\n    )\n)\n\nresult = layout.extract(image)\n</code></pre>"},{"location":"developer-guide/#multi-backend-models","title":"Multi-Backend Models","text":"<p>Models that support multiple backends (e.g., Qwen = PyTorch, VLLM, MLX, API):</p> <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenPyTorchConfig,\n    QwenVLLMConfig,\n    QwenMLXConfig,\n    QwenAPIConfig,\n)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Option 1: PyTorch (local HuggingFace)\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nextractor = QwenTextExtractor(\n    backend=QwenPyTorchConfig(\n        model=\"Qwen/Qwen2-VL-7B-Instruct\",\n        device=\"cuda\",\n        trust_remote_code=True,\n        torch_dtype=\"bfloat16\",\n    )\n)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Option 2: VLLM (high-throughput)\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nextractor = QwenTextExtractor(\n    backend=QwenVLLMConfig(\n        model=\"Qwen/Qwen2-VL-7B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n        max_model_len=8192,\n        enforce_eager=False,\n    )\n)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Option 3: MLX (Apple Silicon)\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nextractor = QwenTextExtractor(\n    backend=QwenMLXConfig(\n        model=\"Qwen/Qwen2-VL-7B-Instruct-MLX\",\n        quantization=\"4bit\",\n    )\n)\n\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n# Option 4: API (hosted or proxy)\n# \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nextractor = QwenTextExtractor(\n    backend=QwenAPIConfig(\n        model=\"qwen2-vl-7b\",\n        api_key=\"YOUR_API_KEY\",\n        base_url=\"https://api.provider.com/v1\",  # Custom endpoint\n        rate_limit=20,\n        timeout=30,\n    )\n)\n\n# Task parameters in .extract()\nresult = extractor.extract(\n    image,\n    output_format=\"markdown\",\n    include_layout=True,\n    custom_prompt=None,\n)\n</code></pre>"},{"location":"developer-guide/#config-class-naming-convention","title":"Config Class Naming Convention","text":"Model Type Config Naming Example Single-backend <code>{Model}Config</code> <code>DocLayoutYOLOConfig</code> Multi-backend PyTorch <code>{Model}PyTorchConfig</code> <code>QwenPyTorchConfig</code> Multi-backend VLLM <code>{Model}VLLMConfig</code> <code>QwenVLLMConfig</code> Multi-backend MLX <code>{Model}MLXConfig</code> <code>QwenMLXConfig</code> Multi-backend API <code>{Model}APIConfig</code> <code>QwenAPIConfig</code>"},{"location":"developer-guide/#model-backend-support-matrix","title":"Model-Backend Support Matrix","text":"Model PyTorch VLLM MLX API Layout Analysis DocLayoutYOLO \u2705 \u274c \u274c \u274c RTDETRLayoutDetector \u2705 \u274c \u274c \u274c SuryaLayoutDetector \u2705 \u274c \u274c \u274c QwenLayoutDetector \u2705 \u2705 \u2705 \u2705 VLMLayoutDetector \u274c \u274c \u274c \u2705 Text Extraction QwenTextExtractor \u2705 \u2705 \u2705 \u2705 DotsOCRTextExtractor \u2705 \u2705 \u2705 \u274c ChandraTextExtractor \u2705 \u2705 \u2705 \u274c GemmaTextExtractor \u2705 \u2705 \u2705 \u2705 VLMTextExtractor \u274c \u274c \u274c \u2705 OCR Extraction TesseractOCR \u2705 \u274c \u274c \u274c SuryaOCR \u2705 \u274c \u274c \u274c QwenOCR \u2705 \u2705 \u2705 \u2705"},{"location":"developer-guide/#architecture","title":"Architecture","text":""},{"location":"developer-guide/#system-overview","title":"System Overview","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502    Document Loading (Internal)      \u2502\n\u2502  pypdfium2, PyMuPDF, pdfplumber     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Task Layer                  \u2502\n\u2502  Layout, OCR, Text, Table, Math...  \u2502\n\u2502  (Each model has its own configs)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n              \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502      Inference Layer                \u2502\n\u2502  PyTorch, VLLM, MLX, LiteLLM        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"developer-guide/#directory-structure","title":"Directory Structure","text":"<pre><code>omnidocs/\n\u251c\u2500\u2500 __init__.py                 # Export Document\n\u251c\u2500\u2500 document.py                 # Document class (stateless)\n\u2502\n\u251c\u2500\u2500 tasks/\n\u2502   \u251c\u2500\u2500 layout_analysis/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py         # Export models + configs\n\u2502   \u2502   \u251c\u2500\u2500 base.py             # BaseLayoutExtractor\n\u2502   \u2502   \u251c\u2500\u2500 models.py           # LayoutBox, LayoutOutput (Pydantic)\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 doc_layout_yolo.py  # DocLayoutYOLO + DocLayoutYOLOConfig\n\u2502   \u2502   \u251c\u2500\u2500 rtdetr.py           # RTDETRLayoutDetector + RTDETRConfig\n\u2502   \u2502   \u251c\u2500\u2500 surya.py            # SuryaLayoutDetector + SuryaLayoutConfig\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 qwen.py             # QwenLayoutDetector\n\u2502   \u2502   \u2514\u2500\u2500 qwen/               # Qwen backend configs\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u251c\u2500\u2500 pytorch.py      # QwenPyTorchConfig\n\u2502   \u2502       \u251c\u2500\u2500 vllm.py         # QwenVLLMConfig\n\u2502   \u2502       \u251c\u2500\u2500 mlx.py          # QwenMLXConfig\n\u2502   \u2502       \u2514\u2500\u2500 api.py          # QwenAPIConfig\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 ocr_extraction/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 base.py\n\u2502   \u2502   \u251c\u2500\u2500 models.py           # OCROutput, TextBlock (Pydantic)\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 tesseract.py        # TesseractOCR + TesseractConfig\n\u2502   \u2502   \u251c\u2500\u2500 paddle.py           # PaddleOCR + PaddleOCRConfig\n\u2502   \u2502   \u251c\u2500\u2500 easyocr.py          # EasyOCR + EasyOCRConfig\n\u2502   \u2502   \u251c\u2500\u2500 surya.py            # SuryaOCR + SuryaOCRConfig\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 qwen.py             # QwenOCR\n\u2502   \u2502   \u2514\u2500\u2500 qwen/               # Qwen backend configs\n\u2502   \u2502       \u2514\u2500\u2500 ...\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 text_extraction/\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u251c\u2500\u2500 base.py\n\u2502   \u2502   \u251c\u2500\u2500 models.py           # TextOutput (Pydantic)\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 vlm_extractor.py    # VLMTextExtractor + VLMTextConfig (API-only)\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 qwen.py             # QwenTextExtractor\n\u2502   \u2502   \u251c\u2500\u2500 qwen/               # Qwen backend configs\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 pytorch.py      # QwenPyTorchConfig\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 vllm.py         # QwenVLLMConfig\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 mlx.py          # QwenMLXConfig\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 api.py          # QwenAPIConfig\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 dotsocr.py          # DotsOCRTextExtractor\n\u2502   \u2502   \u251c\u2500\u2500 dotsocr/            # DotsOCR backend configs\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 pytorch.py      # DotsOCRPyTorchConfig\n\u2502   \u2502   \u2502   \u251c\u2500\u2500 vllm.py         # DotsOCRVLLMConfig\n\u2502   \u2502   \u2502   \u2514\u2500\u2500 mlx.py          # DotsOCRMLXConfig (no API)\n\u2502   \u2502   \u2502\n\u2502   \u2502   \u251c\u2500\u2500 chandra.py          # ChandraTextExtractor\n\u2502   \u2502   \u251c\u2500\u2500 gemma.py            # GemmaTextExtractor\n\u2502   \u2502   \u251c\u2500\u2500 granite.py          # GraniteDoclingOCR\n\u2502   \u2502   \u251c\u2500\u2500 hunyuan.py          # HunyuanTextExtractor\n\u2502   \u2502   \u251c\u2500\u2500 lighton.py          # LightOnOCRExtractor\n\u2502   \u2502   \u251c\u2500\u2500 mineru.py           # MinerUOCRExtractor\n\u2502   \u2502   \u251c\u2500\u2500 nanonuts.py         # NanonutsOCRExtractor\n\u2502   \u2502   \u251c\u2500\u2500 olmo.py             # OlmOCRExtractor\n\u2502   \u2502   \u2514\u2500\u2500 paddle.py           # PaddleTextExtractor\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 table_extraction/\n\u2502   \u2502   \u251c\u2500\u2500 table_transformer.py\n\u2502   \u2502   \u251c\u2500\u2500 surya_table.py\n\u2502   \u2502   \u251c\u2500\u2500 qwen.py\n\u2502   \u2502   \u2514\u2500\u2500 vlm_extractor.py\n\u2502   \u2502\n\u2502   \u251c\u2500\u2500 math_expression_extraction/\n\u2502   \u2502   \u251c\u2500\u2500 unimernet.py\n\u2502   \u2502   \u251c\u2500\u2500 qwen.py\n\u2502   \u2502   \u2514\u2500\u2500 vlm_extractor.py\n\u2502   \u2502\n\u2502   \u2514\u2500\u2500 structured_output_extraction/\n\u2502       \u2514\u2500\u2500 vlm_extractor.py\n\u2502\n\u251c\u2500\u2500 inference/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 base.py                 # Base backend classes\n\u2502   \u251c\u2500\u2500 pytorch.py              # PyTorch inference utilities\n\u2502   \u251c\u2500\u2500 vllm.py                 # VLLM inference utilities\n\u2502   \u251c\u2500\u2500 mlx.py                  # MLX inference utilities\n\u2502   \u2514\u2500\u2500 api.py                  # LiteLLM/API utilities\n\u2502\n\u251c\u2500\u2500 workflows/\n\u2502   \u2514\u2500\u2500 document_workflow.py\n\u2502\n\u2514\u2500\u2500 utils/\n    \u251c\u2500\u2500 visualization.py\n    \u2514\u2500\u2500 export.py\n</code></pre>"},{"location":"developer-guide/#task-distinctions","title":"Task Distinctions","text":""},{"location":"developer-guide/#critical-clarifications","title":"\u26a0\ufe0f Critical Clarifications","text":"Component Role Output Examples Document Loading Load PDFs/images PIL Images + Metadata <code>Document.from_pdf()</code> OCR Extraction Text + bounding boxes <code>OCROutput(text_blocks=[...])</code> TesseractOCR, SuryaOCR, QwenOCR Text Extraction Markdown/HTML export <code>TextOutput(content, format)</code> QwenTextExtractor, DotsOCRTextExtractor Layout Analysis Detect structure <code>LayoutOutput(bboxes=[...])</code> DocLayoutYOLO, QwenLayoutDetector <p>Important: - PyMuPDF, PDFPlumber, pypdfium2 are internal to Document - NOT separate extractors - OCR returns text WITH bounding boxes - Text Extraction returns formatted text (MD/HTML) WITHOUT bboxes</p>"},{"location":"developer-guide/#ocr-vs-text-extraction","title":"OCR vs Text Extraction","text":"<pre><code># \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# OCR Extraction - Text + Bounding Boxes\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfrom omnidocs.tasks.ocr_extraction import SuryaOCR, SuryaOCRConfig\n\nocr = SuryaOCR(config=SuryaOCRConfig(device=\"cuda\"))\nresult = ocr.extract(image)\n\n# Output: OCROutput\nfor text_block in result.text_blocks:\n    print(f\"Text: {text_block.text}\")\n    print(f\"BBox: {text_block.bbox}\")\n    print(f\"Confidence: {text_block.confidence}\")\n\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Text Extraction - Markdown/HTML Export\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig, QwenAPIConfig\n\n# Local inference with PyTorch\nextractor = QwenTextExtractor(\n    backend=QwenPyTorchConfig(\n        model=\"Qwen/Qwen2-VL-7B-Instruct\",\n        device=\"cuda\",\n    )\n)\n\n# OR API inference\nextractor = QwenTextExtractor(\n    backend=QwenAPIConfig(\n        model=\"qwen2-vl-7b\",\n        api_key=\"YOUR_API_KEY\",\n        base_url=\"https://api.provider.com/v1\",\n    )\n)\n\n# Task parameters in .extract()\nresult = extractor.extract(\n    image,\n    output_format=\"markdown\",      # \"markdown\" or \"html\"\n    include_layout=True,           # Include layout information\n    custom_prompt=None,            # Override default prompt\n)\n\n# Output: TextOutput\nprint(result.content)      # Full markdown/html\nprint(result.format)       # \"markdown\" or \"html\"\n</code></pre>"},{"location":"developer-guide/#layout-detection-fixed-vs-flexible-models","title":"Layout Detection: Fixed vs Flexible Models","text":""},{"location":"developer-guide/#model-categories","title":"Model Categories","text":"<p>OmniDocs layout detectors fall into two categories:</p> Category Models Label Support Use Case Fixed Labels DocLayoutYOLO, RT-DETR, Surya Predefined only Fast, specialized detection Flexible VLM Qwen, Florence-2, VLMLayoutDetector Custom labels via prompting Adaptable to any document type"},{"location":"developer-guide/#fixed-label-models","title":"Fixed Label Models","text":"<p>Examples: DocLayoutYOLO, RTDETRLayoutDetector, SuryaLayoutDetector</p> <p>These models are trained on specific label sets and cannot detect custom elements.</p> <pre><code>from omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\n\nlayout = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = layout.extract(image)\n\n# Fixed labels only:\n# - title\n# - text\n# - list\n# - table\n# - figure\n# - caption\n# - formula\n</code></pre> <p>Characteristics: - \u2705 Fast inference - \u2705 Highly accurate on standard elements - \u274c Cannot detect custom elements (code blocks, sidebars, etc.) - \u274c Fixed label set (no flexibility)</p>"},{"location":"developer-guide/#flexible-vlm-models","title":"Flexible VLM Models","text":"<p>Examples: QwenLayoutDetector, Florence2LayoutDetector, VLMLayoutDetector</p> <p>These models use vision-language prompting and can detect ANY custom layout elements.</p>"},{"location":"developer-guide/#basic-usage-default-labels","title":"Basic Usage (Default Labels)","text":"<pre><code>from omnidocs.tasks.layout_analysis import QwenLayoutDetector\nfrom omnidocs.tasks.layout_analysis.qwen import QwenPyTorchConfig\n\nlayout = QwenLayoutDetector(\n    backend=QwenPyTorchConfig(model=\"Qwen/Qwen2-VL-7B\")\n)\n\n# Standard labels (same as fixed models)\nresult = layout.extract(image)\n# Returns: title, text, table, figure, etc.\n</code></pre>"},{"location":"developer-guide/#custom-labels-simple-strings","title":"Custom Labels (Simple Strings)","text":"<pre><code># Detect custom elements via simple strings\nresult = layout.extract(\n    image,\n    custom_labels=[\"code_block\", \"sidebar\", \"pull_quote\", \"diagram\"]\n)\n\nfor box in result.bboxes:\n    print(f\"{box.label}: {box.bbox}\")\n    # code_block: [x1, y1, x2, y2]\n    # sidebar: [x1, y1, x2, y2]\n</code></pre>"},{"location":"developer-guide/#custom-labels-structured","title":"Custom Labels (Structured)","text":"<p>For advanced use cases, use <code>CustomLabel</code> with metadata:</p> <pre><code>from omnidocs.tasks.layout_analysis import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_analysis.qwen import QwenPyTorchConfig\n\nlayout = QwenLayoutDetector(\n    backend=QwenPyTorchConfig(model=\"Qwen/Qwen2-VL-7B\")\n)\n\n# Structured labels with metadata\nresult = layout.extract(\n    image,\n    custom_labels=[\n        CustomLabel(\n            name=\"code_block\",\n            description=\"Programming source code areas\",\n            detection_prompt=\"Regions with monospace text and syntax highlighting\",\n            color=\"#2ecc71\",\n        ),\n        CustomLabel(\n            name=\"sidebar\",\n            description=\"Sidebar or callout content\",\n            detection_prompt=\"Boxed regions with supplementary information\",\n            color=\"#3498db\",\n        ),\n        CustomLabel(\n            name=\"pull_quote\",\n            description=\"Highlighted quotations\",\n            detection_prompt=\"Large formatted quotes in different font/color\",\n            color=\"#e74c3c\",\n        ),\n    ]\n)\n\n# Access metadata\nfor box in result.bboxes:\n    print(f\"Label: {box.label.name}\")\n    print(f\"Description: {box.label.description}\")\n    print(f\"Color: {box.label.color}\")\n</code></pre>"},{"location":"developer-guide/#customlabel-type-definition","title":"CustomLabel Type Definition","text":"<pre><code>from pydantic import BaseModel, Field\nfrom typing import Optional\n\nclass CustomLabel(BaseModel):\n    \"\"\"Custom layout label definition for flexible VLM models.\"\"\"\n\n    name: str = Field(..., description=\"Label identifier (e.g., 'code_block')\")\n\n    description: Optional[str] = Field(\n        default=None,\n        description=\"Human-readable description\"\n    )\n\n    detection_prompt: Optional[str] = Field(\n        default=None,\n        description=\"Custom prompt hint for detection\"\n    )\n\n    color: Optional[str] = Field(\n        default=None,\n        description=\"Visualization color (hex or name)\"\n    )\n\n    class Config:\n        extra = \"allow\"  # Users can add custom fields\n</code></pre>"},{"location":"developer-guide/#reusable-label-sets","title":"Reusable Label Sets","text":"<pre><code>from omnidocs.tasks.layout_analysis import CustomLabel\n\nclass TechnicalDocLabels:\n    \"\"\"Reusable labels for technical documentation.\"\"\"\n\n    CODE_BLOCK = CustomLabel(\n        name=\"code_block\",\n        description=\"Source code listings\",\n        color=\"#2ecc71\"\n    )\n\n    API_REFERENCE = CustomLabel(\n        name=\"api_reference\",\n        description=\"API documentation tables\",\n        color=\"#3498db\"\n    )\n\n    DIAGRAM = CustomLabel(\n        name=\"diagram\",\n        description=\"Architecture diagrams\",\n        color=\"#9b59b6\"\n    )\n\n    @classmethod\n    def all(cls):\n        return [cls.CODE_BLOCK, cls.API_REFERENCE, cls.DIAGRAM]\n\n# Use across projects\nresult = layout.extract(image, custom_labels=TechnicalDocLabels.all())\n</code></pre>"},{"location":"developer-guide/#user-extensions","title":"User Extensions","text":"<p>Users can extend <code>CustomLabel</code> with custom fields:</p> <pre><code>from omnidocs.tasks.layout_analysis import CustomLabel\n\nclass MyLabel(CustomLabel):\n    priority: int = 1          # Custom field\n    requires_ocr: bool = True  # Custom field\n\nresult = layout.extract(\n    image,\n    custom_labels=[\n        MyLabel(\n            name=\"important_section\",\n            description=\"High-priority content\",\n            priority=10,\n            requires_ocr=True,\n        )\n    ]\n)\n\n# Access custom fields\nfor box in result.bboxes:\n    print(f\"Priority: {box.label.priority}\")\n    print(f\"Requires OCR: {box.label.requires_ocr}\")\n</code></pre>"},{"location":"developer-guide/#comparison","title":"Comparison","text":"Feature Fixed Models Flexible VLMs Speed \u26a1 Fast \ud83d\udc22 Slower Accuracy (standard) \u2b50\u2b50\u2b50 High \u2b50\u2b50 Good Custom labels \u274c No \u2705 Yes String labels \u274c No \u2705 Yes Structured labels \u274c No \u2705 Yes (CustomLabel) Label metadata \u274c No \u2705 Yes Detection prompts \u274c No \u2705 Yes Use case Standard docs Any document type"},{"location":"developer-guide/#document-loading","title":"Document Loading","text":""},{"location":"developer-guide/#design-decision-stateless-document","title":"Design Decision: Stateless Document","text":"<p>Document is SOURCE DATA only - it does NOT store task results.</p> <p>Rationale: - Clean separation: Document = loaded PDF/images, Tasks = analysis results - Memory efficient: Document doesn't grow with analysis - User control: Users decide what to cache and how - Flexibility: Works with any caching strategy</p>"},{"location":"developer-guide/#document-api","title":"Document API","text":"<pre><code>from omnidocs import Document\n\n# Load from various sources\ndoc = Document.from_pdf(\"file.pdf\", dpi=150, page_range=(0, 4))\ndoc = Document.from_url(\"https://example.com/doc.pdf\")\ndoc = Document.from_bytes(pdf_bytes, filename=\"doc.pdf\")\ndoc = Document.from_image(\"page.png\")\ndoc = Document.from_images([\"page1.png\", \"page2.png\"])\n\n# Properties (metadata only)\ndoc.page_count          # Number of pages\ndoc.metadata            # DocumentMetadata object\ndoc.pages               # List[Image.Image] - all pages\ndoc.text                # Full text (lazy extraction, cached)\n\n# Access specific pages\npage_img = doc.get_page(0)              # 0-indexed\npage_text = doc.get_page_text(1)        # 1-indexed\npage_size = doc.get_page_size(0)        # Dimensions\n\n# Iterate (memory efficient)\nfor page_img in doc.iter_pages():\n    process(page_img)\n\n# Utilities\ndoc.save_images(\"output/\", prefix=\"page\", format=\"PNG\")\ndoc.to_dict()\ndoc.clear_cache()       # Free cached page images\n</code></pre>"},{"location":"developer-guide/#usage-patterns","title":"Usage Patterns","text":""},{"location":"developer-guide/#pattern-1-single-backend-model-simple","title":"Pattern 1: Single-Backend Model (Simple)","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\n\n# Load document\ndoc = Document.from_pdf(\"paper.pdf\")\n\n# Single-backend model - just use config=\nlayout = DocLayoutYOLO(\n    config=DocLayoutYOLOConfig(\n        device=\"cuda\",\n        img_size=1024,\n    )\n)\n\n# Process\nfor i in range(doc.page_count):\n    page = doc.get_page(i)\n    result = layout.extract(page)\n\n    for box in result.bboxes:\n        print(f\"{box.label}: {box.bbox}\")\n</code></pre>"},{"location":"developer-guide/#pattern-2-multi-backend-model-flexible","title":"Pattern 2: Multi-Backend Model (Flexible)","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenPyTorchConfig,\n    QwenVLLMConfig,\n    QwenAPIConfig,\n)\n\ndoc = Document.from_pdf(\"paper.pdf\")\n\n# Choose backend based on environment\nimport os\n\nif os.getenv(\"USE_VLLM\"):\n    backend = QwenVLLMConfig(\n        model=\"Qwen/Qwen2-VL-7B-Instruct\",\n        tensor_parallel_size=2,\n    )\nelif os.getenv(\"USE_API\"):\n    backend = QwenAPIConfig(\n        model=\"qwen2-vl-7b\",\n        api_key=os.getenv(\"API_KEY\"),\n        base_url=os.getenv(\"API_BASE_URL\"),\n    )\nelse:\n    backend = QwenPyTorchConfig(\n        model=\"Qwen/Qwen2-VL-7B-Instruct\",\n        device=\"cuda\",\n    )\n\nextractor = QwenTextExtractor(backend=backend)\n\n# Process with task params in extract()\nfor i in range(doc.page_count):\n    page = doc.get_page(i)\n    result = extractor.extract(\n        page,\n        output_format=\"markdown\",\n        include_layout=True,\n    )\n    print(result.content)\n</code></pre>"},{"location":"developer-guide/#pattern-3-api-only-models-vlmtextextractor","title":"Pattern 3: API-Only Models (VLMTextExtractor)","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import VLMTextExtractor, VLMTextConfig\n\ndoc = Document.from_pdf(\"file.pdf\")\n\n# Generic VLM extractor for API-only models (Gemini, GPT-4, Claude)\nextractor = VLMTextExtractor(\n    config=VLMTextConfig(\n        model=\"gemini-1.5-flash\",      # or \"gpt-4o\", \"claude-3-sonnet\"\n        api_key=\"YOUR_API_KEY\",\n        base_url=None,                  # Optional custom endpoint\n        rate_limit=20,\n    )\n)\n\nresult = extractor.extract(\n    doc.get_page(0),\n    output_format=\"markdown\",\n    custom_prompt=\"Extract all text preserving structure.\",\n)\n</code></pre>"},{"location":"developer-guide/#pattern-4-mixed-pipeline","title":"Pattern 4: Mixed Pipeline","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\nfrom omnidocs.tasks.table_extraction import TableTransformer, TableTransformerConfig\n\ndoc = Document.from_pdf(\"research_paper.pdf\")\n\n# Different models for different tasks\nlayout = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\n\ntext = QwenTextExtractor(\n    backend=QwenPyTorchConfig(\n        model=\"Qwen/Qwen2-VL-7B-Instruct\",\n        device=\"cuda\",\n    )\n)\n\ntable = TableTransformer(config=TableTransformerConfig(device=\"cuda\"))\n\n# Process based on detected layout\npage = doc.get_page(0)\nlayout_result = layout.extract(page)\n\nfor box in layout_result.bboxes:\n    region = page.crop(box.bbox)\n\n    if box.label == \"text\":\n        result = text.extract(region, output_format=\"markdown\")\n    elif box.label == \"table\":\n        result = table.extract(region)\n\n    print(f\"{box.label}: {result}\")\n</code></pre>"},{"location":"developer-guide/#complete-examples","title":"Complete Examples","text":""},{"location":"developer-guide/#example-1-sanskrit-document-processing","title":"Example 1: Sanskrit Document Processing","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenAPIConfig\n\n# Load document\ndoc = Document.from_pdf(\n    \"Mayavada_khandanam.pdf\",\n    dpi=150,\n    page_range=(0, 4)\n)\n\n# Setup extractors\nlayout = DocLayoutYOLO(\n    config=DocLayoutYOLOConfig(device=\"cuda\", confidence=0.25)\n)\n\ntext_extractor = QwenTextExtractor(\n    backend=QwenAPIConfig(\n        model=\"qwen2-vl-72b\",\n        api_key=\"YOUR_API_KEY\",\n        rate_limit=10,\n    )\n)\n\n# Process each page\nall_results = {}\n\nfor page_num in range(doc.page_count):\n    page = doc.get_page(page_num)\n\n    # Detect layout\n    layout_result = layout.extract(page)\n\n    # Extract text from text regions\n    page_results = []\n    for box in layout_result.bboxes:\n        if box.label == \"text\":\n            region = page.crop(box.bbox)\n            text_result = text_extractor.extract(\n                region,\n                output_format=\"markdown\",\n                custom_prompt=\"Extract Sanskrit/Hindi text accurately.\",\n            )\n            page_results.append({\n                \"bbox\": box.bbox,\n                \"text\": text_result.content,\n            })\n\n    all_results[f\"page_{page_num}\"] = page_results\n</code></pre>"},{"location":"developer-guide/#example-2-high-throughput-with-vllm","title":"Example 2: High-Throughput with VLLM","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenVLLMConfig\n\n# VLLM for batch processing\nextractor = QwenTextExtractor(\n    backend=QwenVLLMConfig(\n        model=\"Qwen/Qwen2-VL-7B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n        max_model_len=8192,\n    )\n)\n\n# Process many documents efficiently\ndocuments = [\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"]\n\nfor doc_path in documents:\n    doc = Document.from_pdf(doc_path)\n\n    for i in range(doc.page_count):\n        result = extractor.extract(\n            doc.get_page(i),\n            output_format=\"markdown\",\n        )\n        # Save result...\n</code></pre>"},{"location":"developer-guide/#example-3-apple-silicon-with-mlx","title":"Example 3: Apple Silicon with MLX","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenMLXConfig\n\n# MLX for Apple Silicon\nextractor = QwenTextExtractor(\n    backend=QwenMLXConfig(\n        model=\"Qwen/Qwen2-VL-7B-Instruct-MLX\",\n        quantization=\"4bit\",\n    )\n)\n\ndoc = Document.from_pdf(\"document.pdf\")\n\nresult = extractor.extract(\n    doc.get_page(0),\n    output_format=\"markdown\",\n)\n</code></pre>"},{"location":"developer-guide/#example-4-structured-output-extraction","title":"Example 4: Structured Output Extraction","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.structured_output_extraction import VLMStructuredExtractor, VLMStructuredConfig\nfrom pydantic import BaseModel\nfrom typing import List\n\n# Define schema\nclass Invoice(BaseModel):\n    vendor: str\n    invoice_number: str\n    date: str\n    total_amount: float\n    line_items: List[dict]\n\n# Setup extractor\nextractor = VLMStructuredExtractor(\n    config=VLMStructuredConfig(\n        model=\"gpt-4o\",\n        api_key=\"YOUR_API_KEY\",\n    )\n)\n\ndoc = Document.from_pdf(\"invoice.pdf\")\n\n# Extract with schema\nresult = extractor.extract(\n    doc.get_page(0),\n    output_model=Invoice,\n)\n\n# Typed, validated output\nprint(f\"Vendor: {result.data.vendor}\")\nprint(f\"Total: ${result.data.total_amount}\")\n</code></pre>"},{"location":"developer-guide/#import-reference","title":"Import Reference","text":""},{"location":"developer-guide/#complete-import-guide","title":"Complete Import Guide","text":"<pre><code># \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Document Loading\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfrom omnidocs import Document\n\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Layout Analysis\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfrom omnidocs.tasks.layout_analysis import (\n    # Single-backend models (config included)\n    DocLayoutYOLO, DocLayoutYOLOConfig,\n    RTDETRLayoutDetector, RTDETRConfig,\n    SuryaLayoutDetector, SuryaLayoutConfig,\n\n    # Multi-backend model\n    QwenLayoutDetector,\n\n    # API-only\n    VLMLayoutDetector, VLMLayoutConfig,\n\n    # Custom label support\n    CustomLabel,\n)\n\n# Qwen layout backend configs\nfrom omnidocs.tasks.layout_analysis.qwen import (\n    QwenPyTorchConfig,\n    QwenVLLMConfig,\n    QwenMLXConfig,\n    QwenAPIConfig,\n)\n\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# OCR Extraction (text + bboxes)\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfrom omnidocs.tasks.ocr_extraction import (\n    # Single-backend models\n    TesseractOCR, TesseractConfig,\n    PaddleOCR, PaddleOCRConfig,\n    EasyOCR, EasyOCRConfig,\n    SuryaOCR, SuryaOCRConfig,\n\n    # Multi-backend model\n    QwenOCR,\n\n    # API-only\n    VLMOCRExtractor, VLMOCRConfig,\n)\n\n# Qwen OCR backend configs\nfrom omnidocs.tasks.ocr_extraction.qwen import (\n    QwenPyTorchConfig,\n    QwenVLLMConfig,\n    QwenMLXConfig,\n    QwenAPIConfig,\n)\n\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Text Extraction (MD/HTML)\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfrom omnidocs.tasks.text_extraction import (\n    # Multi-backend models\n    QwenTextExtractor,\n    DotsOCRTextExtractor,\n    ChandraTextExtractor,\n    GemmaTextExtractor,\n    GraniteDoclingOCR,\n    HunyuanTextExtractor,\n    LightOnOCRExtractor,\n    MinerUOCRExtractor,\n    NanonutsOCRExtractor,\n    OlmOCRExtractor,\n    PaddleTextExtractor,\n\n    # API-only\n    VLMTextExtractor, VLMTextConfig,\n)\n\n# Qwen text extraction backend configs\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenPyTorchConfig,\n    QwenVLLMConfig,\n    QwenMLXConfig,\n    QwenAPIConfig,\n)\n\n# DotsOCR backend configs (no API)\nfrom omnidocs.tasks.text_extraction.dotsocr import (\n    DotsOCRPyTorchConfig,\n    DotsOCRVLLMConfig,\n    DotsOCRMLXConfig,\n)\n\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Table Extraction\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfrom omnidocs.tasks.table_extraction import (\n    TableTransformer, TableTransformerConfig,\n    SuryaTable, SuryaTableConfig,\n    QwenTableExtractor,\n    VLMTableExtractor, VLMTableConfig,\n)\n\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Math Expression Extraction\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfrom omnidocs.tasks.math_expression_extraction import (\n    UniMERNet, UniMERNetConfig,\n    QwenMathExtractor,\n    VLMMathExtractor, VLMMathConfig,\n)\n\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Structured Output Extraction\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfrom omnidocs.tasks.structured_output_extraction import (\n    VLMStructuredExtractor, VLMStructuredConfig,\n)\n\n\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# Workflows (Optional)\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\nfrom omnidocs.workflows import DocumentWorkflow\n</code></pre>"},{"location":"developer-guide/#implementation-roadmap","title":"Implementation Roadmap","text":""},{"location":"developer-guide/#phase-1-core-infrastructure","title":"Phase 1: Core Infrastructure","text":"<p>Goals: Base classes and config system</p> <ul> <li>[ ] Base extractor classes with <code>.extract()</code> method</li> <li>[ ] Pydantic config classes pattern</li> <li>[ ] Pydantic output models (LayoutOutput, OCROutput, TextOutput)</li> <li>[ ] Document class (stateless)</li> </ul> <p>Deliverables: - <code>omnidocs/document.py</code> - <code>omnidocs/tasks/*/base.py</code> - <code>omnidocs/tasks/*/models.py</code></p>"},{"location":"developer-guide/#phase-2-single-backend-models","title":"Phase 2: Single-Backend Models","text":"<p>Goals: Implement models with single backend</p> <ul> <li>[ ] DocLayoutYOLO + DocLayoutYOLOConfig</li> <li>[ ] SuryaOCR + SuryaOCRConfig</li> <li>[ ] UniMERNet + UniMERNetConfig</li> <li>[ ] TableTransformer + TableTransformerConfig</li> </ul>"},{"location":"developer-guide/#phase-3-multi-backend-models","title":"Phase 3: Multi-Backend Models","text":"<p>Goals: Implement models with multiple backends</p> <ul> <li>[ ] QwenTextExtractor + all backend configs</li> <li>[ ] DotsOCRTextExtractor + backend configs</li> <li>[ ] Backend-specific inference utilities</li> </ul>"},{"location":"developer-guide/#phase-4-api-only-models","title":"Phase 4: API-Only Models","text":"<p>Goals: Generic VLM wrappers</p> <ul> <li>[ ] VLMTextExtractor for Gemini, GPT-4, Claude</li> <li>[ ] VLMStructuredExtractor with schema support</li> <li>[ ] LiteLLM integration</li> </ul>"},{"location":"developer-guide/#phase-5-testing-documentation","title":"Phase 5: Testing &amp; Documentation","text":"<p>Goals: Comprehensive testing and docs</p> <ul> <li>[ ] Unit tests for all extractors</li> <li>[ ] Integration tests</li> <li>[ ] API documentation</li> <li>[ ] Tutorial notebooks</li> </ul>"},{"location":"developer-guide/#summary","title":"Summary","text":""},{"location":"developer-guide/#key-design-decisions","title":"\u2705 Key Design Decisions","text":"Decision Choice Rationale Import Pattern Class-based Direct, explicit, type-safe Method Name <code>.extract()</code> for all Consistent, predictable Config Style Model-specific IDE autocomplete, clear discoverability Init vs Extract Config at init, task params at extract Clear separation Document Design Stateless Separation of concerns Backend Discovery Config classes exist = supported Obvious, no guessing"},{"location":"developer-guide/#config-parameter-naming","title":"Config Parameter Naming","text":"Model Type Parameter Example Single-backend <code>config=</code> <code>DocLayoutYOLO(config=...)</code> Multi-backend <code>backend=</code> <code>QwenTextExtractor(backend=...)</code> API-only <code>config=</code> <code>VLMTextExtractor(config=...)</code> <p>Last Updated: January 20, 2026 Status: \u2705 Design Complete - Ready for Implementation Maintainer: Adithya S Kolavi</p>"}]}