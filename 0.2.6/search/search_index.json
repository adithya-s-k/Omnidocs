{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Index","text":"<p> Unified Python toolkit for visual document processing </p>"},{"location":"#install","title":"Install","text":"<pre><code>pip install omnidocs[pytorch]\n</code></pre>"},{"location":"#extract-text-in-4-lines","title":"Extract Text in 4 Lines","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n\ndoc = Document.from_pdf(\"document.pdf\")\nextractor = QwenTextExtractor(backend=QwenPyTorchConfig(device=\"cuda\"))\nresult = extractor.extract(doc.get_page(0), output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"#what-omnidocs-does","title":"What OmniDocs Does","text":"Task What You Get Example Models Text Extraction Markdown/HTML from documents Qwen3-VL, DotsOCR, Nanonets OCR2 Layout Analysis Bounding boxes for titles, tables, figures DocLayoutYOLO, RT-DETR, Qwen Layout OCR Text + coordinates Tesseract, EasyOCR, PaddleOCR Table Extraction Structured table data (rows, columns, cells) TableFormer Reading Order Logical reading sequence Rule-based R-tree"},{"location":"#core-design","title":"Core Design","text":"<pre><code>Image \u2192 Extractor.extract() \u2192 Pydantic Output\n</code></pre> <ul> <li>One API: <code>.extract()</code> for every task</li> <li>Type-Safe: Pydantic configs with IDE autocomplete</li> <li>Multi-Backend: PyTorch, VLLM, MLX, API</li> <li>Stateless: Document loads data, you manage results</li> </ul>"},{"location":"#choose-your-backend","title":"Choose Your Backend","text":"Backend Install Best For PyTorch <code>pip install omnidocs[pytorch]</code> Development, single GPU VLLM <code>pip install omnidocs[vllm]</code> Production, high throughput MLX <code>pip install omnidocs[mlx]</code> Apple Silicon (M1/M2/M3) API <code>pip install omnidocs[api]</code> No GPU, cloud-based"},{"location":"#whats-available","title":"What's Available","text":"Model Task PyTorch VLLM MLX API Qwen3-VL Text, Layout \u2705 \u2705 \u2705 \u2705 DotsOCR Text \u2705 \u2705 -- \u2705 Nanonets OCR2 Text \u2705 \u2705 \u2705 -- DocLayoutYOLO Layout \u2705 -- -- -- RT-DETR Layout \u2705 -- -- -- TableFormer Table \u2705 -- -- -- Tesseract OCR \u2705 -- -- -- EasyOCR OCR \u2705 -- -- -- PaddleOCR OCR \u2705 -- -- -- Rule-based Reading Order \u2705 -- -- --"},{"location":"#coming-soon","title":"Coming Soon","text":"Model Task Status Granite Docling Text \ud83d\udd1c Scripts ready MinerU VL Text \ud83d\udd1c Scripts ready Surya OCR, Layout \ud83d\udd1c Planned <p>See Roadmap for full tracking.</p>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li> <p>Getting Started</p> <p>Install, configure, and run your first extraction</p> </li> <li> <p>Concepts</p> <p>Architecture, configs, backends, and design decisions</p> </li> <li> <p>Usage</p> <p>Tasks, models, batch processing, and deployment</p> </li> </ul>"},{"location":"#quick-reference","title":"Quick Reference","text":""},{"location":"#single-backend-model-eg-doclayoutyolo","title":"Single-Backend Model (e.g., DocLayoutYOLO)","text":"<pre><code>from omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\n\nlayout = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = layout.extract(image)\n</code></pre>"},{"location":"#multi-backend-model-eg-qwen","title":"Multi-Backend Model (e.g., Qwen)","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig  # or VLLMConfig, MLXConfig, APIConfig\n\nextractor = QwenTextExtractor(backend=QwenPyTorchConfig(device=\"cuda\"))\nresult = extractor.extract(image, output_format=\"markdown\")\n</code></pre>"},{"location":"#links","title":"Links","text":"<ul> <li>GitHub</li> <li>Issues</li> <li>Contributing</li> </ul> Get Started"},{"location":"LICENSE/","title":"LICENSE","text":"<p>Apache License Version 2.0, January 2004 http://www.apache.org/licenses/</p>"},{"location":"LICENSE/#terms-and-conditions-for-use-reproduction-and-distribution","title":"TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION","text":""},{"location":"LICENSE/#1-definitions","title":"1. Definitions","text":"<p>\"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.</p>"},{"location":"LICENSE/#2-grant-of-copyright-license","title":"2. Grant of Copyright License","text":"<p>Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.</p>"},{"location":"LICENSE/#3-grant-of-patent-license","title":"3. Grant of Patent License","text":"<p>Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.</p>"},{"location":"LICENSE/#4-redistribution","title":"4. Redistribution","text":"<p>You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:</p> <ol> <li>You must give any other recipients of the Work or Derivative Works a copy of this License; and</li> <li>You must cause any modified files to carry prominent notices stating that You changed the files; and</li> <li>You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and</li> <li>If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.</li> </ol> <p>You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.</p>"},{"location":"LICENSE/#5-submission-of-contributions","title":"5. Submission of Contributions","text":"<p>Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.</p>"},{"location":"LICENSE/#6-trademarks","title":"6. Trademarks","text":"<p>This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.</p>"},{"location":"LICENSE/#7-disclaimer-of-warranty","title":"7. Disclaimer of Warranty","text":"<p>Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.</p>"},{"location":"LICENSE/#8-limitation-of-liability","title":"8. Limitation of Liability","text":"<p>In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.</p>"},{"location":"LICENSE/#9-accepting-warranty-or-additional-liability","title":"9. Accepting Warranty or Additional Liability","text":"<p>While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.</p>"},{"location":"LICENSE/#end-of-terms-and-conditions","title":"END OF TERMS AND CONDITIONS","text":"<p>Copyright 2025 OmniDocs Contributors</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"ROADMAP/","title":"Roadmap","text":"<p>Status: \u2705 Done | \ud83d\udea7 WIP | \ud83d\udd1c Soon | \u274c N/A</p>"},{"location":"ROADMAP/#text-extraction-models","title":"Text Extraction Models","text":"Model Params PyTorch VLLM MLX API Qwen3-VL 2-32B \u2705 \u2705 \u2705 \u2705 DotsOCR 1.7B \u2705 \u2705 \u274c \u2705 Nanonets OCR2 3B \u2705 \u2705 \u2705 \u274c Granite-Docling 258M \ud83d\udd1c \ud83d\udd1c \u274c \u274c MinerU VL 1.2B \ud83d\udd1c \ud83d\udd1c \ud83d\udd1c \u274c LightOnOCR-2 1B \ud83d\udd1c \ud83d\udd1c \u274c \u274c Chandra 9B \ud83d\udd1c \ud83d\udd1c \u274c \u274c olmOCR-2 7B \ud83d\udd1c \ud83d\udd1c \u274c \ud83d\udd1c DeepSeek-OCR-2 3B \ud83d\udd1c \ud83d\udd1c \ud83d\udd1c \ud83d\udd1c"},{"location":"ROADMAP/#layout-analysis-models","title":"Layout Analysis Models","text":"Model Params PyTorch VLLM MLX API DocLayoutYOLO - \u2705 \u274c \u274c \u274c RT-DETR - \u2705 \u274c \u274c \u274c Qwen Layout 2-32B \u2705 \u2705 \u2705 \u2705 Surya Layout - \ud83d\udd1c \u274c \u274c \u274c Florence-2 - \ud83d\udd1c \u274c \u274c \ud83d\udd1c"},{"location":"ROADMAP/#ocr-models","title":"OCR Models","text":"Model Params PyTorch VLLM MLX API Tesseract - \u2705 \u274c \u274c \u274c EasyOCR - \u2705 \u274c \u274c \u274c PaddleOCR - \u2705 \u274c \u274c \u274c Surya OCR - \ud83d\udd1c \u274c \u274c \u274c GOT-OCR2 700M \ud83d\udd1c \u274c \u274c \u274c"},{"location":"ROADMAP/#table-extraction-models","title":"Table Extraction Models","text":"Model Params PyTorch VLLM MLX API TableFormer - \u2705 \u274c \u274c \u274c TableTransformer - \ud83d\udd1c \u274c \u274c \u274c Surya-Table - \ud83d\udd1c \u274c \u274c \u274c"},{"location":"ROADMAP/#reading-order-models","title":"Reading Order Models","text":"Model Params PyTorch VLLM MLX API Rule-based (R-tree) - \u2705 \u274c \u274c \u274c"},{"location":"ROADMAP/#tasks","title":"Tasks","text":"Task Status Document Loading \u2705 Text Extraction \u2705 Layout Analysis \u2705 OCR Extraction \u2705 Table Extraction \u2705 Reading Order \u2705 Math Recognition \ud83d\udd1c Structured Output \ud83d\udd1c Chart Understanding \ud83d\udd1c"},{"location":"ROADMAP/#infrastructure","title":"Infrastructure","text":"Component Status Document class \u2705 Pydantic configs \u2705 Multi-backend \u2705 Batch processing \u2705 Modal deployment \u2705 Testing framework \u2705"},{"location":"ROADMAP/#scripts-ready-pending-integration","title":"Scripts Ready (Pending Integration)","text":"<p>These models have working scripts but aren't yet integrated into OmniDocs:</p> Model Task Scripts Granite Docling Text VLLM, HF, MLX MinerU VL Text VLLM, MLX"},{"location":"ROADMAP/#under-consideration","title":"Under Consideration","text":"<p>Models being evaluated for future integration:</p>"},{"location":"ROADMAP/#high-priority","title":"High Priority","text":"Model Use Case Why Marker Full pipeline Uses Surya, good tables Granite Vision 3.3 Document understanding IBM, good charts Surya OCR + Layout Multi-language, modern"},{"location":"ROADMAP/#specialized-use-cases","title":"Specialized Use Cases","text":"Use Case Models Under Review Handwriting TrOCR, Surya Scientific Papers Nougat, Marker Asian Languages PaddleOCR-VL (109 langs) Edge/Mobile Granite-Docling (258M) Forms &amp; Receipts DeepSeek-OCR-2"},{"location":"ROADMAP/#framework-integration","title":"Framework Integration","text":"Framework Status deepdoctection \ud83d\udd1c Docling \ud83d\udd1c Marker \ud83d\udd1c"},{"location":"ROADMAP/#benchmarks-reference","title":"Benchmarks Reference","text":""},{"location":"ROADMAP/#olmocr-bench-higher-better","title":"OlmOCR-Bench (Higher = Better)","text":"Model Score Params LightOnOCR-2 83.2 1B Chandra 83.1 9B olmOCR-2 82.4 7B DotsOCR 79.1 1.7B Nanonets OCR2 ~78 3B DeepSeek-OCR 75.4 3B"},{"location":"ROADMAP/#speed-pagessecond-on-h100","title":"Speed (Pages/Second on H100)","text":"Model Speed LightOnOCR-2 5.7 PaddleOCR-VL 3.3 DeepSeek-OCR 2.3"},{"location":"ROADMAP/#next-up","title":"Next Up","text":"<ol> <li>\ud83d\udd1c Granite Docling - Integration from scripts</li> <li>\ud83d\udd1c MinerU VL - Integration from scripts</li> <li>\ud83d\udd1c Math Recognition - UniMERNet or Qwen</li> <li>\ud83d\udd1c Surya - Multi-language OCR + Layout</li> </ol> <p>For detailed model specs, see ROADMAP_DETAILED.md.</p> <p>Last Updated: February 2026</p>"},{"location":"ROADMAP_DETAILED/","title":"OmniDocs Development Roadmap","text":""},{"location":"ROADMAP_DETAILED/#target-model-support","title":"\ud83d\udce6 Target Model Support","text":"<p>Research Date: February 2026 Status: Comprehensive model research completed Models Ordered By: Release date (newest first within each size category)</p>"},{"location":"ROADMAP_DETAILED/#quick-reference-model-capabilities-backend-support","title":"\ud83c\udfaf Quick Reference: Model Capabilities &amp; Backend Support","text":""},{"location":"ROADMAP_DETAILED/#comprehensive-model-comparison-table","title":"Comprehensive Model Comparison Table","text":"Model Size PyTorch VLLM MLX OpenAI API Tasks Release DeepSeek-OCR-2 3B \u2705 \u2705 \u2705 \u2705 T, O, Tab, F Jan 2026 LightOnOCR-2-1B 1B \u2705 \u2705 \u274c \u274c T, O Jan 2026 LightOnOCR-2-1B-bbox 1B \u2705 \u2705 \u274c \u274c T, L, O Jan 2026 OCRFlux-3B 3B \u2705 \u2705 \u274c \u274c T, O Jan 2026 Qwen3-VL-2B 2B \u2705 \u2705 \u2705 \u2705 T, L, S, O, Tab Oct 2025 Qwen3-VL-4B 4B \u2705 \u2705 \u2705 \u274c T, L, S, O, Tab Oct 2025 Qwen3-VL-8B 8B \u2705 \u2705 \u2705 \u2705 T, L, S, O, Tab, F Oct 2025 Qwen3-VL-32B 32B \u2705 \u2705 \u2705 \u2705 T, L, S, O, Tab, F Oct 2025 olmOCR-2-7B 7B \u2705 \u2705 \u274c \u2705 T, O, Tab, F Oct 2025 PaddleOCR-VL 900M \u2705 \u26a0\ufe0f \u274c \u274c T, L, O, Tab, F Oct 2025 LightOnOCR-1B 1B \u2705 \u274c \u274c \u274c T, O Oct 2025 Granite-Vision-3.3-2B 2B \u2705 \u274c \u274c \u274c T, L, Tab, Chart Jun 2025 Gemma-3-4B-IT 4B \u2705 \u274c \u274c \u2705 T, S, O 2025 Granite-Docling-258M 258M \u2705 \u26a0\ufe0f \u2705 \u274c T, L, Tab, F Dec 2024 dots.ocr 1.7B \u2705 \u2705 \u274c \u274c T, L, Tab, F, O Dec 2024 DeepSeek-OCR 3B \u2705 \u2705 \u2705 \u2705 T, O, Tab Oct 2024 Chandra 9B \u2705 \u2705 \u274c \u274c T, L, O, Tab, F 2024 MinerU2.5-2509-1.2B 1.2B \u2705 \u2705 \u2705 \u274c T, L, Tab, F, O Sep 2024 GOT-OCR2.0 700M \u2705 \u274c \u274c \u274c T, O, F, Tab Sep 2024 Nanonets-OCR2-3B 3B \u2705 \u2705 \u2705 \u274c T, F, O 2024 Qwen2.5-VL-3B 3B \u2705 \u2705 \u2705 \u2705 T, L, S, O 2024 Qwen2.5-VL-7B 7B \u2705 \u2705 \u2705 \u2705 T, L, S, O, Tab 2024 Qwen2.5-VL-32B 32B \u2705 \u2705 \u2705 \u2705 T, L, S, O, Tab 2024 <p>Legend: - Tasks: T=Text Extract, L=Layout, O=OCR, S=Structured, Tab=Table, F=Formula, Chart=Chart Understanding - \u2705 = Fully supported | \u26a0\ufe0f = Limited/Partial support | \u274c = Not supported</p>"},{"location":"ROADMAP_DETAILED/#backend-details","title":"Backend Details","text":""},{"location":"ROADMAP_DETAILED/#pytorch-support","title":"PyTorch Support","text":"<ul> <li>All models support PyTorch via HuggingFace Transformers</li> <li>Primary development backend for all models</li> <li>Requirements: <code>transformers&gt;=4.46</code>, <code>torch&gt;=2.0</code></li> </ul>"},{"location":"ROADMAP_DETAILED/#vllm-support-high-throughput-production","title":"VLLM Support (High-Throughput Production)","text":"<p>Fully Supported (\u2705): - Qwen3-VL Series (vllm&gt;=0.11.0) - Qwen2.5-VL Series - DeepSeek-OCR (official upstream) - dots.ocr (recommended, vllm&gt;=0.9.1) - MinerU2.5 - olmOCR-2 (via olmOCR toolkit) - Chandra - LightOnOCR-2-1B (vllm&gt;=0.11.1) - Nanonets-OCR2-3B</p> <p>Limited Support (\u26a0\ufe0f): - Granite-Docling-258M (untied weights required) - PaddleOCR-VL (possible but not officially confirmed)</p> <p>Not Supported (\u274c): - GOT-OCR2.0 - Gemma-3-4B-IT - LightOnOCR-1B (legacy)</p>"},{"location":"ROADMAP_DETAILED/#mlx-support-apple-silicon-m1m2m3","title":"MLX Support (Apple Silicon M1/M2/M3+)","text":"<p>Fully Supported via mlx-community (\u2705): - Qwen3-VL Series - Collection   - 2B, 4B, 8B, 32B (4-bit, 8-bit variants) - Qwen2.5-VL Series - Collection   - 3B, 7B, 32B, 72B (4-bit, 8-bit variants) - DeepSeek-OCR - 4-bit, 8-bit - Granite-Docling-258M - Official MLX - MinerU2.5 - bf16 - Nanonets-OCR2-3B - 4-bit</p> <p>Usage: <pre><code>pip install mlx-vlm\npython -m mlx_vlm.generate --model mlx-community/Qwen3-VL-8B-Instruct-4bit \\\n  --prompt \"Extract text from this document\" --image doc.png\n</code></pre></p>"},{"location":"ROADMAP_DETAILED/#openai-compatible-api-providers","title":"OpenAI-Compatible API Providers","text":"<p>OpenRouter (openrouter.ai): - \u2705 Qwen3-VL-235B-A22B ($0.45/$3.50 per M tokens) - \u2705 Qwen3-VL-30B-A3B - \u2705 Qwen2.5-VL-3B (SOTA visual understanding) - \u2705 Qwen2.5-VL-32B (structured outputs, math) - \u2705 Qwen2.5-VL-72B (best overall)</p> <p>Novita AI (novita.ai): - \u2705 DeepSeek-OCR (Model Page) - \u2705 Qwen2.5-VL-72B (OCR + scientific reasoning) - \u2705 Qwen3-VL-8B ($0.08/$0.50 per M tokens)</p> <p>Together AI (together.ai): - \u2705 Various vision-language models - \u2705 Lightweight models with multilingual support</p> <p>Replicate (replicate.com): - \u2705 Vision models collection - \u2705 Pay-per-use inference</p> <p>Others: - DeepInfra: olmOCR-2-7B - Parasail: olmOCR-2-7B - Cirrascale: olmOCR-2-7B</p> <p>API Integration Example: <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenAPIConfig\n\n# OpenRouter\nextractor = QwenTextExtractor(\n    backend=QwenAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=\"YOUR_OPENROUTER_KEY\",\n        base_url=\"https://openrouter.ai/api/v1\"\n    )\n)\n\n# Novita AI\nextractor = QwenTextExtractor(\n    backend=QwenAPIConfig(\n        model=\"novita/qwen3-vl-8b-instruct\",\n        api_key=\"YOUR_NOVITA_KEY\",\n        base_url=\"https://api.novita.ai/v3/openai\"\n    )\n)\n</code></pre></p>"},{"location":"ROADMAP_DETAILED/#task-capability-matrix","title":"Task Capability Matrix","text":"Task Description Model Count Top Models Text Extract (T) Document \u2192 Markdown/HTML 18 LightOnOCR-2, Chandra, Qwen3-VL-8B Layout (L) Structure detection with bboxes 8 Qwen3-VL-8B, Chandra, MinerU2.5 OCR (O) Text + bbox coordinates 15 LightOnOCR-2, olmOCR-2, Chandra Structured (S) Schema-based extraction 5 Qwen3-VL (all), Qwen2.5-VL (all), Gemma-3 Table (Tab) Table detection/extraction 12 Qwen3-VL-8B, DeepSeek-OCR, olmOCR-2 Formula (F) Math expression recognition 8 Nanonets-OCR2, Qwen3-VL-8B, GOT-OCR2.0"},{"location":"ROADMAP_DETAILED/#model-overview-by-task-capability","title":"Model Overview by Task Capability","text":""},{"location":"ROADMAP_DETAILED/#task-categories","title":"Task Categories","text":"Task Description Model Count text_extract Document to Markdown/HTML conversion 18 layout Document structure detection with bounding boxes 8 ocr Text extraction with bbox coordinates 6 structured Schema-based data extraction 5 table Table detection and extraction 4 formula Mathematical expression recognition 3"},{"location":"ROADMAP_DETAILED/#latest-models-january-2026","title":"\ud83c\udd95 Latest Models (January 2026)","text":""},{"location":"ROADMAP_DETAILED/#deepseek-ocr-2","title":"DeepSeek-OCR-2","text":"<p>Released: January 27, 2026 | Parameters: 3B | License: MIT</p> <p>HuggingFace: deepseek-ai/DeepSeek-OCR-2</p> <p>Description: State-of-the-art 3B-parameter vision-language model with new DeepEncoder architecture. Unlike traditional OCR systems, DeepSeek OCR 2 focuses on image-to-text with stronger visual reasoning.</p> <p>Key Features: - DeepEncoder: 380M vision encoder (80M SAM-base + 300M CLIP-large) - 97% precision at 10\u00d7 visual token compression - ~60% accuracy at 20\u00d7 compression - Strong document understanding beyond text extraction</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM (high throughput) - \u2705 MLX (4-bit, 8-bit) - \u2705 API (Novita AI, OpenRouter)</p> <p>Tasks: <code>text_extract</code>, <code>ocr</code>, <code>table</code>, <code>formula</code></p> <p>Links: - Model Card - GitHub</p>"},{"location":"ROADMAP_DETAILED/#lightonocr-2-1b","title":"LightOnOCR-2-1B","text":"<p>Released: January 19, 2026 | Parameters: 1B | License: Apache 2.0</p> <p>HuggingFace: lightonai/LightOnOCR-2-1B</p> <p>Description: Second-generation 1B-parameter end-to-end vision-language OCR model. SOTA conversion of PDF renders to clean text without multi-stage pipelines.</p> <p>Key Features: - 83.2 on OlmOCR-Bench (SOTA, beats 9B Chandra) - 5.7 pages/second on H100 (~493K pages/day) - &lt;$0.01 per 1,000 pages at cloud pricing - Bbox variant for figure/image localization</p> <p>Model Variants: - <code>LightOnOCR-2-1B</code> - Default OCR - <code>LightOnOCR-2-1B-bbox</code> - Best localization - <code>LightOnOCR-2-1B-bbox-soup</code> - Balanced</p> <p>Backends Supported: - \u2705 PyTorch (transformers from source) - \u2705 VLLM (vllm&gt;=0.11.1)</p> <p>Tasks: <code>text_extract</code>, <code>ocr</code>, <code>layout</code> (bbox variants)</p> <p>Links: - Blog Post - Demo</p>"},{"location":"ROADMAP_DETAILED/#ocrflux-3b","title":"OCRFlux-3B","text":"<p>Released: January 2026 | Parameters: 3B | License: Apache 2.0</p> <p>HuggingFace: Fine-tuned from Qwen2.5-VL-3B-Instruct</p> <p>Description: Multimodal LLM for converting PDFs and images to clean Markdown. Runs efficiently on consumer hardware (GTX 3090).</p> <p>Key Features: - Compact 3B architecture - Clean Markdown output - Consumer GPU compatible - Based on Qwen2.5-VL</p> <p>Backends Supported: - \u2705 PyTorch - \u2705 VLLM</p> <p>Tasks: <code>text_extract</code>, <code>ocr</code></p>"},{"location":"ROADMAP_DETAILED/#core-models-by-size-release-date","title":"\ud83c\udfaf Core Models (By Size &amp; Release Date)","text":""},{"location":"ROADMAP_DETAILED/#ultra-compact-models-1b-parameters","title":"Ultra-Compact Models (&lt;1B Parameters)","text":""},{"location":"ROADMAP_DETAILED/#ibm-granite-docling-258m","title":"IBM Granite-Docling-258M","text":"<p>Released: December 2024 | Parameters: 258M | License: Apache 2.0</p> <p>HuggingFace: ibm-granite/granite-docling-258M</p> <p>Description: Ultra-compact vision-language model (VLM) for converting documents to machine-readable formats while fully preserving layout, tables, equations, and lists. Built on Idefics3 architecture with siglip2-base-patch16-512 vision encoder and Granite 165M LLM.</p> <p>Key Features: - End-to-end document understanding at 258M parameters - Handles inline/floating math, code, table structure - Rivals systems several times its size - Extremely cost-effective</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 MLX (Apple Silicon) - ibm-granite/granite-docling-258M-mlx - \u2705 WebGPU - Demo Space</p> <p>Integration: <pre><code>pip install docling  # Automatically downloads model\n</code></pre></p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>pillow</code>, <code>docling</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>table</code>, <code>formula</code></p> <p>Links: - Model Card - MLX Version - Demo Space - Official Docs - Collection</p>"},{"location":"ROADMAP_DETAILED/#2-stepfun-ai-got-ocr20","title":"2. stepfun-ai GOT-OCR2.0","text":"<p>Released: September 2024 | Parameters: 700M | License: Apache 2.0</p> <p>HuggingFace: stepfun-ai/GOT-OCR2_0</p> <p>Description: General OCR Theory model for multilingual OCR on plain documents, scene text, formatted documents, tables, charts, mathematical formulas, geometric shapes, molecular formulas, and sheet music.</p> <p>Key Features: - Interactive OCR with region-specific recognition (coordinate or color-based) - Plain text OCR + formatted text OCR (markdown, LaTeX) - Multi-page document processing - Wide range of specialized content types</p> <p>Model Variations: - stepfun-ai/GOT-OCR2_0 - Original with custom code - stepfun-ai/GOT-OCR-2.0-hf - HuggingFace-native transformers integration</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 Custom inference pipeline</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>pillow</code></p> <p>Tasks: <code>text_extract</code>, <code>ocr</code>, <code>formula</code>, <code>table</code></p> <p>Links: - Model Card - HF-Native Version</p>"},{"location":"ROADMAP_DETAILED/#compact-models-1-2b-parameters","title":"Compact Models (1-2B Parameters)","text":""},{"location":"ROADMAP_DETAILED/#3-rednote-hilab-dotsocr","title":"3. rednote-hilab dots.ocr","text":"<p>Released: December 2024 | Parameters: 1.7B | License: MIT</p> <p>HuggingFace: rednote-hilab/dots.ocr</p> <p>Description: Multilingual documents parsing model based on 1.7B LLM with SOTA performance. Provides faster inference than many high-performing models based on larger foundations.</p> <p>Key Features: - Task switching via prompt alteration only - Competitive detection vs traditional models (DocLayout-YOLO) - Built-in VLLM support for high throughput - Released with paper arXiv:2512.02498</p> <p>Model Variations: - rednote-hilab/dots.ocr - Full model - rednote-hilab/dots.ocr.base - Base variant</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM (Recommended for production) - vLLM 0.9.1+</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>vllm&gt;=0.9.1</code> (recommended)</p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>table</code>, <code>formula</code>, <code>ocr</code></p> <p>Links: - Model Card - GitHub - Live Demo - Paper - Collection</p>"},{"location":"ROADMAP_DETAILED/#4-paddlepaddle-paddleocr-vl","title":"4. PaddlePaddle PaddleOCR-VL","text":"<p>Released: October 2025 | Parameters: 900M | License: Apache 2.0</p> <p>HuggingFace: PaddlePaddle/PaddleOCR-VL</p> <p>Description: Ultra-compact multilingual documents parsing VLM with SOTA performance. Integrates NaViT-style dynamic resolution visual encoder with ERNIE-4.5-0.3B language model.</p> <p>Key Features: - Supports 109 languages - Excels in recognizing complex elements (text, tables, formulas, charts) - Minimal resource consumption - Fast inference speeds - SOTA in page-level parsing and element-level recognition</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers - officially integrated) - \u2705 PaddlePaddle framework</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>paddlepaddle</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>ocr</code>, <code>table</code>, <code>formula</code></p> <p>Links: - Model Card - Online Demo - Collection - Transformers Docs - GitHub - PaddleOCR</p>"},{"location":"ROADMAP_DETAILED/#5-lighton-ai-lightonocr-series","title":"5. LightOn AI LightOnOCR Series","text":"<p>Released: January 2025 (v2), October 2025 (v1) | Parameters: 1B | License: Apache 2.0</p> <p>HuggingFace Models: - lightonai/LightOnOCR-2-1B - Recommended for OCR - lightonai/LightOnOCR-2-1B-bbox - Best localization - lightonai/LightOnOCR-2-1B-bbox-soup - Balanced OCR + bbox - lightonai/LightOnOCR-1B-1025 - Legacy v1</p> <p>Description: Compact, end-to-end vision-language model for OCR and document understanding. State-of-the-art accuracy in its weight class while being several times faster than larger VLMs.</p> <p>Key Features: - LightOnOCR-2-1B: SOTA on OlmOCR-Bench (83.2 \u00b1 0.9), outperforms Chandra-9B - Performance: 3.3\u00d7 faster than Chandra, 1.7\u00d7 faster than OlmOCR, 5\u00d7 faster than dots.ocr - Variants: OCR-only, bbox-capable (figure/image localization), and balanced checkpoints - Paper: arXiv:2601.14251</p> <p>Model Comparison: | Model | Use Case | Bbox Support | |-------|----------|--------------| | LightOnOCR-2-1B | Default for PDF\u2192Text/Markdown | \u274c | | LightOnOCR-2-1B-bbox | Best localization of figures/images | \u2705 Best | | LightOnOCR-2-1B-bbox-soup | Balanced OCR + localization | \u2705 Balanced |</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers - upstream support) - \u26a0\ufe0f Requires transformers from source for v2 (not yet in stable release)</p> <p>Quantized Versions: - GGUF format</p> <p>Dependencies: <code>transformers&gt;=4.48</code> (from source for v2), <code>torch</code>, <code>pillow</code></p> <p>Tasks: <code>text_extract</code>, <code>ocr</code>, <code>layout</code> (bbox variants only)</p> <p>Links: - LightOnOCR-2 Blog - LightOnOCR-1 Blog - Demo Space - Paper (arXiv) - Organization</p>"},{"location":"ROADMAP_DETAILED/#6-opendatalab-mineru25","title":"6. opendatalab MinerU2.5","text":"<p>Released: September 2024 | Parameters: 1.2B | License: Apache 2.0</p> <p>HuggingFace: opendatalab/MinerU2.5-2509-1.2B</p> <p>Description: Decoupled vision-language model for efficient high-resolution document parsing with state-of-the-art accuracy and low computational overhead.</p> <p>Key Features: - Two-stage parsing: global layout analysis on downsampled images \u2192 fine-grained content recognition on native-resolution crops - Outperforms Gemini-2.5 Pro, Qwen2.5-VL-72B, GPT-4o, MonkeyOCR, dots.ocr, PP-StructureV3 - Large-scale diverse data engine for pretraining/fine-tuning - New performance records in text, formula, table recognition, and reading order</p> <p>Model Variations: - opendatalab/MinerU2.5-2509-1.2B - Official model - mlx-community/MinerU2.5-2509-1.2B-bf16 - MLX for Apple Silicon - Mungert/MinerU2.5-2509-1.2B-GGUF - GGUF quantized</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM (with OpenAI API specs) - \u2705 MLX (Apple Silicon)</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>vllm</code> (optional)</p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>table</code>, <code>formula</code>, <code>ocr</code></p> <p>Links: - Model Card - Paper (arXiv:2509.22186) - MLX Version - GGUF Version</p>"},{"location":"ROADMAP_DETAILED/#small-models-2-4b-parameters","title":"Small Models (2-4B Parameters)","text":""},{"location":"ROADMAP_DETAILED/#7-qwen3-vl-2b-instruct","title":"7. Qwen3-VL-2B-Instruct","text":"<p>Released: October 2025 | Parameters: 2B | License: Apache 2.0</p> <p>HuggingFace: Qwen/Qwen3-VL-2B-Instruct</p> <p>Description: Multimodal LLM from Alibaba Cloud's Qwen team with comprehensive upgrades: superior text understanding/generation, deeper visual perception/reasoning, extended context, and stronger agent interaction.</p> <p>Key Features: - Dense and MoE architectures that scale from edge to cloud - Instruct and reasoning-enhanced \"Thinking\" editions - Enhanced spatial and video dynamics comprehension - Part of Qwen3-VL multimodal retrieval framework (arXiv:2601.04720, 2026)</p> <p>Model Variations: - Qwen/Qwen3-VL-2B-Instruct - Instruction-tuned - Qwen/Qwen3-VL-2B-Thinking - Reasoning-enhanced - Qwen/Qwen3-VL-2B-Instruct-GGUF - Quantized GGUF</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM - \u2705 MLX (via mlx-community) - \u2705 API (via cloud providers)</p> <p>Dependencies: <code>transformers&gt;=4.46</code>, <code>torch</code>, <code>qwen-vl-utils</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>structured</code>, <code>ocr</code>, <code>table</code></p> <p>Links: - Model Card - GitHub - Collection - GGUF Version</p>"},{"location":"ROADMAP_DETAILED/#8-deepseek-ocr","title":"8. DeepSeek-OCR","text":"<p>Released: October 2024 | Parameters: ~3B | License: MIT</p> <p>HuggingFace: deepseek-ai/DeepSeek-OCR</p> <p>Description: High-accuracy OCR model from DeepSeek-AI for extracting text from complex visual inputs (documents, screenshots, receipts, natural scenes).</p> <p>Key Features: - Built for real-world documents: PDFs, forms, tables, handwritten/noisy text - Outputs clean, structured Markdown - VLLM support upstream - ~2500 tokens/s on A100 with vLLM - Paper: arXiv:2510.18234</p> <p>Model Variations: - deepseek-ai/DeepSeek-OCR - Official BF16 (~6.7 GB) - NexaAI/DeepSeek-OCR-GGUF - Quantized GGUF</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM (officially supported)</p> <p>Requirements: - Python 3.12.9 + CUDA 11.8 - <code>torch==2.6.0</code>, <code>transformers==4.46.3</code>, <code>flash-attn==2.7.3</code> - L4 / A100 GPUs (\u226516 GB VRAM)</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>vllm</code>, <code>flash-attn</code>, <code>einops</code></p> <p>Tasks: <code>text_extract</code>, <code>ocr</code>, <code>table</code></p> <p>Links: - Model Card - GitHub - GGUF Version - Demo Space</p>"},{"location":"ROADMAP_DETAILED/#9-nanonets-ocr2-3b","title":"9. Nanonets-OCR2-3B","text":"<p>Released: 2024 | Parameters: 3B | License: Apache 2.0</p> <p>HuggingFace: nanonets/Nanonets-OCR2-3B</p> <p>Description: State-of-the-art image-to-markdown OCR model that transforms documents into structured markdown with intelligent content recognition and semantic tagging, optimized for LLM downstream processing.</p> <p>Key Features: - LaTeX equation recognition (inline $...$ and display $$...$$) - Intelligent image description with structured tags (logos, charts, graphs) - 125K context window - ~7.53 GB model size</p> <p>Model Variations: - nanonets/Nanonets-OCR2-3B - Full BF16 - Mungert/Nanonets-OCR2-3B-GGUF - GGUF quantized - mlx-community/Nanonets-OCR2-3B-4bit - MLX 4-bit - yasserrmd/Nanonets-OCR2-3B - Ollama format</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 MLX (Apple Silicon) - \u2705 Ollama</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>pillow</code></p> <p>Tasks: <code>text_extract</code>, <code>formula</code>, <code>ocr</code></p> <p>Links: - Model Card - GGUF Version - MLX 4-bit - Ollama</p>"},{"location":"ROADMAP_DETAILED/#10-qwen3-vl-4b-instruct","title":"10. Qwen3-VL-4B-Instruct","text":"<p>Released: October 2025 | Parameters: 4B | License: Apache 2.0</p> <p>HuggingFace: Qwen/Qwen3-VL-4B-Instruct</p> <p>Description: Mid-size Qwen3-VL model with balanced performance and efficiency. Part of comprehensive multimodal model series with text understanding, visual reasoning, and agent capabilities.</p> <p>Model Variations: - Qwen/Qwen3-VL-4B-Instruct - Instruction-tuned - Qwen/Qwen3-VL-4B-Thinking - Reasoning-enhanced</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM - \u2705 MLX (via mlx-community) - \u2705 API (via cloud providers)</p> <p>Dependencies: <code>transformers&gt;=4.46</code>, <code>torch</code>, <code>qwen-vl-utils</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>structured</code>, <code>ocr</code>, <code>table</code></p> <p>Links: - Collection - GitHub</p>"},{"location":"ROADMAP_DETAILED/#11-google-gemma-3-4b-it","title":"11. Google Gemma-3-4B-IT","text":"<p>Released: 2025 | Parameters: 4B | License: Gemma License</p> <p>HuggingFace: google/gemma-3-4b-it</p> <p>Description: Lightweight, state-of-the-art multimodal model from Google built from same research/technology as Gemini. Handles text and image input, generates text output.</p> <p>Key Features: - 128K context window - Multilingual support (140+ languages) - SigLIP image encoder (896\u00d7896 square images) - Gemma-3-4B-IT beats Gemma-2-27B-IT on benchmarks</p> <p>Model Variations: - google/gemma-3-4b-it - Instruction-tuned (vision-capable) - google/gemma-3-4b-pt - Pre-trained base - google/gemma-3-4b-it-qat-q4_0-gguf - Quantized GGUF - bartowski/google_gemma-3-4b-it-GGUF - Community GGUF</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 Google AI SDK - \u2705 API (Google AI Studio)</p> <p>Dependencies: <code>transformers&gt;=4.46</code>, <code>torch</code>, <code>pillow</code></p> <p>Tasks: <code>text_extract</code>, <code>structured</code>, <code>ocr</code></p> <p>Links: - Model Card - Blog Post - Transformers Docs - Google Docs - DeepMind Page</p>"},{"location":"ROADMAP_DETAILED/#medium-models-7-9b-parameters","title":"Medium Models (7-9B Parameters)","text":""},{"location":"ROADMAP_DETAILED/#12-allenai-olmocr-2-7b-1025","title":"12. allenai olmOCR-2-7B-1025","text":"<p>Released: October 2025 | Parameters: 7B | License: Apache 2.0</p> <p>HuggingFace: allenai/olmOCR-2-7B-1025</p> <p>Description: State-of-the-art OCR for English-language digitized print documents. Fine-tuned from Qwen2.5-VL-7B-Instruct using olmOCR-mix-1025 dataset + GRPO RL training.</p> <p>Key Features: - 82.4 points on olmOCR-Bench (SOTA for real-world documents) - Substantial improvements where OCR often fails (math equations, tables, tricky cases) - Boosted via reinforcement learning (GRPO)</p> <p>Model Variations: - allenai/olmOCR-2-7B-1025 - Full BF16 version - allenai/olmOCR-2-7B-1025-FP8 - Recommended FP8 quantized (practical use except fine-tuning) - bartowski/allenai_olmOCR-2-7B-1025-GGUF - GGUF quantized - richardyoung/olmOCR-2-7B-1025-GGUF - Alternative GGUF</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM (recommended via olmOCR toolkit) - \u2705 API (DeepInfra, Parasail, Cirrascale)</p> <p>Best Usage: Via olmOCR toolkit with VLLM for efficient inference at scale (millions of documents).</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>vllm</code>, <code>olmocr</code> (toolkit Tasks: <code>text_extract</code>, <code>ocr</code>, <code>table</code>, <code>formula</code></p> <p>Links: - Model Card - FP8 Version - Blog Post - GGUF (bartowski)</p>"},{"location":"ROADMAP_DETAILED/#13-qwen3-vl-8b-instruct","title":"13. Qwen3-VL-8B-Instruct","text":"<p>Released: October 2025 | Parameters: 8B | License: Apache 2.0</p> <p>HuggingFace: Qwen/Qwen3-VL-8B-Instruct</p> <p>Description: Primary model in Qwen3-VL series with optimal balance of performance and efficiency. Enhanced document parsing over Qwen2.5-VL with improved visual perception, text understanding, and advanced reasoning.</p> <p>Key Features: - Custom layout label support (flexible VLM) - Extended context length - Enhanced spatial and video comprehension - Stronger agent interaction capabilities</p> <p>Model Variations: - Qwen/Qwen3-VL-8B-Instruct - Instruction-tuned - Qwen/Qwen3-VL-8B-Thinking - Reasoning-enhanced</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM - \u2705 MLX (Apple Silicon) - mlx-community/Qwen3-VL-8B-Instruct-4bit - \u2705 API (Novita AI, OpenRouter, etc.)</p> <p>API Providers: - Novita AI: Context 131K tokens, Max output 33K tokens   - Pricing: $0.08/M input tokens, $0.50/M output tokens</p> <p>Dependencies: <code>transformers&gt;=4.46</code>, <code>torch</code>, <code>qwen-vl-utils</code>, <code>vllm</code> (optional)</p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>structured</code>, <code>ocr</code>, <code>table</code>, <code>formula</code></p> <p>Links: - Model Card - Collection - GitHub - MLX 4-bit</p>"},{"location":"ROADMAP_DETAILED/#14-datalab-to-chandra","title":"14. datalab-to Chandra","text":"<p>Released: 2024 | Parameters: 9B | License: Apache 2.0</p> <p>HuggingFace: datalab-to/chandra</p> <p>Description: OCR model handling complex tables, forms, and handwriting with full layout preservation. Uses Qwen3VL for document understanding.</p> <p>Key Features: - 83.1 \u00b1 0.9 overall on OlmOCR benchmark (outperforms DeepSeek OCR, dots.ocr, olmOCR) - Strong grounding capabilities - Supports 40+ languages - Layout-aware output with bbox coordinates for every text block, table, and image - Outputs in HTML, Markdown, and JSON with detailed layout</p> <p>Use Cases: - Handwritten forms - Mathematical notation - Multi-column layouts - Complex tables</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM (production throughput)</p> <p>Installation: <pre><code>pip install chandra-ocr\n</code></pre></p> <p>Model Variations: - datalab-to/chandra - Official model - noctrex/Chandra-OCR-GGUF - GGUF quantized</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>vllm</code> (optional), <code>chandra-ocr</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>ocr</code>, <code>table</code>, <code>formula</code></p> <p>Links: - Model Card - GitHub - Blog Post - DeepWiki Docs - GGUF Version</p>"},{"location":"ROADMAP_DETAILED/#large-models-32b-parameters","title":"Large Models (32B+ Parameters)","text":""},{"location":"ROADMAP_DETAILED/#15-qwen3-vl-32b-instruct","title":"15. Qwen3-VL-32B-Instruct","text":"<p>Released: October 2025 | Parameters: 32B | License: Apache 2.0</p> <p>HuggingFace: Qwen/Qwen3-VL-32B-Instruct</p> <p>Description: Largest Qwen3-VL model with maximum performance for complex document understanding and multimodal reasoning tasks.</p> <p>Key Features: - Superior performance on complex documents - Extended context length - Enhanced reasoning capabilities - Production-grade for demanding applications</p> <p>Model Variations: - Qwen/Qwen3-VL-32B-Instruct - Instruction-tuned - Qwen/Qwen3-VL-32B-Thinking - Reasoning-enhanced</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM (recommended for production) - \u2705 API (cloud providers)</p> <p>GPU Requirements: A100 40GB+ or multi-GPU setup</p> <p>Dependencies: <code>transformers&gt;=4.46</code>, <code>torch</code>, <code>qwen-vl-utils</code>, <code>vllm</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>structured</code>, <code>ocr</code>, <code>table</code>, <code>formula</code></p> <p>Links: - Model Card - Collection - GitHub</p>"},{"location":"ROADMAP_DETAILED/#specialized-models","title":"Specialized Models","text":""},{"location":"ROADMAP_DETAILED/#16-docling-projectdocling-models","title":"16. docling-project/docling-models","text":"<p>Released: 2024 | Parameters: Various | License: Apache 2.0</p> <p>HuggingFace: docling-project/docling-models</p> <p>Description: Collection of models powering the Docling PDF document conversion package. Includes layout detection (RT-DETR) and table structure recognition (TableFormer).</p> <p>Models Included: 1. Layout Model: RT-DETR for detecting document components    - Labels: Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title 2. TableFormer Model: Table structure identification from images</p> <p>Note: Superseded by granite-docling-258M for end-to-end document conversion (receives updates and support).</p> <p>Backends Supported: - \u2705 PyTorch (via Docling library)</p> <p>Integration: <pre><code>pip install docling\n</code></pre></p> <p>Dependencies: <code>docling</code>, <code>transformers</code>, <code>torch</code></p> <p>Tasks: <code>layout</code>, <code>table</code></p> <p>Links: - Model Card - Vision Models Docs - SmolDocling (legacy)</p>"},{"location":"ROADMAP_DETAILED/#optional-models-legacyalternative","title":"\ud83d\udce6 Optional Models (Legacy/Alternative)","text":""},{"location":"ROADMAP_DETAILED/#qwen-25-vl-series-previous-generation","title":"Qwen 2.5-VL Series (Previous Generation)","text":""},{"location":"ROADMAP_DETAILED/#qwen25-vl-3b-instruct","title":"Qwen2.5-VL-3B-Instruct","text":"<p>Released: 2024 | Parameters: 3B | License: Apache 2.0</p> <p>HuggingFace: Qwen/Qwen2.5-VL-3B-Instruct</p> <p>Description: Previous generation Qwen VLM with strong visual understanding, agentic capabilities, video understanding (1+ hour), and structured outputs.</p> <p>Key Features: - Analyzes texts, charts, icons, graphics, layouts - Visual agent capabilities (computer use, phone use) - Video comprehension with temporal segment pinpointing - ViT architecture with SwiGLU and RMSNorm - Dynamic resolution + dynamic FPS sampling</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM - \u2705 MLX - \u2705 API</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>qwen-vl-utils</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>structured</code>, <code>ocr</code></p> <p>Links: - Model Card - Collection</p>"},{"location":"ROADMAP_DETAILED/#qwen25-vl-7b-instruct","title":"Qwen2.5-VL-7B-Instruct","text":"<p>Released: 2024 | Parameters: 7B | License: Apache 2.0</p> <p>HuggingFace: Qwen/Qwen2.5-VL-7B-Instruct</p> <p>Description: Mid-size Qwen2.5-VL model with same capabilities as 3B variant but enhanced performance.</p> <p>Model Variations: - Qwen/Qwen2.5-VL-7B-Instruct - Official - unsloth/Qwen2.5-VL-7B-Instruct-GGUF - GGUF quantized - nvidia/Qwen2.5-VL-7B-Instruct-NVFP4 - NVIDIA FP4 optimized</p> <p>Backends Supported: - \u2705 PyTorch (HuggingFace Transformers) - \u2705 VLLM - \u2705 MLX - \u2705 API</p> <p>Dependencies: <code>transformers</code>, <code>torch</code>, <code>qwen-vl-utils</code></p> <p>Tasks: <code>text_extract</code>, <code>layout</code>, <code>structured</code>, <code>ocr</code>, <code>table</code></p> <p>Links: - Model Card - Collection - GGUF Version</p>"},{"location":"ROADMAP_DETAILED/#model-comparison-summary","title":"\ud83d\udcca Model Comparison Summary","text":""},{"location":"ROADMAP_DETAILED/#by-release-date-2024-2026","title":"By Release Date (2024-2026)","text":"Model Release Params Benchmark Score LightOnOCR-2-1B Jan 2025 1B 83.2 (OlmOCR) dots.ocr Dec 2024 1.7B 79.1 (OlmOCR) Granite-Docling-258M Dec 2024 258M N/A Chandra 2024 9B 83.1 (OlmOCR) Qwen3-VL Series Oct 2025 2-32B SOTA PaddleOCR-VL Oct 2025 900M SOTA olmOCR-2-7B Oct 2025 7B 82.4 (OlmOCR) DeepSeek-OCR Oct 2024 3B 75.4 (OlmOCR) GOT-OCR2.0 Sep 2024 700M N/A MinerU2.5 Sep 2024 1.2B SOTA"},{"location":"ROADMAP_DETAILED/#by-performance-olmocr-bench","title":"By Performance (OlmOCR-Bench)","text":"Rank Model Score Params 1 LightOnOCR-2-1B 83.2 \u00b1 0.9 1B 2 Chandra 83.1 \u00b1 0.9 9B 3 olmOCR-2-7B 82.4 7B 4 dots.ocr 79.1 1.7B 5 olmOCR (v1) 78.5 7B 6 DeepSeek-OCR 75.4 \u00b1 1.0 3B"},{"location":"ROADMAP_DETAILED/#by-speed-relative-performance","title":"By Speed (Relative Performance)","text":"Model Speed Multiplier Params LightOnOCR-2-1B Fastest baseline 1B PaddleOCR-VL 1.73\u00d7 slower 900M DeepSeek-OCR (vLLM) 1.73\u00d7 slower 3B olmOCR-2 1.7\u00d7 slower 7B Chandra 3.3\u00d7 slower 9B dots.ocr 5\u00d7 slower 1.7B"},{"location":"ROADMAP_DETAILED/#backend-support-matrix","title":"\ud83d\udd27 Backend Support Matrix","text":"Model PyTorch VLLM MLX API GGUF Granite-Docling-258M \u2705 \u274c \u2705 \u274c \u274c dots.ocr \u2705 \u2705 \u274c \u274c \u274c GOT-OCR2.0 \u2705 \u274c \u274c \u274c \u274c PaddleOCR-VL \u2705 \u274c \u274c \u274c \u274c MinerU2.5 \u2705 \u2705 \u2705 \u274c \u2705 LightOnOCR-2-1B \u2705 \u274c \u274c \u274c \u2705 Qwen3-VL (all) \u2705 \u2705 \u2705 \u2705 \u2705 DeepSeek-OCR \u2705 \u2705 \u274c \u274c \u2705 Nanonets-OCR2-3B \u2705 \u274c \u2705 \u274c \u2705 Gemma-3-4B-IT \u2705 \u274c \u274c \u2705 \u2705 olmOCR-2-7B \u2705 \u2705 \u274c \u2705 \u2705 Chandra \u2705 \u2705 \u274c \u274c \u2705 Qwen2.5-VL (all) \u2705 \u2705 \u2705 \u2705 \u2705"},{"location":"ROADMAP_DETAILED/#recommended-model-selection-guide","title":"\ud83d\udcda Recommended Model Selection Guide","text":""},{"location":"ROADMAP_DETAILED/#by-use-case","title":"By Use Case","text":"Use Case Recommended Model Why Edge/Mobile Deployment Granite-Docling-258M Ultra-compact (258M), MLX support Fast OCR (CPU) LightOnOCR-2-1B Fastest in class, SOTA accuracy Multilingual Documents PaddleOCR-VL 109 languages, minimal resources High-Throughput Serving dots.ocr + VLLM Built for VLLM, fast inference Best Accuracy (English) LightOnOCR-2-1B or Chandra SOTA on OlmOCR-Bench Custom Layout Detection Qwen3-VL-8B Flexible VLM with prompt-based labels Production Balanced Qwen3-VL-8B or olmOCR-2-7B Performance + reliability Complex Documents Chandra or Qwen3-VL-32B Handles tables, forms, handwriting Apple Silicon (M1/M2/M3) Granite-Docling-258M (MLX) Native MLX optimization Cost-Effective API Qwen3-VL-8B (Novita) $0.08/M tokens input"},{"location":"ROADMAP_DETAILED/#quick-start-examples","title":"\ud83d\ude80 Quick Start Examples","text":""},{"location":"ROADMAP_DETAILED/#ultra-compact-258m-granite-docling","title":"Ultra-Compact (258M) - Granite-Docling","text":"<pre><code>from omnidocs.tasks.text_extraction import GraniteDoclingOCR, GraniteDoclingConfig\n\nextractor = GraniteDoclingOCR(\n    config=GraniteDoclingConfig(device=\"cuda\")\n)\nresult = extractor.extract(image, output_format=\"markdown\")\n</code></pre>"},{"location":"ROADMAP_DETAILED/#fastest-ocr-1b-lightonocr-2","title":"Fastest OCR (1B) - LightOnOCR-2","text":"<pre><code>from omnidocs.tasks.text_extraction import LightOnOCRExtractor, LightOnOCRConfig\n\nextractor = LightOnOCRExtractor(\n    config=LightOnOCRConfig(\n        model=\"lightonai/LightOnOCR-2-1B\",\n        device=\"cuda\"\n    )\n)\nresult = extractor.extract(image, output_format=\"markdown\")\n</code></pre>"},{"location":"ROADMAP_DETAILED/#high-throughput-17b-dotsocr-vllm","title":"High-Throughput (1.7B) - dots.ocr + VLLM","text":"<pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nextractor = DotsOCRTextExtractor(\n    backend=DotsOCRVLLMConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.9\n    )\n)\nresult = extractor.extract(image, output_format=\"markdown\")\n</code></pre>"},{"location":"ROADMAP_DETAILED/#best-accuracy-7-9b-olmocr-2-or-chandra","title":"Best Accuracy (7-9B) - olmOCR-2 or Chandra","text":"<pre><code>from omnidocs.tasks.text_extraction import OlmOCRExtractor, ChandraTextExtractor\nfrom omnidocs.tasks.text_extraction.olm import OlmOCRVLLMConfig\nfrom omnidocs.tasks.text_extraction.chandra import ChandraPyTorchConfig\n\n# Option 1: olmOCR-2-7B with VLLM\nextractor = OlmOCRExtractor(\n    backend=OlmOCRVLLMConfig(\n        model=\"allenai/olmOCR-2-7B-1025-FP8\",\n        tensor_parallel_size=1\n    )\n)\n\n# Option 2: Chandra-9B\nextractor = ChandraTextExtractor(\n    backend=ChandraPyTorchConfig(\n        model=\"datalab-to/chandra\",\n        device=\"cuda\"\n    )\n)\n</code></pre>"},{"location":"ROADMAP_DETAILED/#flexible-custom-layouts-8b-qwen3-vl","title":"Flexible Custom Layouts (8B) - Qwen3-VL","text":"<pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenPyTorchConfig\n\nlayout = QwenLayoutDetector(\n    backend=QwenPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\"\n    )\n)\n\nresult = layout.extract(\n    image,\n    custom_labels=[\"code_block\", \"sidebar\", \"diagram\"]\n)\n</code></pre>"},{"location":"ROADMAP_DETAILED/#current-focus-layout-analysis-models","title":"\ud83c\udfaf Current Focus: Layout Analysis Models","text":""},{"location":"ROADMAP_DETAILED/#phase-1-multi-backend-vlm-integration","title":"Phase 1: Multi-Backend VLM Integration","text":""},{"location":"ROADMAP_DETAILED/#1-qwen3-vl-8b-instruct-integration","title":"1. Qwen3-VL-8B-Instruct Integration","text":"<p>Status: \ud83d\udfe1 In Progress</p> <p>Integrate Qwen3-VL-8B-Instruct for flexible layout detection with custom label support across all backends.</p> <p>Key Features: - Enhanced document parsing over Qwen2.5-VL - Improved visual perception and text understanding - Advanced reasoning capabilities - Custom layout label support</p>"},{"location":"ROADMAP_DETAILED/#implementation-checklist","title":"Implementation Checklist:","text":"<ul> <li>[ ] HuggingFace/PyTorch Backend (<code>QwenLayoutDetector</code> + <code>QwenPyTorchConfig</code>)</li> </ul> <p>Model: <code>Qwen/Qwen3-VL-8B-Instruct</code></p> <p>Config Class: <code>omnidocs/tasks/layout_analysis/qwen/pytorch.py</code> <pre><code>class QwenPyTorchConfig(BaseModel):\n    model: str = \"Qwen/Qwen3-VL-8B-Instruct\"\n    device: str = \"cuda\"\n    torch_dtype: Literal[\"auto\", \"float16\", \"bfloat16\"] = \"auto\"\n    attn_implementation: Optional[str] = None  # \"flash_attention_2\" if available\n    cache_dir: Optional[str] = None\n</code></pre></p> <p>Dependencies:   - <code>torch</code>, <code>transformers</code>   - <code>qwen-vl-utils</code> (model-specific utility)</p> <p>Reference Implementation: See <code>scripts/layout/modal_qwen3_vl_layout.py</code> in the repository</p> <p>Testing:   - Validate on synthetic document images   - Compare detection accuracy with ground truth   - Test custom label support</p> <ul> <li>[ ] VLLM Backend (<code>QwenVLLMConfig</code>)</li> </ul> <p>Model: <code>Qwen/Qwen3-VL-8B-Instruct</code></p> <p>Config Class: <code>omnidocs/tasks/layout_analysis/qwen/vllm.py</code> <pre><code>class QwenVLLMConfig(BaseModel):\n    model: str = \"Qwen/Qwen3-VL-8B-Instruct\"\n    tensor_parallel_size: int = 1\n    gpu_memory_utilization: float = 0.9\n    max_model_len: Optional[int] = None\n    trust_remote_code: bool = True\n</code></pre></p> <p>Dependencies:   - <code>vllm&gt;=0.4.0</code>   - <code>torch&gt;=2.0</code></p> <p>Use Case: High-throughput batch processing (10+ documents/second)</p> <p>Modal Config:   - GPU: <code>A10G:1</code> (minimum), <code>A100:1</code> (recommended for production)   - Image: VLLM GPU Image with flash-attn</p> <p>Testing:   - Benchmark throughput vs PyTorch   - Validate output consistency   - Test batch processing</p> <ul> <li>[ ] MLX Backend (<code>QwenMLXConfig</code>)</li> </ul> <p>Model: <code>mlx-community/Qwen3-VL-8B-Instruct-4bit</code></p> <p>Config Class: <code>omnidocs/tasks/layout_analysis/qwen/mlx.py</code> <pre><code>class QwenMLXConfig(BaseModel):\n    model: str = \"mlx-community/Qwen3-VL-8B-Instruct-4bit\"\n    quantization: Literal[\"4bit\", \"8bit\"] = \"4bit\"\n    max_tokens: int = 4096\n</code></pre></p> <p>Dependencies:   - <code>mlx&gt;=0.10</code>   - <code>mlx-lm&gt;=0.10</code></p> <p>Platform: Apple Silicon only (M1/M2/M3+)</p> <p>Use Case: Local development and testing on macOS</p> <p>Note: \u26a0\ufe0f DO NOT deploy MLX to Modal - local development only</p> <ul> <li>[ ] API Backend (<code>QwenAPIConfig</code>)</li> </ul> <p>Model: <code>qwen3-vl-8b-instruct</code></p> <p>Config Class: <code>omnidocs/tasks/layout_analysis/qwen/api.py</code> <pre><code>class QwenAPIConfig(BaseModel):\n    model: str = \"novita/qwen3-vl-8b-instruct\"\n    api_key: str\n    base_url: Optional[str] = None\n    max_tokens: int = 4096\n    temperature: float = 0.1\n</code></pre></p> <p>Provider: Novita AI   - Context Length: 131K tokens   - Max Output: 33K tokens   - Pricing:     - Input: $0.08/M tokens     - Output: $0.50/M tokens</p> <p>Dependencies:   - <code>litellm&gt;=1.30</code>   - <code>openai&gt;=1.0</code></p> <p>Use Case:   - Serverless deployments   - No GPU infrastructure required   - Cost-effective for low-volume processing</p> <ul> <li>[ ] Main Extractor Class (<code>omnidocs/tasks/layout_analysis/qwen.py</code>)</li> </ul> <p>Implement unified <code>QwenLayoutDetector</code> class:   <pre><code>from typing import Union, List, Optional\nfrom PIL import Image\nfrom .base import BaseLayoutExtractor\nfrom .models import LayoutOutput\nfrom .qwen import (\n    QwenPyTorchConfig,\n    QwenVLLMConfig,\n    QwenMLXConfig,\n    QwenAPIConfig,\n)\n\nQwenBackendConfig = Union[\n    QwenPyTorchConfig,\n    QwenVLLMConfig,\n    QwenMLXConfig,\n    QwenAPIConfig,\n]\n\nclass QwenLayoutDetector(BaseLayoutExtractor):\n    \"\"\"Flexible VLM-based layout detector with custom label support.\"\"\"\n\n    def __init__(self, backend: QwenBackendConfig):\n        self.backend_config = backend\n        self._backend = self._create_backend()\n\n    def extract(\n        self,\n        image: Image.Image,\n        custom_labels: Optional[List[str]] = None,\n    ) -&gt; LayoutOutput:\n        \"\"\"\n        Detect layout elements with optional custom labels.\n\n        Args:\n            image: PIL Image\n            custom_labels: Optional custom layout categories\n                Default: [\"title\", \"paragraph\", \"table\", \"figure\",\n                         \"caption\", \"formula\", \"list\"]\n\n        Returns:\n            LayoutOutput with detected bounding boxes\n        \"\"\"\n        # Implementation...\n</code></pre></p> <ul> <li>[ ] Integration Tests</li> </ul> <p>Test suite covering:   - All backend configurations   - Custom label functionality   - Cross-backend output consistency   - Edge cases (empty images, single elements, complex layouts)</p> <ul> <li> <p>[ ] Documentation</p> </li> <li> <p>API reference with examples for each backend</p> </li> <li>Performance comparison table (PyTorch vs VLLM vs MLX vs API)</li> <li>Migration guide from Qwen2.5-VL</li> <li> <p>Custom label usage examples</p> </li> <li> <p>[ ] Modal Deployment Script</p> </li> </ul> <p>Create production-ready deployment:   - <code>scripts/layout_omnidocs/modal_qwen_layout_vllm_online.py</code>   - Web endpoint for layout detection API   - Batch processing support   - Monitoring and logging</p>"},{"location":"ROADMAP_DETAILED/#phase-2-additional-layout-models","title":"Phase 2: Additional Layout Models","text":""},{"location":"ROADMAP_DETAILED/#2-rt-detr-layout-detector","title":"2. RT-DETR Layout Detector","text":"<ul> <li>[ ] Single-Backend Implementation (PyTorch only)</li> <li>Model: <code>RT-DETR</code> (Facebook AI)</li> <li>Fixed label support (COCO-based)</li> <li>Real-time detection optimization</li> </ul>"},{"location":"ROADMAP_DETAILED/#3-surya-layout-detector","title":"3. Surya Layout Detector","text":"<ul> <li>[ ] Single-Backend Implementation (PyTorch only)</li> <li>Model: <code>vikp/surya_layout</code></li> <li>Multi-language document support</li> <li>Optimized for speed</li> </ul>"},{"location":"ROADMAP_DETAILED/#4-florence-2-layout-detector","title":"4. Florence-2 Layout Detector","text":"<ul> <li>[ ] Multi-Backend Implementation</li> <li>HuggingFace/PyTorch backend</li> <li>API backend (Microsoft Azure)</li> <li>Object detection + dense captioning</li> </ul>"},{"location":"ROADMAP_DETAILED/#future-phases","title":"\ud83d\udd2e Future Phases","text":"<p>Additional task categories will be added after layout analysis is complete:</p> <ul> <li>OCR Extraction: Surya-OCR, PaddleOCR, Qwen-OCR</li> <li>Text Extraction: VLM-based Markdown/HTML extraction</li> <li>Table Extraction: Table Transformer, Surya-Table</li> <li>Math Expression Extraction: UniMERNet, Surya-Math</li> <li>Advanced Features: Reading order, image captioning, chart understanding</li> <li>Package &amp; Distribution: PyPI publishing, comprehensive documentation</li> </ul>"},{"location":"ROADMAP_DETAILED/#success-metrics-layout-analysis","title":"\ud83c\udfaf Success Metrics (Layout Analysis)","text":""},{"location":"ROADMAP_DETAILED/#performance-targets","title":"Performance Targets","text":"Metric Target Current Layout Detection Accuracy (mAP) &gt;90% TBD Inference Speed (PyTorch) &lt;2s per page TBD Inference Speed (VLLM) &lt;0.5s per page TBD Custom Label Support 100% functional TBD"},{"location":"ROADMAP_DETAILED/#quality-targets","title":"Quality Targets","text":"<ul> <li>[ ] Type hints coverage: 100%</li> <li>[ ] Docstring coverage: 100%</li> <li>[ ] Test coverage: &gt;80%</li> <li>[ ] All backends tested on production data</li> <li>[ ] Cross-backend output consistency validated</li> </ul>"},{"location":"ROADMAP_DETAILED/#infrastructure","title":"\ud83d\udd27 Infrastructure","text":""},{"location":"ROADMAP_DETAILED/#modal-deployment-standards","title":"Modal Deployment Standards","text":"<p>Consistency Requirements (as per CLAUDE.md):</p> <ul> <li>Volume Name: <code>omnidocs</code></li> <li>Secret Name: <code>adithya-hf-wandb</code></li> <li>CUDA Version: <code>12.4.0-devel-ubuntu22.04</code></li> <li>Python Version: <code>3.11</code> (3.12 for Qwen3-VL)</li> <li>Cache Directory: <code>/data/.cache</code> (HuggingFace)</li> <li>Model Cache: <code>/data/omnidocs_models</code></li> <li>Dependency Management: <code>.uv_pip_install()</code> (NO version pinning)</li> </ul>"},{"location":"ROADMAP_DETAILED/#gpu-configurations","title":"GPU Configurations","text":"GPU Use Case Cost (est.) <code>A10G:1</code> Development &amp; Testing $0.60/hr <code>A100:1</code> Production Inference $3.00/hr <code>A100:2</code> High-Throughput VLLM $6.00/hr"},{"location":"ROADMAP_DETAILED/#references","title":"\ud83d\udcda References","text":""},{"location":"ROADMAP_DETAILED/#design-documents","title":"Design Documents","text":"<ul> <li>Backend Architecture - Core design principles (see <code>IMPLEMENTATION_PLAN/BACKEND_ARCHITECTURE.md</code>)</li> <li>Developer Experience (DevEx) - API design and patterns (see <code>IMPLEMENTATION_PLAN/DEVEX.md</code>)</li> <li>Claude Development Guide - Implementation standards (see <code>CLAUDE.md</code> in repo root)</li> </ul>"},{"location":"ROADMAP_DETAILED/#external-resources","title":"External Resources","text":"<ul> <li>Qwen3-VL Model Card</li> <li>Qwen3-VL MLX (4bit)</li> <li>Modal Documentation</li> <li>UV Package Manager</li> </ul>"},{"location":"ROADMAP_DETAILED/#notes","title":"\ud83d\udcdd Notes","text":""},{"location":"ROADMAP_DETAILED/#implementation-order-rationale","title":"Implementation Order Rationale","text":"<ol> <li>Qwen3-VL Priority: Multi-backend support demonstrates v2.0 architecture</li> <li>RT-DETR: Fast fixed-label detection for production use</li> <li>Surya: Multi-language support and speed optimization</li> <li>Florence-2: Microsoft's advanced VLM capabilities</li> </ol>"},{"location":"ROADMAP_DETAILED/#breaking-changes-from-v10","title":"Breaking Changes from v1.0","text":"<ul> <li>String-based factory pattern removed (use class imports)</li> <li>Document class is now stateless (doesn't store results)</li> <li>Config classes are model-specific (not generic)</li> <li>Backend selection via config type (not string parameter)</li> </ul> <p>Last Updated: January 21, 2026 Maintainer: Adithya S Kolavi Version: 2.0.0-dev</p>"},{"location":"concepts/","title":"Concepts","text":"<p>Understanding OmniDocs architecture, config patterns, backend system, and document model.</p>"},{"location":"concepts/#architecture-overview","title":"Architecture Overview","text":""},{"location":"concepts/#core-flow","title":"Core Flow","text":"<pre><code>User Code\n    \u2193\nDocument.from_pdf() \u2192 PIL Images\n    \u2193\nExtractor(config) \u2192 Load model with backend\n    \u2193\nextractor.extract(image) \u2192 Pydantic output\n</code></pre>"},{"location":"concepts/#design-principles","title":"Design Principles","text":"Principle What It Means Unified API <code>.extract()</code> for all tasks Class imports <code>from omnidocs.tasks.x import Model</code> (no string factories) Type-safe configs Pydantic validation, IDE autocomplete Stateless Document Document = source data, not results Config = capability Available configs show supported backends Init vs Extract Config sets hardware, extract sets task params"},{"location":"concepts/#component-architecture","title":"Component Architecture","text":"<pre><code>omnidocs/\n\u251c\u2500\u2500 document.py            # Document class (stateless)\n\u251c\u2500\u2500 tasks/\n\u2502   \u251c\u2500\u2500 text_extraction/   # Text \u2192 Markdown/HTML\n\u2502   \u251c\u2500\u2500 layout_extraction/ # Structure detection\n\u2502   \u251c\u2500\u2500 ocr_extraction/    # Text + bounding boxes\n\u2502   \u251c\u2500\u2500 table_extraction/  # Table structure extraction\n\u2502   \u251c\u2500\u2500 reading_order/     # Logical reading sequence\n\u2502   \u2514\u2500\u2500 ...\n\u2514\u2500\u2500 inference/\n    \u251c\u2500\u2500 pytorch.py         # HuggingFace/torch\n    \u251c\u2500\u2500 vllm.py            # High-throughput\n    \u251c\u2500\u2500 mlx.py             # Apple Silicon\n    \u2514\u2500\u2500 api.py             # LiteLLM\n</code></pre>"},{"location":"concepts/#config-pattern","title":"Config Pattern","text":""},{"location":"concepts/#single-backend-models","title":"Single-Backend Models","text":"<p>Models that support only one backend (typically PyTorch):</p> <pre><code>from omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\n\n# Pattern: {Model}Config \u2192 config= parameter\nlayout = DocLayoutYOLO(\n    config=DocLayoutYOLOConfig(\n        device=\"cuda\",\n        confidence=0.25,\n    )\n)\n</code></pre>"},{"location":"concepts/#multi-backend-models","title":"Multi-Backend Models","text":"<p>Models that support multiple backends:</p> <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenPyTorchConfig,   # Local GPU\n    QwenVLLMConfig,      # High throughput\n    QwenMLXConfig,       # Apple Silicon\n    QwenAPIConfig,       # Cloud API\n)\n\n# Pattern: {Model}{Backend}Config \u2192 backend= parameter\nextractor = QwenTextExtractor(\n    backend=QwenPyTorchConfig(device=\"cuda\")\n)\n</code></pre>"},{"location":"concepts/#config-naming","title":"Config Naming","text":"Model Type Naming Pattern Parameter Single-backend <code>{Model}Config</code> <code>config=</code> Multi-backend <code>{Model}{Backend}Config</code> <code>backend=</code>"},{"location":"concepts/#what-goes-where","title":"What Goes Where","text":"<p>Init (config/backend) - Model name/path - Device (cuda, cpu, mps) - Quantization, dtype - Backend-specific settings</p> <p>Extract (method params) - Output format (markdown, html) - Custom prompts - Task-specific options - Per-call settings</p> <pre><code># Init: hardware/model setup\nextractor = QwenTextExtractor(\n    backend=QwenPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n)\n\n# Extract: task parameters\nresult = extractor.extract(\n    image,\n    output_format=\"markdown\",\n    include_layout=True,\n)\n</code></pre>"},{"location":"concepts/#backend-system","title":"Backend System","text":""},{"location":"concepts/#backend-comparison","title":"Backend Comparison","text":"Backend Use Case Requirements PyTorch Development, local GPU CUDA 12+ or CPU VLLM Production, high throughput NVIDIA GPU 24GB+ MLX Apple Silicon development M1/M2/M3 Mac API No GPU, cloud-first API key + internet"},{"location":"concepts/#backend-selection","title":"Backend Selection","text":"<pre><code># PyTorch - development default\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\nbackend = QwenPyTorchConfig(device=\"cuda\", torch_dtype=\"bfloat16\")\n\n# VLLM - production throughput\nfrom omnidocs.tasks.text_extraction.qwen import QwenVLLMConfig\nbackend = QwenVLLMConfig(tensor_parallel_size=2, gpu_memory_utilization=0.9)\n\n# MLX - Apple Silicon\nfrom omnidocs.tasks.text_extraction.qwen import QwenMLXConfig\nbackend = QwenMLXConfig(quantization=\"4bit\")\n\n# API - cloud\nfrom omnidocs.tasks.text_extraction.qwen import QwenAPIConfig\nbackend = QwenAPIConfig(api_key=\"sk-...\", base_url=\"https://...\")\n</code></pre>"},{"location":"concepts/#switching-backends","title":"Switching Backends","text":"<p>OmniDocs makes it easy to switch - only the config changes:</p> <pre><code># Development: PyTorch\nextractor = QwenTextExtractor(\n    backend=QwenPyTorchConfig(device=\"cuda\")\n)\n\n# Production: switch to VLLM\nextractor = QwenTextExtractor(\n    backend=QwenVLLMConfig(tensor_parallel_size=2)\n)\n\n# Same .extract() API works for both\nresult = extractor.extract(image, output_format=\"markdown\")\n</code></pre>"},{"location":"concepts/#discoverability","title":"Discoverability","text":"<p>Available backends = importable config classes:</p> <pre><code># Check what backends a model supports\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenPyTorchConfig,  # \u2713 PyTorch supported\n    QwenVLLMConfig,     # \u2713 VLLM supported\n    QwenMLXConfig,      # \u2713 MLX supported\n    QwenAPIConfig,      # \u2713 API supported\n)\n\n# If import fails \u2192 backend not supported for that model\n</code></pre>"},{"location":"concepts/#document-model","title":"Document Model","text":""},{"location":"concepts/#design-stateless","title":"Design: Stateless","text":"<p>Document contains source data only, not analysis results.</p> <p>Why? - Clean separation of concerns - User controls caching strategy - Memory efficient - Works with any workflow</p> <pre><code>doc = Document.from_pdf(\"file.pdf\")  # Just loads PDF\nresult = extractor.extract(doc.get_page(0))  # User manages result\n</code></pre>"},{"location":"concepts/#loading-methods","title":"Loading Methods","text":"<pre><code>from omnidocs import Document\n\n# From file\ndoc = Document.from_pdf(\"file.pdf\", dpi=150)\n\n# From URL\ndoc = Document.from_url(\"https://example.com/doc.pdf\")\n\n# From bytes\ndoc = Document.from_bytes(pdf_bytes, filename=\"doc.pdf\")\n\n# From images\ndoc = Document.from_image(\"page.png\")\ndoc = Document.from_images([\"p1.png\", \"p2.png\"])\n</code></pre>"},{"location":"concepts/#lazy-loading","title":"Lazy Loading","text":"<p>Pages render on demand, then cache:</p> <pre><code>doc = Document.from_pdf(\"large.pdf\")  # Fast: no rendering yet\n\npage = doc.get_page(0)  # Renders now (~200ms)\npage = doc.get_page(0)  # Cached: instant\n</code></pre>"},{"location":"concepts/#memory-management","title":"Memory Management","text":"<pre><code># Efficient iteration (one page at a time)\nfor page in doc.iter_pages():\n    result = extractor.extract(page)\n    save(result)\n\n# Clear cache for large documents\ndoc.clear_cache()        # All pages\ndoc.clear_cache(page=0)  # Specific page\n\n# Context manager\nwith Document.from_pdf(\"file.pdf\") as doc:\n    # Use doc\n    pass  # Auto-closes\n</code></pre>"},{"location":"concepts/#metadata","title":"Metadata","text":"<pre><code>doc.page_count              # Number of pages\ndoc.metadata.source_type    # \"file\", \"url\", \"bytes\"\ndoc.metadata.file_name      # Filename\ndoc.metadata.file_size      # Size in bytes\ndoc.metadata.format         # \"pdf\", \"png\", etc.\ndoc.to_dict()               # Serialize metadata\n</code></pre>"},{"location":"concepts/#key-patterns","title":"Key Patterns","text":""},{"location":"concepts/#pattern-1-single-page","title":"Pattern 1: Single Page","text":"<pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresult = extractor.extract(doc.get_page(0))\n</code></pre>"},{"location":"concepts/#pattern-2-all-pages","title":"Pattern 2: All Pages","text":"<pre><code>for i, page in enumerate(doc.iter_pages()):\n    result = extractor.extract(page)\n    save(f\"page_{i}.md\", result.content)\n</code></pre>"},{"location":"concepts/#pattern-3-memory-control","title":"Pattern 3: Memory Control","text":"<pre><code>for i, page in enumerate(doc.iter_pages()):\n    result = extractor.extract(page)\n    save(result)\n    if i % 10 == 0:\n        doc.clear_cache()  # Free every 10 pages\n</code></pre>"},{"location":"concepts/#pattern-4-environment-based-backend","title":"Pattern 4: Environment-Based Backend","text":"<pre><code>import os\n\nif os.getenv(\"USE_VLLM\"):\n    backend = QwenVLLMConfig(tensor_parallel_size=2)\nelif os.getenv(\"USE_API\"):\n    backend = QwenAPIConfig(api_key=os.getenv(\"API_KEY\"))\nelse:\n    backend = QwenPyTorchConfig(device=\"cuda\")\n\nextractor = QwenTextExtractor(backend=backend)\n</code></pre>"},{"location":"concepts/#trade-offs","title":"Trade-offs","text":"Choice Option A Option B Speed vs Quality 2B model (fast) 8B+ model (accurate) Setup vs Throughput PyTorch (simple) VLLM (10x faster) Privacy vs Convenience Local (private) API (no setup) Memory vs Speed Lazy loading Load all pages"},{"location":"concepts/#summary","title":"Summary","text":"Concept Key Point Architecture Image \u2192 Extractor \u2192 Pydantic output Configs Single-backend: <code>config=</code>, Multi-backend: <code>backend=</code> Backends PyTorch (dev), VLLM (prod), MLX (Mac), API (cloud) Document Stateless, lazy-loaded, user manages results"},{"location":"getting-started/","title":"Getting Started","text":"<p>Everything you need to install OmniDocs, choose a backend, and extract your first document.</p>"},{"location":"getting-started/#1-install","title":"1. Install","text":"<pre><code># PyTorch (recommended for most users)\npip install omnidocs[pytorch]\n\n# Or with uv (faster)\nuv pip install omnidocs[pytorch]\n</code></pre> <p>Other backends: <pre><code>pip install omnidocs[vllm]   # High-throughput production\npip install omnidocs[mlx]    # Apple Silicon\npip install omnidocs[api]    # Cloud-based, no GPU\npip install omnidocs[all]    # Everything\n</code></pre></p> <p>Requirements: - Python 3.10, 3.11, or 3.12 - 4GB RAM minimum (8GB+ recommended) - GPU optional but recommended</p> <p>Verify: <pre><code>from omnidocs import Document\nprint(\"Ready!\")\n</code></pre></p>"},{"location":"getting-started/#2-choose-your-backend","title":"2. Choose Your Backend","text":"<pre><code>Mac with Apple Silicon? \u2192 MLX\nProcessing 100+ docs/day? \u2192 VLLM\nHave NVIDIA GPU? \u2192 PyTorch\nNo GPU? \u2192 API\n</code></pre> Backend Speed Cost GPU Best For PyTorch 1-2s/page Free Optional Development VLLM 0.1s/page Free Required Production scale MLX 2-3s/page Free No (Mac) Apple Silicon API 3-5s/page $0.01-0.10/doc No Quick testing"},{"location":"getting-started/#3-your-first-extraction","title":"3. Your First Extraction","text":""},{"location":"getting-started/#load-a-document","title":"Load a Document","text":"<pre><code>from omnidocs import Document\n\n# From PDF\ndoc = Document.from_pdf(\"document.pdf\")\n\n# From image\ndoc = Document.from_image(\"page.png\")\n\n# From URL\ndoc = Document.from_url(\"https://example.com/doc.pdf\")\n\nprint(f\"Pages: {doc.page_count}\")\n</code></pre>"},{"location":"getting-started/#extract-text","title":"Extract Text","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n\n# Initialize extractor (downloads model on first run)\nextractor = QwenTextExtractor(\n    backend=QwenPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-2B-Instruct\",  # Small, fast\n        device=\"cuda\",  # or \"cpu\", \"mps\"\n    )\n)\n\n# Extract first page\nresult = extractor.extract(doc.get_page(0), output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"getting-started/#complete-example","title":"Complete Example","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n\n# Load\ndoc = Document.from_pdf(\"research_paper.pdf\")\n\n# Initialize once\nextractor = QwenTextExtractor(\n    backend=QwenPyTorchConfig(device=\"cuda\")\n)\n\n# Process all pages\nfor i in range(doc.page_count):\n    result = extractor.extract(doc.get_page(i), output_format=\"markdown\")\n    with open(f\"page_{i+1}.md\", \"w\") as f:\n        f.write(result.content)\n    print(f\"Page {i+1} done\")\n</code></pre>"},{"location":"getting-started/#4-backend-configuration","title":"4. Backend Configuration","text":""},{"location":"getting-started/#pytorch","title":"PyTorch","text":"<pre><code>from omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n\nconfig = QwenPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-2B-Instruct\",  # 2B for speed, 8B for quality\n    device=\"cuda\",                       # \"cuda\", \"cpu\", \"mps\"\n    torch_dtype=\"bfloat16\",             # \"auto\", \"float16\", \"bfloat16\"\n)\n</code></pre>"},{"location":"getting-started/#vllm","title":"VLLM","text":"<pre><code>from omnidocs.tasks.text_extraction.qwen import QwenVLLMConfig\n\nconfig = QwenVLLMConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    tensor_parallel_size=1,      # GPUs to use\n    gpu_memory_utilization=0.9,  # Memory fraction\n)\n</code></pre>"},{"location":"getting-started/#mlx-mac-only","title":"MLX (Mac only)","text":"<pre><code>from omnidocs.tasks.text_extraction.qwen import QwenMLXConfig\n\nconfig = QwenMLXConfig(\n    model=\"Qwen/Qwen3-VL-2B-Instruct\",\n    quantization=\"4bit\",  # \"4bit\", \"8bit\", or None\n)\n</code></pre>"},{"location":"getting-started/#api","title":"API","text":"<pre><code>from omnidocs.tasks.text_extraction.qwen import QwenAPIConfig\n\nconfig = QwenAPIConfig(\n    model=\"qwen3-vl-8b\",\n    api_key=\"YOUR_API_KEY\",\n    base_url=\"https://api.provider.com/v1\",\n)\n</code></pre>"},{"location":"getting-started/#5-working-with-documents","title":"5. Working with Documents","text":""},{"location":"getting-started/#document-class","title":"Document Class","text":"<pre><code>from omnidocs import Document\n\ndoc = Document.from_pdf(\"file.pdf\", dpi=150)  # dpi: 72-300\n\n# Properties\ndoc.page_count        # Number of pages\ndoc.metadata          # DocumentMetadata object\ndoc.text              # Full text (lazy, cached)\n\n# Access pages\npage = doc.get_page(0)           # Single page (0-indexed)\nfor page in doc.iter_pages():    # Memory efficient iteration\n    process(page)\n\n# Memory management\ndoc.clear_cache()     # Free rendered pages\ndoc.close()           # Release resources\n</code></pre>"},{"location":"getting-started/#large-documents","title":"Large Documents","text":"<pre><code># Load specific pages only\ndoc = Document.from_pdf(\"huge.pdf\", page_range=(0, 50))\n\n# Process with memory control\nfor i, page in enumerate(doc.iter_pages()):\n    result = extractor.extract(page)\n    save_result(result)\n\n    if i % 10 == 0:\n        doc.clear_cache()  # Free memory every 10 pages\n</code></pre>"},{"location":"getting-started/#6-output-formats","title":"6. Output Formats","text":"<pre><code># Markdown (default)\nresult = extractor.extract(page, output_format=\"markdown\")\n# Output: # Heading\\n\\nParagraph text...\n\n# HTML\nresult = extractor.extract(page, output_format=\"html\")\n# Output: &lt;h1&gt;Heading&lt;/h1&gt;&lt;p&gt;Paragraph text...&lt;/p&gt;\n\n# Access result\nprint(result.content)   # The extracted text\nprint(result.format)    # \"markdown\" or \"html\"\n</code></pre>"},{"location":"getting-started/#7-quick-troubleshooting","title":"7. Quick Troubleshooting","text":"<p>\"CUDA out of memory\" <pre><code># Use smaller model\nconfig = QwenPyTorchConfig(model=\"Qwen/Qwen3-VL-2B-Instruct\")\n</code></pre></p> <p>\"Model download slow\" - First run downloads ~4-16GB. Cached after.</p> <p>\"No GPU\" <pre><code>pip install omnidocs[api]\n# Then use QwenAPIConfig\n</code></pre></p> <p>\"Page out of range\" <pre><code>if page_num &lt; doc.page_count:\n    page = doc.get_page(page_num)\n</code></pre></p>"},{"location":"getting-started/#8-other-tasks","title":"8. Other Tasks","text":"<p>OmniDocs supports more than just text extraction:</p>"},{"location":"getting-started/#layout-analysis","title":"Layout Analysis","text":"<pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\ndetector = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nlayout = detector.extract(page)\n\nfor box in layout.bboxes:\n    print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre>"},{"location":"getting-started/#table-extraction","title":"Table Extraction","text":"<pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\nextractor = TableFormerExtractor(config=TableFormerConfig(device=\"cuda\"))\nresult = extractor.extract(table_image)\ndf = result.to_dataframe()\n</code></pre>"},{"location":"getting-started/#reading-order","title":"Reading Order","text":"<pre><code>from omnidocs.tasks.reading_order import RuleBasedReadingOrderPredictor\n\npredictor = RuleBasedReadingOrderPredictor()\nreading_order = predictor.predict(layout, ocr_result)\ntext = reading_order.get_full_text()\n</code></pre>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Concepts - Understand the architecture</li> <li>Tasks - All available tasks (text, layout, OCR, tables, reading order)</li> <li>Models - All available models and their configurations</li> <li>Usage - Tasks, models, and workflows</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Complete guide to installing OmniDocs and its dependencies.</p>"},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.10, 3.11, or 3.12</li> <li>OS: Linux, macOS, Windows</li> <li>RAM: 4GB minimum (8GB+ recommended)</li> <li>GPU: Optional but recommended for VLM models</li> </ul>"},{"location":"installation/#quick-install","title":"Quick Install","text":"<pre><code># PyPI (recommended)\npip install omnidocs\n\n# Or with uv (faster)\nuv pip install omnidocs\n</code></pre> <p>Verify installation: <pre><code>from omnidocs import Document\nprint(\"OmniDocs installed successfully!\")\n</code></pre></p>"},{"location":"installation/#install-with-extras","title":"Install with Extras","text":"<p>OmniDocs uses optional dependencies to keep the base install lightweight.</p> <pre><code># Core + PyTorch (most users)\npip install omnidocs[pytorch]\n\n# Core + VLLM (high throughput)\npip install omnidocs[vllm]\n\n# Core + MLX (Apple Silicon)\npip install omnidocs[mlx]\n\n# Core + API support\npip install omnidocs[api]\n\n# Everything\npip install omnidocs[all]\n</code></pre>"},{"location":"installation/#what-each-extra-includes","title":"What Each Extra Includes","text":"Extra Includes Best For <code>pytorch</code> PyTorch, Transformers, Accelerate Local GPU inference <code>vllm</code> VLLM, PyTorch Production, high throughput <code>mlx</code> MLX, mlx-lm Apple Silicon (M1/M2/M3) <code>api</code> LiteLLM, httpx Cloud-based, no GPU <code>ocr</code> Tesseract bindings, EasyOCR, PaddleOCR OCR tasks <code>all</code> All of the above Full functionality"},{"location":"installation/#install-from-source","title":"Install from Source","text":"<p>For development or latest features:</p> <pre><code># Clone repository\ngit clone https://github.com/adithya-s-k/Omnidocs.git\ncd Omnidocs\n\n# Install with uv (recommended)\nuv sync\n\n# Or with pip\npip install -e .\n</code></pre>"},{"location":"installation/#backend-specific-setup","title":"Backend-Specific Setup","text":""},{"location":"installation/#pytorch-backend","title":"PyTorch Backend","text":"<p>Most common setup for local GPU inference.</p> <pre><code>pip install omnidocs[pytorch]\n</code></pre> <p>Requirements: - NVIDIA GPU with CUDA support (optional but recommended) - CUDA 11.8+ for GPU acceleration</p> <p>Verify: <pre><code>import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\n</code></pre></p>"},{"location":"installation/#vllm-backend","title":"VLLM Backend","text":"<p>High-throughput inference for production.</p> <pre><code>pip install omnidocs[vllm]\n</code></pre> <p>Requirements: - Linux (VLLM has limited Windows/macOS support) - NVIDIA GPU with 16GB+ VRAM - CUDA 12.0+</p> <p>Verify: <pre><code>from vllm import LLM\nprint(\"VLLM installed successfully!\")\n</code></pre></p>"},{"location":"installation/#mlx-backend","title":"MLX Backend","text":"<p>Optimized for Apple Silicon Macs.</p> <pre><code>pip install omnidocs[mlx]\n</code></pre> <p>Requirements: - macOS with Apple Silicon (M1/M2/M3/M4) - macOS 13.0+</p> <p>Verify: <pre><code>import mlx.core as mx\nprint(f\"MLX installed, default device: {mx.default_device()}\")\n</code></pre></p>"},{"location":"installation/#api-backend","title":"API Backend","text":"<p>No GPU required - uses cloud APIs.</p> <pre><code>pip install omnidocs[api]\n</code></pre> <p>Supported providers: - OpenRouter - Together AI - Fireworks AI - Any OpenAI-compatible API</p> <p>Setup: <pre><code>import os\nos.environ[\"OPENROUTER_API_KEY\"] = \"your-key\"\n</code></pre></p>"},{"location":"installation/#ocr-dependencies","title":"OCR Dependencies","text":""},{"location":"installation/#tesseract","title":"Tesseract","text":"<p>Tesseract requires a system installation:</p> <p>macOS: <pre><code>brew install tesseract\n</code></pre></p> <p>Ubuntu/Debian: <pre><code>sudo apt install tesseract-ocr\n</code></pre></p> <p>Windows: Download from UB-Mannheim/tesseract</p> <p>Install additional languages: <pre><code># Ubuntu\nsudo apt install tesseract-ocr-chi-sim  # Chinese\nsudo apt install tesseract-ocr-jpn      # Japanese\nsudo apt install tesseract-ocr-ara      # Arabic\n\n# macOS\nbrew install tesseract-lang\n</code></pre></p> <p>Verify: <pre><code>tesseract --version\ntesseract --list-langs\n</code></pre></p>"},{"location":"installation/#easyocr","title":"EasyOCR","text":"<pre><code>pip install easyocr\n</code></pre> <p>Downloads models automatically on first use (~100MB per language).</p>"},{"location":"installation/#paddleocr","title":"PaddleOCR","text":"<pre><code>pip install paddlepaddle paddleocr\n</code></pre> <p>For GPU support: <pre><code>pip install paddlepaddle-gpu paddleocr\n</code></pre></p>"},{"location":"installation/#flash-attention-optional","title":"Flash Attention (Optional)","text":"<p>Flash Attention 2 accelerates VLM inference. Optional but recommended for production.</p>"},{"location":"installation/#requirements_1","title":"Requirements","text":"<ul> <li>CUDA 11.8+ (12.3+ for FA3)</li> <li>PyTorch 2.0+</li> <li>NVIDIA GPU with compute capability 7.0+ (V100, A100, RTX 3090, etc.)</li> <li>Linux</li> </ul>"},{"location":"installation/#option-1-pre-built-wheels-recommended","title":"Option 1: Pre-built Wheels (Recommended)","text":"<p>Download matching wheel from Flash Attention Releases:</p> <pre><code># Example: Python 3.12, CUDA 12, PyTorch 2.5\npip install https://github.com/Dao-AILab/flash-attention/releases/download/v2.8.3/flash_attn-2.8.3+cu12torch2.5cxx11abiFALSE-cp312-cp312-linux_x86_64.whl\n</code></pre> <p>Wheel naming: - <code>cp312</code> = Python 3.12 (cp311 for 3.11, cp310 for 3.10) - <code>cu12</code> = CUDA 12.x (cu118 for CUDA 11.8) - <code>torch2.5</code> = PyTorch 2.5.x</p>"},{"location":"installation/#option-2-compile-from-pypi","title":"Option 2: Compile from PyPI","text":"<pre><code>pip install flash-attn --no-build-isolation\n</code></pre> <p>Speed up compilation: <pre><code>MAX_JOBS=4 pip install flash-attn --no-build-isolation\n</code></pre></p>"},{"location":"installation/#option-3-build-from-source","title":"Option 3: Build from Source","text":"<pre><code>git clone https://github.com/Dao-AILab/flash-attention.git\ncd flash-attention\npip install ninja\npython setup.py install\n</code></pre>"},{"location":"installation/#verify-flash-attention","title":"Verify Flash Attention","text":"<pre><code>import torch\nfrom flash_attn import flash_attn_func\n\nprint(f\"Flash Attention installed: {flash_attn_func is not None}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n</code></pre>"},{"location":"installation/#skip-flash-attention","title":"Skip Flash Attention","text":"<p>If installation fails, use VLLM backend instead (includes optimized attention):</p> <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenVLLMConfig\n\n# VLLM includes optimized attention\nextractor = QwenTextExtractor(\n    backend=QwenVLLMConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n)\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#cuda-out-of-memory","title":"\"CUDA out of memory\"","text":"<pre><code># Use smaller model\npip install omnidocs[pytorch]\n# Then use Qwen3-VL-2B instead of 8B\n</code></pre>"},{"location":"installation/#no-module-named-omnidocs","title":"\"No module named 'omnidocs'\"","text":"<pre><code># Reinstall\npip uninstall omnidocs\npip install omnidocs\n</code></pre>"},{"location":"installation/#tesseract-not-found","title":"\"tesseract not found\"","text":"<pre><code># Install system package\nbrew install tesseract        # macOS\nsudo apt install tesseract-ocr  # Linux\n</code></pre>"},{"location":"installation/#pytorchcuda-version-mismatch","title":"PyTorch/CUDA version mismatch","text":"<pre><code># Check versions\npython -c \"import torch; print(torch.__version__, torch.version.cuda)\"\n\n# Reinstall matching versions\npip install torch torchvision --index-url https://download.pytorch.org/whl/cu121\n</code></pre>"},{"location":"installation/#flash-attention-compilation-fails","title":"Flash Attention compilation fails","text":"<pre><code># Install build tools\nsudo apt install build-essential ninja-build\n\n# Or use pre-built wheel instead\n</code></pre>"},{"location":"installation/#environment-setup","title":"Environment Setup","text":""},{"location":"installation/#recommended-virtual-environment","title":"Recommended: Virtual Environment","text":"<pre><code># Create environment\npython -m venv .venv\nsource .venv/bin/activate  # Linux/macOS\n# or\n.venv\\Scripts\\activate     # Windows\n\n# Install\npip install omnidocs[pytorch]\n</code></pre>"},{"location":"installation/#with-uv-faster","title":"With uv (Faster)","text":"<pre><code># Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create environment and install\nuv venv\nsource .venv/bin/activate\nuv pip install omnidocs[pytorch]\n</code></pre>"},{"location":"installation/#with-conda","title":"With conda","text":"<pre><code>conda create -n omnidocs python=3.11\nconda activate omnidocs\npip install omnidocs[pytorch]\n</code></pre>"},{"location":"installation/#verify-full-installation","title":"Verify Full Installation","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\n# Load a test image\nfrom PIL import Image\nimport requests\nfrom io import BytesIO\n\nurl = \"https://raw.githubusercontent.com/adithya-s-k/OmniDocs/main/assets/sample_page.png\"\nresponse = requests.get(url)\nimage = Image.open(BytesIO(response.content))\n\n# Test layout extraction\nlayout = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cpu\"))\nresult = layout.extract(image)\n\nprint(f\"Detected {len(result.bboxes)} elements\")\nprint(\"OmniDocs is ready!\")\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li>Getting Started - First extraction in 5 minutes</li> <li>Usage - Tasks, models, and workflows</li> </ul>"},{"location":"contributing/","title":"Contributing to OmniDocs","text":"<p>Thank you for your interest in contributing to OmniDocs!</p>"},{"location":"contributing/#quick-links","title":"Quick Links","text":"<ul> <li>Development Workflow - 6-phase implementation process</li> <li>Testing Guide - How to run tests (local, MLX, Modal GPU)</li> <li>Adding Models - How to add new model support</li> <li>Style Guide - Code standards and conventions</li> </ul>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#1-clone-the-repository","title":"1. Clone the repository","text":"<pre><code>git clone https://github.com/adithya-s-k/OmniDocs.git\ncd OmniDocs/Omnidocs\n</code></pre>"},{"location":"contributing/#2-install-dependencies-with-uv","title":"2. Install dependencies with uv","text":"<pre><code># Install uv if you don't have it\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Sync dependencies\nuv sync\n</code></pre>"},{"location":"contributing/#3-run-tests","title":"3. Run tests","text":"<pre><code># Run pytest unit tests\nuv run pytest tests/ -v\n\n# Run local CPU/MLX tests\nuv run python -m tests.runners.local_runner \\\n    --image tests/fixtures/images/test_simple.png \\\n    --cpu-only\n\n# See full testing guide\n</code></pre> <p>For comprehensive testing instructions including GPU tests on Modal, see the Testing Guide.</p>"},{"location":"contributing/#project-structure","title":"Project Structure","text":"<pre><code>Omnidocs/\n\u251c\u2500\u2500 omnidocs/          # Main package\n\u2502   \u251c\u2500\u2500 document.py    # Document loading\n\u2502   \u251c\u2500\u2500 tasks/         # Task extractors\n\u2502   \u251c\u2500\u2500 inference/     # Backend implementations\n\u2502   \u2514\u2500\u2500 utils/         # Utilities\n\u251c\u2500\u2500 tests/             # Test suite\n\u2502   \u251c\u2500\u2500 fixtures/      # Test data (PDFs, images)\n\u2502   \u2514\u2500\u2500 tasks/         # Task tests\n\u2514\u2500\u2500 docs/              # Documentation\n</code></pre>"},{"location":"contributing/#design-documents","title":"Design Documents","text":"<p>Read Before Implementing</p> <p>Before implementing new features, read the architecture docs:</p> <ul> <li>Architecture Overview - System design</li> <li>Backend System - Multi-backend support</li> <li>Config Pattern - Configuration design</li> </ul>"},{"location":"contributing/#building-documentation","title":"Building Documentation","text":"<pre><code># Install docs dependencies\nuv sync --group docs\n\n# Serve docs with live reload\nuv run mkdocs serve\n\n# Open http://127.0.0.1:8000\n</code></pre>"},{"location":"contributing/#need-help","title":"Need Help?","text":"<ul> <li>Open an issue</li> <li>Check the Roadmap</li> </ul>"},{"location":"contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the Apache 2.0 License.</p>"},{"location":"contributing/adding-models/","title":"Adding New Models to OmniDocs","text":"<p>This guide provides step-by-step instructions for integrating a new model into OmniDocs.</p>"},{"location":"contributing/adding-models/#before-you-start","title":"Before You Start","text":"<ol> <li>Read Workflow - Understand the 6-phase process</li> <li>Read IMPLEMENTATION_PLAN/BACKEND_ARCHITECTURE.md</li> <li>Verify the model you want to add doesn't already exist</li> </ol>"},{"location":"contributing/adding-models/#step-1-create-experiment-script","title":"Step 1: Create Experiment Script","text":"<p>Create a standalone test script in <code>scripts/</code> to verify the model works.</p>"},{"location":"contributing/adding-models/#for-gpu-models-pytorchvllm","title":"For GPU Models (PyTorch/VLLM)","text":"<p>Create <code>scripts/text_extract/modal_mymodel_pytorch.py</code>:</p> <pre><code>import modal\nfrom pathlib import Path\n\napp = modal.App(\"test-mymodel\")\n\n# Standard CUDA configuration\ncuda_version = \"12.4.0\"\nflavor = \"devel\"\noperating_sys = \"ubuntu22.04\"\ntag = f\"{cuda_version}-{flavor}-{operating_sys}\"\n\n# Base image (cached across scripts)\nBASE_IMAGE = (\n    modal.Image.from_registry(f\"nvidia/cuda:{tag}\", add_python=\"3.12\")\n    .apt_install(\"libgl1-mesa-glx\", \"libglib2.0-0\")\n    .uv_pip_install(\n        \"torch==2.5.1\",\n        \"torchvision\",\n        \"torchaudio\",\n        \"transformers\",\n        \"pillow\",\n        \"numpy\",\n        \"pydantic\",\n        \"huggingface_hub\",\n        \"accelerate\",\n    )\n    .env({\n        \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",\n        \"HF_HOME\": \"/data/.cache\",\n    })\n)\n\n# Model-specific dependencies\nIMAGE = BASE_IMAGE.uv_pip_install(\"mymodel-package\")\n\nvolume = modal.Volume.from_name(\"omnidocs\", create_if_missing=True)\nsecret = modal.Secret.from_name(\"adithya-hf-wandb\")\n\n@app.function(\n    image=IMAGE,\n    gpu=\"A10G:1\",\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=600,\n)\ndef test_model():\n    \"\"\"Test model loading and inference.\"\"\"\n    import torch\n    from transformers import AutoModelForCausalLM, AutoProcessor\n    from PIL import Image\n    import requests\n    from io import BytesIO\n\n    MODEL_NAME = \"vendor/mymodel\"\n\n    # Load model\n    print(\"Loading model...\")\n    processor = AutoProcessor.from_pretrained(\n        MODEL_NAME,\n        trust_remote_code=True,\n    )\n\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        trust_remote_code=True,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n    ).eval()\n\n    # Load test image\n    print(\"Loading test image...\")\n    response = requests.get(\"https://example.com/test.png\")\n    image = Image.open(BytesIO(response.content))\n\n    # Run inference\n    print(\"Running inference...\")\n    inputs = processor(text=\"Extract text\", images=image, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_new_tokens=4096)\n\n    result = processor.decode(outputs[0], skip_special_tokens=True)\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"RESULT:\")\n    print(\"=\"*60)\n    print(result)\n\n    return {\"success\": True, \"length\": len(result)}\n\n@app.local_entrypoint()\ndef main():\n    result = test_model.remote()\n    print(f\"\\nTest completed: {result['success']}\")\n</code></pre>"},{"location":"contributing/adding-models/#for-api-models-local","title":"For API Models (Local)","text":"<p>Create <code>scripts/text_extract/litellm_mymodel_text.py</code>:</p> <pre><code>import os\nfrom PIL import Image\nfrom openai import OpenAI\nimport base64\nfrom io import BytesIO\n\ndef encode_image(image: Image.Image) -&gt; str:\n    \"\"\"Encode PIL Image to base64.\"\"\"\n    buffered = BytesIO()\n    image.save(buffered, format=\"PNG\")\n    img_bytes = buffered.getvalue()\n    return f\"data:image/png;base64,{base64.b64encode(img_bytes).decode()}\"\n\n# Initialize client\nclient = OpenAI(\n    base_url=\"https://openrouter.ai/api/v1\",\n    api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n)\n\n# Test image\nimage = Image.open(\"test.png\")\n\n# Run inference\nresponse = client.chat.completions.create(\n    model=\"vendor/mymodel\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"image_url\", \"image_url\": {\"url\": encode_image(image)}},\n                {\"type\": \"text\", \"text\": \"Extract text in markdown format\"}\n            ]\n        }\n    ],\n    max_tokens=4096,\n)\n\nprint(response.choices[0].message.content)\n</code></pre>"},{"location":"contributing/adding-models/#run-and-validate","title":"Run and Validate","text":"<pre><code>cd scripts/text_extract/\n\n# GPU models\nmodal run modal_mymodel_pytorch.py\n\n# API models (local)\npython litellm_mymodel_text.py\n</code></pre> <p>Checklist: - [ ] Model loads - [ ] Inference runs - [ ] Output is reasonable - [ ] Error handling works</p>"},{"location":"contributing/adding-models/#step-2-decide-single-or-multi-backend","title":"Step 2: Decide: Single or Multi-Backend?","text":""},{"location":"contributing/adding-models/#single-backend-model","title":"Single-Backend Model","text":"<p>When: Model only works with one inference backend (e.g., YOLO-based models)</p> <p>File Structure: <pre><code>omnidocs/tasks/text_extraction/\n\u251c\u2500\u2500 mymodel.py           # Config + Extractor in same file\n</code></pre></p> <p>Example: <pre><code>from pydantic import BaseModel, Field\n\nclass MyModelConfig(BaseModel):\n    \"\"\"Config for MyModel (PyTorch only).\"\"\"\n    device: str = Field(default=\"cuda\")\n    model_name: str = Field(default=\"vendor/mymodel\")\n    class Config:\n        extra = \"forbid\"\n\nclass MyModelTextExtractor:\n    def __init__(self, config: MyModelConfig):\n        self.config = config\n        # Load model\n\n    def extract(self, image, output_format=\"markdown\"):\n        # Extraction logic\n</code></pre></p>"},{"location":"contributing/adding-models/#multi-backend-model","title":"Multi-Backend Model","text":"<p>When: Model can use multiple backends (PyTorch, VLLM, MLX, API)</p> <p>File Structure: <pre><code>omnidocs/tasks/text_extraction/\n\u251c\u2500\u2500 mymodel.py           # Main extractor class\n\u2514\u2500\u2500 mymodel/             # Backend configurations\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 pytorch.py       # MyModelPyTorchConfig\n    \u251c\u2500\u2500 vllm.py          # MyModelVLLMConfig\n    \u251c\u2500\u2500 mlx.py           # MyModelMLXConfig\n    \u2514\u2500\u2500 api.py           # MyModelAPIConfig\n</code></pre></p> <p>Example: <pre><code>from typing import Union\n\nQwenBackendConfig = Union[\n    MyModelPyTorchConfig,\n    MyModelVLLMConfig,\n    MyModelMLXConfig,\n    MyModelAPIConfig,\n]\n\nclass MyModelTextExtractor:\n    def __init__(self, backend: MyModelBackendConfig):\n        self.backend_config = backend\n        self._backend = self._create_backend()\n\n    def _create_backend(self):\n        if isinstance(self.backend_config, MyModelPyTorchConfig):\n            # Create PyTorch backend\n        elif isinstance(self.backend_config, MyModelVLLMConfig):\n            # Create VLLM backend\n        # ...\n</code></pre></p>"},{"location":"contributing/adding-models/#step-3-create-config-classes","title":"Step 3: Create Config Classes","text":""},{"location":"contributing/adding-models/#single-backend-config","title":"Single-Backend Config","text":"<pre><code># omnidocs/tasks/text_extraction/mymodel.py\n\nfrom pydantic import BaseModel, Field\nfrom typing import Literal, Optional\n\nclass MyModelConfig(BaseModel):\n    \"\"\"Configuration for MyModel (PyTorch only).\"\"\"\n\n    # Required\n    model: str = Field(\n        default=\"vendor/mymodel-8b\",\n        description=\"Model identifier\"\n    )\n\n    # Optional with defaults\n    device: str = Field(\n        default=\"cuda\",\n        description=\"Device to run on (cuda/cpu)\"\n    )\n\n    torch_dtype: Literal[\"float16\", \"bfloat16\", \"float32\"] = Field(\n        default=\"bfloat16\",\n        description=\"Torch data type\"\n    )\n\n    # Numeric with validation\n    max_new_tokens: int = Field(\n        default=4096,\n        ge=1,\n        le=32768,\n        description=\"Maximum tokens to generate\"\n    )\n\n    class Config:\n        extra = \"forbid\"  # CRITICAL: Catch typos\n</code></pre>"},{"location":"contributing/adding-models/#multi-backend-configs","title":"Multi-Backend Configs","text":"<pre><code># omnidocs/tasks/text_extraction/mymodel/pytorch.py\n\nclass MyModelPyTorchConfig(BaseModel):\n    \"\"\"PyTorch backend for MyModel.\"\"\"\n\n    model: str = Field(..., description=\"Model ID\")\n    device: str = Field(default=\"cuda\")\n    torch_dtype: Literal[\"float16\", \"bfloat16\", \"float32\"] = Field(default=\"bfloat16\")\n    device_map: Optional[str] = Field(default=\"auto\")\n\n    class Config:\n        extra = \"forbid\"\n\n# omnidocs/tasks/text_extraction/mymodel/vllm.py\n\nclass MyModelVLLMConfig(BaseModel):\n    \"\"\"VLLM backend for MyModel.\"\"\"\n\n    model: str = Field(..., description=\"Model ID\")\n    tensor_parallel_size: int = Field(default=1, ge=1)\n    gpu_memory_utilization: float = Field(default=0.9, ge=0.1, le=1.0)\n    max_model_len: int = Field(default=32768)\n\n    class Config:\n        extra = \"forbid\"\n\n# omnidocs/tasks/text_extraction/mymodel/api.py\n\nclass MyModelAPIConfig(BaseModel):\n    \"\"\"API backend for MyModel.\"\"\"\n\n    model: str = Field(..., description=\"API model name\")\n    api_key: str = Field(..., description=\"API key\")\n    base_url: Optional[str] = Field(None, description=\"API base URL\")\n\n    class Config:\n        extra = \"forbid\"\n</code></pre>"},{"location":"contributing/adding-models/#step-4-create-extractor-class","title":"Step 4: Create Extractor Class","text":""},{"location":"contributing/adding-models/#single-backend-extractor","title":"Single-Backend Extractor","text":"<pre><code># omnidocs/tasks/text_extraction/mymodel.py\n\nfrom .base import BaseTextExtractor\nfrom .models import TextOutput\nfrom PIL import Image\n\nclass MyModelTextExtractor(BaseTextExtractor):\n    \"\"\"MyModel text extractor (PyTorch only).\"\"\"\n\n    def __init__(self, config: MyModelConfig):\n        self.config = config\n        self._load_model()\n\n    def _load_model(self):\n        \"\"\"Load model with PyTorch.\"\"\"\n        import torch\n        from transformers import AutoModelForCausalLM, AutoProcessor\n\n        self.processor = AutoProcessor.from_pretrained(\n            self.config.model,\n            trust_remote_code=True,\n        )\n\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.config.model,\n            trust_remote_code=True,\n            torch_dtype=torch.bfloat16 if self.config.torch_dtype == \"bfloat16\" else torch.float16,\n            device_map=self.config.device,\n        ).eval()\n\n    def extract(\n        self,\n        image: Image.Image,\n        output_format: str = \"markdown\",\n        **kwargs\n    ) -&gt; TextOutput:\n        \"\"\"Extract text from image.\"\"\"\n        import torch\n\n        prompt = self._get_prompt(output_format)\n\n        inputs = self.processor(\n            text=prompt,\n            images=image,\n            return_tensors=\"pt\"\n        ).to(self.model.device)\n\n        with torch.no_grad():\n            outputs = self.model.generate(\n                **inputs,\n                max_new_tokens=self.config.max_new_tokens,\n            )\n\n        raw_output = self.processor.decode(outputs[0], skip_special_tokens=True)\n\n        return TextOutput(\n            content=raw_output,\n            format=output_format,\n            raw_output=raw_output,\n        )\n\n    def _get_prompt(self, output_format: str) -&gt; str:\n        \"\"\"Get extraction prompt.\"\"\"\n        if output_format == \"markdown\":\n            return \"Extract text in markdown format...\"\n        elif output_format == \"html\":\n            return \"Extract text in HTML format...\"\n        else:\n            return \"Extract all text...\"\n</code></pre>"},{"location":"contributing/adding-models/#multi-backend-extractor","title":"Multi-Backend Extractor","text":"<pre><code># omnidocs/tasks/text_extraction/mymodel.py\n\nfrom typing import Union, Optional\nfrom .base import BaseTextExtractor\nfrom .models import TextOutput\nfrom omnidocs.tasks.text_extraction.mymodel import (\n    MyModelPyTorchConfig,\n    MyModelVLLMConfig,\n    MyModelAPIConfig,\n)\n\nMyModelBackendConfig = Union[\n    MyModelPyTorchConfig,\n    MyModelVLLMConfig,\n    MyModelAPIConfig,\n]\n\nclass MyModelTextExtractor(BaseTextExtractor):\n    \"\"\"MyModel text extractor with multi-backend support.\"\"\"\n\n    def __init__(self, backend: MyModelBackendConfig):\n        self.backend_config = backend\n        self._backend = self._create_backend()\n\n    def _create_backend(self):\n        \"\"\"Create appropriate backend based on config type.\"\"\"\n        if isinstance(self.backend_config, MyModelPyTorchConfig):\n            try:\n                from omnidocs.inference.pytorch import PyTorchInference\n            except ImportError:\n                raise ImportError(\n                    \"PyTorch backend requires torch and transformers. \"\n                    \"Install with: pip install omnidocs[pytorch]\"\n                )\n            return PyTorchInference(self.backend_config)\n\n        elif isinstance(self.backend_config, MyModelVLLMConfig):\n            try:\n                from omnidocs.inference.vllm import VLLMInference\n            except ImportError:\n                raise ImportError(\n                    \"VLLM backend requires vllm. \"\n                    \"Install with: pip install omnidocs[vllm]\"\n                )\n            return VLLMInference(self.backend_config)\n\n        elif isinstance(self.backend_config, MyModelAPIConfig):\n            try:\n                from omnidocs.inference.api import APIInference\n            except ImportError:\n                raise ImportError(\n                    \"API backend requires openai. \"\n                    \"Install with: pip install omnidocs[api]\"\n                )\n            return APIInference(self.backend_config)\n\n        else:\n            raise TypeError(f\"Unknown backend: {type(self.backend_config)}\")\n\n    def extract(\n        self,\n        image: Image.Image,\n        output_format: str = \"markdown\",\n        **kwargs\n    ) -&gt; TextOutput:\n        \"\"\"Extract text from image.\"\"\"\n        prompt = self._get_prompt(output_format)\n        raw_output = self._backend.infer(image, prompt)\n\n        return TextOutput(\n            content=raw_output,\n            format=output_format,\n            raw_output=raw_output,\n        )\n</code></pre>"},{"location":"contributing/adding-models/#step-5-update-package-exports","title":"Step 5: Update Package Exports","text":"<p>Edit <code>omnidocs/tasks/text_extraction/__init__.py</code>:</p> <pre><code># Existing imports\nfrom .base import BaseTextExtractor\nfrom .models import TextOutput\n\n# New imports - single backend\nfrom .mymodel import MyModelTextExtractor, MyModelConfig\n\n# OR - multi-backend\nfrom .mymodel import MyModelTextExtractor\nfrom .mymodel import (\n    MyModelPyTorchConfig,\n    MyModelVLLMConfig,\n    MyModelAPIConfig,\n)\n\n__all__ = [\n    # Base\n    \"BaseTextExtractor\",\n    \"TextOutput\",\n\n    # Existing\n    \"QwenTextExtractor\",\n    \"DotsOCRTextExtractor\",\n\n    # New\n    \"MyModelTextExtractor\",\n    \"MyModelConfig\",  # or MyModelPyTorchConfig, MyModelVLLMConfig, etc.\n]\n</code></pre>"},{"location":"contributing/adding-models/#step-6-add-dependencies","title":"Step 6: Add Dependencies","text":"<pre><code>cd Omnidocs/\n\n# Add model-specific package\nuv add --group pytorch mymodel-package\n\n# Sync virtual environment\nuv sync\n</code></pre>"},{"location":"contributing/adding-models/#step-7-write-tests","title":"Step 7: Write Tests","text":"<p>Create <code>Omnidocs/tests/tasks/text_extraction/test_mymodel.py</code>:</p> <pre><code>import pytest\nfrom omnidocs.tasks.text_extraction import MyModelTextExtractor, MyModelConfig\n\nclass TestMyModelConfig:\n    \"\"\"Test configuration validation.\"\"\"\n\n    def test_valid_config(self):\n        config = MyModelConfig(model=\"vendor/mymodel\")\n        assert config.device == \"cuda\"\n        assert config.torch_dtype == \"bfloat16\"\n\n    def test_invalid_dtype(self):\n        with pytest.raises(ValueError):\n            MyModelConfig(model=\"vendor/mymodel\", torch_dtype=\"invalid\")\n\n    def test_extra_fields_forbidden(self):\n        with pytest.raises(ValueError):\n            MyModelConfig(model=\"vendor/mymodel\", invalid_param=\"value\")\n\nclass TestMyModelExtractor:\n    \"\"\"Test extractor functionality.\"\"\"\n\n    @pytest.fixture\n    def sample_image(self):\n        from PIL import Image\n        return Image.new(\"RGB\", (800, 600), color=\"white\")\n\n    def test_extract_markdown(self, sample_image):\n        config = MyModelConfig(model=\"vendor/mymodel\", device=\"cpu\")\n        extractor = MyModelTextExtractor(config=config)\n\n        result = extractor.extract(sample_image, output_format=\"markdown\")\n\n        assert result.format.value == \"markdown\"\n        assert isinstance(result.content, str)\n</code></pre> <p>Run tests: <pre><code>cd Omnidocs/\nuv run pytest tests/tasks/text_extraction/test_mymodel.py -v\n</code></pre></p>"},{"location":"contributing/adding-models/#step-8-integration-test","title":"Step 8: Integration Test","text":"<p>Create <code>scripts/text_extract_omnidocs/modal_mymodel_text_hf.py</code>:</p> <pre><code>\"\"\"Test MyModel through Omnidocs package on Modal.\"\"\"\n\nimport modal\n\napp = modal.App(\"test-mymodel-omnidocs\")\n\n# Standard image with Omnidocs\nOMNIDOCS_IMAGE = (\n    modal.Image.from_registry(\"nvidia/cuda:12.4.0-devel-ubuntu22.04\", add_python=\"3.12\")\n    .apt_install(\"libgl1-mesa-glx\", \"libglib2.0-0\")\n    .uv_pip_install(\"torch\", \"transformers\", \"pillow\")\n    .run_commands(\"uv pip install -e /pkg/Omnidocs --system\")\n    .env({\"HF_HOME\": \"/data/.cache\"})\n)\n\nvolume = modal.Volume.from_name(\"omnidocs\", create_if_missing=True)\nsecret = modal.Secret.from_name(\"adithya-hf-wandb\")\npkg_mount = modal.Mount.from_local_dir(\n    Path(__file__).parent.parent.parent / \"Omnidocs\",\n    remote_path=\"/pkg/Omnidocs\"\n)\n\n@app.function(\n    image=OMNIDOCS_IMAGE,\n    gpu=\"A10G:1\",\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    mounts=[pkg_mount],\n    timeout=600,\n)\ndef test_omnidocs_mymodel():\n    \"\"\"Test MyModel through Omnidocs.\"\"\"\n    from omnidocs.tasks.text_extraction import MyModelTextExtractor\n    from omnidocs.tasks.text_extraction.mymodel import MyModelPyTorchConfig\n    from PIL import Image\n\n    extractor = MyModelTextExtractor(\n        config=MyModelPyTorchConfig(\n            model=\"vendor/mymodel\",\n            device=\"cuda\",\n        )\n    )\n\n    test_image = Image.new(\"RGB\", (800, 600), color=\"white\")\n    result = extractor.extract(test_image, output_format=\"markdown\")\n\n    assert result.format.value == \"markdown\"\n    assert len(result.content) &gt; 0\n\n    return {\"success\": True, \"length\": len(result.content)}\n\n@app.local_entrypoint()\ndef main():\n    result = test_omnidocs_mymodel.remote()\n    print(f\"Test passed: {result}\")\n</code></pre>"},{"location":"contributing/adding-models/#step-9-lint-checks","title":"Step 9: Lint Checks","text":"<pre><code>cd Omnidocs/\n\n# Format code\nuv run black omnidocs/\n\n# Type checking\nuv run mypy omnidocs/\n</code></pre>"},{"location":"contributing/adding-models/#step-10-submit-pr","title":"Step 10: Submit PR","text":"<p>Follow Workflow - Phase 5: Pull Request.</p>"},{"location":"contributing/adding-models/#checklist","title":"Checklist","text":"<ul> <li>[ ] Experiment script created and tested</li> <li>[ ] Backend type chosen (single or multi)</li> <li>[ ] Config classes written with validation</li> <li>[ ] Extractor class implemented</li> <li>[ ] Package exports updated</li> <li>[ ] Dependencies added (uv add)</li> <li>[ ] Unit tests &gt;80% coverage</li> <li>[ ] Integration test passing on Modal</li> <li>[ ] Black formatting applied</li> <li>[ ] Mypy checks passing</li> <li>[ ] PR created and reviewed</li> </ul>"},{"location":"contributing/adding-models/#next-steps","title":"Next Steps","text":"<p>After PR approval and merge, follow Workflow - Phase 6: Release to publish the new version.</p>"},{"location":"contributing/style-guide/","title":"Style Guide","text":"<p>OmniDocs follows consistent code and documentation standards to maintain quality and clarity.</p>"},{"location":"contributing/style-guide/#code-style","title":"Code Style","text":""},{"location":"contributing/style-guide/#type-hints","title":"Type Hints","text":"<p>All public APIs must have complete type hints:</p> <pre><code># \u2705 GOOD\ndef extract(self, image: Image.Image, output_format: str = \"markdown\") -&gt; TextOutput:\n    \"\"\"Extract text from image.\"\"\"\n    pass\n\n# \u274c BAD\ndef extract(self, image, output_format=\"markdown\"):\n    \"\"\"Extract text from image.\"\"\"\n    pass\n</code></pre>"},{"location":"contributing/style-guide/#docstrings","title":"Docstrings","text":"<p>Use Google-style docstrings for all public classes and methods:</p> <pre><code>def extract(\n    self,\n    image: Image.Image,\n    output_format: str = \"markdown\",\n    include_layout: bool = False,\n) -&gt; TextOutput:\n    \"\"\"Extract text from document image.\n\n    Converts document images to formatted text using the configured model\n    and backend.\n\n    Args:\n        image: PIL Image to extract text from.\n        output_format: Output format (\"markdown\" or \"html\").\n        include_layout: Include layout information in output.\n\n    Returns:\n        TextOutput containing extracted content.\n\n    Raises:\n        ValueError: If output_format is invalid.\n        RuntimeError: If model is not loaded.\n\n    Example:\n        &gt;&gt;&gt; extractor = QwenTextExtractor(backend=config)\n        &gt;&gt;&gt; result = extractor.extract(image, output_format=\"markdown\")\n        &gt;&gt;&gt; print(result.content)\n    \"\"\"\n</code></pre>"},{"location":"contributing/style-guide/#pydantic-configs","title":"Pydantic Configs","text":"<p>All config classes must follow these rules:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import Literal, Optional\n\nclass MyConfig(BaseModel):\n    \"\"\"Clear docstring explaining purpose.\"\"\"\n\n    # Required parameters\n    required_param: str = Field(\n        ...,  # Indicates required\n        description=\"Description of parameter\"\n    )\n\n    # Optional with defaults\n    optional_param: str = Field(\n        default=\"default_value\",\n        description=\"Description\"\n    )\n\n    # Enum/Literal for fixed choices\n    dtype: Literal[\"float16\", \"bfloat16\", \"float32\"] = Field(\n        default=\"bfloat16\",\n        description=\"Data type\"\n    )\n\n    # Numeric with bounds\n    confidence: float = Field(\n        default=0.5,\n        ge=0.0,\n        le=1.0,\n        description=\"Confidence threshold\"\n    )\n\n    # Optional nullable\n    cache_dir: Optional[str] = Field(\n        default=None,\n        description=\"Optional cache directory\"\n    )\n\n    class Config:\n        extra = \"forbid\"  # CRITICAL: Catch typos\n</code></pre> <p>Rules: - \u2705 All parameters use <code>Field(...)</code> - \u2705 All parameters have descriptions - \u2705 Type hints for everything - \u2705 Validation rules (ge, le, Literal) - \u2705 <code>extra = \"forbid\"</code> to catch mistakes - \u2705 Class-level docstring</p>"},{"location":"contributing/style-guide/#error-handling","title":"Error Handling","text":"<p>Provide informative error messages with installation instructions:</p> <pre><code># \u2705 GOOD\nif isinstance(self.backend_config, QwenPyTorchConfig):\n    try:\n        from omnidocs.inference.pytorch import PyTorchInference\n    except ImportError:\n        raise ImportError(\n            \"PyTorch backend requires torch and transformers. \"\n            \"Install with: pip install omnidocs[pytorch]\"\n        )\n    return PyTorchInference(self.backend_config)\n\n# \u274c BAD\nfrom omnidocs.inference.pytorch import PyTorchInference  # Hard requirement\n</code></pre>"},{"location":"contributing/style-guide/#testing-standards","title":"Testing Standards","text":""},{"location":"contributing/style-guide/#test-structure","title":"Test Structure","text":"<p>Use pytest fixtures and clear test names:</p> <pre><code>import pytest\nfrom unittest.mock import Mock, patch\n\nclass TestMyExtractor:\n    \"\"\"Test suite for MyExtractor.\"\"\"\n\n    @pytest.fixture\n    def sample_image(self):\n        \"\"\"Create sample test image.\"\"\"\n        from PIL import Image\n        return Image.new(\"RGB\", (800, 600), color=\"white\")\n\n    @pytest.fixture\n    def config(self):\n        \"\"\"Create test config.\"\"\"\n        return MyConfig(model=\"test-model\", device=\"cpu\")\n\n    def test_valid_config(self, config):\n        \"\"\"Test that valid config initializes.\"\"\"\n        assert config.device == \"cpu\"\n        assert config.model == \"test-model\"\n\n    def test_invalid_param_raises(self):\n        \"\"\"Test that invalid params raise ValidationError.\"\"\"\n        with pytest.raises(ValueError):\n            MyConfig(model=\"test\", invalid_param=\"value\")\n\n    def test_extract_returns_output(self, sample_image, config):\n        \"\"\"Test that extract method returns Output.\"\"\"\n        extractor = MyExtractor(config=config)\n        result = extractor.extract(sample_image)\n\n        assert isinstance(result, MyOutput)\n        assert len(result.content) &gt; 0\n\n    @pytest.mark.skipif(\n        not torch.cuda.is_available(),\n        reason=\"Requires GPU\"\n    )\n    def test_extract_on_gpu(self, sample_image, config):\n        \"\"\"Test GPU extraction.\"\"\"\n        config.device = \"cuda\"\n        extractor = MyExtractor(config=config)\n        result = extractor.extract(sample_image)\n        assert result is not None\n</code></pre>"},{"location":"contributing/style-guide/#test-coverage","title":"Test Coverage","text":"<p>Target &gt;80% coverage for all code:</p> <pre><code># Run tests with coverage\nuv run pytest tests/ --cov=omnidocs --cov-report=html\n\n# Check coverage report\nopen htmlcov/index.html\n</code></pre>"},{"location":"contributing/style-guide/#assertion-best-practices","title":"Assertion Best Practices","text":"<pre><code># \u2705 GOOD - Specific assertions\nassert result.format.value == \"markdown\"\nassert len(result.content) &gt; 100\nassert result.content_length == len(result.content)\n\n# \u274c BAD - Generic assertions\nassert result is not None\nassert bool(result)\n</code></pre>"},{"location":"contributing/style-guide/#documentation-standards","title":"Documentation Standards","text":""},{"location":"contributing/style-guide/#markdown-style","title":"Markdown Style","text":"<pre><code># Main Title (H1)\n\nBrief introductory paragraph explaining purpose.\n\n## Section (H2)\n\nSubsection content.\n\n### Subsection (H3)\n\nUse code blocks for examples:\n\n\\`\\`\\`python\n# Python code example\nresult = extractor.extract(image)\n\\`\\`\\`\n\n### Lists\n\n- Bullet point 1\n- Bullet point 2\n  - Nested point\n  - Another nested\n\n1. Numbered item 1\n2. Numbered item 2\n\n### Tables\n\n| Header 1 | Header 2 |\n|----------|----------|\n| Cell 1   | Cell 2   |\n\n### Links\n\n[Link text](../path/to/file.md)\n</code></pre>"},{"location":"contributing/style-guide/#code-examples","title":"Code Examples","text":"<p>All code examples must be: 1. Complete - Can be copy-pasted and run 2. Runnable - All imports included 3. Correct - Verified against actual API 4. Documented - Comments explaining key parts</p> <pre><code>### Example: Extract Text\n\nHere's a complete example:\n\n\\`\\`\\`python\nfrom omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n\n# Load document\ndoc = Document.from_pdf(\"document.pdf\")\n\n# Create extractor\nextractor = QwenTextExtractor(\n    backend=QwenPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n    )\n)\n\n# Extract text from first page\nresult = extractor.extract(\n    doc.get_page(0),\n    output_format=\"markdown\"\n)\n\n# Print result\nprint(result.content)\n\\`\\`\\`\n</code></pre>"},{"location":"contributing/style-guide/#documentation-requirements","title":"Documentation Requirements","text":"<p>Every user-facing documentation file must include:</p> <ol> <li>Title (H1) - Clear, descriptive</li> <li>Summary (1-2 paragraphs) - What is this doc about?</li> <li>Table of Contents - For long docs (&gt;2000 words)</li> <li>Main Content - Progressive disclosure (simple \u2192 complex)</li> <li>Code Examples - 3-5 complete, runnable examples</li> <li>Comparison Table - When/what/why decisions</li> <li>Troubleshooting - Common issues and solutions</li> <li>See Also - Links to related docs</li> <li>YAML Frontmatter - For AI-friendly parsing</li> </ol> <pre><code>---\ntitle: \"Task Name\"\ndescription: \"Short description\"\ncategory: \"guides\"\ndifficulty: \"intermediate\"\ntime_estimate: \"15 minutes\"\nkeywords: [\"text extraction\", \"markdown\", \"pdf\"]\n---\n\n# Task Name\n\nBrief intro (1-2 sentences).\n\n## When to Use This Guide\n\n## Quick Example\n\n## Main Content\n\n## Advanced Features\n\n## Troubleshooting\n\n## Next Steps\n</code></pre>"},{"location":"contributing/style-guide/#git-commit-standards","title":"Git &amp; Commit Standards","text":""},{"location":"contributing/style-guide/#branch-naming","title":"Branch Naming","text":"<pre><code>feature/add-qwen-support\nbugfix/fix-memory-leak\ndocs/update-installation-guide\nrefactor/simplify-backend-system\n</code></pre>"},{"location":"contributing/style-guide/#commit-messages","title":"Commit Messages","text":"<p>Use imperative mood, no capitals, no periods:</p> <pre><code># \u2705 GOOD\nfeat: add Qwen text extraction support\nfix: prevent GPU memory leak in batch processing\ndocs: update installation guide\nrefactor: simplify backend selection logic\n\n# \u274c BAD\nAdded Qwen support\nFix memory leak\nUpdate docs\nRefactored backend code\n</code></pre>"},{"location":"contributing/style-guide/#commit-content","title":"Commit Content","text":"<ul> <li>One logical change per commit</li> <li>All tests passing</li> <li>Code properly formatted</li> <li>No debugging code or comments</li> </ul>"},{"location":"contributing/style-guide/#python-code-formatting","title":"Python Code Formatting","text":""},{"location":"contributing/style-guide/#use-black","title":"Use Black","text":"<pre><code>uv run black omnidocs/ tests/\n</code></pre> <p>Black settings (in <code>pyproject.toml</code>): <pre><code>[tool.black]\nline-length = 100\ntarget-version = [\"py310\", \"py311\", \"py312\"]\n</code></pre></p>"},{"location":"contributing/style-guide/#use-mypy-for-type-checking","title":"Use MyPy for Type Checking","text":"<pre><code>uv run mypy omnidocs/\n</code></pre> <p>MyPy settings (in <code>pyproject.toml</code>): <pre><code>[tool.mypy]\npython_version = \"3.10\"\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = false\ndisallow_incomplete_defs = true\n</code></pre></p>"},{"location":"contributing/style-guide/#import-ordering","title":"Import Ordering","text":"<p>Use isort (configured in pyproject.toml):</p> <pre><code># Standard library\nimport os\nimport sys\nfrom pathlib import Path\n\n# Third party\nimport numpy as np\nimport torch\nfrom transformers import AutoModel\nfrom pydantic import BaseModel\n\n# Local\nfrom omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\n</code></pre>"},{"location":"contributing/style-guide/#quality-checklist","title":"Quality Checklist","text":"<p>Before submitting code:</p> <ul> <li>[ ] All tests pass (<code>uv run pytest</code>)</li> <li>[ ] Coverage &gt;80% (<code>uv run pytest --cov</code>)</li> <li>[ ] Code formatted (<code>uv run black omnidocs/</code>)</li> <li>[ ] Type checked (<code>uv run mypy omnidocs/</code>)</li> <li>[ ] No hard-coded paths or secrets</li> <li>[ ] Docstrings complete</li> <li>[ ] Error messages helpful</li> <li>[ ] Examples runnable</li> <li>[ ] No debug print statements</li> <li>[ ] Git history clean</li> </ul>"},{"location":"contributing/style-guide/#common-patterns","title":"Common Patterns","text":""},{"location":"contributing/style-guide/#backend-selection-pattern","title":"Backend Selection Pattern","text":"<pre><code>def _create_backend(self):\n    \"\"\"Create appropriate backend.\"\"\"\n    if isinstance(self.backend_config, MyModelPyTorchConfig):\n        from omnidocs.inference.pytorch import PyTorchInference\n        return PyTorchInference(self.backend_config)\n    # ... other backends\n    else:\n        raise TypeError(f\"Unknown backend: {type(self.backend_config)}\")\n</code></pre>"},{"location":"contributing/style-guide/#config-validation-pattern","title":"Config Validation Pattern","text":"<pre><code>class MyConfig(BaseModel):\n    param: str = Field(..., description=\"...\")\n\n    @validator(\"param\")\n    def validate_param(cls, v):\n        if v not in [\"valid1\", \"valid2\"]:\n            raise ValueError(f\"Invalid param: {v}\")\n        return v\n\n    class Config:\n        extra = \"forbid\"\n</code></pre>"},{"location":"contributing/style-guide/#test-fixture-pattern","title":"Test Fixture Pattern","text":"<pre><code>@pytest.fixture\ndef config(self):\n    \"\"\"Create test config.\"\"\"\n    return MyConfig(\n        model=\"test-model\",\n        device=\"cpu\",  # Use CPU for tests\n    )\n\n@pytest.fixture\ndef extractor(self, config):\n    \"\"\"Create test extractor.\"\"\"\n    return MyExtractor(config=config)\n</code></pre>"},{"location":"contributing/style-guide/#when-in-doubt","title":"When in Doubt","text":"<ul> <li>Check existing code in <code>/omnidocs/</code> - follow established patterns</li> <li>Read Concepts - understand architecture</li> <li>Check tests in <code>/tests/</code> - see testing patterns</li> <li>Run <code>black</code> and <code>mypy</code> - follow their output</li> </ul> <p>Questions? Open an issue or check CLAUDE.md for development guide.</p>"},{"location":"contributing/testing/","title":"Testing Guide","text":"<p>This guide covers how to run tests for OmniDocs across different platforms and backends.</p>"},{"location":"contributing/testing/#test-architecture","title":"Test Architecture","text":"<p>OmniDocs uses a multi-tier testing approach:</p> Tier Platform Tests Command Local CPU Any machine CPU-based extractors <code>uv run python -m tests.runners.local_runner --cpu-only</code> Local MLX Apple Silicon MLX extractors <code>uv run python -m tests.runners.local_runner --mlx</code> Modal GPU Cloud (L4/A10G) VLLM, PyTorch GPU <code>modal run scripts/.../modal_runner.py</code> pytest Any Unit/integration <code>uv run pytest</code>"},{"location":"contributing/testing/#directory-structure","title":"Directory Structure","text":"<pre><code>Omnidocs/tests/\n\u251c\u2500\u2500 fixtures/\n\u2502   \u2514\u2500\u2500 images/              # Test images\n\u2502       \u2514\u2500\u2500 test_simple.png\n\u251c\u2500\u2500 standalone/              # Standalone test scripts\n\u2502   \u251c\u2500\u2500 text_extraction/\n\u2502   \u2502   \u251c\u2500\u2500 qwen_vllm.py\n\u2502   \u2502   \u251c\u2500\u2500 qwen_pytorch.py\n\u2502   \u2502   \u251c\u2500\u2500 qwen_mlx.py\n\u2502   \u2502   \u251c\u2500\u2500 nanonets_vllm.py\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 layout_extraction/\n\u2502   \u2502   \u251c\u2500\u2500 doclayout_yolo_cpu.py\n\u2502   \u2502   \u251c\u2500\u2500 doclayout_yolo_gpu.py\n\u2502   \u2502   \u251c\u2500\u2500 rtdetr_cpu.py\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u251c\u2500\u2500 ocr_extraction/\n\u2502   \u251c\u2500\u2500 table_extraction/\n\u2502   \u2514\u2500\u2500 reading_order/\n\u251c\u2500\u2500 runners/\n\u2502   \u251c\u2500\u2500 local_runner.py      # Local test runner\n\u2502   \u251c\u2500\u2500 registry.py          # Test registry\n\u2502   \u2514\u2500\u2500 report.py            # Result reporting\n\u251c\u2500\u2500 integration/             # pytest integration tests\n\u2514\u2500\u2500 utils/                   # Test utilities\n</code></pre>"},{"location":"contributing/testing/#running-local-tests","title":"Running Local Tests","text":""},{"location":"contributing/testing/#prerequisites","title":"Prerequisites","text":"<pre><code>cd Omnidocs\n\n# Install with test dependencies\nuv sync --group dev\n\n# For MLX tests (Apple Silicon only)\nuv sync --group mlx\n\n# For OCR tests\nuv sync --group ocr\n</code></pre>"},{"location":"contributing/testing/#using-the-local-runner","title":"Using the Local Runner","text":"<p>The local runner executes standalone test scripts on your machine.</p> <pre><code># Basic usage - run all CPU tests\nuv run python -m tests.runners.local_runner \\\n    --image tests/fixtures/images/test_simple.png \\\n    --cpu-only\n\n# Run all MLX tests (Apple Silicon)\nuv run python -m tests.runners.local_runner \\\n    --image tests/fixtures/images/test_simple.png \\\n    --mlx\n\n# Filter by task\nuv run python -m tests.runners.local_runner \\\n    --image tests/fixtures/images/test_simple.png \\\n    --task layout_extraction \\\n    --cpu-only\n\n# Run specific test\nuv run python -m tests.runners.local_runner \\\n    --image tests/fixtures/images/test_simple.png \\\n    --test doclayout_yolo_cpu\n</code></pre>"},{"location":"contributing/testing/#local-runner-options","title":"Local Runner Options","text":"Option Description Example <code>--image</code> Path to test image (required) <code>tests/fixtures/images/test_simple.png</code> <code>--cpu-only</code> Run only CPU tests <code>--mlx</code> Run only MLX tests <code>--task</code> Filter by task type <code>text_extraction</code>, <code>layout_extraction</code> <code>--test</code> Run specific test <code>doclayout_yolo_cpu</code> <code>--output</code> Output JSON file <code>results.json</code>"},{"location":"contributing/testing/#example-output","title":"Example Output","text":"<pre><code>Running 4 tests\nImage: tests/fixtures/images/test_simple.png\n---------------------------------------------------------------------------\n  Running qwen_layout_mlx... [PASS] (3.78s)\n  Running qwen_layout_api... [FAIL] (0.00s)\n    Error: api_key required\n  Running doclayout_yolo_cpu... [PASS] (0.46s)\n  Running rtdetr_cpu... [PASS] (0.90s)\n\n===========================================================================\nSUMMARY: 3 passed, 1 failed (13.2s)\n===========================================================================\n</code></pre>"},{"location":"contributing/testing/#running-gpu-tests-on-modal","title":"Running GPU Tests on Modal","text":"<p>GPU tests run on Modal cloud infrastructure with NVIDIA GPUs.</p>"},{"location":"contributing/testing/#prerequisites_1","title":"Prerequisites","text":"<ol> <li> <p>Install Modal CLI:    <pre><code>pip install modal\n</code></pre></p> </li> <li> <p>Authenticate:    <pre><code>modal setup\n</code></pre></p> </li> <li> <p>Create HuggingFace secret (for model downloads):    <pre><code>modal secret create adithya-hf-wandb HF_TOKEN=your_hf_token\n</code></pre></p> </li> </ol>"},{"location":"contributing/testing/#using-the-modal-runner","title":"Using the Modal Runner","text":"<pre><code>cd /path/to/omnidocs_Master\n\n# List all available tests\nmodal run scripts/text_extract_omnidocs/modal_runner.py --list-tests\n\n# Run a specific test\nmodal run scripts/text_extract_omnidocs/modal_runner.py --test qwen_vllm\n\n# Run all tests\nmodal run scripts/text_extract_omnidocs/modal_runner.py --run-all\n</code></pre>"},{"location":"contributing/testing/#available-modal-tests","title":"Available Modal Tests","text":""},{"location":"contributing/testing/#text-extraction","title":"Text Extraction","text":"Test Backend GPU Model <code>qwen_vllm</code> VLLM L4 Qwen3-VL-4B <code>qwen_pytorch</code> PyTorch L4 Qwen3-VL-4B <code>nanonets_vllm</code> VLLM L4 Nanonets-OCR-s <code>nanonets_pytorch</code> PyTorch L4 Nanonets-OCR-s <code>dotsocr_vllm</code> VLLM L4 dots.ocr <code>dotsocr_pytorch</code> PyTorch L4 dots.ocr"},{"location":"contributing/testing/#layout-extraction","title":"Layout Extraction","text":"Test Backend GPU Model <code>qwen_layout_vllm</code> VLLM L4 Qwen3-VL-4B <code>qwen_layout_pytorch</code> PyTorch L4 Qwen3-VL-4B <code>doclayout_yolo_gpu</code> PyTorch L4 DocLayoutYOLO <code>rtdetr_gpu</code> PyTorch L4 RTDETR"},{"location":"contributing/testing/#example-output_1","title":"Example Output","text":"<pre><code>Running test: doclayout_yolo_gpu\n============================================================\nTesting DocLayoutYOLO with GPU\n============================================================\nModel load time: 8.34s\nInference time: 2.27s\n\n--- Detected Layout Elements ---\nNumber of boxes: 8\n  1. LayoutLabel.TITLE: conf=0.32\n  2. LayoutLabel.TEXT: conf=0.72\n  ...\n\n============================================================\nTEST RESULTS\n============================================================\n  status: success\n  test: doclayout_yolo_gpu\n  backend: pytorch_gpu\n  model: DocLayoutYOLO\n  num_boxes: 8\n  load_time: 8.34\n  inference_time: 2.27\n</code></pre>"},{"location":"contributing/testing/#running-pytest","title":"Running pytest","text":"<p>For unit tests and integration tests:</p> <pre><code>cd Omnidocs\n\n# Run all tests\nuv run pytest\n\n# Run with specific markers\nuv run pytest -m \"cpu\"              # CPU-only tests\nuv run pytest -m \"not slow\"         # Skip slow tests\nuv run pytest -m \"layout_extraction\" # Layout tests only\n\n# Run specific test file\nuv run pytest tests/integration/test_layout_extractors.py -v\n\n# Run with coverage\nuv run pytest --cov=omnidocs\n</code></pre>"},{"location":"contributing/testing/#pytest-markers","title":"pytest Markers","text":"<p>Defined in <code>pyproject.toml</code>:</p> Marker Description <code>slow</code> Long-running tests (network, large files) <code>gpu</code> Requires GPU <code>cpu</code> CPU-only tests <code>vllm</code> VLLM backend tests <code>pytorch</code> PyTorch backend tests <code>mlx</code> MLX backend tests (Apple Silicon) <code>api</code> API backend tests <code>text_extraction</code> Text extraction task <code>layout_extraction</code> Layout extraction task <code>ocr_extraction</code> OCR extraction task <code>table_extraction</code> Table extraction task <code>reading_order</code> Reading order task <code>integration</code> Integration tests requiring model inference"},{"location":"contributing/testing/#writing-tests","title":"Writing Tests","text":""},{"location":"contributing/testing/#standalone-test-template","title":"Standalone Test Template","text":"<p>Create a new test in <code>tests/standalone/&lt;task&gt;/&lt;model&gt;_&lt;backend&gt;.py</code>:</p> <pre><code>\"\"\"\nModel Name - Backend\n\nUsage:\n    python -m tests.standalone.&lt;task&gt;.&lt;model&gt;_&lt;backend&gt; path/to/image.png\n\"\"\"\nimport sys\nimport time\nfrom pathlib import Path\nfrom PIL import Image\n\n\ndef run_extraction(img: Image.Image) -&gt; dict:\n    \"\"\"Run extraction and return results.\"\"\"\n    from omnidocs.tasks.&lt;task&gt; import MyExtractor, MyConfig\n\n    start = time.time()\n    extractor = MyExtractor(config=MyConfig(device=\"cpu\"))\n    load_time = time.time() - start\n\n    start = time.time()\n    result = extractor.extract(img)\n    inference_time = time.time() - start\n\n    return {\n        \"result\": result,\n        \"load_time\": load_time,\n        \"inference_time\": inference_time,\n    }\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) &lt; 2:\n        print(\"Usage: python -m tests.standalone.&lt;task&gt;.&lt;model&gt;_&lt;backend&gt; &lt;image_path&gt;\")\n        sys.exit(1)\n\n    img = Image.open(sys.argv[1])\n    result = run_extraction(img)\n    print(f\"Load time: {result['load_time']:.2f}s\")\n    print(f\"Inference time: {result['inference_time']:.2f}s\")\n</code></pre>"},{"location":"contributing/testing/#register-the-test","title":"Register the Test","text":"<p>Add to <code>tests/runners/registry.py</code>:</p> <pre><code>from .registry import TestSpec, Backend, Task\n\n# Add to TEST_REGISTRY list\nTestSpec(\n    name=\"mymodel_cpu\",\n    module=\"&lt;task&gt;.mymodel_cpu\",\n    backend=Backend.PYTORCH_CPU,\n    task=Task.&lt;TASK&gt;,\n    gpu_type=None,  # None for CPU tests\n),\n</code></pre>"},{"location":"contributing/testing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"contributing/testing/#vllm-multiprocessing-error","title":"VLLM Multiprocessing Error","text":"<p>If you see <code>Cannot re-initialize CUDA in forked subprocess</code>:</p> <pre><code>import os\nos.environ[\"VLLM_USE_V1\"] = \"0\"\nos.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n</code></pre>"},{"location":"contributing/testing/#flash-attention-version-mismatch","title":"Flash Attention Version Mismatch","text":"<p>Use <code>attn_implementation=\"sdpa\"</code> instead of <code>flash_attention_2</code>:</p> <pre><code>config = MyConfig(attn_implementation=\"sdpa\")\n</code></pre>"},{"location":"contributing/testing/#mlx-tests-fail-on-non-apple-hardware","title":"MLX Tests Fail on Non-Apple Hardware","text":"<p>MLX only works on Apple Silicon. Skip with:</p> <pre><code>uv run python -m tests.runners.local_runner --cpu-only  # Exclude MLX\n</code></pre>"},{"location":"contributing/testing/#api-tests-need-credentials","title":"API Tests Need Credentials","text":"<p>API tests require environment variables:</p> <pre><code>export OPENROUTER_API_KEY=your_key\nuv run python -m tests.runners.local_runner --test qwen_layout_api\n</code></pre>"},{"location":"contributing/testing/#test-results-reference","title":"Test Results Reference","text":""},{"location":"contributing/testing/#text-extraction-performance-modal-l4-gpu","title":"Text Extraction Performance (Modal L4 GPU)","text":"Model Backend Load Time Inference Time Qwen3-VL-4B VLLM 84s 7.0s Qwen3-VL-4B PyTorch 54s 6.2s Nanonets-OCR-s VLLM 194s 8.4s Nanonets-OCR-s PyTorch 44s 6.3s DotsOCR VLLM 94s 10.0s DotsOCR PyTorch 42s 11.4s"},{"location":"contributing/testing/#layout-extraction-performance","title":"Layout Extraction Performance","text":"Model Backend Load Time Inference Time Qwen Layout VLLM (L4) 237s 27.2s Qwen Layout PyTorch (L4) 54s 13.3s Qwen Layout MLX (local) 8.8s 13.7s DocLayoutYOLO GPU (L4) 8.3s 2.3s DocLayoutYOLO CPU (local) 0.5s 0.3s RTDETR GPU (L4) 12.4s 1.9s RTDETR CPU (local) 0.9s 0.5s"},{"location":"contributing/testing/#table-extraction-performance","title":"Table Extraction Performance","text":"Model Backend Load Time Inference Time TableFormer (fast) CPU (local) 0.5s 0.3s TableFormer (accurate) CPU (local) 0.5s 0.9s TableFormer (fast) GPU (L4) 8s 0.2s TableFormer (accurate) GPU (L4) 8s 0.5s"},{"location":"contributing/testing/#reading-order-performance","title":"Reading Order Performance","text":"Model Backend Load Time Inference Time Rule-based CPU (local) &lt;0.1s &lt;0.1s"},{"location":"contributing/workflow/","title":"Implementation Workflow","text":"<p>This guide walks through the complete 6-phase workflow for adding new features to OmniDocs.</p>"},{"location":"contributing/workflow/#overview","title":"Overview","text":"<pre><code>Issue &amp; Planning\n    \u2193\nExperimentation (scripts/)\n    \u2193\nIntegration (Omnidocs/)\n    \u2193\nTesting &amp; Validation\n    \u2193\nPull Request &amp; Review\n    \u2193\nVersion Release\n</code></pre>"},{"location":"contributing/workflow/#phase-1-issue-planning","title":"Phase 1: Issue &amp; Planning","text":""},{"location":"contributing/workflow/#create-github-issue","title":"Create GitHub Issue","text":"<p>Create a new issue with this template:</p> <pre><code>**Title**: Add [Model/Task Name] Support\n\n**Description**:\n- **Task Type**: Text Extraction / Layout Analysis / OCR / etc.\n- **Model**: [Model name]\n- **Backends**: PyTorch / VLLM / MLX / API\n- **Use Case**: [Brief description]\n\n**References**:\n- Model Card: [HuggingFace link]\n- Paper: [arXiv link if applicable]\n\n**Checklist**:\n- [ ] Create implementation plan\n- [ ] Experiment in scripts/\n- [ ] Integrate into Omnidocs/\n- [ ] Write tests\n- [ ] Pass lint checks\n- [ ] Create PR\n</code></pre>"},{"location":"contributing/workflow/#read-design-documents","title":"Read Design Documents","text":"<p>Before implementing ANYTHING, read these:</p> <ol> <li><code>IMPLEMENTATION_PLAN/BACKEND_ARCHITECTURE.md</code> - Backend system design</li> <li><code>IMPLEMENTATION_PLAN/DEVEX.md</code> - API design principles</li> <li><code>CLAUDE.md</code> - Development guide and standards</li> </ol>"},{"location":"contributing/workflow/#write-implementation-plan","title":"Write Implementation Plan","text":"<p>Add a comment to your issue with:</p> <p><pre><code>## Implementation Plan\n\n### Architecture\n- Single-backend or multi-backend?\n- Which backends to support?\n- Config class names?\n\n### File Structure\n</code></pre> omnidocs/tasks/text_extraction/ \u251c\u2500\u2500 mymodel.py           # Main extractor \u2514\u2500\u2500 mymodel/             # Configs (if multi-backend)     \u251c\u2500\u2500 pytorch.py     \u251c\u2500\u2500 vllm.py     \u2514\u2500\u2500 api.py <pre><code>### Dependencies\nList all new packages to add\n\n### Timeline\nEstimate effort for each phase\n</code></pre></p>"},{"location":"contributing/workflow/#phase-2-experimentation-scripts","title":"Phase 2: Experimentation (scripts/)","text":""},{"location":"contributing/workflow/#create-experiment-scripts","title":"Create Experiment Scripts","text":"<p>For GPU models, create <code>scripts/text_extract/modal_mymodel_pytorch.py</code>:</p> <pre><code>import modal\n\napp = modal.App(\"omnidocs-mymodel-test\")\n\ncuda_version = \"12.4.0\"\nflavor = \"devel\"\noperating_sys = \"ubuntu22.04\"\ntag = f\"{cuda_version}-{flavor}-{operating_sys}\"\n\n# Base image (cached)\nBASE_IMAGE = (\n    modal.Image.from_registry(f\"nvidia/cuda:{tag}\", add_python=\"3.12\")\n    .apt_install(\"libgl1-mesa-glx\", \"libglib2.0-0\")\n    .uv_pip_install(\"torch\", \"transformers\", \"pillow\")\n    .env({\"HF_HOME\": \"/data/.cache\"})\n)\n\n# Model-specific layer\nIMAGE = BASE_IMAGE.uv_pip_install(\"mymodel-package\")\n\nvolume = modal.Volume.from_name(\"omnidocs\", create_if_missing=True)\nsecret = modal.Secret.from_name(\"adithya-hf-wandb\")\n\n@app.function(image=IMAGE, gpu=\"A10G:1\", volumes={\"/data\": volume}, secrets=[secret])\ndef test_inference():\n    # Test code here\n    pass\n\n@app.local_entrypoint()\ndef main():\n    test_inference.remote()\n</code></pre> <p>Naming Convention: - <code>modal_{model}_{backend}.py</code> - GPU-based (PyTorch/VLLM) - <code>litellm_{model}_{task}.py</code> - API-based (local) - <code>mlx_{model}_{task}.py</code> - Apple Silicon (local)</p>"},{"location":"contributing/workflow/#run-and-validate","title":"Run and Validate","text":"<pre><code>cd scripts/text_extract/\nmodal run modal_mymodel_pytorch.py\n</code></pre> <p>Validation Checklist: - [ ] Model loads successfully - [ ] Inference produces expected output - [ ] Performance is acceptable - [ ] Error handling works - [ ] Different input types work</p>"},{"location":"contributing/workflow/#document-findings","title":"Document Findings","text":"<p>In the GitHub issue, comment with: - Model size and memory requirements - Inference speed (tokens/sec or pages/sec) - Quality observations - Recommended configurations</p>"},{"location":"contributing/workflow/#phase-3-integration-omnidocs","title":"Phase 3: Integration (Omnidocs/)","text":""},{"location":"contributing/workflow/#step-1-determine-backend-support","title":"Step 1: Determine Backend Support","text":"<p>Is this single-backend or multi-backend?</p> <p>Single-Backend (e.g., DocLayoutYOLO): - One config class in same file as extractor - Use <code>config=</code> parameter</p> <p>Multi-Backend (e.g., Qwen): - Multiple config classes in subfolder - Use <code>backend=</code> parameter with Union type</p>"},{"location":"contributing/workflow/#step-2-create-config-classes","title":"Step 2: Create Config Classes","text":"<p>Single-Backend (in <code>omnidocs/tasks/text_extraction/mymodel.py</code>):</p> <pre><code>from pydantic import BaseModel, Field\n\nclass MyModelConfig(BaseModel):\n    \"\"\"Configuration for MyModel.\"\"\"\n\n    model: str = Field(default=\"vendor/mymodel\", description=\"Model ID\")\n    device: str = Field(default=\"cuda\", description=\"Device\")\n    dtype: str = Field(default=\"bfloat16\", description=\"Data type\")\n\n    class Config:\n        extra = \"forbid\"\n</code></pre> <p>Multi-Backend (in <code>omnidocs/tasks/text_extraction/mymodel/pytorch.py</code>):</p> <pre><code>class MyModelPyTorchConfig(BaseModel):\n    \"\"\"PyTorch backend config.\"\"\"\n    model: str = Field(..., description=\"Model ID\")\n    device: str = Field(default=\"cuda\")\n    torch_dtype: str = Field(default=\"bfloat16\")\n    class Config:\n        extra = \"forbid\"\n</code></pre> <p>Config Rules: - \u2705 All parameters with <code>Field(...)</code> - \u2705 All parameters have descriptions - \u2705 Type hints for everything - \u2705 Validation rules (ge, le, Literal) - \u2705 <code>extra = \"forbid\"</code> to catch typos - \u2705 Pydantic docstring for class</p>"},{"location":"contributing/workflow/#step-3-create-extractor-class","title":"Step 3: Create Extractor Class","text":"<pre><code>from typing import Union, Optional\nfrom .base import BaseTextExtractor\nfrom .models import TextOutput\n\nclass MyModelTextExtractor(BaseTextExtractor):\n    \"\"\"MyModel text extractor.\"\"\"\n\n    def __init__(self, backend: MyModelBackendConfig):\n        self.backend_config = backend\n        self._backend = self._create_backend()\n\n    def _create_backend(self):\n        \"\"\"Create backend based on config type.\"\"\"\n        if isinstance(self.backend_config, MyModelPyTorchConfig):\n            from omnidocs.inference.pytorch import PyTorchInference\n            return PyTorchInference(self.backend_config)\n        # ... other backends\n\n    def extract(self, image, output_format=\"markdown\", **kwargs) -&gt; TextOutput:\n        \"\"\"Extract text from image.\"\"\"\n        # Implementation\n        pass\n</code></pre>"},{"location":"contributing/workflow/#step-4-update-exports","title":"Step 4: Update Exports","text":"<p>Edit <code>omnidocs/tasks/text_extraction/__init__.py</code>:</p> <pre><code>from .mymodel import MyModelTextExtractor\nfrom .mymodel import (\n    MyModelPyTorchConfig,\n    MyModelVLLMConfig,\n    # ...\n)\n\n__all__ = [\n    \"MyModelTextExtractor\",\n    \"MyModelPyTorchConfig\",\n    # ...\n]\n</code></pre>"},{"location":"contributing/workflow/#step-5-add-dependencies","title":"Step 5: Add Dependencies","text":"<pre><code>cd Omnidocs/\nuv add --group pytorch mymodel-package\nuv sync\n</code></pre>"},{"location":"contributing/workflow/#phase-4-testing-validation","title":"Phase 4: Testing &amp; Validation","text":""},{"location":"contributing/workflow/#write-unit-tests","title":"Write Unit Tests","text":"<p>Create <code>Omnidocs/tests/tasks/text_extraction/test_mymodel.py</code>:</p> <pre><code>import pytest\nfrom omnidocs.tasks.text_extraction import MyModelTextExtractor\nfrom omnidocs.tasks.text_extraction.mymodel import MyModelPyTorchConfig\n\nclass TestMyModelConfig:\n    \"\"\"Test config validation.\"\"\"\n\n    def test_valid_config(self):\n        config = MyModelPyTorchConfig(model=\"vendor/mymodel\")\n        assert config.device == \"cuda\"\n\n    def test_invalid_param(self):\n        with pytest.raises(ValueError):\n            MyModelPyTorchConfig(\n                model=\"vendor/mymodel\",\n                invalid_param=\"value\"  # Should raise error\n            )\n\nclass TestMyModelExtractor:\n    \"\"\"Test extractor functionality.\"\"\"\n\n    @pytest.fixture\n    def sample_image(self):\n        from PIL import Image\n        return Image.new(\"RGB\", (800, 600))\n\n    def test_extract_markdown(self, sample_image):\n        config = MyModelPyTorchConfig(model=\"vendor/mymodel\", device=\"cuda\")\n        extractor = MyModelTextExtractor(backend=config)\n        result = extractor.extract(sample_image, output_format=\"markdown\")\n\n        assert result.format.value == \"markdown\"\n        assert len(result.content) &gt; 0\n</code></pre> <p>Coverage Target: &gt;80%</p>"},{"location":"contributing/workflow/#integration-test","title":"Integration Test","text":"<p>Create <code>scripts/text_extract_omnidocs/modal_mymodel_text_hf.py</code>:</p> <pre><code>\"\"\"Test MyModel through Omnidocs package on Modal.\"\"\"\n\n@app.function(image=OMNIDOCS_IMAGE, gpu=\"A10G:1\", ...)\ndef test_omnidocs_mymodel():\n    from omnidocs.tasks.text_extraction import MyModelTextExtractor\n    from omnidocs.tasks.text_extraction.mymodel import MyModelPyTorchConfig\n\n    extractor = MyModelTextExtractor(\n        backend=MyModelPyTorchConfig(model=\"vendor/mymodel\", device=\"cuda\")\n    )\n\n    result = extractor.extract(image, output_format=\"markdown\")\n    assert result.format.value == \"markdown\"\n    return {\"success\": True, \"length\": len(result.content)}\n</code></pre>"},{"location":"contributing/workflow/#lint-checks","title":"Lint Checks","text":"<pre><code>cd Omnidocs/\n\n# Format code\nuv run black omnidocs/ tests/\n\n# Type checking\nuv run mypy omnidocs/\n\n# Fix any issues before proceeding\n</code></pre>"},{"location":"contributing/workflow/#phase-5-pull-request","title":"Phase 5: Pull Request","text":""},{"location":"contributing/workflow/#create-feature-branch","title":"Create Feature Branch","text":"<pre><code>git checkout main\ngit pull origin main\ngit checkout -b feature/add-mymodel-support\n</code></pre>"},{"location":"contributing/workflow/#commit-changes","title":"Commit Changes","text":"<pre><code>git add Omnidocs/omnidocs/tasks/text_extraction/mymodel.py\ngit add Omnidocs/omnidocs/tasks/text_extraction/mymodel/\ngit add Omnidocs/tests/tasks/text_extraction/test_mymodel.py\ngit add Omnidocs/pyproject.toml\ngit add scripts/text_extract/modal_mymodel_pytorch.py\ngit add scripts/text_extract_omnidocs/test_mymodel.py\n\ngit commit -m \"$(cat &lt;&lt;'EOF'\nfeat: add MyModel text extraction support\n\nAdds MyModelTextExtractor with PyTorch backend:\n- Complete text extraction to Markdown/HTML\n- Pydantic config validation\n- Integration with Document class\n- Unit tests (&gt;80% coverage)\n- Modal deployment scripts\n\nTesting:\n- Unit tests passing\n- Integration tests on Modal verified\n- Black formatting applied\n- Mypy type checks passing\nEOF\n)\"\n</code></pre> <p>Important: - NO <code>Co-Authored-By</code> attribution - NO AI/Claude mentions - Commits attributed to repository owner only</p>"},{"location":"contributing/workflow/#push-and-create-pr","title":"Push and Create PR","text":"<pre><code>git push origin feature/add-mymodel-support\n\ngh pr create \\\n  --title \"Add MyModel Text Extraction\" \\\n  --body \"Adds MyModel support with PyTorch backend.\n\n## Changes\n- MyModelTextExtractor class\n- PyTorch configuration\n- Unit and integration tests\n- Modal deployment scripts\n\n## Testing\n- [x] Unit tests passing (&gt;80%)\n- [x] Integration tests on Modal\n- [x] Lint checks (black, mypy)\n- [x] Documentation updated\"\n</code></pre>"},{"location":"contributing/workflow/#iterate-on-review","title":"Iterate on Review","text":"<ol> <li>Address reviewer feedback</li> <li>Push updates to same branch</li> <li>Re-run lint and tests</li> <li>Request re-review</li> </ol>"},{"location":"contributing/workflow/#phase-6-version-release","title":"Phase 6: Version &amp; Release","text":""},{"location":"contributing/workflow/#update-version","title":"Update Version","text":"<p>Edit <code>Omnidocs/pyproject.toml</code>:</p> <pre><code>[project]\nname = \"omnidocs\"\nversion = \"2.2.0\"  # Increment MINOR for new feature\n</code></pre>"},{"location":"contributing/workflow/#update-changelog","title":"Update Changelog","text":"<p>Edit <code>Omnidocs/CHANGELOG.md</code>:</p> <pre><code>## [2.2.0] - 2026-02-15\n\n### Added\n- **MyModel Text Extraction**: PyTorch backend for efficient text extraction\n  - Markdown and HTML output formats\n  - Integration with Document class\n  - Unit tests with &gt;80% coverage\n</code></pre>"},{"location":"contributing/workflow/#create-git-tag","title":"Create Git Tag","text":"<pre><code>git checkout main\ngit pull origin main\ngit tag -a v2.2.0 -m \"Release v2.2.0: Add MyModel text extraction\"\ngit push origin v2.2.0\n</code></pre>"},{"location":"contributing/workflow/#build-and-publish","title":"Build and Publish","text":"<pre><code>cd Omnidocs/\n\n# Build distribution\nuv build\n\n# Publish to PyPI\nuv publish\n</code></pre>"},{"location":"contributing/workflow/#create-github-release","title":"Create GitHub Release","text":"<pre><code>gh release create v2.2.0 \\\n  --title \"v2.2.0: MyModel Text Extraction\" \\\n  --notes \"Added MyModel support for efficient text extraction\"\n</code></pre>"},{"location":"contributing/workflow/#summary-checklist","title":"Summary Checklist","text":""},{"location":"contributing/workflow/#planning","title":"Planning","text":"<ul> <li>[ ] GitHub issue created</li> <li>[ ] Design docs read</li> <li>[ ] Implementation plan written</li> </ul>"},{"location":"contributing/workflow/#experimentation","title":"Experimentation","text":"<ul> <li>[ ] Experiment script in scripts/</li> <li>[ ] Modal/local execution successful</li> <li>[ ] Findings documented</li> </ul>"},{"location":"contributing/workflow/#integration","title":"Integration","text":"<ul> <li>[ ] Config classes created</li> <li>[ ] Extractor class implemented</li> <li>[ ] init.py exports updated</li> <li>[ ] Dependencies added (uv add)</li> </ul>"},{"location":"contributing/workflow/#testing","title":"Testing","text":"<ul> <li>[ ] Unit tests &gt;80% coverage</li> <li>[ ] Integration test in *_omnidocs/</li> <li>[ ] Modal test successful</li> <li>[ ] Lint checks passing (black, mypy)</li> </ul>"},{"location":"contributing/workflow/#pr","title":"PR","text":"<ul> <li>[ ] Feature branch created</li> <li>[ ] Changes committed</li> <li>[ ] PR created with description</li> <li>[ ] CI/CD checks pass</li> <li>[ ] Feedback addressed</li> </ul>"},{"location":"contributing/workflow/#release","title":"Release","text":"<ul> <li>[ ] Version bumped</li> <li>[ ] Changelog updated</li> <li>[ ] Git tag created</li> <li>[ ] Package published to PyPI</li> <li>[ ] GitHub release created</li> </ul>"},{"location":"contributing/workflow/#next-see-adding-models-for-step-by-step-model-integration","title":"Next: See Adding Models for step-by-step model integration.","text":""},{"location":"guides/","title":"Guides","text":"<p>Task-oriented tutorials for common OmniDocs workflows.</p>"},{"location":"guides/#by-task","title":"By Task","text":"Guide What You'll Do Time Text Extraction Convert documents to Markdown/HTML 5 min Layout Analysis Detect titles, tables, figures 5 min OCR Extraction Get text with coordinates 5 min Batch Processing Process 100+ documents 10 min Modal Deployment Deploy to cloud GPUs 15 min"},{"location":"guides/#quick-examples","title":"Quick Examples","text":""},{"location":"guides/#text-extraction","title":"Text Extraction","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n\ndoc = Document.from_pdf(\"paper.pdf\")\nextractor = QwenTextExtractor(backend=QwenPyTorchConfig(device=\"cuda\"))\nresult = extractor.extract(doc.get_page(0), output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"guides/#layout-analysis","title":"Layout Analysis","text":"<pre><code>from omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\n\nlayout = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = layout.extract(image)\nfor elem in result.elements:\n    print(f\"{elem.label}: {elem.bbox}\")\n</code></pre>"},{"location":"guides/#ocr-extraction","title":"OCR Extraction","text":"<pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractConfig\n\nocr = TesseractOCR(config=TesseractConfig(languages=[\"eng\"]))\nresult = ocr.extract(image)\nfor block in result.text_blocks:\n    print(f\"{block.text} @ {block.bbox}\")\n</code></pre>"},{"location":"guides/#batch-processing","title":"Batch Processing","text":"<pre><code>from pathlib import Path\n\nextractor = QwenTextExtractor(backend=QwenPyTorchConfig(device=\"cuda\"))\n\nfor pdf in Path(\"docs/\").glob(\"*.pdf\"):\n    doc = Document.from_pdf(str(pdf))\n    for i, page in enumerate(doc.iter_pages()):\n        result = extractor.extract(page)\n        (pdf.stem / f\"page_{i}.md\").write_text(result.content)\n</code></pre>"},{"location":"guides/#common-workflows","title":"Common Workflows","text":""},{"location":"guides/#academic-paper-processing","title":"Academic Paper Processing","text":"<ol> <li>Layout: DocLayoutYOLO (0.1-0.2s/page)</li> <li>Text: Qwen3-VL-8B (2-4s/page)</li> <li>Result: Structure + content</li> </ol>"},{"location":"guides/#batch-document-processing","title":"Batch Document Processing","text":"<ol> <li>Layout: DocLayoutYOLO</li> <li>Text: DotsOCR + VLLM (multi-GPU)</li> <li>Result: 5-10k docs/hour</li> </ol>"},{"location":"guides/#form-field-extraction","title":"Form Field Extraction","text":"<ol> <li>Layout: Qwen Layout (custom labels)</li> <li>OCR: Tesseract per field</li> <li>Result: Structured form data</li> </ol>"},{"location":"guides/#performance-reference","title":"Performance Reference","text":"Task Model Device Speed Text Extraction Qwen3-VL-8B A10G 2-3s/page Text Extraction Qwen3-VL-8B CPU 15-30s/page Layout Detection DocLayoutYOLO A10G 0.1-0.2s/page OCR Tesseract CPU 0.5-1s/page"},{"location":"guides/#troubleshooting","title":"Troubleshooting","text":"<p>Out of memory \u2192 Use smaller model (2B instead of 8B)</p> <p>Slow inference \u2192 Use VLLM backend for batches</p> <p>Poor accuracy \u2192 Try larger model or different model</p> <p>Missing elements \u2192 Adjust confidence threshold</p> <p>See individual guides for detailed troubleshooting.</p>"},{"location":"guides/batch-processing/","title":"Batch Processing Guide","text":"<p>Process multiple documents efficiently at scale. This guide covers batch loading, processing patterns, memory optimization, progress tracking, and GPU deployment.</p>"},{"location":"guides/batch-processing/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Built-in Batch Utilities (NEW)</li> <li>Batch Loading</li> <li>Processing Patterns</li> <li>Memory Optimization</li> <li>Progress Tracking</li> <li>Error Handling</li> <li>Performance Benchmarks</li> <li>Troubleshooting</li> </ul>"},{"location":"guides/batch-processing/#built-in-batch-utilities","title":"Built-in Batch Utilities","text":"<p>OmniDocs provides built-in utilities for common batch processing workflows. These handle document loading, progress tracking, and result aggregation automatically.</p>"},{"location":"guides/batch-processing/#documentbatch","title":"DocumentBatch","text":"<p>Load multiple PDFs from a directory or list of paths.</p> <pre><code>from omnidocs import DocumentBatch\n\n# Load from directory\nbatch = DocumentBatch.from_directory(\"pdfs/\")\nprint(f\"Found {batch.count} documents\")\n\n# Load from explicit paths\nbatch = DocumentBatch.from_paths([\"doc1.pdf\", \"doc2.pdf\", \"doc3.pdf\"])\n\n# With pattern matching\nbatch = DocumentBatch.from_directory(\"pdfs/\", pattern=\"invoice_*.pdf\")\n\n# Recursive search\nbatch = DocumentBatch.from_directory(\"documents/\", recursive=True)\n\n# Iterate over documents\nfor doc in batch:\n    for page in doc.iter_pages():\n        result = extractor.extract(page, output_format=\"markdown\")\n</code></pre>"},{"location":"guides/batch-processing/#process_directory","title":"process_directory()","text":"<p>One-liner for processing all PDFs in a directory.</p> <pre><code>from omnidocs import process_directory\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize extractor\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(device=\"cuda\")\n)\n\n# Process entire directory\nresults = process_directory(\n    \"pdfs/\",\n    extractor,\n    output_dir=\"results/\",  # Save JSON per document\n    output_format=\"markdown\",\n)\n\nprint(f\"Processed {results.document_count} documents, {results.total_pages} pages\")\n</code></pre>"},{"location":"guides/batch-processing/#process_document","title":"process_document()","text":"<p>Process all pages of a single document.</p> <pre><code>from omnidocs import Document, process_document\n\ndoc = Document.from_pdf(\"paper.pdf\")\nresult = process_document(doc, extractor, output_format=\"markdown\")\n\n# Access results\nfor page_result in result.all_results:\n    print(page_result.content[:100])\n\n# Save to file\nresult.save_json(\"paper_results.json\")\n</code></pre>"},{"location":"guides/batch-processing/#progress-tracking-with-callbacks","title":"Progress Tracking with Callbacks","text":"<pre><code>from omnidocs import DocumentBatch, process_directory\n\n# Document-level progress\ndef doc_progress(filename, current, total):\n    print(f\"[{current}/{total}] Processing {filename}\")\n\nresults = process_directory(\n    \"pdfs/\",\n    extractor,\n    progress_callback=doc_progress,\n)\n\n# Page-level progress with iter_with_progress\nbatch = DocumentBatch.from_directory(\"pdfs/\")\n\nfor doc in batch.iter_with_progress(lambda c, t, f: print(f\"[{c}/{t}] {f}\")):\n    for page in doc.iter_pages():\n        result = extractor.extract(page)\n</code></pre>"},{"location":"guides/batch-processing/#result-aggregation","title":"Result Aggregation","text":"<pre><code>from omnidocs import DocumentResult, BatchResult, merge_text_results\n\n# Manual result collection\ndoc_result = DocumentResult(source_path=\"paper.pdf\", page_count=10)\ndoc_result.add_page_result(0, text_output)\ndoc_result.add_page_result(1, text_output)\n\n# Merge all page content\nall_results = doc_result.all_results\nfull_text = merge_text_results(all_results, separator=\"\\n\\n---\\n\\n\")\n\n# Batch results\nbatch_result = BatchResult()\nbatch_result.add_document_result(\"paper1\", doc_result1)\nbatch_result.add_document_result(\"paper2\", doc_result2)\n\n# Save everything\nbatch_result.save_json(\"all_results.json\")\n</code></pre>"},{"location":"guides/batch-processing/#extractor-batch-methods","title":"Extractor Batch Methods","text":"<p>All extractors support <code>batch_extract()</code> and <code>extract_document()</code> methods.</p> <pre><code>from omnidocs import Document\n\ndoc = Document.from_pdf(\"paper.pdf\")\n\n# Extract all pages at once\nresults = extractor.extract_document(\n    doc,\n    output_format=\"markdown\",\n    progress_callback=lambda c, t: print(f\"Page {c}/{t}\"),\n)\n\n# Or with explicit image list\nimages = list(doc.iter_pages())\nresults = extractor.batch_extract(images, output_format=\"markdown\")\n</code></pre>"},{"location":"guides/batch-processing/#batch-loading","title":"Batch Loading","text":""},{"location":"guides/batch-processing/#load-from-directory","title":"Load from Directory","text":"<p>Load all images or PDFs from a directory.</p> <pre><code>from pathlib import Path\nfrom omnidocs import Document\nfrom PIL import Image\n\n# Find all image files\nimage_dir = Path(\"documents/images\")\nimage_paths = sorted(\n    list(image_dir.glob(\"*.png\")) +\n    list(image_dir.glob(\"*.jpg\")) +\n    list(image_dir.glob(\"*.jpeg\"))\n)\n\nprint(f\"Found {len(image_paths)} images\")\n\n# Load as PIL Images\nimages = [Image.open(p) for p in image_paths]\n\n# Load PDFs\npdf_dir = Path(\"documents/pdfs\")\npdf_paths = sorted(pdf_dir.glob(\"*.pdf\"))\n\ndocuments = [Document.from_pdf(p) for p in pdf_paths]\nprint(f\"Found {len(documents)} PDFs with {sum(d.page_count for d in documents)} total pages\")\n</code></pre>"},{"location":"guides/batch-processing/#lazy-loading-for-large-batches","title":"Lazy Loading for Large Batches","text":"<p>Don't load all images upfront - load as needed to save memory.</p> <pre><code>from pathlib import Path\nfrom PIL import Image\n\nimage_dir = Path(\"documents/\")\nimage_paths = sorted(image_dir.glob(\"*.png\"))\n\n# Generator: loads images on-demand\ndef image_generator(paths):\n    \"\"\"Generator that yields images one at a time.\"\"\"\n    for path in paths:\n        yield Image.open(path)\n\n# Usage: iterate without loading all at once\nfor idx, image in enumerate(image_generator(image_paths)):\n    print(f\"Processing image {idx+1}/{len(image_paths)}\")\n    # Process one image, then load next\n    # image is garbage collected automatically\n</code></pre>"},{"location":"guides/batch-processing/#load-with-metadata","title":"Load with Metadata","text":"<p>Track source information for each batch item.</p> <pre><code>from pathlib import Path\nfrom PIL import Image\nfrom dataclasses import dataclass\nfrom typing import Dict, Any\n\n@dataclass\nclass BatchItem:\n    \"\"\"Container for batch item with metadata.\"\"\"\n    path: Path\n    image: Image.Image\n    metadata: Dict[str, Any]\n\n# Load with metadata\nitems = []\nfor image_path in image_paths:\n    image = Image.open(image_path)\n    item = BatchItem(\n        path=image_path,\n        image=image,\n        metadata={\n            \"filename\": image_path.name,\n            \"size_bytes\": image_path.stat().st_size,\n            \"dimensions\": image.size,\n            \"format\": image.format,\n        }\n    )\n    items.append(item)\n\nprint(f\"Loaded {len(items)} items with metadata\")\n</code></pre>"},{"location":"guides/batch-processing/#processing-patterns","title":"Processing Patterns","text":""},{"location":"guides/batch-processing/#pattern-1-simple-loop","title":"Pattern 1: Simple Loop","text":"<p>Process items sequentially (smallest memory footprint).</p> <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom pathlib import Path\nfrom PIL import Image\nimport time\n\n# Initialize extractor once\nconfig = QwenTextPyTorchConfig(device=\"cuda\")\nextractor = QwenTextExtractor(backend=config)\n\n# Load image paths\nimages = sorted(Path(\"images/\").glob(\"*.png\"))\n\n# Process sequentially\nresults = []\nstart = time.time()\n\nfor idx, image_path in enumerate(images):\n    image = Image.open(image_path)\n    result = extractor.extract(image, output_format=\"markdown\")\n    results.append({\n        \"path\": str(image_path),\n        \"content_length\": result.content_length,\n        \"word_count\": result.word_count,\n    })\n\nelapsed = time.time() - start\nprint(f\"Processed {len(results)} images in {elapsed:.1f}s\")\nprint(f\"Average: {elapsed/len(images):.2f}s per image\")\n</code></pre>"},{"location":"guides/batch-processing/#pattern-2-batched-processing","title":"Pattern 2: Batched Processing","text":"<p>Group images into batches (more efficient for VLLM).</p> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\nfrom pathlib import Path\nfrom PIL import Image\n\n# Use VLLM for better batch efficiency\nconfig = QwenTextVLLMConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    max_tokens=4096,\n)\nextractor = QwenTextExtractor(backend=config)\n\n# Load images\nimages = [Image.open(p) for p in sorted(Path(\"images/\").glob(\"*.png\"))]\n\n# Process in batches\nbatch_size = 4\nresults = []\n\nfor batch_idx in range(0, len(images), batch_size):\n    batch = images[batch_idx:batch_idx + batch_size]\n    print(f\"Processing batch {batch_idx//batch_size + 1}\")\n\n    for image in batch:\n        result = extractor.extract(image, output_format=\"markdown\")\n        results.append(result)\n\nprint(f\"Processed {len(results)} images\")\n</code></pre>"},{"location":"guides/batch-processing/#pattern-3-pdf-with-multiple-pages","title":"Pattern 3: PDF with Multiple Pages","text":"<p>Process all pages of multiple PDFs.</p> <pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom pathlib import Path\n\n# Initialize\nconfig = QwenTextPyTorchConfig(device=\"cuda\")\nextractor = QwenTextExtractor(backend=config)\n\n# Load PDFs\npdf_files = sorted(Path(\"pdfs/\").glob(\"*.pdf\"))\n\n# Process all pages\nall_results = []\n\nfor pdf_path in pdf_files:\n    print(f\"Processing {pdf_path.name}\")\n    doc = Document.from_pdf(pdf_path)\n\n    for page_idx in range(doc.page_count):\n        page_image = doc.get_page(page_idx)\n        result = extractor.extract(page_image, output_format=\"markdown\")\n\n        all_results.append({\n            \"pdf\": pdf_path.name,\n            \"page\": page_idx + 1,\n            \"word_count\": result.word_count,\n            \"content\": result.content,\n        })\n\nprint(f\"Processed {sum(d['page_count'] for d in documents)} pages total\")\n</code></pre>"},{"location":"guides/batch-processing/#pattern-4-parallel-processing-per-document","title":"Pattern 4: Parallel Processing (Per-Document)","text":"<p>Use multiprocessing for CPU-bound preprocessing.</p> <pre><code>from multiprocessing import Pool\nfrom PIL import Image\nfrom pathlib import Path\n\ndef preprocess_image(image_path):\n    \"\"\"Preprocess a single image.\"\"\"\n    image = Image.open(image_path)\n\n    # Resize if needed\n    if image.width &lt; 1024:\n        image = image.resize((image.width * 2, image.height * 2))\n\n    # Convert to RGB\n    if image.mode != \"RGB\":\n        image = image.convert(\"RGB\")\n\n    return image_path, image\n\n# Parallel preprocessing\nimage_paths = sorted(Path(\"images/\").glob(\"*.png\"))\n\nwith Pool(4) as pool:  # 4 processes\n    results = pool.map(preprocess_image, image_paths)\n\nprint(f\"Preprocessed {len(results)} images\")\n\n# Then process with GPU (sequential, since we only have 1 GPU)\nconfig = QwenTextPyTorchConfig(device=\"cuda\")\nextractor = QwenTextExtractor(backend=config)\n\nfor path, image in results:\n    result = extractor.extract(image, output_format=\"markdown\")\n    # Process...\n</code></pre>"},{"location":"guides/batch-processing/#memory-optimization","title":"Memory Optimization","text":""},{"location":"guides/batch-processing/#monitor-gpu-memory","title":"Monitor GPU Memory","text":"<pre><code>import torch\n\nprint(\"GPU Memory:\")\nprint(f\"  Allocated: {torch.cuda.memory_allocated()/1e9:.1f}GB\")\nprint(f\"  Reserved: {torch.cuda.memory_reserved()/1e9:.1f}GB\")\nprint(f\"  Available: {torch.cuda.get_device_properties(0).total_memory/1e9:.1f}GB\")\n\n# Clear cache between batches\ntorch.cuda.empty_cache()\nprint(\"Cache cleared\")\n</code></pre>"},{"location":"guides/batch-processing/#optimize-model-configuration","title":"Optimize Model Configuration","text":"<pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Memory-optimized configuration\nconfig = QwenTextPyTorchConfig(\n    device=\"cuda\",\n    torch_dtype=\"float16\",  # Half precision (less memory)\n    max_new_tokens=2048,  # Smaller context (less memory)\n)\n</code></pre>"},{"location":"guides/batch-processing/#process-in-streaming-fashion","title":"Process in Streaming Fashion","text":"<p>Never keep all results in memory - stream to disk.</p> <pre><code>import json\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom pathlib import Path\nfrom PIL import Image\n\nconfig = QwenTextPyTorchConfig(device=\"cuda\")\nextractor = QwenTextExtractor(backend=config)\n\nimages = sorted(Path(\"images/\").glob(\"*.png\"))\n\n# Stream results to JSON Lines file\noutput_file = \"results.jsonl\"\n\nwith open(output_file, \"w\") as f:\n    for image_path in images:\n        image = Image.open(image_path)\n        result = extractor.extract(image, output_format=\"markdown\")\n\n        # Write immediately (don't accumulate in memory)\n        record = {\n            \"path\": str(image_path),\n            \"content_length\": result.content_length,\n            \"word_count\": result.word_count,\n        }\n        f.write(json.dumps(record) + \"\\n\")\n\n# Results are on disk, not in memory\nprint(f\"Streamed results to {output_file}\")\n\n# Read results later\nresults = []\nwith open(output_file) as f:\n    for line in f:\n        results.append(json.loads(line))\n</code></pre>"},{"location":"guides/batch-processing/#garbage-collection","title":"Garbage Collection","text":"<p>Explicitly free memory between batches.</p> <pre><code>import gc\nimport torch\n\nfor batch_idx, images in enumerate(batches):\n    # Process batch\n    for image in images:\n        result = extractor.extract(image)\n\n    # Free memory\n    del images\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    print(f\"Batch {batch_idx + 1} complete, memory freed\")\n</code></pre>"},{"location":"guides/batch-processing/#progress-tracking","title":"Progress Tracking","text":""},{"location":"guides/batch-processing/#simple-counter","title":"Simple Counter","text":"<pre><code>images = sorted(Path(\"images/\").glob(\"*.png\"))\ntotal = len(images)\n\nfor idx, image_path in enumerate(images, 1):\n    image = Image.open(image_path)\n    result = extractor.extract(image)\n\n    # Print progress\n    print(f\"[{idx}/{total}] {image_path.name}\", end=\" \")\n    print(f\"\u2713 {result.word_count} words\")\n</code></pre> <p>Output: <pre><code>[1/100] document_1.png \u2713 245 words\n[2/100] document_2.png \u2713 312 words\n[3/100] document_3.png \u2713 189 words\n</code></pre></p>"},{"location":"guides/batch-processing/#progress-bar-with-tqdm","title":"Progress Bar with tqdm","text":"<pre><code>from tqdm import tqdm\nfrom pathlib import Path\nfrom PIL import Image\n\nimages = sorted(Path(\"images/\").glob(\"*.png\"))\n\nfor image_path in tqdm(images, desc=\"Processing\"):\n    image = Image.open(image_path)\n    result = extractor.extract(image)\n    # Process...\n</code></pre> <p>Output: <pre><code>Processing: 45%|\u2588\u2588\u2588\u2588\u258c     | 45/100 [5:23&lt;6:32, 8.22s/it]\n</code></pre></p>"},{"location":"guides/batch-processing/#detailed-progress-with-eta","title":"Detailed Progress with ETA","text":"<pre><code>import time\nfrom pathlib import Path\nfrom PIL import Image\n\nimages = sorted(Path(\"images/\").glob(\"*.png\"))\ntotal = len(images)\n\nstart_time = time.time()\n\nfor idx, image_path in enumerate(images, 1):\n    image = Image.open(image_path)\n    result = extractor.extract(image)\n\n    # Calculate metrics\n    elapsed = time.time() - start_time\n    avg_time = elapsed / idx\n    remaining = (total - idx) * avg_time\n    remaining_mins = remaining / 60\n\n    # Print progress\n    percent = 100 * idx / total\n    print(f\"[{idx:3d}/{total}] {percent:5.1f}% \"\n          f\"{image_path.name:20} \"\n          f\"ETA: {remaining_mins:5.1f}min\")\n</code></pre> <p>Output: <pre><code>[  1/100]   1.0% document_1.png         ETA:  8.2min\n[ 10/100]  10.0% document_10.png        ETA:  7.4min\n[ 50/100]  50.0% document_50.png        ETA:  3.7min\n[100/100] 100.0% document_100.png       ETA:  0.0min\n</code></pre></p>"},{"location":"guides/batch-processing/#save-progress-periodically","title":"Save Progress Periodically","text":"<pre><code>import json\nfrom pathlib import Path\nfrom PIL import Image\n\nimages = sorted(Path(\"images/\").glob(\"*.png\"))\ncheckpoint_file = \"progress.json\"\n\n# Load existing progress\nif checkpoint_file.exists():\n    with open(checkpoint_file) as f:\n        completed = set(json.load(f).get(\"completed\", []))\nelse:\n    completed = set()\n\nresults = []\n\nfor image_path in images:\n    if str(image_path) in completed:\n        print(f\"Skipping {image_path.name} (already processed)\")\n        continue\n\n    image = Image.open(image_path)\n    result = extractor.extract(image)\n    results.append({\n        \"path\": str(image_path),\n        \"word_count\": result.word_count,\n    })\n\n    # Save progress periodically\n    completed.add(str(image_path))\n    if len(results) % 10 == 0:\n        with open(checkpoint_file, \"w\") as f:\n            json.dump({\"completed\": list(completed)}, f)\n        print(f\"Saved progress: {len(completed)}/{len(images)} completed\")\n\n# Final save\nwith open(checkpoint_file, \"w\") as f:\n    json.dump({\"completed\": list(completed)}, f)\n</code></pre>"},{"location":"guides/batch-processing/#error-handling","title":"Error Handling","text":""},{"location":"guides/batch-processing/#graceful-degradation","title":"Graceful Degradation","text":"<pre><code>from pathlib import Path\nfrom PIL import Image\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nimages = sorted(Path(\"images/\").glob(\"*.png\"))\nresults = []\nerrors = []\n\nfor image_path in images:\n    try:\n        image = Image.open(image_path)\n        result = extractor.extract(image)\n        results.append({\"path\": str(image_path), \"success\": True})\n\n    except torch.cuda.OutOfMemoryError:\n        logger.error(f\"OOM on {image_path.name}\")\n        errors.append({\"path\": str(image_path), \"error\": \"OOM\"})\n        torch.cuda.empty_cache()\n\n    except Exception as e:\n        logger.error(f\"Error on {image_path.name}: {e}\")\n        errors.append({\"path\": str(image_path), \"error\": str(e)})\n\nprint(f\"\\nResults: {len(results)} succeeded, {len(errors)} failed\")\n\nif errors:\n    print(\"\\nFailed items:\")\n    for error in errors:\n        print(f\"  {error['path']}: {error['error']}\")\n</code></pre>"},{"location":"guides/batch-processing/#retry-on-error","title":"Retry on Error","text":"<pre><code>from tenacity import retry, stop_after_attempt, wait_exponential\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=2, max=10),\n)\ndef extract_with_retry(extractor, image):\n    \"\"\"Extract with automatic retry on failure.\"\"\"\n    try:\n        return extractor.extract(image)\n    except Exception as e:\n        logger.warning(f\"Extraction failed: {e}, retrying...\")\n        raise\n\n# Use in batch processing\nfor image_path in images:\n    try:\n        image = Image.open(image_path)\n        result = extract_with_retry(extractor, image)\n        results.append(result)\n    except Exception as e:\n        logger.error(f\"Failed after retries: {image_path}: {e}\")\n</code></pre>"},{"location":"guides/batch-processing/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"guides/batch-processing/#typical-performance","title":"Typical Performance","text":"<p>Processing a standard page (300 DPI, ~2000x3000px):</p> <p>PyTorch (Single GPU): - Model load: ~2-3 seconds (one-time) - Per-page latency: ~2-3 seconds - Throughput: ~1 page/second - GPU Memory: ~16GB</p> <p>VLLM (Single GPU): - Model load: ~5-8 seconds (one-time) - Per-page latency: ~2-3 seconds - Throughput: ~1-2 pages/second (batched) - GPU Memory: ~20GB</p> <p>Multi-GPU VLLM: - Model load: ~8-12 seconds - Per-page latency: ~1-2 seconds - Throughput: ~2-4 pages/second (batched) - GPU Memory: ~10GB per GPU</p>"},{"location":"guides/batch-processing/#100-document-benchmark","title":"100-Document Benchmark","text":"<p>Processing 100 pages (typical):</p> <pre><code>import time\n\nimages = [...] # 100 images\n\n# PyTorch\nconfig = QwenTextPyTorchConfig(device=\"cuda\")\nextractor = QwenTextExtractor(backend=config)\n\nstart = time.time()\nfor image in images:\n    result = extractor.extract(image)\nelapsed = time.time() - start\n\nprint(f\"PyTorch: {elapsed:.1f}s ({elapsed/100:.2f}s per page)\")\n# Expected: ~3-4 minutes total\n\n# VLLM\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\n\nconfig = QwenTextVLLMConfig(\n    tensor_parallel_size=1,\n    max_tokens=4096,\n)\nextractor = QwenTextExtractor(backend=config)\n\nstart = time.time()\nfor image in images:\n    result = extractor.extract(image)\nelapsed = time.time() - start\n\nprint(f\"VLLM: {elapsed:.1f}s ({elapsed/100:.2f}s per page)\")\n# Expected: ~2-3 minutes total\n</code></pre>"},{"location":"guides/batch-processing/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/batch-processing/#out-of-memory-during-batch-processing","title":"Out of Memory During Batch Processing","text":"<p>Problem: CUDA OOM after processing several documents.</p> <p>Solutions: 1. Reduce batch size 2. Process one item at a time 3. Use smaller model 4. Clear cache between items</p> <pre><code># Solution 1: Reduce batch\nfor batch in batches:\n    for image in batch[:2]:  # Process 2 at a time instead of 4\n        result = extractor.extract(image)\n\n# Solution 2: Clear cache\ntorch.cuda.empty_cache()\ngc.collect()\n\n# Solution 3: Use smaller model\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-7B-Instruct\",  # Smaller\n    device=\"cuda\",\n)\n\n# Solution 4: Lower token limit\nconfig = QwenTextPyTorchConfig(\n    device=\"cuda\",\n    max_new_tokens=2048,  # Reduced\n)\n</code></pre>"},{"location":"guides/batch-processing/#very-slow-processing","title":"Very Slow Processing","text":"<p>Problem: Processing taking much longer than expected.</p> <p>Solutions: 1. Check GPU utilization 2. Use VLLM instead of PyTorch 3. Reduce image resolution 4. Verify model is on GPU</p> <pre><code>import torch\nimport subprocess\n\n# Check GPU usage\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=utilization.gpu\", \"--format=csv,noheader\"],\n    capture_output=True, text=True\n)\ngpu_util = result.stdout.strip()\nprint(f\"GPU Utilization: {gpu_util}%\")\n\nif gpu_util &lt; \"50%\":\n    # GPU not being fully used - try VLLM\n    from omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\n    config = QwenTextVLLMConfig()\n    extractor = QwenTextExtractor(backend=config)\n\n# Verify model on GPU\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Current device: {torch.cuda.current_device()}\")\n</code></pre>"},{"location":"guides/batch-processing/#variable-processing-times","title":"Variable Processing Times","text":"<p>Problem: Some documents take much longer to process.</p> <p>Solutions: 1. Check image sizes 2. Set token limit 3. Log processing times</p> <pre><code>import time\n\nfor image_path in images:\n    start = time.time()\n    image = Image.open(image_path)\n\n    # Check size\n    if image.size[0] &gt; 4000:\n        print(f\"Warning: Large image {image.size}, may be slow\")\n\n    result = extractor.extract(image)\n    elapsed = time.time() - start\n\n    # Flag slow items\n    if elapsed &gt; 5:\n        print(f\"Slow: {image_path.name} took {elapsed:.1f}s\")\n\n    # Limit tokens for very large documents\n    if result.word_count &gt; 5000:\n        print(f\"Very long output: {result.word_count} words\")\n</code></pre>"},{"location":"guides/batch-processing/#failed-documents","title":"Failed Documents","text":"<p>Problem: Some documents fail to process.</p> <p>Solutions: 1. Check file integrity 2. Try with different model 3. Check image format</p> <pre><code>from PIL import Image\nimport traceback\n\nfor image_path in images:\n    try:\n        # Verify image\n        image = Image.open(image_path)\n        image.verify()\n\n        # Reload (verify closes the file)\n        image = Image.open(image_path)\n\n        # Try extraction\n        result = extractor.extract(image)\n\n    except Exception as e:\n        print(f\"Failed {image_path.name}:\")\n        traceback.print_exc()\n\n        # Try alternative\n        try:\n            # Fallback to Tesseract (simple OCR)\n            from omnidocs.tasks.ocr_extraction import Tesseract\n            ocr = Tesseract()\n            result = ocr.extract(image)\n            print(\"  Fallback succeeded with Tesseract\")\n        except:\n            print(\"  Fallback also failed\")\n</code></pre> <p>Next Steps: - See Text Extraction Guide for extraction configuration - See Deployment Guide for scaling batches on GPU - See OCR Guide for text with locations</p>"},{"location":"guides/deployment-modal/","title":"Modal Deployment Guide","text":"<p>Deploy OmniDocs inference at scale on Modal serverless GPUs. This guide covers setup, configuration, deployment patterns, and cost optimization.</p>"},{"location":"guides/deployment-modal/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Why Modal for OmniDocs</li> <li>Standard Setup</li> <li>Basic Deployment</li> <li>Multi-GPU Deployment</li> <li>Production Patterns</li> <li>Monitoring &amp; Logging</li> <li>Cost Optimization</li> <li>Troubleshooting</li> </ul>"},{"location":"guides/deployment-modal/#why-modal-for-omnidocs","title":"Why Modal for OmniDocs","text":"<p>Modal is ideal for OmniDocs because:</p> <ol> <li>No Infrastructure Management - Modal handles GPU provisioning, networking, and scaling</li> <li>Pay Per Use - Only pay for actual GPU time, not idle time</li> <li>Automatic Scaling - Handle traffic spikes without manual scaling</li> <li>Pre-built GPU Images - CUDA, drivers, PyTorch pre-installed</li> <li>Distributed Processing - Process multiple documents in parallel</li> <li>Easy CLI - Deploy with single <code>modal run</code> command</li> </ol> <p>Cost Comparison: - Self-managed GPU: $500-2000/month (always on) - Modal (batch processing): $0.30-1.00 per hour of GPU time - For 100 documents (3 hours GPU time): ~$1.00</p>"},{"location":"guides/deployment-modal/#standard-setup","title":"Standard Setup","text":""},{"location":"guides/deployment-modal/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>Install Modal CLI: <pre><code>pip install modal\n</code></pre></p> </li> <li> <p>Authenticate: <pre><code>modal token new\n# Or use existing token\nmodal token set\n</code></pre></p> </li> <li> <p>Create Modal workspace (optional): <pre><code>modal workspace create my-workspace\nmodal workspace use my-workspace\n</code></pre></p> </li> </ol>"},{"location":"guides/deployment-modal/#standard-configuration","title":"Standard Configuration","text":"<p>Every OmniDocs Modal script uses this standard setup:</p> <pre><code>import modal\nfrom pathlib import Path\n\n# ============= Configuration =============\n\n# Model settings\nMODEL_NAME = \"Qwen/Qwen3-VL-8B-Instruct\"  # or other models\nGPU_CONFIG = \"A10G:1\"  # GPU type and count\n\n# Cache directories\nMODEL_CACHE_DIR = \"/data/omnidocs_models\"\n\n# CUDA settings (keep consistent across all scripts)\ncuda_version = \"12.4.0\"\nflavor = \"devel\"\noperating_sys = \"ubuntu22.04\"\ntag = f\"{cuda_version}-{flavor}-{operating_sys}\"\n\n# ============= Build Modal Image =============\n\nIMAGE = (\n    modal.Image.from_registry(f\"nvidia/cuda:{tag}\", add_python=\"3.12\")\n    .apt_install(\"libgl1-mesa-glx\", \"libglib2.0-0\")\n    # Base dependencies first (gets cached)\n    .uv_pip_install(\n        \"torch\",\n        \"torchvision\",\n        \"torchaudio\",\n        \"transformers\",\n        \"pillow\",\n        \"numpy\",\n        \"pydantic\",\n        \"huggingface_hub\",\n        \"hf_transfer\",\n        \"accelerate\",\n    )\n    # Model-specific dependencies\n    .uv_pip_install(\"qwen-vl-utils\")\n    .env({\n        \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",\n        \"HF_HOME\": \"/data/.cache\",\n        \"OMNIDOCS_MODEL_CACHE\": MODEL_CACHE_DIR,\n    })\n)\n\n# ============= Modal Setup =============\n\nvolume = modal.Volume.from_name(\"omnidocs\", create_if_missing=True)\nhuggingface_secret = modal.Secret.from_name(\"adithya-hf-wandb\")\n\napp = modal.App(\"omnidocs-deployment\")\n</code></pre> <p>Key Points: - Volume Name: Always use <code>\"omnidocs\"</code> for consistency - Secret Name: Always use <code>\"adithya-hf-wandb\"</code> (contains HF token) - Python Version: <code>3.12</code> for latest compatibility - GPU: <code>A10G:1</code> is standard (adjust as needed) - Timeout: Default 600s (10 min), increase for long documents</p>"},{"location":"guides/deployment-modal/#environment-variables-for-deployment","title":"Environment Variables for Deployment","text":"<p>Set up required secrets:</p> <pre><code># Create HuggingFace secret (one-time)\nmodal secret create adithya-hf-wandb \\\n  --key HF_TOKEN \\\n  --value \"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxx\"\n\n# Create volume for model caching (one-time)\nmodal volume create omnidocs\n</code></pre>"},{"location":"guides/deployment-modal/#basic-deployment","title":"Basic Deployment","text":""},{"location":"guides/deployment-modal/#example-1-simple-text-extraction","title":"Example 1: Simple Text Extraction","text":"<p>Deploy a basic text extraction function.</p> <pre><code>import modal\nfrom typing import Dict, Any\nfrom pathlib import Path\n\n# ============= Configuration =============\ncuda_version = \"12.4.0\"\nflavor = \"devel\"\noperating_sys = \"ubuntu22.04\"\ntag = f\"{cuda_version}-{flavor}-{operating_sys}\"\n\nIMAGE = (\n    modal.Image.from_registry(f\"nvidia/cuda:{tag}\", add_python=\"3.12\")\n    .apt_install(\"libgl1-mesa-glx\", \"libglib2.0-0\")\n    .uv_pip_install(\n        \"torch\", \"torchvision\", \"transformers\", \"pillow\", \"numpy\",\n        \"pydantic\", \"huggingface_hub\", \"accelerate\",\n    )\n    .uv_pip_install(\"qwen-vl-utils\")\n    .env({\n        \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",\n        \"HF_HOME\": \"/data/.cache\",\n    })\n)\n\nvolume = modal.Volume.from_name(\"omnidocs\", create_if_missing=True)\nsecret = modal.Secret.from_name(\"adithya-hf-wandb\")\n\napp = modal.App(\"omnidocs-text-extraction\")\n\n# ============= Modal Function =============\n\n@app.function(\n    image=IMAGE,\n    gpu=\"A10G:1\",\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=600,\n)\ndef extract_text(image_bytes: bytes) -&gt; Dict[str, Any]:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image_bytes: Image file contents (PNG/JPG)\n\n    Returns:\n        Dict with extracted text and metadata\n    \"\"\"\n    from omnidocs.tasks.text_extraction import QwenTextExtractor\n    from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n    from PIL import Image\n    import io\n\n    # Load image\n    image = Image.open(io.BytesIO(image_bytes))\n\n    # Initialize extractor\n    config = QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n    )\n    extractor = QwenTextExtractor(backend=config)\n\n    # Extract\n    result = extractor.extract(image, output_format=\"markdown\")\n\n    return {\n        \"success\": True,\n        \"content_length\": result.content_length,\n        \"word_count\": result.word_count,\n        \"content\": result.content,\n    }\n\n# ============= Local Entrypoint =============\n\n@app.local_entrypoint()\ndef main():\n    \"\"\"Test the deployment.\"\"\"\n    from pathlib import Path\n\n    # Test with a sample image\n    test_image_path = \"test_document.png\"\n    with open(test_image_path, \"rb\") as f:\n        image_bytes = f.read()\n\n    # Run extraction\n    result = extract_text.remote(image_bytes)\n\n    print(f\"Extraction succeeded: {result['success']}\")\n    print(f\"Content length: {result['content_length']} chars\")\n    print(f\"Word count: {result['word_count']}\")\n    print(f\"\\nContent preview:\")\n    print(result['content'][:500])\n</code></pre> <p>Deploy: <pre><code># Test locally\npython script.py\n\n# Or run on Modal GPU\nmodal run script.py\n</code></pre></p>"},{"location":"guides/deployment-modal/#example-2-batch-processing-with-progress","title":"Example 2: Batch Processing with Progress","text":"<p>Deploy a batch processor with progress tracking.</p> <pre><code>import modal\nfrom typing import List, Dict, Any\nimport time\n\n# ... (image and app setup as above)\n\n@app.function(\n    image=IMAGE,\n    gpu=\"A10G:1\",\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=1800,  # 30 min for large batches\n)\ndef process_batch(image_bytes_list: List[bytes]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Process a batch of images.\n\n    Args:\n        image_bytes_list: List of image byte strings\n\n    Returns:\n        Processing results and statistics\n    \"\"\"\n    from omnidocs.tasks.text_extraction import QwenTextExtractor\n    from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n    from PIL import Image\n    import io\n\n    # Initialize once (expensive)\n    config = QwenTextPyTorchConfig(device=\"cuda\")\n    extractor = QwenTextExtractor(backend=config)\n\n    results = []\n    start_time = time.time()\n\n    for idx, image_bytes in enumerate(image_bytes_list, 1):\n        try:\n            image = Image.open(io.BytesIO(image_bytes))\n            result = extractor.extract(image, output_format=\"markdown\")\n\n            results.append({\n                \"index\": idx,\n                \"success\": True,\n                \"word_count\": result.word_count,\n                \"content_length\": result.content_length,\n            })\n\n            # Progress\n            elapsed = time.time() - start_time\n            avg_time = elapsed / idx\n            remaining = (len(image_bytes_list) - idx) * avg_time\n            print(f\"[{idx}/{len(image_bytes_list)}] {remaining/60:.1f}min remaining\")\n\n        except Exception as e:\n            results.append({\n                \"index\": idx,\n                \"success\": False,\n                \"error\": str(e),\n            })\n\n    total_time = time.time() - start_time\n\n    return {\n        \"total_time\": total_time,\n        \"num_images\": len(image_bytes_list),\n        \"succeeded\": sum(1 for r in results if r[\"success\"]),\n        \"failed\": sum(1 for r in results if not r[\"success\"]),\n        \"results\": results,\n    }\n\n@app.local_entrypoint()\ndef main():\n    \"\"\"Test batch processing.\"\"\"\n    from pathlib import Path\n\n    # Load images\n    image_dir = Path(\"test_images/\")\n    image_paths = sorted(image_dir.glob(\"*.png\"))[:5]  # Test with 5\n\n    image_bytes_list = [\n        open(p, \"rb\").read()\n        for p in image_paths\n    ]\n\n    # Process batch\n    result = process_batch.remote(image_bytes_list)\n\n    print(f\"\\nResults:\")\n    print(f\"  Succeeded: {result['succeeded']}/{result['num_images']}\")\n    print(f\"  Failed: {result['failed']}/{result['num_images']}\")\n    print(f\"  Total time: {result['total_time']:.1f}s\")\n    print(f\"  Average: {result['total_time']/result['num_images']:.2f}s per image\")\n</code></pre>"},{"location":"guides/deployment-modal/#multi-gpu-deployment","title":"Multi-GPU Deployment","text":""},{"location":"guides/deployment-modal/#example-1-vllm-with-tensor-parallelism","title":"Example 1: VLLM with Tensor Parallelism","text":"<p>Use VLLM to distribute inference across multiple GPUs.</p> <pre><code>import modal\n\n# Use larger GPU for tensor parallelism\nGPU_CONFIG = \"A10G:2\"  # 2 GPUs\n\nIMAGE = (\n    modal.Image.from_registry(f\"nvidia/cuda:12.4.0-devel-ubuntu22.04\", add_python=\"3.12\")\n    .apt_install(\"libopenmpi-dev\", \"libnuma-dev\", \"libgl1-mesa-glx\", \"libglib2.0-0\")\n    .uv_pip_install(\n        \"torch\", \"transformers\", \"pillow\", \"pydantic\",\n        \"huggingface_hub\", \"accelerate\",\n    )\n    # VLLM for multi-GPU inference\n    .uv_pip_install(\"vllm\")\n    .run_commands(\"uv pip install flash-attn --no-build-isolation --system\")\n    .env({\n        \"HF_HUB_ENABLE_HF_TRANSFER\": \"1\",\n        \"HF_HOME\": \"/data/.cache\",\n    })\n)\n\nvolume = modal.Volume.from_name(\"omnidocs\", create_if_missing=True)\nsecret = modal.Secret.from_name(\"adithya-hf-wandb\")\n\napp = modal.App(\"omnidocs-vllm-2gpu\")\n\n@app.function(\n    image=IMAGE,\n    gpu=GPU_CONFIG,  # 2 GPUs\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=600,\n)\ndef extract_vllm_2gpu(image_bytes: bytes) -&gt; Dict[str, Any]:\n    \"\"\"\n    Extract using VLLM with 2-GPU tensor parallelism.\n    \"\"\"\n    from omnidocs.tasks.text_extraction import QwenTextExtractor\n    from omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\n    from PIL import Image\n    import io\n\n    image = Image.open(io.BytesIO(image_bytes))\n\n    # Configure for 2 GPUs\n    config = QwenTextVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,  # Distribute across 2 GPUs\n        gpu_memory_utilization=0.9,\n        max_tokens=4096,\n    )\n    extractor = QwenTextExtractor(backend=config)\n\n    result = extractor.extract(image, output_format=\"markdown\")\n\n    return {\n        \"success\": True,\n        \"word_count\": result.word_count,\n        \"model\": \"VLLM (2-GPU tensor parallel)\",\n    }\n</code></pre>"},{"location":"guides/deployment-modal/#example-2-multi-function-with-load-balancing","title":"Example 2: Multi-Function with Load Balancing","text":"<p>Deploy multiple functions to handle parallel requests.</p> <pre><code>import modal\nfrom typing import Dict, Any\n\n# ... (image and app setup)\n\n# Create 3 independent extract functions\nfor func_idx in range(3):\n    @app.function(\n        image=IMAGE,\n        gpu=\"A10G:1\",\n        volumes={\"/data\": volume},\n        secrets=[secret],\n        timeout=600,\n        name=f\"extract_{func_idx}\",\n    )\n    def extract_text(image_bytes: bytes, _func_idx=func_idx) -&gt; Dict[str, Any]:\n        # Same implementation as before\n        # Modal will create 3 independent functions\n        from omnidocs.tasks.text_extraction import QwenTextExtractor\n        from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n        from PIL import Image\n        import io\n\n        image = Image.open(io.BytesIO(image_bytes))\n        config = QwenTextPyTorchConfig(device=\"cuda\")\n        extractor = QwenTextExtractor(backend=config)\n        result = extractor.extract(image, output_format=\"markdown\")\n\n        return {\n            \"success\": True,\n            \"word_count\": result.word_count,\n            \"worker\": _func_idx,\n        }\n\n@app.local_entrypoint()\ndef main():\n    \"\"\"Process 3 images in parallel.\"\"\"\n    import concurrent.futures\n\n    # Get function references\n    extract_0 = modal.Function.lookup(\"omnidocs-multiworker\", \"extract_0\")\n    extract_1 = modal.Function.lookup(\"omnidocs-multiworker\", \"extract_1\")\n    extract_2 = modal.Function.lookup(\"omnidocs-multiworker\", \"extract_2\")\n\n    functions = [extract_0, extract_1, extract_2]\n\n    # Prepare 3 test images\n    image_bytes_list = [\n        open(f\"test_{i}.png\", \"rb\").read()\n        for i in range(3)\n    ]\n\n    # Process in parallel\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        futures = [\n            executor.submit(func.remote, img_bytes)\n            for func, img_bytes in zip(functions, image_bytes_list)\n        ]\n        results = [f.result() for f in futures]\n\n    print(f\"Processed {len(results)} images in parallel\")\n    for result in results:\n        print(f\"  Worker {result['worker']}: {result['word_count']} words\")\n</code></pre>"},{"location":"guides/deployment-modal/#production-patterns","title":"Production Patterns","text":""},{"location":"guides/deployment-modal/#scheduled-processing","title":"Scheduled Processing","text":"<p>Run batch processing on a schedule.</p> <pre><code>import modal\nfrom datetime import datetime\n\n# ... (image and app setup)\n\n@app.function(\n    image=IMAGE,\n    gpu=\"A10G:1\",\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=3600,\n    schedule=modal.Period(days=1),  # Daily at midnight UTC\n)\ndef daily_batch_processing():\n    \"\"\"Process accumulated documents daily.\"\"\"\n    from pathlib import Path\n    from omnidocs.tasks.text_extraction import QwenTextExtractor\n    from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n    from PIL import Image\n    import json\n\n    # Find new documents\n    inbox_dir = Path(\"/data/inbox\")\n    processed_dir = Path(\"/data/processed\")\n    results_dir = Path(\"/data/results\")\n\n    results_dir.mkdir(exist_ok=True)\n\n    # Initialize extractor\n    config = QwenTextPyTorchConfig(device=\"cuda\")\n    extractor = QwenTextExtractor(backend=config)\n\n    # Process documents\n    for image_path in inbox_dir.glob(\"*.png\"):\n        image = Image.open(image_path)\n        result = extractor.extract(image, output_format=\"markdown\")\n\n        # Save result\n        result_file = results_dir / f\"{image_path.stem}.json\"\n        with open(result_file, \"w\") as f:\n            json.dump({\n                \"filename\": image_path.name,\n                \"word_count\": result.word_count,\n                \"content_length\": result.content_length,\n                \"processed_at\": datetime.now().isoformat(),\n            }, f)\n\n        # Move to processed\n        image_path.rename(processed_dir / image_path.name)\n\n    print(f\"Daily processing complete\")\n</code></pre>"},{"location":"guides/deployment-modal/#webhook-handler","title":"Webhook Handler","text":"<p>Accept requests from an external service.</p> <pre><code>import modal\nfrom typing import Dict, Any\nfrom fastapi import FastAPI\n\n# ... (image setup)\n\nweb_app = FastAPI()\n\n# Create Modal web endpoint\n@app.function(\n    image=IMAGE,\n    gpu=\"A10G:1\",\n    volumes={\"/data\": volume},\n    secrets=[secret],\n)\n@modal.web_endpoint(method=\"POST\")\ndef extract_from_url(request: Dict[str, str]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Accept image URL and extract text.\n\n    POST /extract_from_url\n    {\"image_url\": \"https://example.com/doc.png\"}\n    \"\"\"\n    import requests\n    from omnidocs.tasks.text_extraction import QwenTextExtractor\n    from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n    from PIL import Image\n    import io\n\n    # Download image\n    response = requests.get(request[\"image_url\"], timeout=30)\n    image = Image.open(io.BytesIO(response.content))\n\n    # Extract\n    config = QwenTextPyTorchConfig(device=\"cuda\")\n    extractor = QwenTextExtractor(backend=config)\n    result = extractor.extract(image, output_format=\"markdown\")\n\n    return {\n        \"success\": True,\n        \"word_count\": result.word_count,\n        \"content\": result.content,\n    }\n</code></pre> <p>Deploy and test: <pre><code># Deploy\nmodal deploy script.py\n\n# Get URL\nmodal app list\n\n# Test\ncurl -X POST https://your-workspace.modal.run/extract_from_url \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"image_url\":\"https://example.com/doc.png\"}'\n</code></pre></p>"},{"location":"guides/deployment-modal/#monitoring-logging","title":"Monitoring &amp; Logging","text":""},{"location":"guides/deployment-modal/#log-extraction-progress","title":"Log Extraction Progress","text":"<pre><code>import modal\nimport logging\nfrom typing import List\n\n# ... (image and app setup)\n\n@app.function(\n    image=IMAGE,\n    gpu=\"A10G:1\",\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=1800,\n)\ndef process_with_logging(image_bytes_list: List[bytes]) -&gt; Dict[str, Any]:\n    \"\"\"Process with detailed logging.\"\"\"\n    import logging\n\n    # Configure logging\n    logging.basicConfig(\n        level=logging.INFO,\n        format='[%(asctime)s] %(levelname)s: %(message)s',\n    )\n    logger = logging.getLogger(__name__)\n\n    from omnidocs.tasks.text_extraction import QwenTextExtractor\n    from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n    from PIL import Image\n    import io\n    import time\n    import torch\n\n    logger.info(f\"Starting batch processing: {len(image_bytes_list)} images\")\n    logger.info(f\"GPU: {torch.cuda.get_device_name(0)}\")\n\n    config = QwenTextPyTorchConfig(device=\"cuda\")\n    extractor = QwenTextExtractor(backend=config)\n\n    results = []\n    start_time = time.time()\n\n    for idx, image_bytes in enumerate(image_bytes_list, 1):\n        try:\n            iter_start = time.time()\n\n            image = Image.open(io.BytesIO(image_bytes))\n            result = extractor.extract(image, output_format=\"markdown\")\n\n            iter_time = time.time() - iter_start\n\n            logger.info(f\"[{idx}/{len(image_bytes_list)}] \"\n                       f\"Processed in {iter_time:.2f}s, \"\n                       f\"{result.word_count} words\")\n\n            results.append({\"success\": True})\n\n        except Exception as e:\n            logger.error(f\"[{idx}] Failed: {e}\")\n            results.append({\"success\": False, \"error\": str(e)})\n\n    total_time = time.time() - start_time\n    logger.info(f\"Batch complete: {total_time:.1f}s total, \"\n               f\"{total_time/len(image_bytes_list):.2f}s per image\")\n\n    return {\"results\": results}\n</code></pre>"},{"location":"guides/deployment-modal/#monitor-gpu-memory","title":"Monitor GPU Memory","text":"<pre><code>@app.function(\n    image=IMAGE,\n    gpu=\"A10G:1\",\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=600,\n)\ndef extract_with_memory_monitoring(image_bytes: bytes):\n    \"\"\"Extract with GPU memory monitoring.\"\"\"\n    import torch\n\n    def log_memory(label):\n        allocated = torch.cuda.memory_allocated() / 1e9\n        reserved = torch.cuda.memory_reserved() / 1e9\n        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n        print(f\"{label}: allocated={allocated:.1f}GB, \"\n              f\"reserved={reserved:.1f}GB, total={total:.1f}GB\")\n\n    log_memory(\"Initial\")\n\n    # Load model\n    from omnidocs.tasks.text_extraction import QwenTextExtractor\n    from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n    config = QwenTextPyTorchConfig(device=\"cuda\")\n    extractor = QwenTextExtractor(backend=config)\n\n    log_memory(\"After model load\")\n\n    # Extract\n    from PIL import Image\n    import io\n\n    image = Image.open(io.BytesIO(image_bytes))\n    result = extractor.extract(image, output_format=\"markdown\")\n\n    log_memory(\"After extraction\")\n\n    return {\"success\": True}\n</code></pre>"},{"location":"guides/deployment-modal/#cost-optimization","title":"Cost Optimization","text":""},{"location":"guides/deployment-modal/#gpu-selection","title":"GPU Selection","text":"GPU $/hour Ideal For A10G $0.35 General purpose, fast A40 $1.10 Large models, high VRAM T4 $0.15 Budget processing L40S $1.25 High-end graphics <p>Recommendation: A10G (sweet spot of price/performance)</p>"},{"location":"guides/deployment-modal/#cost-calculation","title":"Cost Calculation","text":"<pre><code>Cost per document = (Model load time + Processing time) \u00d7 $/hour GPU\n\nExample (A10G @ $0.35/hour):\n- Model load: 2 seconds (one-time amortized across batch)\n- Per-image: 3 seconds\n- Batch of 100: (2 + 100*3) / 3600 hours \u00d7 $0.35/hour \u2248 $0.03 per image\n- 100 documents: ~$3 total GPU cost\n</code></pre>"},{"location":"guides/deployment-modal/#batch-size-optimization","title":"Batch Size Optimization","text":"<p>Larger batches = lower per-item cost (amortize model load).</p> <pre><code># Calculation\nmodel_load_time = 2  # seconds\nper_image_time = 3   # seconds\ngpu_cost_per_hour = 0.35  # dollars\nbatch_sizes = [1, 5, 10, 50, 100]\n\nprint(\"Batch Size | Total Time | Cost per Image\")\nprint(\"-\" * 40)\n\nfor batch_size in batch_sizes:\n    total_time = (model_load_time + batch_size * per_image_time) / 3600\n    cost_per_image = total_time * gpu_cost_per_hour / batch_size\n    print(f\"{batch_size:10} | {total_time*3600:9.0f}s | ${cost_per_image:.4f}\")\n\n# Output:\n# Batch Size | Total Time | Cost per Image\n# ----------------------------------------\n#          1 |          5s | $0.0005\n#          5 |         17s | $0.0012\n#         10 |         32s | $0.0031\n#         50 |        152s | $0.0149\n#        100 |        302s | $0.0293\n</code></pre> <p>Takeaway: Batch size of ~50 is optimal (amortizes load, avoids timeout issues)</p>"},{"location":"guides/deployment-modal/#spot-instances","title":"Spot Instances","text":"<p>Use cheaper Spot instances for non-critical batches.</p> <pre><code>import modal\n\n@app.function(\n    image=IMAGE,\n    gpu=modal.gpu.A10G(count=1, spot=True),  # Use Spot instance\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=600,\n)\ndef extract_spot(image_bytes: bytes):\n    # Same as regular extraction\n    pass\n\n# Cost: ~60% cheaper than on-demand\n</code></pre>"},{"location":"guides/deployment-modal/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/deployment-modal/#model-download-stuck","title":"Model Download Stuck","text":"<p>Problem: Model stuck downloading.</p> <p>Solution: Set HF token and increase timeout.</p> <pre><code># Ensure HF token is set\nmodal secret create adithya-hf-wandb \\\n  --key HF_TOKEN \\\n  --value \"your-token\"\n\n# Increase timeout for initial runs\n@app.function(\n    ...,\n    timeout=1800,  # 30 minutes for first run\n)\n</code></pre>"},{"location":"guides/deployment-modal/#cuda-out-of-memory","title":"CUDA Out of Memory","text":"<p>Problem: CUDA OOM during extraction.</p> <p>Solution: Use larger GPU or reduce model size.</p> <pre><code># Option 1: Use larger GPU\ngpu=modal.gpu.A40(count=1)  # 48GB VRAM vs 24GB on A10G\n\n# Option 2: Use smaller model\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-7B-Instruct\",  # 7B instead of 8B\n)\n\n# Option 3: Use VLLM with tensor parallelism\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\nconfig = QwenTextVLLMConfig(\n    tensor_parallel_size=2,  # Split across 2 GPUs\n)\ngpu=modal.gpu.A10G(count=2)\n</code></pre>"},{"location":"guides/deployment-modal/#timeout-errors","title":"Timeout Errors","text":"<p>Problem: Function times out.</p> <p>Solution: Increase timeout or reduce batch size.</p> <pre><code>@app.function(\n    ...,\n    timeout=1800,  # Increase to 30 minutes\n)\n\n# Or reduce batch size\nbatch_size = 10  # Process 10 at a time instead of 100\n</code></pre>"},{"location":"guides/deployment-modal/#network-errors","title":"Network Errors","text":"<p>Problem: Intermittent network failures.</p> <p>Solution: Add retry logic.</p> <pre><code>from tenacity import retry, stop_after_attempt, wait_exponential\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n@retry(\n    stop=stop_after_attempt(3),\n    wait=wait_exponential(multiplier=1, min=2, max=10),\n)\ndef download_with_retry(image_url: str):\n    import requests\n    response = requests.get(image_url, timeout=30)\n    response.raise_for_status()\n    return response.content\n</code></pre> <p>Next Steps: - See Batch Processing Guide for local batch patterns - See Text Extraction Guide for model configuration - Modal docs: https://modal.com/docs</p>"},{"location":"guides/layout-analysis/","title":"Layout Analysis Guide","text":"<p>Detect document structure and element boundaries using layout detection models. This guide explains how to extract layout information, work with labels, and build advanced document processing pipelines.</p>"},{"location":"guides/layout-analysis/#table-of-contents","title":"Table of Contents","text":"<ul> <li>What is Layout Detection</li> <li>Available Models</li> <li>Fixed vs Custom Labels</li> <li>Basic Usage</li> <li>Filtering and Analyzing Results</li> <li>Visualization</li> <li>Advanced: Custom Labels</li> <li>Troubleshooting</li> </ul>"},{"location":"guides/layout-analysis/#what-is-layout-detection","title":"What is Layout Detection","text":"<p>Layout detection identifies document regions (elements) and classifies them by type. Unlike OCR (which extracts text) or text extraction (which formats content), layout detection provides structural information.</p> <p>Output: List of bounding boxes with labels and confidence scores.</p> <p>Use Cases: - Document structure analysis - Segmentation for downstream processing - Building multi-stage pipelines (layout \u2192 text \u2192 output) - Understanding document semantics - Filtering unwanted elements (headers, footers, artifacts)</p>"},{"location":"guides/layout-analysis/#available-models","title":"Available Models","text":""},{"location":"guides/layout-analysis/#1-doclayoutyolo-fast-fixed-labels","title":"1. DocLayoutYOLO (Fast, fixed labels)","text":"<p>YOLO-based detector optimized for speed and accuracy on document layouts.</p> <p>Strengths: - Extremely fast (~0.3-0.5 sec per page) - High accuracy on standard document elements - Single-backend (PyTorch only) - Low memory requirements</p> <p>Weaknesses: - Fixed labels only (no custom categories) - Less accurate on irregular documents - May struggle with non-English text</p> <p>Fixed Labels: - <code>title</code> - Document/section titles - <code>text</code> - Body paragraphs - <code>list</code> - Bullet or numbered lists - <code>table</code> - Data tables - <code>figure</code> - Images, diagrams, charts - <code>caption</code> - Figure/table captions - <code>formula</code> - Mathematical equations - <code>footnote</code> - Footnotes - <code>page_header</code> - Page headers - <code>page_footer</code> - Page footers - <code>unknown</code> - Unclassifiable elements</p>"},{"location":"guides/layout-analysis/#2-rt-detr-accuracy-focused-fixed-labels","title":"2. RT-DETR (Accuracy-focused, fixed labels)","text":"<p>High-accuracy detector with stronger backbone, but slower than YOLO.</p> <p>Strengths: - Higher accuracy than YOLO - Good on challenging document types - Handles small elements better</p> <p>Weaknesses: - Slower (~1-2 sec per page) - Higher memory requirements - Fixed labels only</p> <p>Same fixed labels as DocLayoutYOLO.</p>"},{"location":"guides/layout-analysis/#3-qwenlayoutdetector-flexible-custom-labels","title":"3. QwenLayoutDetector (Flexible, custom labels)","text":"<p>Vision-language model supporting custom layout labels.</p> <p>Strengths: - Flexible custom labels (define your own) - Strong on diverse document types - Multi-backend support (PyTorch, VLLM, MLX, API) - Better on irregular layouts</p> <p>Weaknesses: - Slower than YOLO (~2-3 sec per page) - Higher VRAM requirements - Requires more GPU memory</p> <p>Supports: - Standard LayoutLabel enum - Custom labels (unlimited) - Mixed standard + custom labels</p>"},{"location":"guides/layout-analysis/#fixed-vs-custom-labels","title":"Fixed vs Custom Labels","text":""},{"location":"guides/layout-analysis/#fixed-labels-doclayoutyolo-rt-detr","title":"Fixed Labels (DocLayoutYOLO, RT-DETR)","text":"<p>Predefined categories that the model recognizes during training.</p> <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\n# Fixed labels are built-in and cannot be changed\nconfig = DocLayoutYOLOConfig(device=\"cuda\")\ndetector = DocLayoutYOLO(config=config)\n\nresult = detector.extract(image)\n\n# Result contains elements with fixed labels\nfor element in result.elements:\n    print(f\"{element.label}: {element.bbox}\")\n    # Labels will always be from: title, text, list, table, figure, caption, formula, etc.\n</code></pre> <p>Available Fixed Labels:</p> Label Use Case Typical Content <code>title</code> Document/section heading \"Chapter 1\", \"Introduction\" <code>text</code> Body paragraphs Main content text <code>list</code> Bullet or numbered lists \"- Item 1\", \"1. Point A\" <code>table</code> Data tables and grids Tabular data <code>figure</code> Images, diagrams, charts Photos, graphics, plots <code>caption</code> Figure/table descriptions \"Figure 1: Title\" <code>formula</code> Mathematical equations LaTeX, equations <code>footnote</code> Bottom-of-page notes Footnotes, endnotes <code>page_header</code> Page header regions Running headers <code>page_footer</code> Page footer regions Running footers <code>unknown</code> Unclassifiable elements Artifacts, noise"},{"location":"guides/layout-analysis/#custom-labels-qwenlayoutdetector-only","title":"Custom Labels (QwenLayoutDetector only)","text":"<p>Define your own label categories for domain-specific documents.</p> <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\nfrom omnidocs.tasks.layout_extraction import CustomLabel\n\n# Define custom labels\ncustom_labels = [\n    CustomLabel(name=\"code_block\", description=\"Code snippets\"),\n    CustomLabel(name=\"sidebar\", description=\"Sidebar content\"),\n    CustomLabel(name=\"pull_quote\", description=\"Quoted text\"),\n]\n\nconfig = QwenLayoutPyTorchConfig(device=\"cuda\")\ndetector = QwenLayoutDetector(backend=config)\n\nresult = detector.extract(image, custom_labels=custom_labels)\n\n# Result contains elements with your custom labels\nfor element in result.elements:\n    print(f\"{element.label}: {element.bbox}\")\n    # Labels will be: code_block, sidebar, pull_quote, or standard labels\n</code></pre>"},{"location":"guides/layout-analysis/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/layout-analysis/#example-1-fast-layout-detection-doclayoutyolo","title":"Example 1: Fast Layout Detection (DocLayoutYOLO)","text":"<p>Detect layout using the fast YOLO-based model.</p> <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs import Document\nfrom PIL import Image\n\n# Load image\nimage = Image.open(\"document_page.png\")\n\n# Initialize detector (fastest option)\nconfig = DocLayoutYOLOConfig(\n    device=\"cuda\",  # or \"cpu\"\n    img_size=1024,\n    confidence=0.25,  # Detection confidence threshold\n)\ndetector = DocLayoutYOLO(config=config)\n\n# Extract layout\nresult = detector.extract(image)\n\nprint(f\"Detected {len(result.elements)} layout elements\")\n\n# Display results\nfor element in result.elements:\n    print(f\"  {element.label:12} @ {element.bbox} (confidence: {element.confidence:.2f})\")\n\n# Count by label\nfrom collections import Counter\nlabel_counts = Counter(e.label for e in result.elements)\nprint(f\"\\nSummary: {dict(label_counts)}\")\n</code></pre> <p>Output Example: <pre><code>Detected 12 layout elements\n  title         @ [50, 20, 500, 60] (confidence: 0.98)\n  text          @ [50, 80, 950, 300] (confidence: 0.95)\n  figure        @ [50, 320, 450, 650] (confidence: 0.92)\n  caption       @ [50, 660, 450, 700] (confidence: 0.88)\n  text          @ [480, 320, 950, 600] (confidence: 0.94)\n  table         @ [50, 720, 950, 1000] (confidence: 0.91)\n\nSummary: {'title': 1, 'text': 2, 'figure': 1, 'caption': 1, 'table': 1, ...}\n</code></pre></p>"},{"location":"guides/layout-analysis/#example-2-extract-from-pdf-with-multiple-pages","title":"Example 2: Extract from PDF with Multiple Pages","text":"<p>Process all pages of a PDF document.</p> <pre><code>from omnidocs import Document\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom pathlib import Path\n\n# Load PDF\ndoc = Document.from_pdf(\"document.pdf\")\nprint(f\"Loaded {doc.page_count} pages\")\n\n# Initialize detector\nconfig = DocLayoutYOLOConfig(device=\"cuda\")\ndetector = DocLayoutYOLO(config=config)\n\n# Process all pages\npage_layouts = []\nfor page_idx in range(doc.page_count):\n    page_image = doc.get_page(page_idx)\n    result = detector.extract(page_image)\n    page_layouts.append({\n        \"page\": page_idx + 1,\n        \"num_elements\": len(result.elements),\n        \"elements\": result.elements,\n    })\n    print(f\"Page {page_idx + 1}: {len(result.elements)} elements\")\n\n# Summary statistics\ntotal_elements = sum(p[\"num_elements\"] for p in page_layouts)\nprint(f\"\\nTotal: {total_elements} elements across {doc.page_count} pages\")\n</code></pre>"},{"location":"guides/layout-analysis/#example-3-high-accuracy-detection-rt-detr","title":"Example 3: High-Accuracy Detection (RT-DETR)","text":"<p>Use the more accurate RT-DETR model for challenging documents.</p> <pre><code>from omnidocs.tasks.layout_extraction import RTDETRLayoutDetector, RTDETRLayoutConfig\nfrom PIL import Image\n\nimage = Image.open(\"complex_document.png\")\n\n# Use RT-DETR for better accuracy (slower)\nconfig = RTDETRLayoutConfig(\n    device=\"cuda\",\n    confidence=0.3,  # Lower confidence threshold for more detections\n)\ndetector = RTDETRLayoutDetector(config=config)\n\nresult = detector.extract(image)\n\n# Filter by confidence\nhigh_confidence = [e for e in result.elements if e.confidence &gt;= 0.9]\nprint(f\"High confidence detections: {len(high_confidence)}/{len(result.elements)}\")\n\nfor element in high_confidence:\n    print(f\"  {element.label:12} {element.bbox} (conf: {element.confidence:.3f})\")\n</code></pre>"},{"location":"guides/layout-analysis/#example-4-custom-labels-with-qwen","title":"Example 4: Custom Labels with Qwen","text":"<p>Use Qwen to detect custom document elements.</p> <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\nfrom omnidocs.tasks.layout_extraction import CustomLabel\nfrom PIL import Image\n\nimage = Image.open(\"technical_document.png\")\n\n# Define domain-specific labels\ncustom_labels = [\n    CustomLabel(name=\"code_block\", description=\"Syntax-highlighted code\"),\n    CustomLabel(name=\"api_doc\", description=\"API documentation/reference\"),\n    CustomLabel(name=\"note_box\", description=\"Important note or warning\"),\n    CustomLabel(name=\"example\", description=\"Code example or usage\"),\n]\n\n# Initialize with PyTorch backend\nconfig = QwenLayoutPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cuda\",\n)\ndetector = QwenLayoutDetector(backend=config)\n\n# Extract with custom labels\nresult = detector.extract(image, custom_labels=custom_labels)\n\n# Analyze custom labels\ncustom_elements = [e for e in result.elements if e.label in [l.name for l in custom_labels]]\nprint(f\"Found {len(custom_elements)} domain-specific elements\")\n\nfor element in custom_elements:\n    print(f\"  {element.label:12} {element.bbox}\")\n</code></pre>"},{"location":"guides/layout-analysis/#filtering-and-analyzing-results","title":"Filtering and Analyzing Results","text":""},{"location":"guides/layout-analysis/#filter-by-label","title":"Filter by Label","text":"<p>Extract only specific types of elements.</p> <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\ndetector = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = detector.extract(image)\n\n# Get only text elements\ntext_elements = [e for e in result.elements if e.label == \"text\"]\nprint(f\"Text blocks: {len(text_elements)}\")\n\n# Get tables and figures\nvisual_elements = [e for e in result.elements if e.label in [\"table\", \"figure\"]]\nprint(f\"Visual elements: {len(visual_elements)}\")\n\n# Exclude page artifacts (headers, footers)\ncontent_elements = [\n    e for e in result.elements\n    if e.label not in [\"page_header\", \"page_footer\", \"unknown\"]\n]\nprint(f\"Main content elements: {len(content_elements)}\")\n</code></pre>"},{"location":"guides/layout-analysis/#filter-by-confidence","title":"Filter by Confidence","text":"<p>Exclude low-confidence detections.</p> <pre><code># Keep only high-confidence detections\nmin_confidence = 0.8\nfiltered = [e for e in result.elements if e.confidence &gt;= min_confidence]\n\nprint(f\"Original: {len(result.elements)} elements\")\nprint(f\"Filtered (confidence &gt;= {min_confidence}): {len(filtered)} elements\")\n\n# Analyze confidence distribution\nconfidences = [e.confidence for e in result.elements]\nprint(f\"Confidence range: {min(confidences):.2f} - {max(confidences):.2f}\")\nprint(f\"Average: {sum(confidences)/len(confidences):.2f}\")\n</code></pre>"},{"location":"guides/layout-analysis/#filter-by-bounding-box","title":"Filter by Bounding Box","text":"<p>Find elements in specific image regions.</p> <pre><code># Find elements in top half of page\nimage_height = image.height\ntop_half_elements = [\n    e for e in result.elements\n    if e.bbox[1] &lt; image_height / 2  # y1 &lt; height/2\n]\nprint(f\"Elements in top half: {len(top_half_elements)}\")\n\n# Find elements in specific region (e.g., sidebar)\ndef in_region(bbox, region):\n    \"\"\"Check if element overlaps with region.\"\"\"\n    x1, y1, x2, y2 = bbox\n    rx1, ry1, rx2, ry2 = region\n    # Check overlap\n    return not (x2 &lt; rx1 or x1 &gt; rx2 or y2 &lt; ry1 or y1 &gt; ry2)\n\nsidebar_region = (0, 0, 200, 1024)  # Left 200px\nsidebar_elements = [e for e in result.elements if in_region(e.bbox, sidebar_region)]\nprint(f\"Elements in sidebar region: {len(sidebar_elements)}\")\n</code></pre>"},{"location":"guides/layout-analysis/#analyze-element-sizes","title":"Analyze Element Sizes","text":"<p>Check element dimensions for quality control.</p> <pre><code># Calculate element sizes\nelement_sizes = []\nfor element in result.elements:\n    bbox = element.bbox  # [x1, y1, x2, y2]\n    width = bbox[2] - bbox[0]\n    height = bbox[3] - bbox[1]\n    area = width * height\n    element_sizes.append({\n        \"label\": element.label,\n        \"width\": width,\n        \"height\": height,\n        \"area\": area,\n    })\n\n# Find largest elements\nlargest = sorted(element_sizes, key=lambda e: e[\"area\"], reverse=True)[:5]\nprint(\"Largest elements:\")\nfor elem in largest:\n    print(f\"  {elem['label']}: {elem['width']:.0f}x{elem['height']:.0f} ({elem['area']:.0f} px\u00b2)\")\n\n# Find anomalies (very small or very large)\navg_area = sum(e[\"area\"] for e in element_sizes) / len(element_sizes)\nanomalies = [e for e in element_sizes if e[\"area\"] &lt; avg_area * 0.1 or e[\"area\"] &gt; avg_area * 10]\nprint(f\"Anomalous elements: {len(anomalies)}\")\n</code></pre>"},{"location":"guides/layout-analysis/#visualization","title":"Visualization","text":""},{"location":"guides/layout-analysis/#visualize-detections","title":"Visualize Detections","text":"<p>Draw bounding boxes on the image.</p> <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom PIL import Image, ImageDraw\nimport random\n\nimage = Image.open(\"document.png\")\ndetector = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = detector.extract(image)\n\n# Create visualization\nvis_image = image.copy()\ndraw = ImageDraw.Draw(vis_image)\n\n# Color map for labels\ncolors = {\n    \"title\": \"#FF0000\",\n    \"text\": \"#00FF00\",\n    \"table\": \"#0000FF\",\n    \"figure\": \"#FFFF00\",\n    \"list\": \"#FF00FF\",\n    \"caption\": \"#00FFFF\",\n}\n\n# Draw bounding boxes\nfor element in result.elements:\n    bbox = element.bbox\n    label = element.label\n    color = colors.get(label, \"#FFFFFF\")\n\n    # Draw rectangle\n    draw.rectangle(bbox, outline=color, width=2)\n\n    # Draw label\n    draw.text((bbox[0], bbox[1]-10), label, fill=color)\n\n# Save visualization\nvis_image.save(\"layout_visualization.png\")\nprint(\"Saved visualization to layout_visualization.png\")\n</code></pre>"},{"location":"guides/layout-analysis/#use-built-in-visualization-if-available","title":"Use Built-in Visualization (if available)","text":"<p>Some models provide built-in visualization.</p> <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\ndetector = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = detector.extract(image)\n\n# If the model supports visualization\nif hasattr(result, 'visualize'):\n    vis_image = result.visualize(image)\n    vis_image.save(\"layout_visualization.png\")\nelse:\n    print(\"Visualization not available for this model\")\n</code></pre>"},{"location":"guides/layout-analysis/#create-mask-images","title":"Create Mask Images","text":"<p>Generate segmentation masks for each label.</p> <pre><code>from PIL import Image, ImageDraw\nimport numpy as np\n\n# Label to color mapping\nlabel_colors = {\n    \"title\": (255, 0, 0),\n    \"text\": (0, 255, 0),\n    \"table\": (0, 0, 255),\n    \"figure\": (255, 255, 0),\n    \"list\": (255, 0, 255),\n    \"caption\": (0, 255, 255),\n}\n\n# Create mask image\nmask = Image.new(\"RGB\", image.size, color=(255, 255, 255))\ndraw = ImageDraw.Draw(mask)\n\n# Draw filled rectangles\nfor element in result.elements:\n    color = label_colors.get(element.label, (128, 128, 128))\n    draw.rectangle(element.bbox, fill=color)\n\n# Save and display\nmask.save(\"layout_mask.png\")\n\n# Create overlay\noverlay = Image.blend(image, mask, alpha=0.5)\noverlay.save(\"layout_overlay.png\")\nprint(\"Saved mask and overlay\")\n</code></pre>"},{"location":"guides/layout-analysis/#advanced-custom-labels","title":"Advanced: Custom Labels","text":""},{"location":"guides/layout-analysis/#multi-stage-pipeline-with-custom-labels","title":"Multi-Stage Pipeline with Custom Labels","text":"<p>Detect custom elements, then extract text from specific types.</p> <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\nfrom omnidocs.tasks.layout_extraction import CustomLabel\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom PIL import Image, ImageDraw\nimport numpy as np\n\nimage = Image.open(\"document.png\")\n\n# Stage 1: Layout detection with custom labels\ncustom_labels = [\n    CustomLabel(name=\"code_snippet\"),\n    CustomLabel(name=\"data_table\"),\n    CustomLabel(name=\"api_reference\"),\n]\n\nlayout_config = QwenLayoutPyTorchConfig(device=\"cuda\")\nlayout_detector = QwenLayoutDetector(backend=layout_config)\nlayout_result = layout_detector.extract(image, custom_labels=custom_labels)\n\n# Group elements by type\ncode_snippets = [e for e in layout_result.elements if e.label == \"code_snippet\"]\ndata_tables = [e for e in layout_result.elements if e.label == \"data_table\"]\napi_docs = [e for e in layout_result.elements if e.label == \"api_reference\"]\n\nprint(f\"Found: {len(code_snippets)} code snippets, \"\n      f\"{len(data_tables)} tables, \"\n      f\"{len(api_docs)} API references\")\n\n# Stage 2: Extract text from specific regions\ntext_config = QwenTextPyTorchConfig(device=\"cuda\")\ntext_extractor = QwenTextExtractor(backend=text_config)\n\n# Process code snippets with special handling\nfor snippet in code_snippets:\n    x1, y1, x2, y2 = snippet.bbox\n    snippet_img = image.crop((x1, y1, x2, y2))\n\n    # Use specialized prompt for code\n    code_prompt = \"Extract the code from this code snippet. Format as a code block with language identifier.\"\n    result = text_extractor.extract(\n        snippet_img,\n        output_format=\"markdown\",\n        custom_prompt=code_prompt,\n    )\n    print(f\"Code snippet:\\n{result.content}\\n\")\n</code></pre>"},{"location":"guides/layout-analysis/#custom-labels-with-constraints","title":"Custom Labels with Constraints","text":"<p>Add validation and constraints to custom labels.</p> <pre><code>from omnidocs.tasks.layout_extraction import CustomLabel\n\n# Create labels with metadata\nlabels = [\n    CustomLabel(\n        name=\"warning_box\",\n        description=\"Important warning or alert\",\n        color=\"#FF0000\",  # Custom metadata\n    ),\n    CustomLabel(\n        name=\"tip_box\",\n        description=\"Helpful tip or best practice\",\n        color=\"#00FF00\",\n    ),\n    CustomLabel(\n        name=\"example_code\",\n        description=\"Code example or snippet\",\n        color=\"#0000FF\",\n    ),\n]\n\n# Use in extraction\ndetector = QwenLayoutDetector(backend=config)\nresult = detector.extract(image, custom_labels=labels)\n\n# Group by custom category\nwarnings = [e for e in result.elements if e.label == \"warning_box\"]\ntips = [e for e in result.elements if e.label == \"tip_box\"]\nexamples = [e for e in result.elements if e.label == \"example_code\"]\n\nprint(f\"Warnings: {len(warnings)}, Tips: {len(tips)}, Examples: {len(examples)}\")\n</code></pre>"},{"location":"guides/layout-analysis/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/layout-analysis/#missing-elements","title":"Missing Elements","text":"<p>Problem: Some elements not detected.</p> <p>Solutions: 1. Lower <code>confidence</code> threshold 2. Try different model (RT-DETR instead of YOLO) 3. Resize image if very small 4. Check image quality</p> <pre><code># Solution 1: Lower confidence threshold\nconfig = DocLayoutYOLOConfig(device=\"cuda\", confidence=0.15)  # Default: 0.25\n\n# Solution 2: Try RT-DETR\nfrom omnidocs.tasks.layout_extraction import RTDETRLayoutDetector\n\ndetector = RTDETRLayoutDetector(config=RTDETRLayoutConfig(device=\"cuda\"))\nresult = detector.extract(image)\n\n# Solution 3: Resize image\nif image.width &lt; 512:\n    image = image.resize((image.width * 2, image.height * 2))\n\n# Solution 4: Check image quality\nprint(f\"Image size: {image.size}\")\nprint(f\"Image mode: {image.mode}\")\nif image.mode == \"RGBA\":\n    # Convert RGBA to RGB\n    image = image.convert(\"RGB\")\n</code></pre>"},{"location":"guides/layout-analysis/#false-positives","title":"False Positives","text":"<p>Problem: Detecting too many incorrect elements.</p> <p>Solutions: 1. Increase <code>confidence</code> threshold 2. Post-filter by size or region 3. Use different model</p> <pre><code># Solution 1: Increase confidence threshold\nconfig = DocLayoutYOLOConfig(device=\"cuda\", confidence=0.5)  # Higher threshold\n\nresult = detector.extract(image)\n\n# Solution 2: Filter by size\nfiltered = [\n    e for e in result.elements\n    if (e.bbox[2] - e.bbox[0]) &gt; 50  # Width &gt; 50px\n    and (e.bbox[3] - e.bbox[1]) &gt; 20  # Height &gt; 20px\n]\n\n# Solution 3: Try different model\n# If YOLO has too many false positives, try RT-DETR or Qwen\n</code></pre>"},{"location":"guides/layout-analysis/#overlapping-detections","title":"Overlapping Detections","text":"<p>Problem: Elements overlap incorrectly.</p> <p>Solutions: 1. Post-process to remove overlaps 2. Use different confidence threshold 3. Try different model</p> <pre><code>def remove_overlaps(elements, overlap_threshold=0.5):\n    \"\"\"Remove overlapping detections.\"\"\"\n    if not elements:\n        return []\n\n    # Sort by confidence (descending)\n    sorted_elements = sorted(elements, key=lambda e: e.confidence, reverse=True)\n\n    # Keep non-overlapping elements\n    kept = []\n    for elem in sorted_elements:\n        overlaps = False\n        for kept_elem in kept:\n            # Calculate intersection over union\n            # ... (implementation details)\n            pass\n        if not overlaps:\n            kept.append(elem)\n\n    return kept\n\nfiltered = remove_overlaps(result.elements)\nprint(f\"Removed {len(result.elements) - len(filtered)} overlapping detections\")\n</code></pre>"},{"location":"guides/layout-analysis/#slow-inference","title":"Slow Inference","text":"<p>Problem: Layout detection too slow.</p> <p>Solutions: 1. Use faster model (YOLO instead of RT-DETR) 2. Reduce image resolution 3. Use VLLM for Qwen</p> <pre><code># Solution 1: Use YOLO (fastest)\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO\ndetector = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\n\n# Solution 2: Resize image\noriginal_size = image.size\nimage = image.resize((image.width // 2, image.height // 2))\nresult = detector.extract(image)\n# Scale bboxes back\nfor elem in result.elements:\n    elem.bbox = [x * 2 for x in elem.bbox]\n\n# Solution 3: Use VLLM for Qwen\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutVLLMConfig\nconfig = QwenLayoutVLLMConfig(tensor_parallel_size=1)\n</code></pre> <p>Next Steps: - See Text Extraction Guide for extracting content from detected regions - See OCR Extraction Guide for character-level extraction - See Batch Processing Guide for processing many documents</p>"},{"location":"guides/ocr-extraction/","title":"OCR Extraction Guide","text":"<p>Extract text with precise bounding boxes at character, word, line, or block level. This guide covers when to use OCR, available models, multi-language support, and performance considerations.</p>"},{"location":"guides/ocr-extraction/#table-of-contents","title":"Table of Contents","text":"<ul> <li>OCR vs Text Extraction vs Layout</li> <li>Available Models</li> <li>Basic Usage</li> <li>Extracting Bounding Boxes</li> <li>Filtering Results</li> <li>Multi-Language Support</li> <li>Performance Comparison</li> <li>Troubleshooting</li> </ul>"},{"location":"guides/ocr-extraction/#ocr-vs-text-extraction-vs-layout","title":"OCR vs Text Extraction vs Layout","text":"Feature OCR Text Extraction Layout Detection Returns Text + bounding boxes Formatted text only Bounding boxes only Granularity Character/word/line Full document Element-level Location Info Yes (precise) No Yes (element regions) Output Type List of text blocks Single formatted string List of elements Use Case Word spotting, re-OCR, handwriting Document parsing Structure analysis Latency 1-2 sec per page 2-5 sec per page 0.5-1 sec per page Example Output <code>[{\"text\": \"Hello\", \"bbox\": [10, 20, 50, 35]}]</code> <code>\"# Hello\\n\\nWorld\"</code> <code>[{\"label\": \"title\", \"bbox\": [...]}]</code> <p>Choose OCR when: - You need precise character locations - Building re-OCR or correction pipelines - Extracting structured data from tables (get cell coordinates first) - Analyzing handwriting - Building word spotting systems</p> <p>Choose Text Extraction when: - Converting documents to readable format - Extracting full document content - Building markdown/HTML outputs - Focus on content quality over location</p> <p>Choose Layout Detection when: - Understanding document structure - Filtering unwanted elements - Multi-stage processing</p>"},{"location":"guides/ocr-extraction/#available-models","title":"Available Models","text":""},{"location":"guides/ocr-extraction/#model-comparison","title":"Model Comparison","text":"Model Speed Accuracy Languages GPU Req Best For Tesseract \u2b50\u2b50\u2b50\u2b50\u2b50 (Fast) \u2b50\u2b50\u2b50 (Good) 100+ None Legacy, CPU-only EasyOCR \u2b50\u2b50\u2b50 (Medium) \u2b50\u2b50\u2b50\u2b50 (Very Good) 80+ Optional Production use PaddleOCR \u2b50\u2b50\u2b50\u2b50 (Very Fast) \u2b50\u2b50\u2b50\u2b50 (Very Good) 11 Optional Speed-critical, Asian text CRAFT \u2b50\u2b50\u2b50 (Medium) \u2b50\u2b50\u2b50\u2b50 (Very Good) English Optional Scene text detection"},{"location":"guides/ocr-extraction/#1-tesseract-cpu-only","title":"1. Tesseract (CPU-only)","text":"<p>Traditional OCR engine, excellent for clean printed text.</p> <p>Strengths: - No GPU required, CPU-only - Extremely fast - Supports 100+ languages - Proven and reliable - Opensource (Apache 2.0)</p> <p>Weaknesses: - Lower accuracy on complex layouts - Struggles with handwriting - Needs training data for custom fonts</p> <p>When to use: - CPU-only systems (Raspberry Pi, servers) - Clean printed documents - Cost-sensitive applications - Multi-language documents</p> <p>Languages: 100+ (English, Chinese, Arabic, Hindi, etc.)</p>"},{"location":"guides/ocr-extraction/#2-easyocr-gpu-recommended","title":"2. EasyOCR (GPU-recommended)","text":"<p>Deep learning OCR with excellent accuracy.</p> <p>Strengths: - Very high accuracy on diverse text - Supports 80+ languages - Works with or without GPU - Easy API - Good on real-world documents</p> <p>Weaknesses: - Slower than PaddleOCR - Higher memory usage - Requires downloading large models</p> <p>When to use: - High accuracy needed - Mixed language documents - Production systems - Irregular text layouts</p> <p>Languages: English, Chinese, Japanese, Korean, Arabic, Hindi, etc. (80+ total)</p>"},{"location":"guides/ocr-extraction/#3-paddleocr-fastest-with-gpu","title":"3. PaddleOCR (Fastest with GPU)","text":"<p>Lightweight OCR optimized for speed.</p> <p>Strengths: - Fastest inference speed - Small model size - Excellent Asian language support - Works on CPU and GPU - Very efficient</p> <p>Weaknesses: - Fewer languages than EasyOCR - Slightly lower accuracy on English - Limited handwriting support</p> <p>When to use: - Performance-critical applications - Asian language documents - Resource-constrained environments - High-throughput pipelines</p> <p>Languages: English, Chinese, Japanese, Korean, Arabic (main languages)</p>"},{"location":"guides/ocr-extraction/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/ocr-extraction/#example-1-simple-word-level-ocr","title":"Example 1: Simple Word-Level OCR","text":"<p>Extract text with word-level bounding boxes.</p> <pre><code>from omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\nfrom PIL import Image\n\nimage = Image.open(\"document_page.png\")\n\n# Initialize EasyOCR for high accuracy\nconfig = EasyOCRConfig(\n    languages=[\"en\"],  # English only (faster)\n    gpu=True,  # Use GPU if available\n)\nocr = EasyOCR(config=config)\n\n# Extract text with bounding boxes\nresult = ocr.extract(image)\n\nprint(f\"Extracted {len(result.text_blocks)} text blocks\")\n\n# Access text and locations\nfor block in result.text_blocks:\n    print(f\"Text: '{block.text}'\")\n    print(f\"Bbox: {block.bbox}\")\n    print(f\"Confidence: {block.confidence:.2f}\")\n    print()\n</code></pre> <p>Output Example: <pre><code>Extracted 5 text blocks\nText: 'Document'\nBbox: BoundingBox(x1=10, y1=5, x2=120, y2=30)\nConfidence: 0.98\n\nText: 'Title'\nBbox: BoundingBox(x1=10, y1=35, x2=100, y2=55)\nConfidence: 0.97\n\n...\n</code></pre></p>"},{"location":"guides/ocr-extraction/#example-2-fast-cpu-only-ocr-tesseract","title":"Example 2: Fast CPU-Only OCR (Tesseract)","text":"<p>Use Tesseract for fast CPU-only extraction.</p> <pre><code>from omnidocs.tasks.ocr_extraction import Tesseract, TesseractConfig\nfrom PIL import Image\n\nimage = Image.open(\"document_page.png\")\n\n# Initialize Tesseract (CPU only, no GPU)\nconfig = TesseractConfig(\n    language=\"eng\",  # Single language for speed\n    config=\"--psm 3\",  # Page segmentation mode\n)\nocr = Tesseract(config=config)\n\n# Extract\nresult = ocr.extract(image)\n\nprint(f\"Found {len(result.text_blocks)} words\")\n\n# Display results with confidence\nhigh_confidence = [b for b in result.text_blocks if b.confidence &gt; 0.9]\nprint(f\"High confidence blocks: {len(high_confidence)}\")\n\n# Get plain text\nprint(\"\\nExtracted text:\")\nprint(\" \".join(block.text for block in result.text_blocks))\n</code></pre>"},{"location":"guides/ocr-extraction/#example-3-multi-language-ocr","title":"Example 3: Multi-Language OCR","text":"<p>Extract from documents with multiple languages.</p> <pre><code>from omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\nfrom PIL import Image\n\nimage = Image.open(\"multilingual_document.png\")\n\n# Support multiple languages\nconfig = EasyOCRConfig(\n    languages=[\"en\", \"zh\", \"ar\"],  # English, Chinese, Arabic\n    gpu=True,\n)\nocr = EasyOCR(config=config)\n\nresult = ocr.extract(image)\n\n# Group by detected language (if available)\nfor block in result.text_blocks:\n    print(f\"[{block.language}] {block.text}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#example-4-pdf-with-character-level-extraction","title":"Example 4: PDF with Character-Level Extraction","text":"<p>Extract at character granularity from PDF.</p> <pre><code>from omnidocs import Document\nfrom omnidocs.tasks.ocr_extraction import PaddleOCR, PaddleOCRConfig\n\n# Load PDF\ndoc = Document.from_pdf(\"document.pdf\")\n\n# Initialize PaddleOCR for character-level extraction\nconfig = PaddleOCRConfig(\n    languages=[\"en\", \"ch\"],  # English and Chinese\n    gpu=True,\n)\nocr = PaddleOCR(config=config)\n\n# Process first page\npage_image = doc.get_page(0)\nresult = ocr.extract(page_image, granularity=\"character\")\n\n# Access character-level data\nchar_count = len(result.text_blocks)\nprint(f\"Extracted {char_count} characters\")\n\n# Find coordinates of specific character\nfor block in result.text_blocks:\n    if block.text == \"A\":\n        print(f\"Found 'A' at {block.bbox}\")\n        break\n</code></pre>"},{"location":"guides/ocr-extraction/#extracting-bounding-boxes","title":"Extracting Bounding Boxes","text":""},{"location":"guides/ocr-extraction/#get-text-blocks-with-coordinates","title":"Get Text Blocks with Coordinates","text":"<pre><code>from omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\nconfig = EasyOCRConfig(languages=[\"en\"], gpu=True)\nocr = EasyOCR(config=config)\n\nresult = ocr.extract(image)\n\n# Print detailed block information\nfor block in result.text_blocks:\n    x1, y1, x2, y2 = block.bbox.x1, block.bbox.y1, block.bbox.x2, block.bbox.y2\n    width = x2 - x1\n    height = y2 - y1\n\n    print(f\"'{block.text}' @ ({x1:.0f}, {y1:.0f}) \"\n          f\"size: {width:.0f}x{height:.0f} \"\n          f\"conf: {block.confidence:.2f}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#convert-to-normalized-coordinates","title":"Convert to Normalized Coordinates","text":"<p>Convert pixel coordinates to 0-1024 normalized range.</p> <pre><code># Normalize bounding boxes to 0-1024 range\nimage_width, image_height = image.size\nnormalized_blocks = result.get_normalized_blocks()\n\nfor block in normalized_blocks:\n    # Coordinates now in 0-1024 range\n    print(f\"'{block.text}' @ {block.bbox} (normalized)\")\n\n# Manual normalization\nNORM_SIZE = 1024\n\ndef normalize_bbox(bbox, image_size):\n    \"\"\"Convert pixel bbox to normalized 0-1024.\"\"\"\n    img_w, img_h = image_size\n    x1 = int(bbox.x1 * NORM_SIZE / img_w)\n    y1 = int(bbox.y1 * NORM_SIZE / img_h)\n    x2 = int(bbox.x2 * NORM_SIZE / img_w)\n    y2 = int(bbox.y2 * NORM_SIZE / img_h)\n    return (x1, y1, x2, y2)\n\nfor block in result.text_blocks:\n    norm_bbox = normalize_bbox(block.bbox, (image_width, image_height))\n    print(f\"Normalized: {norm_bbox}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#extract-from-specific-regions","title":"Extract from Specific Regions","text":"<p>Get OCR results from a cropped region.</p> <pre><code># Crop image to specific region\nregion_bbox = (100, 100, 500, 400)  # x1, y1, x2, y2\ncropped = image.crop(region_bbox)\n\n# Run OCR on crop\nresult_crop = ocr.extract(cropped)\n\n# Adjust bboxes back to original image coordinates\nx1_offset, y1_offset = region_bbox[0], region_bbox[1]\n\nfor block in result_crop.text_blocks:\n    # Shift coordinates\n    adjusted_bbox = (\n        block.bbox.x1 + x1_offset,\n        block.bbox.y1 + y1_offset,\n        block.bbox.x2 + x1_offset,\n        block.bbox.y2 + y1_offset,\n    )\n    print(f\"'{block.text}' @ {adjusted_bbox}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#filtering-results","title":"Filtering Results","text":""},{"location":"guides/ocr-extraction/#filter-by-confidence","title":"Filter by Confidence","text":"<p>Keep only high-confidence extractions.</p> <pre><code># Filter by confidence threshold\nmin_confidence = 0.85\nconfident_blocks = [\n    b for b in result.text_blocks\n    if b.confidence &gt;= min_confidence\n]\n\nprint(f\"Original: {len(result.text_blocks)} blocks\")\nprint(f\"Filtered (conf &gt;= {min_confidence}): {len(confident_blocks)} blocks\")\n\n# Display confidence distribution\nconfidences = [b.confidence for b in result.text_blocks]\nprint(f\"Confidence range: {min(confidences):.2f} - {max(confidences):.2f}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#filter-by-region","title":"Filter by Region","text":"<p>Extract OCR results from specific image regions.</p> <pre><code>def is_in_region(bbox, region):\n    \"\"\"Check if bbox overlaps with region.\"\"\"\n    rx1, ry1, rx2, ry2 = region\n    return not (bbox.x2 &lt; rx1 or bbox.x1 &gt; rx2 or\n                bbox.y2 &lt; ry1 or bbox.y1 &gt; ry2)\n\n# Top-left region\ntop_left = (0, 0, image.width//2, image.height//2)\ntop_left_blocks = [b for b in result.text_blocks if is_in_region(b.bbox, top_left)]\n\n# Sidebar region\nsidebar = (0, 0, 200, image.height)\nsidebar_blocks = [b for b in result.text_blocks if is_in_region(b.bbox, sidebar)]\n\nprint(f\"Top-left blocks: {len(top_left_blocks)}\")\nprint(f\"Sidebar blocks: {len(sidebar_blocks)}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#filter-by-text-content","title":"Filter by Text Content","text":"<p>Find blocks matching patterns.</p> <pre><code>import re\n\n# Find numbers\nnumber_blocks = [\n    b for b in result.text_blocks\n    if re.match(r'^\\d+$', b.text.strip())\n]\n\n# Find email addresses\nemail_blocks = [\n    b for b in result.text_blocks\n    if re.match(r'^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$', b.text.strip())\n]\n\n# Find specific phrases\nphrase_blocks = [\n    b for b in result.text_blocks\n    if \"important\" in b.text.lower()\n]\n\nprint(f\"Numbers: {len(number_blocks)}\")\nprint(f\"Emails: {len(email_blocks)}\")\nprint(f\"'Important' mentions: {len(phrase_blocks)}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#filter-by-size","title":"Filter by Size","text":"<p>Exclude very small or very large blocks.</p> <pre><code># Calculate block dimensions\ndef get_size(bbox):\n    return (bbox.x2 - bbox.x1, bbox.y2 - bbox.y1)\n\n# Keep medium-sized blocks\nmedium_blocks = []\nfor b in result.text_blocks:\n    width, height = get_size(b.bbox)\n    if 30 &lt; width &lt; 500 and 10 &lt; height &lt; 100:\n        medium_blocks.append(b)\n\nprint(f\"Medium-sized blocks: {len(medium_blocks)}/{len(result.text_blocks)}\")\n\n# Analyze size distribution\nsizes = [get_size(b.bbox) for b in result.text_blocks]\navg_width = sum(w for w, h in sizes) / len(sizes)\navg_height = sum(h for w, h in sizes) / len(sizes)\nprint(f\"Average block size: {avg_width:.0f}x{avg_height:.0f}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#multi-language-support","title":"Multi-Language Support","text":""},{"location":"guides/ocr-extraction/#auto-detect-language","title":"Auto-Detect Language","text":"<p>EasyOCR can auto-detect language.</p> <pre><code>from omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\n# Auto-detect (leave empty or None)\nconfig = EasyOCRConfig(\n    languages=None,  # Auto-detect all languages\n    gpu=True,\n)\nocr = EasyOCR(config=config)\n\nresult = ocr.extract(image)\n\n# Check detected languages\ndetected_langs = set()\nfor block in result.text_blocks:\n    if hasattr(block, 'language'):\n        detected_langs.add(block.language)\n\nprint(f\"Detected languages: {detected_langs}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#process-mixed-language-documents","title":"Process Mixed-Language Documents","text":"<p>Handle documents with multiple languages.</p> <pre><code># Support common languages\nconfig = EasyOCRConfig(\n    languages=[\"en\", \"zh\", \"ar\", \"hi\", \"ja\"],  # English, Chinese, Arabic, Hindi, Japanese\n    gpu=True,\n)\nocr = EasyOCR(config=config)\n\nresult = ocr.extract(image)\n\n# Group results by language\nfrom collections import defaultdict\nby_language = defaultdict(list)\n\nfor block in result.text_blocks:\n    lang = getattr(block, 'language', 'unknown')\n    by_language[lang].append(block)\n\nfor lang, blocks in by_language.items():\n    print(f\"\\n{lang.upper()} ({len(blocks)} blocks):\")\n    for block in blocks[:3]:  # Show first 3\n        print(f\"  {block.text}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#language-specific-optimization","title":"Language-Specific Optimization","text":"<p>Different languages need different models.</p> <pre><code># For English only (fastest)\nconfig_en = EasyOCRConfig(languages=[\"en\"], gpu=True)\n\n# For Asian languages (use PaddleOCR)\nfrom omnidocs.tasks.ocr_extraction import PaddleOCR, PaddleOCRConfig\nconfig_cn = PaddleOCRConfig(languages=[\"ch\"], gpu=True)  # Chinese\n\n# For Arabic/Hebrew (right-to-left)\nconfig_rtl = EasyOCRConfig(languages=[\"ar\"], gpu=True)\n\n# For handwriting\nconfig_hw = EasyOCRConfig(languages=[\"en\"], gpu=True)\n# Note: Most OCR models struggle with handwriting\n</code></pre>"},{"location":"guides/ocr-extraction/#performance-comparison","title":"Performance Comparison","text":""},{"location":"guides/ocr-extraction/#speed-benchmarks","title":"Speed Benchmarks","text":"<p>Processing a typical page (300 DPI, ~2000x3000px):</p> Model CPU GPU Latency Memory Tesseract 0.5-1.0s N/A Very Fast ~100MB PaddleOCR 1-2s 0.3-0.5s Fast ~500MB EasyOCR 2-4s 0.5-1.0s Medium ~1GB"},{"location":"guides/ocr-extraction/#choose-by-speed-requirements","title":"Choose by Speed Requirements","text":"Requirement Model &lt;200ms per page Tesseract or PaddleOCR (GPU) &lt;500ms per page PaddleOCR (GPU) or Tesseract &lt;1s per page EasyOCR (GPU) or PaddleOCR (CPU) 1-2s acceptable EasyOCR (GPU) Accuracy paramount EasyOCR"},{"location":"guides/ocr-extraction/#optimization-for-speed","title":"Optimization for Speed","text":"<pre><code>import time\n\n# Fast configuration\nconfig_fast = PaddleOCRConfig(\n    languages=[\"en\"],  # Single language\n    gpu=True,\n)\nocr = PaddleOCR(config=config_fast)\n\n# Benchmark\nimages = [Image.open(f\"doc{i}.png\") for i in range(5)]\n\nstart = time.time()\nfor img in images:\n    result = ocr.extract(img)\nelapsed = time.time() - start\n\nprint(f\"Processed {len(images)} images in {elapsed:.1f}s\")\nprint(f\"Average: {elapsed/len(images):.2f}s per image\")\n</code></pre>"},{"location":"guides/ocr-extraction/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/ocr-extraction/#low-accuracy","title":"Low Accuracy","text":"<p>Problem: OCR results have many errors.</p> <p>Solutions: 1. Try different model (EasyOCR typically better) 2. Improve image quality 3. Try single language (faster + more accurate)</p> <pre><code># Solution 1: Use EasyOCR (more accurate)\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\nconfig = EasyOCRConfig(languages=[\"en\"], gpu=True)\n\n# Solution 2: Improve image quality\nfrom PIL import Image, ImageEnhance\n\nimg = Image.open(\"noisy_scan.png\")\n\n# Increase contrast\nenhancer = ImageEnhance.Contrast(img)\nimg = enhancer.enhance(1.5)  # 50% more contrast\n\n# Increase sharpness\nenhancer = ImageEnhance.Sharpness(img)\nimg = enhancer.enhance(2.0)  # 2x sharpness\n\n# Resize if too small\nif img.width &lt; 1024:\n    img = img.resize((img.width * 2, img.height * 2), Image.Resampling.LANCZOS)\n\nresult = ocr.extract(img)\n\n# Solution 3: Single language\nconfig = EasyOCRConfig(languages=[\"en\"], gpu=True)  # Just English\n</code></pre>"},{"location":"guides/ocr-extraction/#missing-text","title":"Missing Text","text":"<p>Problem: Some text not detected.</p> <p>Solutions: 1. Check image quality 2. Try lower confidence threshold 3. Use different OCR model</p> <pre><code># Solution 1: Check image\nprint(f\"Image size: {image.size}\")\nprint(f\"Image mode: {image.mode}\")\n\n# Solution 2: Lower confidence\nall_blocks = result.text_blocks  # Includes low confidence\n\nconfidence_dist = [b.confidence for b in result.text_blocks]\nprint(f\"Confidence range: {min(confidence_dist):.2f}-{max(confidence_dist):.2f}\")\n\n# Get even low-confidence blocks\nlow_conf_blocks = [b for b in result.text_blocks if b.confidence &lt; 0.5]\nprint(f\"Low confidence blocks: {len(low_conf_blocks)}\")\n</code></pre>"},{"location":"guides/ocr-extraction/#false-detections","title":"False Detections","text":"<p>Problem: Non-text detected as text.</p> <p>Solutions: 1. Increase confidence threshold 2. Filter by text length 3. Manual post-processing</p> <pre><code># Solution 1: Increase confidence\nhigh_conf = [b for b in result.text_blocks if b.confidence &gt; 0.95]\n\n# Solution 2: Filter short blocks (likely noise)\nMIN_CHARS = 2\nvalid_blocks = [b for b in result.text_blocks if len(b.text) &gt;= MIN_CHARS]\n\n# Solution 3: Remove non-alphabetic text\nimport string\nalpha_blocks = [\n    b for b in result.text_blocks\n    if any(c.isalpha() for c in b.text)\n]\n</code></pre>"},{"location":"guides/ocr-extraction/#slow-performance","title":"Slow Performance","text":"<p>Problem: OCR taking too long.</p> <p>Solutions: 1. Use faster model (PaddleOCR or Tesseract) 2. Reduce image resolution 3. Use GPU 4. Single language</p> <pre><code># Solution 1: Use PaddleOCR (faster)\nfrom omnidocs.tasks.ocr_extraction import PaddleOCR\nocr = PaddleOCR(gpu=True)  # Fastest on GPU\n\n# Solution 2: Reduce resolution\nimage = image.resize((image.width // 2, image.height // 2))\n\n# Solution 3: Ensure GPU enabled\nimport torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n\n# Solution 4: Single language\nconfig = PaddleOCRConfig(languages=[\"en\"], gpu=True)\n</code></pre> <p>Next Steps: - See Text Extraction Guide for formatted document output - See Layout Analysis Guide for document structure - See Batch Processing Guide for processing many documents</p>"},{"location":"guides/text-extraction/","title":"Text Extraction Guide","text":"<p>Extract formatted text content (Markdown/HTML) from document images using vision-language models. This guide covers when to use text extraction, available models, output formats, and practical examples.</p>"},{"location":"guides/text-extraction/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Quick Comparison: Text Extraction vs OCR vs Layout</li> <li>Available Models</li> <li>Basic Usage</li> <li>Output Formats</li> <li>Advanced Features</li> <li>Performance Optimization</li> <li>Troubleshooting</li> </ul>"},{"location":"guides/text-extraction/#quick-comparison","title":"Quick Comparison","text":"Feature Text Extraction OCR Layout Detection Output Formatted text (MD/HTML) Text + bounding boxes Element bounding boxes Use Case Document parsing, markdown export Word/character localization Document structure analysis Models Qwen3-VL, DotsOCR, Nanonets Tesseract, EasyOCR, PaddleOCR DocLayoutYOLO, Qwen-Layout Latency ~2-5 sec per page ~1-2 sec per page ~0.5-1 sec per page Output Type Single string List of text blocks List of bounding boxes Layout Info Optional (DotsOCR only) No Yes (with labels) <p>Choose Text Extraction when: - Converting documents to Markdown/HTML - Extracting complete page content as formatted text - Working with complex documents (multi-column, figures, tables) - You need readable output for downstream processing</p> <p>Choose OCR when: - You need precise character/word locations - Building re-OCR pipelines (e.g., for correction) - Requiring character-level accuracy metrics</p> <p>Choose Layout Detection when: - You need document structure without text content - Building advanced pipelines (layout + text) - Analyzing document semantics</p>"},{"location":"guides/text-extraction/#available-models","title":"Available Models","text":""},{"location":"guides/text-extraction/#1-qwen3-vl-recommended-for-most-cases","title":"1. Qwen3-VL (Recommended for most cases)","text":"<p>High-quality general-purpose vision-language model.</p> <p>Strengths: - Best output quality across diverse documents - Multi-backend support (PyTorch, VLLM, MLX, API) - Consistent Markdown/HTML output - Good at handling complex layouts</p> <p>Backends: - PyTorch: Local GPU inference (single GPU) - VLLM: High-throughput serving (multiple GPUs) - MLX: Apple Silicon (local) - API: Hosted models (cloud)</p> <p>Model Variants: - <code>Qwen/Qwen3-VL-8B-Instruct</code>: Recommended (8B parameters) - <code>Qwen/Qwen3-VL-32B-Instruct</code>: Higher quality (32B, slower, more VRAM)</p>"},{"location":"guides/text-extraction/#2-dotsocr-best-for-technical-documents","title":"2. DotsOCR (Best for technical documents)","text":"<p>Optimized for complex technical documents with precise layout preservation.</p> <p>Strengths: - Layout-aware extraction with bounding boxes - Specialized formatting for tables (HTML) and formulas (LaTeX) - Reading order preservation - 11-category layout detection</p> <p>Weaknesses: - Slower than Qwen (requires layout analysis) - Higher VRAM requirements</p> <p>Backends: - PyTorch: Local GPU inference - VLLM: High-throughput serving - API: Hosted models</p> <p>Output Types: - Structured JSON with layout information - Markdown with coordinate annotations - HTML with bbox attributes</p>"},{"location":"guides/text-extraction/#3-nanonets-coming-soon","title":"3. Nanonets (Coming soon)","text":"<p>Specialized for OCR-quality text extraction.</p>"},{"location":"guides/text-extraction/#basic-usage","title":"Basic Usage","text":""},{"location":"guides/text-extraction/#example-1-simple-markdown-extraction","title":"Example 1: Simple Markdown Extraction","text":"<p>Extract a document page to Markdown using PyTorch backend.</p> <pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom PIL import Image\n\n# Load a single image\nimage = Image.open(\"document_page.png\")\n\n# Initialize extractor with PyTorch backend\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cuda\",  # or \"cpu\"\n    torch_dtype=\"auto\",  # Automatic dtype selection\n)\nextractor = QwenTextExtractor(backend=config)\n\n# Extract text in Markdown format\nresult = extractor.extract(image, output_format=\"markdown\")\n\n# Access the extracted content\nprint(result.content)  # Formatted Markdown text\nprint(result.word_count)  # Number of words\nprint(f\"Model: {result.model_name}\")\n</code></pre>"},{"location":"guides/text-extraction/#example-2-extract-with-layout-information","title":"Example 2: Extract with Layout Information","text":"<p>Use DotsOCR to get text plus layout annotations.</p> <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\nfrom PIL import Image\nimport json\n\nimage = Image.open(\"complex_document.png\")\n\n# Initialize DotsOCR with layout detection\nconfig = DotsOCRPyTorchConfig(\n    device=\"cuda\",\n    max_new_tokens=8192,  # Higher for complex documents\n)\nextractor = DotsOCRTextExtractor(backend=config)\n\n# Extract with layout information\nresult = extractor.extract(image, include_layout=True)\n\n# Access layout elements\nprint(f\"Found {result.num_layout_elements} layout elements\")\nprint(f\"Content length: {result.content_length} characters\")\n\n# Iterate through layout elements\nfor element in result.layout:\n    print(f\"[{element.category}] @{element.bbox}: {element.text[:50]}...\")\n\n# Save layout information to JSON\nlayout_json = [elem.model_dump() for elem in result.layout]\nwith open(\"layout.json\", \"w\") as f:\n    json.dump(layout_json, f, indent=2)\n</code></pre>"},{"location":"guides/text-extraction/#example-3-extract-pdf-document","title":"Example 3: Extract PDF Document","text":"<p>Process multiple pages of a PDF document.</p> <pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom pathlib import Path\n\n# Load PDF document\ndoc = Document.from_pdf(\"multi_page_document.pdf\")\nprint(f\"Loaded PDF with {doc.page_count} pages\")\n\n# Initialize extractor\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cuda\",\n)\nextractor = QwenTextExtractor(backend=config)\n\n# Extract text from all pages\nall_text = []\nfor page_idx in range(min(3, doc.page_count)):  # First 3 pages\n    page_image = doc.get_page(page_idx)\n    result = extractor.extract(page_image, output_format=\"markdown\")\n    all_text.append(result.content)\n    print(f\"Page {page_idx + 1}: {result.word_count} words\")\n\n# Combine results\nfull_document = \"\\n\\n---\\n\\n\".join(all_text)\nprint(f\"\\nTotal content: {len(full_document)} characters\")\n\n# Save to file\nwith open(\"extracted_document.md\", \"w\") as f:\n    f.write(full_document)\n</code></pre>"},{"location":"guides/text-extraction/#example-4-batch-processing-with-progress-tracking","title":"Example 4: Batch Processing with Progress Tracking","text":"<p>Process multiple documents with progress reporting.</p> <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom pathlib import Path\nfrom PIL import Image\nimport time\n\n# Find all image files\nimage_dir = Path(\"documents/\")\nimage_files = list(image_dir.glob(\"*.png\")) + list(image_dir.glob(\"*.jpg\"))\nprint(f\"Found {len(image_files)} images to process\")\n\n# Initialize extractor\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cuda\",\n    max_new_tokens=4096,\n)\nextractor = QwenTextExtractor(backend=config)\n\n# Process with progress tracking\nresults = {}\nstart_time = time.time()\n\nfor idx, image_path in enumerate(image_files, 1):\n    print(f\"[{idx}/{len(image_files)}] Processing {image_path.name}...\", end=\" \")\n\n    try:\n        image = Image.open(image_path)\n        result = extractor.extract(image, output_format=\"markdown\")\n        results[str(image_path)] = {\n            \"content_length\": result.content_length,\n            \"word_count\": result.word_count,\n        }\n        print(f\"\u2713 ({result.word_count} words)\")\n    except Exception as e:\n        print(f\"\u2717 Error: {e}\")\n        results[str(image_path)] = {\"error\": str(e)}\n\n# Summary\nelapsed = time.time() - start_time\nprint(f\"\\nCompleted in {elapsed:.1f}s ({elapsed/len(image_files):.2f}s per image)\")\nprint(f\"Successful: {sum(1 for r in results.values() if 'error' not in r)}\")\n</code></pre>"},{"location":"guides/text-extraction/#output-formats","title":"Output Formats","text":""},{"location":"guides/text-extraction/#markdown-format","title":"Markdown Format","text":"<p>Human-readable format with standard Markdown syntax. Best for documentation and web publishing.</p> <pre><code>result = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n\n# Example output:\n# # Document Title\n#\n# This is the main content with **bold** and *italic* text.\n#\n# ## Section 1\n#\n# - Bullet point 1\n# - Bullet point 2\n#\n# | Column 1 | Column 2 |\n# |----------|----------|\n# | Cell 1   | Cell 2   |\n</code></pre> <p>Advantages: - Human-readable - Git-friendly (version control) - Easy to edit - Good for documentation</p> <p>Limitations: - Loses some layout information - Tables converted to Markdown tables (may lose formatting) - No bounding box information</p>"},{"location":"guides/text-extraction/#html-format","title":"HTML Format","text":"<p>Structured HTML with semantic tags. Better for preserving layout in web contexts.</p> <pre><code>result = extractor.extract(image, output_format=\"html\")\nprint(result.content)\n\n# Example output:\n# &lt;div class=\"document\"&gt;\n#   &lt;h1&gt;Document Title&lt;/h1&gt;\n#   &lt;p&gt;This is the main content with &lt;b&gt;bold&lt;/b&gt; and &lt;i&gt;italic&lt;/i&gt; text.&lt;/p&gt;\n#   &lt;h2&gt;Section 1&lt;/h2&gt;\n#   &lt;ul&gt;\n#     &lt;li&gt;Bullet point 1&lt;/li&gt;\n#     &lt;li&gt;Bullet point 2&lt;/li&gt;\n#   &lt;/ul&gt;\n#   &lt;table&gt;...&lt;/table&gt;\n# &lt;/div&gt;\n</code></pre> <p>Advantages: - Structured and semantic - Better layout preservation - Good for web rendering - Supports nested elements</p> <p>Limitations: - More verbose - Requires HTML parser for processing - Layout information may still be approximate</p>"},{"location":"guides/text-extraction/#plain-text-fallback","title":"Plain Text (Fallback)","text":"<p>Extract plain text without any formatting.</p> <pre><code># Get plain text version\nplain_text = result.plain_text\nprint(plain_text)\n\n# Also available as property:\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\n# ... after extraction ...\nprint(result.plain_text)  # No formatting, just raw text\n</code></pre>"},{"location":"guides/text-extraction/#dotsocr-json-format","title":"DotsOCR JSON Format","text":"<p>Structured JSON with layout information (DotsOCR only).</p> <pre><code>result = extractor.extract(image, output_format=\"json\", include_layout=True)\n\n# Result includes:\n# {\n#   \"content\": \"Full text...\",\n#   \"layout\": [\n#     {\n#       \"bbox\": [100, 50, 400, 80],\n#       \"category\": \"Title\",\n#       \"text\": \"Document Title\"\n#     },\n#     ...\n#   ]\n# }\n</code></pre>"},{"location":"guides/text-extraction/#advanced-features","title":"Advanced Features","text":""},{"location":"guides/text-extraction/#custom-prompts","title":"Custom Prompts","text":"<p>Override the default extraction prompt for specialized use cases.</p> <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\nconfig = QwenTextPyTorchConfig(device=\"cuda\")\nextractor = QwenTextExtractor(backend=config)\n\n# Custom prompt for extractive summarization\ncustom_prompt = \"\"\"\nExtract the most important information from this document image.\nFocus on key facts, numbers, and action items.\nFormat as a concise Markdown list.\n\"\"\"\n\nresult = extractor.extract(\n    image,\n    output_format=\"markdown\",\n    custom_prompt=custom_prompt,\n)\n\nprint(result.content)\n</code></pre>"},{"location":"guides/text-extraction/#temperature-control-pytorch-only","title":"Temperature Control (PyTorch only)","text":"<p>Adjust model creativity/determinism via temperature parameter.</p> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Lower temperature = more deterministic (better for factual extraction)\nconfig = QwenTextPyTorchConfig(\n    device=\"cuda\",\n    temperature=0.1,  # Default: 0.1 (deterministic)\n)\n\n# Higher temperature = more creative (for summarization, etc.)\nconfig_creative = QwenTextPyTorchConfig(\n    device=\"cuda\",\n    temperature=0.7,\n)\n</code></pre>"},{"location":"guides/text-extraction/#backend-switching","title":"Backend Switching","text":"<p>Easily switch between backends without changing extraction code.</p> <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import (\n    QwenTextPyTorchConfig,\n    QwenTextVLLMConfig,\n    QwenTextMLXConfig,\n    QwenTextAPIConfig,\n)\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\n\n# Use PyTorch for single-GPU inference\npytorch_extractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(device=\"cuda\")\n)\nresult1 = pytorch_extractor.extract(image, output_format=\"markdown\")\n\n# Use VLLM for high-throughput inference\nvllm_extractor = QwenTextExtractor(\n    backend=QwenTextVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=1,\n    )\n)\nresult2 = vllm_extractor.extract(image, output_format=\"markdown\")\n\n# Use MLX for Apple Silicon\nmlx_extractor = QwenTextExtractor(\n    backend=QwenTextMLXConfig(device=\"gpu\")\n)\nresult3 = mlx_extractor.extract(image, output_format=\"markdown\")\n\n# Use API for hosted models\napi_extractor = QwenTextExtractor(\n    backend=QwenTextAPIConfig(\n        model=\"qwen3-vl-8b\",\n        api_key=\"your-api-key\",\n        base_url=\"https://api.example.com/v1\",\n    )\n)\nresult4 = api_extractor.extract(image, output_format=\"markdown\")\n\nprint(f\"PyTorch: {result1.word_count} words\")\nprint(f\"VLLM: {result2.word_count} words\")\nprint(f\"MLX: {result3.word_count} words\")\nprint(f\"API: {result4.word_count} words\")\n</code></pre>"},{"location":"guides/text-extraction/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/text-extraction/#model-selection","title":"Model Selection","text":"Model Latency Quality VRAM Speed Qwen3-VL-8B 2-3 sec Excellent 16GB Fast Qwen3-VL-32B 5-8 sec Outstanding 32GB Slow DotsOCR 3-5 sec Very Good (technical) 20GB Medium <p>Recommendation: Start with Qwen3-VL-8B (best quality/speed tradeoff).</p>"},{"location":"guides/text-extraction/#backend-optimization","title":"Backend Optimization","text":"<p>PyTorch (Single GPU): - Best for development and small batches - Load time: ~2-3 seconds - Per-image latency: ~2-3 seconds</p> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cuda\",\n    torch_dtype=\"auto\",  # Let PyTorch choose optimal dtype\n    max_new_tokens=4096,  # Reduce for faster inference\n)\n</code></pre> <p>VLLM (Multi-GPU): - Best for batch processing / high throughput - Load time: ~5-8 seconds (slightly slower but amortizes) - Throughput: 2-4x better than PyTorch for multiple requests</p> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\n\nconfig = QwenTextVLLMConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    tensor_parallel_size=2,  # Use 2 GPUs\n    gpu_memory_utilization=0.9,  # Use 90% of VRAM\n    max_tokens=4096,\n)\n</code></pre> <p>MLX (Apple Silicon): - Best for MacBook development - No GPU-related issues - Slightly slower than VRAM-constrained models</p> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextMLXConfig\n\nconfig = QwenTextMLXConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct-MLX\",\n    device=\"gpu\",\n    quantization=\"4bit\",  # Quantization reduces VRAM\n)\n</code></pre>"},{"location":"guides/text-extraction/#batch-processing-strategy","title":"Batch Processing Strategy","text":"<p>For processing many documents, batch requests to amortize model loading.</p> <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\nfrom pathlib import Path\nfrom PIL import Image\nimport time\n\n# Initialize once (expensive)\nconfig = QwenTextVLLMConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    tensor_parallel_size=1,\n    gpu_memory_utilization=0.85,\n    max_tokens=4096,\n)\nextractor = QwenTextExtractor(backend=config)\n\n# Process many documents (cheap)\nimage_paths = list(Path(\"documents/\").glob(\"*.png\"))\nresults = []\n\nstart = time.time()\nfor image_path in image_paths:\n    image = Image.open(image_path)\n    result = extractor.extract(image, output_format=\"markdown\")\n    results.append(result)\n\nelapsed = time.time() - start\nprint(f\"Processed {len(results)} images in {elapsed:.1f}s\")\nprint(f\"Average: {elapsed/len(results):.2f}s per image\")\n</code></pre>"},{"location":"guides/text-extraction/#token-limit-tuning","title":"Token Limit Tuning","text":"<p>Adjust <code>max_new_tokens</code> based on expected output length.</p> <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# For short documents (&lt; 1000 words)\nconfig_short = QwenTextPyTorchConfig(\n    device=\"cuda\",\n    max_new_tokens=2048,  # Faster\n)\n\n# For medium documents (1000-5000 words)\nconfig_medium = QwenTextPyTorchConfig(\n    device=\"cuda\",\n    max_new_tokens=4096,  # Default\n)\n\n# For long documents (&gt; 5000 words)\nconfig_long = QwenTextPyTorchConfig(\n    device=\"cuda\",\n    max_new_tokens=8192,  # Slower but handles longer docs\n)\n</code></pre>"},{"location":"guides/text-extraction/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guides/text-extraction/#out-of-memory-oom-errors","title":"Out of Memory (OOM) Errors","text":"<p>Problem: CUDA out of memory during inference.</p> <p>Solutions: 1. Reduce <code>max_new_tokens</code> 2. Use smaller model variant (8B instead of 32B) 3. Switch to VLLM with <code>tensor_parallel_size &gt; 1</code> 4. Use quantization (if available)</p> <pre><code># Option 1: Reduce max_new_tokens\nconfig = QwenTextPyTorchConfig(\n    device=\"cuda\",\n    max_new_tokens=2048,  # Reduced from 4096\n)\n\n# Option 2: Smaller model\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",  # Instead of 32B\n    device=\"cuda\",\n)\n\n# Option 3: VLLM with tensor parallelism\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\nconfig = QwenTextVLLMConfig(\n    tensor_parallel_size=2,  # Distribute across 2 GPUs\n    max_tokens=4096,\n)\n</code></pre>"},{"location":"guides/text-extraction/#slow-inference","title":"Slow Inference","text":"<p>Problem: Text extraction takes too long.</p> <p>Solutions: 1. Check GPU utilization (should be &gt;80%) 2. Reduce <code>max_new_tokens</code> 3. Use VLLM instead of PyTorch 4. Use VLLM tensor parallelism</p> <pre><code>import subprocess\n\n# Check GPU usage during extraction\nresult = subprocess.run(\n    [\"nvidia-smi\", \"--query-gpu=utilization.gpu\", \"--format=csv,noheader\"],\n    capture_output=True,\n    text=True\n)\nprint(f\"GPU Utilization: {result.stdout.strip()}%\")\n\n# If &lt;50%, increase batch size or use VLLM\n</code></pre>"},{"location":"guides/text-extraction/#incorrect-or-garbled-output","title":"Incorrect or Garbled Output","text":"<p>Problem: Extracted text is incomplete or corrupted.</p> <p>Solutions: 1. Check image quality (min 1024px width recommended) 2. Verify model downloaded correctly 3. Try with explicit output format</p> <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\n\n# Check image size\nprint(f\"Image size: {image.size}\")  # Should be at least (1024, 768)\n\n# Resize if too small\nif image.width &lt; 1024:\n    image = image.resize((image.width * 2, image.height * 2))\n\n# Try extraction\nconfig = QwenTextPyTorchConfig(device=\"cuda\")\nextractor = QwenTextExtractor(backend=config)\nresult = extractor.extract(image, output_format=\"markdown\")\n\n# Check result\nif len(result.content) &lt; 10:\n    print(\"Warning: Very short output, may indicate extraction failure\")\n    print(f\"Raw output: {result.raw_output}\")\n</code></pre>"},{"location":"guides/text-extraction/#model-download-issues","title":"Model Download Issues","text":"<p>Problem: Model fails to download or load.</p> <p>Solutions: 1. Check internet connection 2. Verify HuggingFace token 3. Set custom cache directory</p> <pre><code>import os\n\n# Set HuggingFace token\nos.environ[\"HF_TOKEN\"] = \"your-token-here\"\n\n# Set custom cache directory\nos.environ[\"HF_HOME\"] = \"/large/disk/hf_cache\"\n\n# Verify download by loading model explicitly\nfrom transformers import AutoTokenizer, AutoModel\n\nmodel_id = \"Qwen/Qwen3-VL-8B-Instruct\"\ntry:\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    print(f\"\u2713 Model {model_id} loaded successfully\")\nexcept Exception as e:\n    print(f\"\u2717 Failed to load model: {e}\")\n</code></pre>"},{"location":"guides/text-extraction/#api-backend-timeouts","title":"API Backend Timeouts","text":"<p>Problem: API requests timeout or fail.</p> <p>Solutions: 1. Increase timeout value 2. Check API credentials 3. Reduce batch size</p> <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextAPIConfig\n\nconfig = QwenTextAPIConfig(\n    model=\"qwen3-vl-8b\",\n    api_key=\"your-api-key\",\n    base_url=\"https://api.example.com/v1\",\n    timeout=60,  # Increase timeout\n    rate_limit=5,  # Reduce concurrent requests\n)\nextractor = QwenTextExtractor(backend=config)\n</code></pre> <p>Next Steps: - See Batch Processing Guide for processing many documents - See Deployment Guide for scaling on Modal - See Layout Analysis Guide for structure-aware extraction</p>"},{"location":"models/","title":"Models","text":"<p>Complete reference for all models available in OmniDocs.</p>"},{"location":"models/#quick-navigation","title":"Quick Navigation","text":""},{"location":"models/#text-extraction","title":"Text Extraction","text":"Model Backends Best For Qwen3-VL PyTorch, VLLM, MLX, API General purpose, multilingual DotsOCR PyTorch, VLLM Layout-aware extraction"},{"location":"models/#layout-analysis","title":"Layout Analysis","text":"Model Backends Best For DocLayoutYOLO PyTorch Fast detection Qwen Layout PyTorch, VLLM, MLX, API Custom labels"},{"location":"models/#ocr","title":"OCR","text":"Model Backends Best For Tesseract CPU Free, offline, 100+ languages"},{"location":"models/#comparison","title":"Comparison","text":"Model Speed Quality Memory Backends Qwen3-VL-2B Fast Good 4GB 4 Qwen3-VL-8B Medium Excellent 16GB 4 DotsOCR Fast Very Good 8GB 2 DocLayoutYOLO Very Fast Good 2-4GB 1 Tesseract Slow (CPU) Good Minimal 1 <p>See full comparison for detailed benchmarks.</p>"},{"location":"models/#choosing-a-model","title":"Choosing a Model","text":""},{"location":"models/#by-task","title":"By Task","text":"<p>Text Extraction - General: Qwen3-VL (any size) - Layout-aware: DotsOCR - Multilingual: Qwen3-VL</p> <p>Layout Detection - Speed: DocLayoutYOLO (0.1s/page) - Custom labels: Qwen Layout - Accuracy: Qwen Layout</p> <p>OCR (with coordinates) - Free/CPU: Tesseract - Accuracy: Surya (coming soon)</p>"},{"location":"models/#by-constraint","title":"By Constraint","text":"<p>Limited GPU (4GB) - Qwen3-VL-2B - DocLayoutYOLO</p> <p>No GPU - Tesseract (CPU) - Qwen3-VL (API backend)</p> <p>Apple Silicon - Qwen3-VL (MLX) - Granite-Docling (coming soon)</p> <p>Production Scale - DotsOCR + VLLM - Qwen3-VL + VLLM</p>"},{"location":"models/#quick-examples","title":"Quick Examples","text":""},{"location":"models/#text-extraction_1","title":"Text Extraction","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n\nextractor = QwenTextExtractor(\n    backend=QwenPyTorchConfig(device=\"cuda\")\n)\nresult = extractor.extract(image, output_format=\"markdown\")\n</code></pre>"},{"location":"models/#layout-detection","title":"Layout Detection","text":"<pre><code>from omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\n\nlayout = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = layout.extract(image)\n</code></pre>"},{"location":"models/#ocr_1","title":"OCR","text":"<pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractConfig\n\nocr = TesseractOCR(config=TesseractConfig(languages=[\"eng\"]))\nresult = ocr.extract(image)\n</code></pre>"},{"location":"models/#coming-soon","title":"Coming Soon","text":"<p>See Roadmap for upcoming models: - LightOnOCR-2 (1B, fastest OCR) - Chandra (9B, best accuracy) - olmOCR-2 (7B, tables/math) - MinerU2.5 (1.2B, MLX support) - Granite-Docling (258M, edge deployment)</p>"},{"location":"models/comparison/","title":"Model Comparison &amp; Selection Guide","text":"<p>A comprehensive comparison of all models available in OmniDocs to help you choose the right tool for your use case.</p>"},{"location":"models/comparison/#text-extraction-models","title":"Text Extraction Models","text":"<p>Models for extracting text content with optional formatting (Markdown/HTML/JSON).</p>"},{"location":"models/comparison/#feature-comparison","title":"Feature Comparison","text":"Feature Qwen3-VL DotsOCR Nanonuts Model Size 2B-32B ~7B ~7B Text Quality Excellent Very Good Very Good Layout Info Basic Detailed (11 cats) Not included Speed Medium Fast Fast Memory 4-40 GB 16 GB 12 GB Multilingual Yes (25+) Limited English-focused Backends PyTorch, VLLM, MLX, API PyTorch, VLLM PyTorch, VLLM Output Formats Markdown, HTML Markdown (with JSON layout) Markdown License Apache 2.0 Open Apache 2.0"},{"location":"models/comparison/#decision-matrix-text-extraction","title":"Decision Matrix: Text Extraction","text":"Use Case Best Choice Why High-quality multilingual docs Qwen3-VL Best text quality, many languages Need layout + text DotsOCR Detailed layout categories with text Fast, English docs Nanonuts Fastest, good quality for English Batch processing DotsOCR + VLLM Good speed with detailed output Cloud/API deployment Qwen3-VL (API) Only option with API backend Apple Silicon only Qwen3-VL (MLX) Only VLM with MLX support"},{"location":"models/comparison/#performance-comparison","title":"Performance Comparison","text":"Model Speed (tok/s) Quality Cost Qwen3-VL-2B 100-150 Good Low (small) Qwen3-VL-8B 50-100 Excellent Medium Qwen3-VL-32B 20-40 Outstanding High DotsOCR 80-120 Very Good Medium Nanonuts 150-200 Good Medium"},{"location":"models/comparison/#layout-analysis-models","title":"Layout Analysis Models","text":"<p>Models for detecting document structure and element regions.</p>"},{"location":"models/comparison/#feature-comparison_1","title":"Feature Comparison","text":"Feature DocLayout-YOLO RT-DETR Qwen Layout Architecture YOLOv10 DETR Vision-Language Speed Very Fast Fast Medium Categories 10 (fixed) 12+ (fixed) Unlimited (custom) Accuracy Good Excellent Excellent Memory 2-4 GB 4-8 GB 8-16 GB Backends PyTorch PyTorch PyTorch, VLLM, MLX, API Custom Labels No No Yes GPU Required Yes Yes Yes (practical) Best For Speed Accuracy Flexibility"},{"location":"models/comparison/#fixed-categories-comparison","title":"Fixed Categories Comparison","text":"<p>DocLayout-YOLO (10): Title, Plain text, Figure, Figure caption, Table, Table caption, Table footnote, Formula, Formula caption, Abandon</p> <p>RT-DETR (12+): Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title, (+ Extended: Document Index, Code, Checkboxes, Forms)</p> <p>Qwen Layout: Standard labels (10) + unlimited custom labels per use case</p>"},{"location":"models/comparison/#decision-matrix-layout-analysis","title":"Decision Matrix: Layout Analysis","text":"Use Case Best Choice Why Batch processing, speed critical DocLayout-YOLO Fastest (0.1-0.2s/page) Academic papers, high precision RT-DETR Excellent accuracy on papers Custom layout categories needed Qwen Layout Only option for custom labels Web page layout Qwen Layout Better understanding of semantic regions Form field detection Qwen Layout Can detect custom field types Production pipeline DocLayout-YOLO Proven, fast, stable"},{"location":"models/comparison/#speed-comparison","title":"Speed Comparison","text":"Model Per-Page Speed Device DocLayout-YOLO 0.1-0.2s Single A10 GPU RT-DETR 0.3-0.5s Single A10 GPU Qwen Layout 2-5s Single A10 GPU Qwen Layout (VLLM) 0.5-1.5s 2x A10 GPU"},{"location":"models/comparison/#ocr-models","title":"OCR Models","text":"<p>Models for extracting text with character/word-level bounding boxes.</p>"},{"location":"models/comparison/#feature-comparison_2","title":"Feature Comparison","text":"Feature Tesseract EasyOCR PaddleOCR Surya Type Traditional Deep Learning Deep Learning Deep Learning Speed Slow (CPU) Medium (GPU) Very Fast Medium Languages 100+ 80+ 80+ Multi Handwriting Poor Medium Medium Excellent GPU Required No Yes Yes Yes Memory CPU 4-6 GB 2-4 GB 6-8 GB Setup System install Python Python Python Accuracy High (printed) Good Excellent (CJK) Best overall"},{"location":"models/comparison/#character-detection-accuracy","title":"Character Detection Accuracy","text":"Model Latin Asian Handwriting Tesseract 95-99% 70-80% 30-50% EasyOCR 90-96% 85-92% 60-70% PaddleOCR 92-97% 94-99% 70-80% Surya 94-98% 88-95% 85-90%"},{"location":"models/comparison/#decision-matrix-ocr","title":"Decision Matrix: OCR","text":"Use Case Best Choice Why Printed English docs Tesseract Fastest (CPU), excellent accuracy Mixed scripts/languages PaddleOCR Best for Asian languages Handwritten documents Surya Best handwriting support Cloud deployment EasyOCR or PaddleOCR Easier setup than Tesseract No GPU available Tesseract Only CPU option Real-time processing PaddleOCR Fastest GPU inference"},{"location":"models/comparison/#performance-comparison_1","title":"Performance Comparison","text":"Model Speed Accuracy Cost Tesseract 2-5s (CPU) 95-99% (printed) Free EasyOCR 1-2s (GPU) 90-96% Free PaddleOCR 0.3-1s (GPU) 92-99% Free Surya 1-3s (GPU) 94-98% Free"},{"location":"models/comparison/#task-specific-recommendations","title":"Task-Specific Recommendations","text":""},{"location":"models/comparison/#use-case-academic-paper-processing","title":"Use Case: Academic Paper Processing","text":"<p>Goal: Extract text and layout from research papers</p> <p>Recommended Pipeline: 1. Layout: DocLayout-YOLO (fast, accurate for papers) 2. Text: Qwen3-VL-8B (high quality, multilingual) 3. Optional: DotsOCR if detailed layout needed</p> <p>Configuration: <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Fast layout detection\nlayout = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\n\n# High-quality text extraction\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n)\n\n# Process\nlayout_result = layout.extract(image)\ntext_result = extractor.extract(image)\n</code></pre></p> <p>Estimated Performance: 2-3 seconds per page, high accuracy</p>"},{"location":"models/comparison/#use-case-document-batch-processing","title":"Use Case: Document Batch Processing","text":"<p>Goal: Extract text from 1000s of documents quickly</p> <p>Recommended Pipeline: 1. Layout: DocLayout-YOLO (for batching) 2. Text: DotsOCR with VLLM (good quality, fast)</p> <p>Configuration: <pre><code># VLLM for batching\nfrom omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nextractor = DotsOCRTextExtractor(\n    backend=DotsOCRVLLMConfig(\n        tensor_parallel_size=2,  # 2 GPUs\n        gpu_memory_utilization=0.85,\n    )\n)\n\n# Process 100+ documents per hour\n</code></pre></p> <p>Estimated Performance: 5-10k documents/hour on 2x A10 GPU</p>"},{"location":"models/comparison/#use-case-handwritten-document-ocr","title":"Use Case: Handwritten Document OCR","text":"<p>Goal: Extract text from handwritten documents</p> <p>Recommended Pipeline: 1. OCR: Surya (best for handwriting) 2. Layout: Qwen Layout with custom labels (if needed)</p> <p>Configuration: <pre><code>from omnidocs.tasks.ocr_extraction import SuryaOCR, SuryaOCRConfig\n\nocr = SuryaOCR(config=SuryaOCRConfig(\n    languages=[\"en\"],\n    det_model=\"en\",\n))\n\nresult = ocr.extract(image)\n</code></pre></p> <p>Estimated Performance: 85%+ handwriting accuracy</p>"},{"location":"models/comparison/#use-case-form-field-extraction","title":"Use Case: Form Field Extraction","text":"<p>Goal: Extract text from form documents with custom field types</p> <p>Recommended Pipeline: 1. Layout: Qwen Layout with custom labels for field types 2. OCR: Tesseract or EasyOCR per field</p> <p>Configuration: <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\ncustom_labels = [\n    CustomLabel(name=\"text_field\"),\n    CustomLabel(name=\"checkbox\"),\n    CustomLabel(name=\"signature_line\"),\n]\n\ndetector = QwenLayoutDetector(\n    backend=QwenLayoutPyTorchConfig(device=\"cuda\")\n)\n\nresult = detector.extract(image, custom_labels=custom_labels)\n</code></pre></p> <p>Estimated Performance: Form processing in 5-10 seconds</p>"},{"location":"models/comparison/#use-case-multilingual-document-processing","title":"Use Case: Multilingual Document Processing","text":"<p>Goal: Process documents in 20+ languages</p> <p>Recommended Pipeline: 1. Text: Qwen3-VL-8B (25+ languages) 2. Fallback: PaddleOCR (80+ languages) for Asian scripts</p> <p>Configuration: <pre><code># Qwen handles most languages\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n)\n</code></pre></p> <p>Supported: English, French, German, Spanish, Russian, Chinese, Japanese, Korean, Arabic, Hindi, Portuguese, Dutch, Polish, Turkish, Greek, Thai, Vietnamese, and more.</p>"},{"location":"models/comparison/#use-case-real-time-document-processing","title":"Use Case: Real-Time Document Processing","text":"<p>Goal: Process documents with &lt;1 second latency</p> <p>Recommended Pipeline: 1. Layout: DocLayout-YOLO (0.1-0.2s) 2. Text: Fast OCR or small VLM</p> <p>Configuration: <pre><code># Fastest layout detection\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\nextractor = DocLayoutYOLO(config=DocLayoutYOLOConfig(\n    device=\"cuda\",\n    img_size=768,  # Smaller for speed\n))\n\nresult = extractor.extract(image)  # &lt;200ms\n</code></pre></p> <p>Estimated Performance: 0.2-1 second per page with layout only</p>"},{"location":"models/comparison/#use-case-cloud-deployment-no-gpu","title":"Use Case: Cloud Deployment (No GPU)","text":"<p>Goal: Deploy document processing in serverless environment</p> <p>Recommended Pipeline: 1. Layout: Not practical (needs GPU) 2. Text: Use API backend or Tesseract (CPU) 3. OCR: Tesseract (CPU-friendly)</p> <p>Configuration: <pre><code># API-based for cloud\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextAPIConfig\n\nextractor = QwenTextExtractor(\n    backend=QwenTextAPIConfig(\n        model=\"qwen3-vl-8b\",\n        api_key=os.getenv(\"QWEN_API_KEY\"),\n    )\n)\n\n# CPU-based OCR\nfrom omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\n\nocr = TesseractOCR(config=TesseractOCRConfig(languages=[\"eng\"]))\n</code></pre></p> <p>Estimated Cost: $0.01-0.10 per document via API</p>"},{"location":"models/comparison/#performance-summary-table","title":"Performance Summary Table","text":""},{"location":"models/comparison/#text-extraction-speed-larger-batch","title":"Text Extraction Speed (larger batch)","text":"Model 1 GPU 2 GPUs (VLLM) Tokens/Sec Qwen3-VL-2B 100-150 250-350 Per-doc Qwen3-VL-8B 50-100 150-250 Per-doc DotsOCR 80-120 200-300 Per-doc Nanonuts 150-200 400-500 Per-doc"},{"location":"models/comparison/#layout-detection-speed","title":"Layout Detection Speed","text":"Model Speed Device DocLayout-YOLO 0.1-0.2s A10 GPU RT-DETR 0.3-0.5s A10 GPU Qwen Layout (PyTorch) 2-5s A10 GPU Qwen Layout (VLLM) 0.5-1.5s 2x A10 GPU"},{"location":"models/comparison/#ocr-speed","title":"OCR Speed","text":"Model Speed (1024x768) Device Tesseract 2-3s CPU EasyOCR 1-2s GPU PaddleOCR 0.3-1s GPU Surya 1-3s GPU"},{"location":"models/comparison/#memory-requirements-summary","title":"Memory Requirements Summary","text":""},{"location":"models/comparison/#vram-requirements","title":"VRAM Requirements","text":"Task Minimal Recommended Optimal Text (Qwen-8B) 8 GB 16 GB 24 GB Layout (DocLayout) 2 GB 4 GB 8 GB Layout (Qwen) 8 GB 16 GB 24 GB OCR (GPU-based) 2 GB 4 GB 8 GB Multi-task pipeline 16 GB 32 GB 40 GB"},{"location":"models/comparison/#cpu-requirements","title":"CPU Requirements","text":"Model CPU Load Parallelization Tesseract Medium Thread-based (4+ cores) EasyOCR Light Not parallelizable DotsOCR Light GPU-bound"},{"location":"models/comparison/#cost-analysis","title":"Cost Analysis","text":""},{"location":"models/comparison/#deployment-costs-per-million-documents","title":"Deployment Costs (per million documents)","text":"Strategy GPU Cost Model Cost Total Self-hosted (PyTorch) $500/month Free $6k/year Self-hosted (VLLM batch) $1000/month Free $12k/year API-based None $1-2/doc $1-2M Hybrid (API + cached) Minimal $0.1-0.5/doc $100k-500k"},{"location":"models/comparison/#development-time","title":"Development Time","text":"Task Effort Models Needed Simple extraction 1 hour 1 (any VLM) Layout + text 2-4 hours 2 (layout + text) Custom layout 4-8 hours Qwen layout + fine-tuning Production pipeline 1-2 weeks 3+ with batching, caching"},{"location":"models/comparison/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"models/comparison/#q-which-model-is-fastest","title":"Q: Which model is fastest?","text":"<p>A: DocLayout-YOLO for layout (0.1-0.2s), PaddleOCR for OCR (0.3-1s), Nanonuts for text (50-80 tok/s)</p>"},{"location":"models/comparison/#q-which-is-most-accurate","title":"Q: Which is most accurate?","text":"<p>A: Qwen3-VL-32B for text, Surya for handwriting, RT-DETR for layout</p>"},{"location":"models/comparison/#q-which-requires-least-gpu","title":"Q: Which requires least GPU?","text":"<p>A: DocLayout-YOLO (2-4 GB), Tesseract (CPU-only)</p>"},{"location":"models/comparison/#q-which-supports-most-languages","title":"Q: Which supports most languages?","text":"<p>A: Tesseract (100+), Qwen (25+), PaddleOCR (80+)</p>"},{"location":"models/comparison/#q-which-is-cheapest-to-run","title":"Q: Which is cheapest to run?","text":"<p>A: Tesseract (free, CPU), DocLayout-YOLO (small GPU model)</p>"},{"location":"models/comparison/#q-best-for-real-time-sub-second","title":"Q: Best for real-time (sub-second)?","text":"<p>A: DocLayout-YOLO for layout only, or PaddleOCR for OCR</p>"},{"location":"models/comparison/#q-best-for-batch-processing","title":"Q: Best for batch processing?","text":"<p>A: DotsOCR or Qwen with VLLM (2-4 GPUs)</p>"},{"location":"models/comparison/#q-can-i-run-without-gpu","title":"Q: Can I run without GPU?","text":"<p>A: Yes - Tesseract (OCR) and API backends (text)</p>"},{"location":"models/comparison/#q-which-is-easiest-to-set-up","title":"Q: Which is easiest to set up?","text":"<p>A: Qwen with PyTorch (single pip install)</p>"},{"location":"models/comparison/#q-production-recommendation","title":"Q: Production recommendation?","text":"<p>A: DocLayout-YOLO + Qwen3-VL-8B on 2x A10 GPU</p>"},{"location":"models/comparison/#migration-guide","title":"Migration Guide","text":""},{"location":"models/comparison/#from-tesseract-to-modern-ocr","title":"From Tesseract to Modern OCR","text":"<pre><code># Old: Tesseract only\nfrom omnidocs.tasks.ocr_extraction import TesseractOCR\n\n# New: Choose based on use case\nfrom omnidocs.tasks.ocr_extraction import (\n    TesseractOCR,  # Printed, many languages\n    PaddleOCR,     # Speed, Asian languages\n    SuryaOCR,      # Handwriting\n)\n</code></pre>"},{"location":"models/comparison/#from-single-model-to-pipeline","title":"From Single-Model to Pipeline","text":"<pre><code># Old: Text extraction only\ntext = extract_text(image)\n\n# New: Layout + text pipeline\nlayout = detect_layout(image)  # Understand structure\ntext = extract_text(image)     # Extract content\n# Combine results for better processing\n</code></pre>"},{"location":"models/comparison/#from-cpu-to-gpu","title":"From CPU to GPU","text":"<pre><code># Old: CPU-based\nocr = TesseractOCR()  # 2-3s per page\n\n# New: GPU-accelerated\nocr = PaddleOCR()     # 0.3-1s per page (10x faster)\n</code></pre>"},{"location":"models/comparison/#see-also","title":"See Also","text":"<ul> <li>Qwen Text Extraction</li> <li>DotsOCR Text Extraction</li> <li>DocLayout-YOLO</li> <li>Qwen Layout Detection</li> <li>Tesseract OCR</li> </ul>"},{"location":"models/layout-analysis/doclayout-yolo/","title":"DocLayout-YOLO Layout Detection","text":""},{"location":"models/layout-analysis/doclayout-yolo/#model-overview","title":"Model Overview","text":"<p>DocLayout-YOLO is a YOLO-based (You Only Look Once) object detector specifically optimized for document layout analysis. It's the fastest layout detection model in OmniDocs, making it ideal for processing large document collections.</p> <p>Model ID: juliozhao/DocLayout-YOLO-DocStructBench Architecture: YOLOv10 (object detection) Training Focus: Academic papers, technical documents, arXiv papers Framework: PyTorch only (no other backends)</p>"},{"location":"models/layout-analysis/doclayout-yolo/#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Fast Inference: 0.1-0.3s per page (fastest in OmniDocs)</li> <li>10 Layout Categories: Title, text, figures, tables, formulas, captions, etc.</li> <li>Fixed Labels: Standardized output across all documents</li> <li>Document-Optimized: Trained on 100K+ academic papers</li> <li>Confidence Scores: Per-detection confidence for filtering</li> </ul>"},{"location":"models/layout-analysis/doclayout-yolo/#limitations","title":"Limitations","text":"<ul> <li>PyTorch only: No VLLM, MLX, or API backends</li> <li>GPU required: No CPU inference (YOLO needs GPU)</li> <li>Fixed categories: Cannot customize labels</li> <li>English-focused: Optimized for English documents</li> <li>Specialized: Best for academic/technical documents</li> <li>Layout only: Does not extract text content (use with OCR/VLM)</li> </ul>"},{"location":"models/layout-analysis/doclayout-yolo/#installation-configuration","title":"Installation &amp; Configuration","text":""},{"location":"models/layout-analysis/doclayout-yolo/#basic-installation","title":"Basic Installation","text":"<pre><code># Install with layout analysis support\npip install omnidocs[pytorch]\n\n# Specifically install doclayout-yolo\npip install doclayout-yolo\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#configuration","title":"Configuration","text":"<pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\nconfig = DocLayoutYOLOConfig(\n    device=\"cuda\",           # GPU required\n    model_path=None,         # Auto-download from HuggingFace\n    img_size=1024,           # Input image size\n    confidence=0.25,         # Detection confidence threshold\n)\n\nextractor = DocLayoutYOLO(config=config)\n</code></pre> <p>Config Parameters:</p> Parameter Type Default Description <code>device</code> str \"cuda\" Device: \"cuda\", \"mps\", \"cpu\" (GPU required) <code>model_path</code> str None Path to model weights, or None for auto-download <code>img_size</code> int 1024 Input size for inference (320-1920) <code>confidence</code> float 0.25 Confidence threshold (0-1, higher = stricter)"},{"location":"models/layout-analysis/doclayout-yolo/#layout-categories-10-fixed","title":"Layout Categories (10 Fixed)","text":"<p>DocLayout-YOLO detects exactly 10 layout element types:</p> Category Description Common Content Title Document/section title \"Introduction\", \"Methods\" Plain text Body paragraph Main content paragraphs Figure Image/diagram (content region) Graphs, plots, photos Figure caption Caption for figures \"Fig. 1: System Overview\" Table Tabular data (content region) Data tables, matrices Table caption Caption for tables \"Table 2: Performance Results\" Table footnote Notes under tables Footnotes, explanations Formula Isolated equation Display math: $E=mc^2$ Formula caption Caption for formulas \"Equation 3.1: Distance metric\" Abandon Elements to ignore Watermarks, page numbers, artifacts"},{"location":"models/layout-analysis/doclayout-yolo/#usage-examples","title":"Usage Examples","text":""},{"location":"models/layout-analysis/doclayout-yolo/#basic-layout-detection","title":"Basic Layout Detection","text":"<pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom PIL import Image\n\n# Initialize\nconfig = DocLayoutYOLOConfig(device=\"cuda\", confidence=0.3)\nextractor = DocLayoutYOLO(config=config)\n\n# Load image\nimage = Image.open(\"document.png\")\n\n# Extract layout\nresult = extractor.extract(image)\n\n# Access results\nprint(f\"Found {result.element_count} elements\")\nprint(f\"Labels: {result.labels_found}\")\n\nfor box in result.bboxes:\n    print(f\"  {box.label.value}: confidence={box.confidence:.2f}\")\n    print(f\"    bbox={box.bbox.to_list()}\")\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#filter-by-confidence","title":"Filter by Confidence","text":"<pre><code># Keep only high-confidence detections\nhigh_conf = result.filter_by_confidence(min_confidence=0.5)\n\nfor box in high_conf:\n    print(f\"{box.label.value} ({box.confidence:.2%})\")\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#filter-by-label","title":"Filter by Label","text":"<pre><code>from omnidocs.tasks.layout_extraction import LayoutLabel\n\n# Extract only text regions\ntext_boxes = result.filter_by_label(LayoutLabel.TEXT)\nprint(f\"Found {len(text_boxes)} text blocks\")\n\n# Extract only figures\nfigures = result.filter_by_label(LayoutLabel.FIGURE)\nfor fig in figures:\n    x1, y1, x2, y2 = fig.bbox.to_xyxy()\n    width = x2 - x1\n    height = y2 - y1\n    print(f\"Figure: {width}x{height} at ({x1}, {y1})\")\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#normalized-coordinates","title":"Normalized Coordinates","text":"<pre><code># Get bounding boxes normalized to 0-1024 scale\nnormalized = result.get_normalized_bboxes()\n\nfor box_dict in normalized:\n    print(f\"{box_dict['label']}:\")\n    print(f\"  bbox (0-1024): {box_dict['bbox']}\")\n    print(f\"  confidence: {box_dict['confidence']:.2f}\")\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#visualization","title":"Visualization","text":"<pre><code>from PIL import Image\n\n# Load original image\nimage = Image.open(\"document.png\")\n\n# Create visualization with bounding boxes\nviz = result.visualize(\n    image,\n    output_path=\"layout_visualization.png\",\n    show_labels=True,\n    show_confidence=True,\n    line_width=2,\n)\n\n# Display\nviz.show()\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#batch-processing","title":"Batch Processing","text":"<pre><code>from pathlib import Path\nimport json\n\n# Process multiple documents\ndoc_dir = Path(\"documents/\")\nresults = {}\n\nfor img_path in sorted(doc_dir.glob(\"*.png\")):\n    print(f\"Processing {img_path.name}...\")\n    image = Image.open(img_path)\n    layout = extractor.extract(image)\n\n    results[img_path.name] = layout.to_dict()\n\n# Save results\nwith open(\"layout_results.json\", \"w\") as f:\n    json.dump(results, f, indent=2)\n\n# Summary statistics\ntotal_elements = sum(r[\"element_count\"] for r in results.values())\navg_elements = total_elements / len(results)\nprint(f\"Total elements: {total_elements}\")\nprint(f\"Average per document: {avg_elements:.1f}\")\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"models/layout-analysis/doclayout-yolo/#speed-comparison","title":"Speed Comparison","text":"Device Image Size Time A10G GPU 1024x1024 0.1-0.2s A10G GPU 2048x2048 0.3-0.5s CPU 1024x1024 5-10s CPU 2048x2048 15-30s"},{"location":"models/layout-analysis/doclayout-yolo/#memory-requirements","title":"Memory Requirements","text":"Batch Size VRAM Device 1 (single) 2-4 GB A10G 2-4 4-8 GB A10G 1 1-2 GB A100"},{"location":"models/layout-analysis/doclayout-yolo/#typical-detection-counts","title":"Typical Detection Counts","text":"Document Type Elements Speed Single page paper 10-30 0.1s Research paper (5pp) 50-150 0.5s Scanned book page 20-40 0.15s"},{"location":"models/layout-analysis/doclayout-yolo/#troubleshooting","title":"Troubleshooting","text":""},{"location":"models/layout-analysis/doclayout-yolo/#model-download-issues","title":"Model Download Issues","text":"<p>Symptom: Model fails to download on first run</p> <p>Solution:</p> <pre><code># Set cache directory\nimport os\nos.environ[\"HF_HOME\"] = \"/path/to/cache\"\n\n# Pre-download the model\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\"juliozhao/DocLayout-YOLO-DocStructBench\")\n\n# Now use extractor (will use cached model)\nextractor = DocLayoutYOLO(config=config)\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#confidence-threshold-tuning","title":"Confidence Threshold Tuning","text":"<p>Symptom: Too many false positives OR missing real elements</p> <p>Solutions:</p> <pre><code># Too many false positives \u2192 increase confidence\nconfig = DocLayoutYOLOConfig(confidence=0.5)  # Stricter\n\n# Missing elements \u2192 decrease confidence\nconfig = DocLayoutYOLOConfig(confidence=0.1)  # More lenient\n\n# Find optimal threshold\nfrom PIL import Image\nimage = Image.open(\"test.png\")\n\nfor conf in [0.1, 0.25, 0.5, 0.75]:\n    config = DocLayoutYOLOConfig(confidence=conf)\n    extractor = DocLayoutYOLO(config=config)\n    result = extractor.extract(image)\n    print(f\"Confidence {conf}: {result.element_count} elements\")\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#image-size-issues","title":"Image Size Issues","text":"<p>Symptom: Poor detection on very large or small images</p> <p>Solutions:</p> <pre><code>from PIL import Image\n\nimage = Image.open(\"document.png\")\nprint(f\"Original size: {image.size}\")\n\n# Resize to standard size for better detection\ntarget_size = 1024\nimage.thumbnail((target_size, target_size), Image.Resampling.LANCZOS)\n\nresult = extractor.extract(image)\nprint(f\"Found {result.element_count} elements\")\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#model-selection-guide","title":"Model Selection Guide","text":""},{"location":"models/layout-analysis/doclayout-yolo/#when-to-use-doclayout-yolo","title":"When to Use DocLayout-YOLO","text":"<p>Best for: - Fast batch processing of large document collections - Academic papers and technical documents - When speed is critical (real-time requirements) - You need layout detection only (will add OCR separately)</p> <p>Not ideal for: - Extracting actual text (use Qwen or DotsOCR) - Complex/unusual layouts (use Qwen layout detector) - Handwritten documents (use Surya) - When you need custom layout categories (use Qwen)</p>"},{"location":"models/layout-analysis/doclayout-yolo/#doclayout-yolo-vs-other-layout-models","title":"DocLayout-YOLO vs Other Layout Models","text":"Feature DocLayout-YOLO RT-DETR Qwen Layout Speed Very Fast Fast Medium Categories 10 (fixed) 12+ (fixed) Unlimited (custom) Backend PyTorch only PyTorch Multi-backend Memory 2-4 GB 4-8 GB 8-16 GB Quality Good Excellent Excellent Use Case Fast detection Precision Flexibility <p>Choose DocLayout-YOLO if: You need fast detection for batch processing Choose Qwen Layout if: You need flexible categories or better quality</p>"},{"location":"models/layout-analysis/doclayout-yolo/#api-reference","title":"API Reference","text":""},{"location":"models/layout-analysis/doclayout-yolo/#doclayoutyoloextract","title":"DocLayoutYOLO.extract()","text":"<pre><code>def extract(image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#layoutoutput-properties","title":"LayoutOutput Properties","text":"<pre><code>result = extractor.extract(image)\n\n# Basic properties\nresult.bboxes              # List[LayoutBox] - all detections\nresult.element_count       # Number of elements\nresult.labels_found        # Set of unique labels\nresult.image_width         # Source image width\nresult.image_height        # Source image height\nresult.model_name          # \"DocLayout-YOLO\"\n\n# Filter methods\nresult.filter_by_label(label)        # Filter by LayoutLabel\nresult.filter_by_confidence(min_conf) # Filter by confidence\n\n# Coordinate conversion\nresult.get_normalized_bboxes()  # Convert to 0-1024 scale\nresult.sort_by_position()       # Sort by reading order\n\n# Export\nresult.to_dict()           # Convert to dictionary\nresult.visualize(image)    # Create visualization\nresult.save_json(path)     # Save to JSON file\nresult.load_json(path)     # Load from JSON file\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#layoutbox-properties","title":"LayoutBox Properties","text":"<pre><code>for box in result.bboxes:\n    box.label             # LayoutLabel enum\n    box.bbox              # BoundingBox object\n    box.confidence        # float (0-1)\n    box.class_id          # int - YOLO class ID\n    box.original_label    # str - original YOLO label\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#boundingbox-methods","title":"BoundingBox Methods","text":"<pre><code>bbox = box.bbox\n\n# Access coordinates\nbbox.x1, bbox.y1          # Top-left corner\nbbox.x2, bbox.y2          # Bottom-right corner\nbbox.width                # Width in pixels\nbbox.height               # Height in pixels\nbbox.area                 # Area in pixels\u00b2\nbbox.center               # (center_x, center_y) tuple\n\n# Convert formats\nbbox.to_list()            # [x1, y1, x2, y2]\nbbox.to_xyxy()            # (x1, y1, x2, y2)\nbbox.to_xywh()            # (x, y, width, height)\n\n# Normalize to 0-1024 range\nnormalized = bbox.to_normalized(image_width, image_height)\n\n# Convert back to absolute\nabsolute = normalized.to_absolute(image_width, image_height)\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#advanced-usage","title":"Advanced Usage","text":""},{"location":"models/layout-analysis/doclayout-yolo/#reading-order-detection","title":"Reading Order Detection","text":"<pre><code># DocLayout-YOLO automatically sorts by position (top to bottom, left to right)\nsorted_result = result.sort_by_position(top_to_bottom=True)\n\nfor i, box in enumerate(sorted_result.bboxes):\n    print(f\"{i+1}. {box.label.value} at ({box.bbox.y1:.0f}, {box.bbox.x1:.0f})\")\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#region-based-processing","title":"Region-Based Processing","text":"<pre><code># Get all elements in upper half of page\nupper_half = [\n    box for box in result.bboxes\n    if box.bbox.y1 &lt; result.image_height // 2\n]\n\n# Get all large elements (&gt; 1/4 page width)\npage_width = result.image_width\nlarge_elements = [\n    box for box in result.bboxes\n    if box.bbox.width &gt; page_width // 4\n]\n\nprint(f\"Upper half: {len(upper_half)} elements\")\nprint(f\"Large: {len(large_elements)} elements\")\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#export-to-different-formats","title":"Export to Different Formats","text":"<pre><code># Save as JSON for downstream processing\nresult.save_json(\"layout.json\")\n\n# Convert to dict for custom serialization\nlayout_dict = result.to_dict()\n\n# Export to COCO format (for computer vision tools)\ncoco_format = {\n    \"images\": [{\n        \"id\": 0,\n        \"width\": result.image_width,\n        \"height\": result.image_height,\n        \"file_name\": \"document.png\"\n    }],\n    \"annotations\": [\n        {\n            \"id\": i,\n            \"image_id\": 0,\n            \"category_id\": box.class_id,\n            \"bbox\": list(box.bbox.to_xywh()),  # COCO format: [x, y, w, h]\n            \"area\": box.bbox.area,\n            \"iscrowd\": 0,\n        }\n        for i, box in enumerate(result.bboxes)\n    ],\n}\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#integration-with-text-extraction","title":"Integration with Text Extraction","text":""},{"location":"models/layout-analysis/doclayout-yolo/#pipeline-layout-ocr","title":"Pipeline: Layout + OCR","text":"<pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\nfrom PIL import Image\n\n# Step 1: Detect layout\nlayout_result = extractor.extract(image)\n\n# Step 2: Extract text from regions\nocr = TesseractOCR(config=TesseractOCRConfig(languages=[\"eng\"]))\n\nfor box in layout_result.bboxes:\n    if box.label.value in [\"text\", \"title\"]:\n        # Crop region\n        x1, y1, x2, y2 = box.bbox.to_xyxy()\n        region = image.crop((x1, y1, x2, y2))\n\n        # OCR the region\n        ocr_result = ocr.extract(region)\n\n        print(f\"{box.label.value}: {ocr_result.full_text}\")\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#pipeline-layout-vlm","title":"Pipeline: Layout + VLM","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Step 1: Detect layout\nlayout_result = extractor.extract(image)\n\n# Step 2: Extract text per element\nextractor_qwen = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(device=\"cuda\")\n)\n\nfor i, box in enumerate(layout_result.bboxes):\n    # Crop region\n    x1, y1, x2, y2 = box.bbox.to_xyxy()\n    region = image.crop((x1, y1, x2, y2))\n\n    # Extract with Qwen\n    result = extractor_qwen.extract(region)\n\n    print(f\"Element {i} ({box.label.value}):\")\n    print(result.content)\n    print()\n</code></pre>"},{"location":"models/layout-analysis/doclayout-yolo/#see-also","title":"See Also","text":"<ul> <li>RT-DETR Layout Detection - Alternative DETR-based model</li> <li>Qwen Layout Detection - For custom categories</li> <li>Comparison Guide - Model selection matrix</li> <li>YOLOv10 Paper</li> </ul>"},{"location":"models/layout-analysis/qwen-layout/","title":"Qwen3-VL Layout Detection","text":""},{"location":"models/layout-analysis/qwen-layout/#model-overview","title":"Model Overview","text":"<p>Qwen3-VL is a Vision-Language Model that can perform flexible layout detection beyond fixed label sets. Unlike DocLayout-YOLO or RT-DETR, Qwen supports custom layout categories while maintaining high accuracy for standard layout analysis tasks.</p> <p>Model Family: Qwen3-VL-2B, Qwen3-VL-4B, Qwen3-VL-8B, Qwen3-VL-32B Repository: Qwen/Qwen3-VL Architecture: Vision Encoder + Language Model Key Feature: Flexible custom labels for domain-specific layout detection</p>"},{"location":"models/layout-analysis/qwen-layout/#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Custom Labels: Define unlimited layout categories beyond standard types</li> <li>VLM-Based: Understands semantic meaning of regions</li> <li>High Accuracy: Better handling of complex/unusual layouts</li> <li>Multi-Backend: PyTorch, VLLM, MLX, API support</li> <li>Confidence Scores: Per-detection confidence for filtering</li> <li>Multilingual: Works with documents in any language</li> </ul>"},{"location":"models/layout-analysis/qwen-layout/#limitations","title":"Limitations","text":"<ul> <li>Slower than YOLO: 5-10x slower than DocLayout-YOLO</li> <li>Requires GPU: No CPU inference practical</li> <li>Memory intensive: 8-16 GB VRAM minimum</li> <li>Less standardized: Labels are user-defined, not fixed enum</li> </ul>"},{"location":"models/layout-analysis/qwen-layout/#supported-backends","title":"Supported Backends","text":"<p>Qwen layout detection supports 4 inference backends:</p> Backend Use Case Speed Setup PyTorch Single document, development 20-50 tok/s Easy VLLM Batch processing 80-150 tok/s Multi-GPU MLX Apple Silicon 10-30 tok/s macOS M1/M3+ API Cloud inference Variable Hosted"},{"location":"models/layout-analysis/qwen-layout/#installation-configuration","title":"Installation &amp; Configuration","text":""},{"location":"models/layout-analysis/qwen-layout/#basic-installation","title":"Basic Installation","text":"<pre><code># Install with PyTorch backend (most common)\npip install omnidocs[pytorch]\n\n# Or with all backends\npip install omnidocs[all]\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#pytorch-backend-configuration","title":"PyTorch Backend Configuration","text":"<pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\nconfig = QwenLayoutPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cuda\",\n    torch_dtype=\"bfloat16\",\n    device_map=\"auto\",\n    max_new_tokens=4096,\n    temperature=0.1,\n)\n\ndetector = QwenLayoutDetector(backend=config)\n</code></pre> <p>PyTorch Config Parameters:</p> Parameter Type Default Description <code>model</code> str \"Qwen/Qwen3-VL-8B-Instruct\" HuggingFace model ID <code>device</code> str \"cuda\" Device: \"cuda\", \"mps\", \"cpu\" <code>torch_dtype</code> str \"auto\" Data type: \"float16\", \"bfloat16\", \"float32\", \"auto\" <code>device_map</code> str \"auto\" Model parallelism strategy <code>use_flash_attention</code> bool False Use Flash Attention 2 (if available) <code>max_new_tokens</code> int 4096 Max tokens to generate <code>temperature</code> float 0.1 Sampling temperature (deterministic output)"},{"location":"models/layout-analysis/qwen-layout/#vllm-backend-configuration","title":"VLLM Backend Configuration","text":"<pre><code>from omnidocs.tasks.layout_extraction.qwen import QwenLayoutVLLMConfig\n\nconfig = QwenLayoutVLLMConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    tensor_parallel_size=1,\n    gpu_memory_utilization=0.9,\n    max_model_len=4096,\n)\n\ndetector = QwenLayoutDetector(backend=config)\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#mlx-backend-configuration-apple-silicon","title":"MLX Backend Configuration (Apple Silicon)","text":"<pre><code>from omnidocs.tasks.layout_extraction.qwen import QwenLayoutMLXConfig\n\nconfig = QwenLayoutMLXConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct-MLX\",\n    quantization=\"4bit\",\n    max_tokens=4096,\n)\n\ndetector = QwenLayoutDetector(backend=config)\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#api-backend-configuration","title":"API Backend Configuration","text":"<pre><code>from omnidocs.tasks.layout_extraction.qwen import QwenLayoutAPIConfig\n\nconfig = QwenLayoutAPIConfig(\n    model=\"qwen3-vl-8b\",\n    api_key=\"your-api-key\",\n    base_url=\"https://api.provider.com/v1\",\n)\n\ndetector = QwenLayoutDetector(backend=config)\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#standard-label-detection","title":"Standard Label Detection","text":""},{"location":"models/layout-analysis/qwen-layout/#using-fixed-standard-labels","title":"Using Fixed Standard Labels","text":"<pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, LayoutLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\nfrom PIL import Image\n\n# Initialize\nconfig = QwenLayoutPyTorchConfig(device=\"cuda\")\ndetector = QwenLayoutDetector(backend=config)\n\n# Load image\nimage = Image.open(\"document.png\")\n\n# Detect standard layout (no custom labels)\nresult = detector.extract(image)\n\n# Access standard labels\nprint(f\"Found {result.element_count} elements\")\nfor box in result.bboxes:\n    print(f\"  {box.label.value}: confidence={box.confidence:.2f}\")\n\n# Filter by standard label type\ntitles = result.filter_by_label(LayoutLabel.TITLE)\ntext_blocks = result.filter_by_label(LayoutLabel.TEXT)\nfigures = result.filter_by_label(LayoutLabel.FIGURE)\ntables = result.filter_by_label(LayoutLabel.TABLE)\n\nprint(f\"Titles: {len(titles)}\")\nprint(f\"Text blocks: {len(text_blocks)}\")\nprint(f\"Figures: {len(figures)}\")\nprint(f\"Tables: {len(tables)}\")\n</code></pre> <p>Standard Layout Labels:</p> Label Description <code>LayoutLabel.TITLE</code> Document/section title <code>LayoutLabel.TEXT</code> Body text paragraph <code>LayoutLabel.LIST</code> Bulleted/numbered list <code>LayoutLabel.FIGURE</code> Image, diagram, plot <code>LayoutLabel.TABLE</code> Tabular data <code>LayoutLabel.CAPTION</code> Figure/table caption <code>LayoutLabel.FORMULA</code> Mathematical equation <code>LayoutLabel.FOOTNOTE</code> Footer note <code>LayoutLabel.PAGE_HEADER</code> Page header <code>LayoutLabel.PAGE_FOOTER</code> Page footer"},{"location":"models/layout-analysis/qwen-layout/#custom-label-detection","title":"Custom Label Detection","text":""},{"location":"models/layout-analysis/qwen-layout/#define-custom-labels","title":"Define Custom Labels","text":"<pre><code>from omnidocs.tasks.layout_extraction import CustomLabel\n\n# Simple custom labels\ncode_block = CustomLabel(name=\"code_block\")\nsidebar = CustomLabel(name=\"sidebar\")\nannotation = CustomLabel(name=\"annotation\")\n\n# Labels with metadata\nabstract = CustomLabel(\n    name=\"abstract\",\n    description=\"Document abstract or summary\",\n    color=\"#E8F4F8\",\n    detection_prompt=\"Look for abstract sections, usually after title\",\n)\n\nrelated_work = CustomLabel(\n    name=\"related_work\",\n    description=\"Related work or background section\",\n    color=\"#FFF3CD\",\n)\n\n# Create list of custom labels\ncustom_labels = [code_block, sidebar, abstract, related_work]\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#extract-with-custom-labels","title":"Extract with Custom Labels","text":"<pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\nfrom PIL import Image\n\ndetector = QwenLayoutDetector(\n    backend=QwenLayoutPyTorchConfig(device=\"cuda\")\n)\n\nimage = Image.open(\"document.png\")\n\n# Detect with custom labels\nresult = detector.extract(\n    image,\n    custom_labels=[code_block, sidebar, abstract],\n)\n\n# Access detections\nfor box in result.bboxes:\n    print(f\"Label: {box.label}\")  # Custom label name\n    print(f\"Bbox: {box.bbox.to_list()}\")\n    print(f\"Confidence: {box.confidence}\")\n    print()\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#mixed-standard-and-custom-labels","title":"Mixed Standard and Custom Labels","text":"<pre><code>from omnidocs.tasks.layout_extraction import LayoutLabel, CustomLabel\n\n# Combine standard and custom labels\nstandard_labels = [LayoutLabel.TITLE, LayoutLabel.TEXT]\ncustom_labels = [\n    CustomLabel(name=\"code_example\"),\n    CustomLabel(name=\"warning_box\"),\n]\n\n# Detect with both\nresult = detector.extract(\n    image,\n    custom_labels=custom_labels,  # Standard labels always included\n)\n\n# All labels present in result\nfor box in result.bboxes:\n    print(f\"Detected: {box.label}\")\n    # Could be: title, text, code_example, warning_box\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#usage-examples","title":"Usage Examples","text":""},{"location":"models/layout-analysis/qwen-layout/#example-1-academic-paper-layout","title":"Example 1: Academic Paper Layout","text":"<pre><code>from omnidocs.tasks.layout_extraction import CustomLabel\nfrom PIL import Image\n\n# Define academic paper custom labels\ncustom_labels = [\n    CustomLabel(\n        name=\"abstract\",\n        description=\"Abstract section\",\n        detection_prompt=\"Find the abstract after the title\",\n    ),\n    CustomLabel(\n        name=\"methodology\",\n        description=\"Methods and experimental setup\",\n    ),\n    CustomLabel(\n        name=\"results_table\",\n        description=\"Results presented as table\",\n    ),\n    CustomLabel(\n        name=\"reference\",\n        description=\"Bibliography and references\",\n    ),\n]\n\n# Detect layout\nresult = detector.extract(\n    image,\n    custom_labels=custom_labels,\n)\n\n# Extract sections\nfor section in custom_labels:\n    elements = [\n        box for box in result.bboxes\n        if box.label == section.name\n    ]\n    if elements:\n        print(f\"Found {section.name}: {len(elements)} element(s)\")\n        for elem in elements:\n            print(f\"  Position: {elem.bbox.to_list()}\")\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#example-2-website-layout-analysis","title":"Example 2: Website Layout Analysis","text":"<pre><code># For web page screenshots\ncustom_labels = [\n    CustomLabel(name=\"header\", description=\"Top navigation bar\"),\n    CustomLabel(name=\"sidebar\", description=\"Left/right sidebar\"),\n    CustomLabel(name=\"main_content\", description=\"Primary content area\"),\n    CustomLabel(name=\"advertisement\", description=\"Ad placement\"),\n    CustomLabel(name=\"footer\", description=\"Footer section\"),\n]\n\nresult = detector.extract(image, custom_labels=custom_labels)\n\n# Map to regions\nregions = {label.name: [] for label in custom_labels}\nfor box in result.bboxes:\n    if box.label in regions:\n        regions[box.label].append(box)\n\nfor region_name, boxes in regions.items():\n    print(f\"{region_name}: {len(boxes)} element(s)\")\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#example-3-form-field-detection","title":"Example 3: Form Field Detection","text":"<pre><code># For forms and structured documents\nform_labels = [\n    CustomLabel(name=\"text_field\", description=\"Text input field\"),\n    CustomLabel(name=\"checkbox\", description=\"Checkbox option\"),\n    CustomLabel(name=\"radio_button\", description=\"Radio button\"),\n    CustomLabel(name=\"dropdown\", description=\"Dropdown select\"),\n    CustomLabel(name=\"required_field\", description=\"Field marked as required (*)\"),\n]\n\nresult = detector.extract(image, custom_labels=form_labels)\n\n# Count field types\nfield_counts = {}\nfor box in result.bboxes:\n    label = str(box.label)\n    field_counts[label] = field_counts.get(label, 0) + 1\n\nfor field_type, count in field_counts.items():\n    print(f\"  {field_type}: {count}\")\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"models/layout-analysis/qwen-layout/#speed-comparison-with-doclayout-yolo","title":"Speed Comparison with DocLayout-YOLO","text":"Model Speed Trade-offs DocLayout-YOLO 0.1-0.2s/page Fast but fixed labels Qwen Layout (PyTorch) 2-5s/page Slower but flexible Qwen Layout (VLLM) 0.5-1.5s/page Better speed with batching"},{"location":"models/layout-analysis/qwen-layout/#memory-requirements","title":"Memory Requirements","text":"Backend Min VRAM Typical Batch PyTorch 8 GB 16 GB 1 VLLM 12 GB 24 GB 2-4 MLX 8 GB 16 GB 1"},{"location":"models/layout-analysis/qwen-layout/#troubleshooting","title":"Troubleshooting","text":""},{"location":"models/layout-analysis/qwen-layout/#custom-labels-not-detected","title":"Custom Labels Not Detected","text":"<p>Symptom: Custom labels return 0 detections</p> <p>Solutions:</p> <pre><code># 1. Provide more detailed descriptions\ncustom_labels = [\n    CustomLabel(\n        name=\"code_block\",\n        description=\"Monospaced font code/programming examples in gray background\",\n        detection_prompt=\"Look for gray-boxed code sections with monospaced text\",\n    ),\n]\n\n# 2. Reduce temperature for more confident predictions\nconfig = QwenLayoutPyTorchConfig(\n    temperature=0.0,  # Most deterministic\n)\n\n# 3. Use larger model variant\nconfig = QwenLayoutPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-32B-Instruct\",\n)\n\n# 4. Check with standard labels first (confidence building)\nresult = detector.extract(image)  # No custom labels\nprint(f\"Found {result.element_count} standard elements\")\n# Then try with custom\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#memory-issues","title":"Memory Issues","text":"<p>Symptom: CUDA out of memory</p> <p>Solutions:</p> <pre><code># Use smaller model\nconfig = QwenLayoutPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-4B-Instruct\",\n)\n\n# Reduce max_new_tokens\nconfig = QwenLayoutPyTorchConfig(\n    max_new_tokens=2048,\n)\n\n# Enable quantization\nconfig = QwenLayoutPyTorchConfig(\n    load_in_4bit=True,\n)\n\n# Use CPU (slow but works)\nconfig = QwenLayoutPyTorchConfig(\n    device=\"cpu\",\n)\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#qwen-layout-vs-doclayout-yolo","title":"Qwen Layout vs DocLayout-YOLO","text":"Aspect Qwen Layout DocLayout-YOLO Custom Labels Yes (unlimited) No (10 fixed) Speed Slower Very fast Accuracy Higher Good Memory 8-16 GB 2-4 GB Backends 4 (PyTorch, VLLM, MLX, API) 1 (PyTorch) Best For Flexibility, custom layouts Speed, batch processing <p>Choose Qwen if: You need custom layout categories or better accuracy Choose DocLayout-YOLO if: You need speed and can use fixed categories</p>"},{"location":"models/layout-analysis/qwen-layout/#api-reference","title":"API Reference","text":""},{"location":"models/layout-analysis/qwen-layout/#qwenlayoutdetectorextract","title":"QwenLayoutDetector.extract()","text":"<pre><code>def extract(\n    image: Union[Image.Image, np.ndarray, str, Path],\n    custom_labels: Optional[List[CustomLabel]] = None,\n) -&gt; LayoutOutput:\n    \"\"\"\n    Extract layout from image with optional custom labels.\n\n    Args:\n        image: Input image\n        custom_labels: List of CustomLabel objects for flexible detection\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#layoutoutput-properties-standard-labels","title":"LayoutOutput Properties (Standard Labels)","text":"<pre><code>result = detector.extract(image)\n\n# Basic info\nresult.bboxes              # List[LayoutBox]\nresult.element_count       # Total detections\nresult.labels_found        # List of detected labels\n\n# Filter by label\nresult.filter_by_label(LayoutLabel.TEXT)\n\n# Convert coordinates\nresult.get_normalized_bboxes()  # 0-1024 scale\nresult.sort_by_position()       # Reading order\n\n# Visualization\nresult.visualize(image, output_path=\"viz.png\")\n\n# Save/load\nresult.save_json(\"layout.json\")\nLayoutOutput.load_json(\"layout.json\")\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#customlabel-properties","title":"CustomLabel Properties","text":"<pre><code>label = CustomLabel(\n    name=\"code_block\",\n    description=\"Code examples\",\n    color=\"#E0E0E0\",\n    detection_prompt=\"Look for monospaced code\",\n)\n\nprint(label.name)               # \"code_block\"\nprint(label.description)        # Description text\nprint(label.color)              # \"#E0E0E0\"\nprint(label.detection_prompt)   # Custom hint\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#advanced-usage","title":"Advanced Usage","text":""},{"location":"models/layout-analysis/qwen-layout/#hierarchical-layout-detection","title":"Hierarchical Layout Detection","text":"<pre><code># Detect first pass: standard labels\nresult_std = detector.extract(image)\n\n# Second pass: custom labels on specific regions\ntext_regions = result_std.filter_by_label(LayoutLabel.TEXT)\n\ncustom_labels = [\n    CustomLabel(name=\"list_item\"),\n    CustomLabel(name=\"definition\"),\n    CustomLabel(name=\"example\"),\n]\n\n# Could refine by cropping and re-detecting each region\nfor text_box in text_regions[:1]:  # First text block\n    x1, y1, x2, y2 = text_box.bbox.to_xyxy()\n    region = image.crop((x1, y1, x2, y2))\n\n    result_detail = detector.extract(\n        region,\n        custom_labels=custom_labels,\n    )\n    print(f\"Found {result_detail.element_count} fine-grained elements\")\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#label-color-mapping-for-visualization","title":"Label Color Mapping for Visualization","text":"<pre><code>from PIL import Image, ImageDraw\n\n# Colors for different label types\nlabel_colors = {\n    \"title\": \"#E74C3C\",        # Red\n    \"text\": \"#3498DB\",         # Blue\n    \"abstract\": \"#2ECC71\",     # Green\n    \"code_block\": \"#95A5A6\",   # Gray\n    \"figure\": \"#9B59B6\",       # Purple\n    \"table\": \"#F39C12\",        # Orange\n}\n\nimage = Image.open(\"document.png\")\nviz = image.copy()\ndraw = ImageDraw.Draw(viz)\n\n# Draw with color mapping\nfor box in result.bboxes:\n    color = label_colors.get(str(box.label), \"#CCCCCC\")\n    coords = box.bbox.to_xyxy()\n    draw.rectangle(coords, outline=color, width=2)\n\nviz.save(\"layout_colored.png\")\n</code></pre>"},{"location":"models/layout-analysis/qwen-layout/#see-also","title":"See Also","text":"<ul> <li>DocLayout-YOLO - Fixed label, fast detector</li> <li>Qwen Text Extraction - Text extraction</li> <li>Comparison Guide - Model selection matrix</li> </ul>"},{"location":"models/ocr-extraction/tesseract/","title":"Tesseract OCR","text":""},{"location":"models/ocr-extraction/tesseract/#model-overview","title":"Model Overview","text":"<p>Tesseract is the leading open-source Optical Character Recognition (OCR) engine, maintained by Google since 2006. It's the most widely deployed OCR solution and excels at printed text in 100+ languages.</p> <p>Project: GitHub tesseract-ocr Architecture: Traditional OCR (legacy and LSTM-based) Training Focus: Printed documents in all major languages Framework: C/C++ with Python bindings</p>"},{"location":"models/ocr-extraction/tesseract/#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Language Support: 100+ languages with high quality</li> <li>Multilingual Documents: Seamlessly handle mixed-language text</li> <li>Word-Level Bounding Boxes: Get exact position of each word</li> <li>Line-Level Grouping: Option to return line-level blocks</li> <li>CPU-Only: No GPU required, runs anywhere</li> <li>Free &amp; Open Source: No license or API costs</li> <li>Configurable: Fine-tuned via OCR engine modes and page segmentation</li> </ul>"},{"location":"models/ocr-extraction/tesseract/#limitations","title":"Limitations","text":"<ul> <li>Printed Text Only: Struggles with handwriting (see Surya for handwritten)</li> <li>CPU-Bound: Slower than GPU-based OCR (2-5 seconds per page)</li> <li>Quality Variance: Heavily dependent on image quality and preprocessing</li> <li>Skewed Documents: Needs de-skewing for rotated documents</li> <li>Low Contrast: Performs poorly on light text or images</li> <li>No Layout Analysis: Returns text only, no structural information (use DocLayout-YOLO for layout)</li> </ul>"},{"location":"models/ocr-extraction/tesseract/#system-installation","title":"System Installation","text":""},{"location":"models/ocr-extraction/tesseract/#required-system-dependencies","title":"Required System Dependencies","text":"<p>Tesseract must be installed at the operating system level before Python can use it.</p> <p>macOS (using Homebrew): <pre><code>brew install tesseract\n</code></pre></p> <p>Ubuntu/Debian: <pre><code>sudo apt-get update\nsudo apt-get install tesseract-ocr\n</code></pre></p> <p>Windows: Download and install from GitHub releases</p> <p>Verify Installation: <pre><code>tesseract --version\n# Should output version and supported languages\n</code></pre></p>"},{"location":"models/ocr-extraction/tesseract/#python-package-installation","title":"Python Package Installation","text":"<pre><code># Install OmniDocs with OCR support\npip install omnidocs[pytorch]\n\n# Or install pytesseract directly\npip install pytesseract\n\n# Verify\npython -c \"import pytesseract; print(pytesseract.get_languages())\"\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#configuration","title":"Configuration","text":""},{"location":"models/ocr-extraction/tesseract/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\n\nconfig = TesseractOCRConfig(\n    languages=[\"eng\"],           # Single language\n    oem=3,                       # OCR Engine Mode (default)\n    psm=3,                       # Page Segmentation Mode\n)\n\nocr = TesseractOCR(config=config)\n</code></pre> <p>Config Parameters:</p> Parameter Type Default Description <code>languages</code> List[str] [\"eng\"] Language codes (e.g., [\"eng\", \"fra\", \"deu\"]) <code>tessdata_dir</code> str None Custom tessdata directory path <code>oem</code> int 3 OCR Engine Mode (0-3) <code>psm</code> int 3 Page Segmentation Mode (0-13) <code>config_params</code> Dict None Additional Tesseract config options"},{"location":"models/ocr-extraction/tesseract/#available-languages","title":"Available Languages","text":"<pre><code># List all installed languages\ntesseract --list-langs\n\n# Sample common languages:\n# eng (English)        fra (French)         deu (German)\n# spa (Spanish)        ita (Italian)        por (Portuguese)\n# chi_sim (Simplified Chinese)  jpn (Japanese)  kor (Korean)\n# ara (Arabic)         rus (Russian)        hin (Hindi)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#ocr-engine-modes-oem","title":"OCR Engine Modes (OEM)","text":"OEM Name Best For Speed 0 Legacy Old documents Fast 1 LSTM Modern text Accurate 2 Legacy+LSTM Mixed quality Medium 3 Default Auto-detect Medium <p>Recommendation: Use OEM=3 (automatic, recommended for most documents)</p>"},{"location":"models/ocr-extraction/tesseract/#page-segmentation-modes-psm","title":"Page Segmentation Modes (PSM)","text":"PSM Description Use Case 0 OSD only Orientation detection only 3 Fully automatic (default) Mixed layouts, images, text 6 Uniform block Single column of text 7 Single text line Single line input 11 Sparse text Scattered text, forms 13 Raw line Treat each line as a word <p>Recommendation: Use PSM=3 for documents, PSM=11 for forms</p>"},{"location":"models/ocr-extraction/tesseract/#usage-examples","title":"Usage Examples","text":""},{"location":"models/ocr-extraction/tesseract/#basic-text-extraction","title":"Basic Text Extraction","text":"<pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\nfrom PIL import Image\n\n# Initialize\nconfig = TesseractOCRConfig(languages=[\"eng\"])\nocr = TesseractOCR(config=config)\n\n# Extract text\nimage = Image.open(\"document.png\")\nresult = ocr.extract(image)\n\n# Access results\nprint(result.full_text)           # Complete extracted text\nprint(result.text_blocks)         # List of TextBlock objects\n\nfor block in result.text_blocks:\n    print(f\"'{block.text}' @ {block.bbox.to_list()} ({block.confidence:.2%})\")\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#line-level-extraction","title":"Line-Level Extraction","text":"<pre><code># Extract at line level (grouped words)\nresult = ocr.extract_lines(image)\n\nfor block in result.text_blocks:\n    print(f\"{block.text}\")\n\n# Useful for:\n# - Preserving line breaks\n# - Document structure\n# - Form processing\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#multilingual-documents","title":"Multilingual Documents","text":"<pre><code># Extract from document with mixed languages\nconfig = TesseractOCRConfig(\n    languages=[\"eng\", \"fra\", \"deu\"],  # English, French, German\n    oem=2,  # Legacy+LSTM for better multilingual support\n)\nocr = TesseractOCR(config=config)\n\nresult = ocr.extract(image)\nprint(f\"Languages detected: {result.languages_detected}\")\nprint(f\"Text: {result.full_text}\")\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#specialized-document-configuration","title":"Specialized Document Configuration","text":"<pre><code># For forms with sparse text\nform_config = TesseractOCRConfig(\n    languages=[\"eng\"],\n    psm=11,  # Sparse text mode\n    config_params={\n        \"tessedit_char_whitelist\": \"0123456789/-.()\",  # Digits, symbols only\n    },\n)\nocr_form = TesseractOCR(config=form_config)\n\nresult = ocr_form.extract(form_image)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#batch-processing","title":"Batch Processing","text":"<pre><code>from pathlib import Path\nimport json\n\n# Process multiple images\ndoc_dir = Path(\"documents/\")\nresults = {}\n\nconfig = TesseractOCRConfig(languages=[\"eng\"])\nocr = TesseractOCR(config=config)\n\nfor img_path in sorted(doc_dir.glob(\"*.png\"))[:10]:\n    print(f\"Processing {img_path.name}...\")\n    image = Image.open(img_path)\n    result = ocr.extract(image)\n\n    results[img_path.name] = {\n        \"text\": result.full_text,\n        \"word_count\": len(result.text_blocks),\n        \"confidence\": sum(\n            b.confidence for b in result.text_blocks\n        ) / len(result.text_blocks) if result.text_blocks else 0,\n    }\n\n# Save results\nwith open(\"ocr_results.json\", \"w\") as f:\n    json.dump(results, f, indent=2)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#image-preprocessing-for-better-results","title":"Image Preprocessing for Better Results","text":"<p>OCR quality depends heavily on image quality. Pre-process images for best results:</p>"},{"location":"models/ocr-extraction/tesseract/#contrast-enhancement","title":"Contrast Enhancement","text":"<pre><code>from PIL import Image, ImageEnhance\n\nimage = Image.open(\"document.png\")\n\n# Increase contrast\nenhancer = ImageEnhance.Contrast(image)\nimage = enhancer.enhance(1.5)  # 1.5x contrast\n\nresult = ocr.extract(image)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#grayscale-conversion","title":"Grayscale Conversion","text":"<pre><code># Convert to grayscale (Tesseract prefers grayscale)\nimage = Image.open(\"document.png\").convert(\"L\")\n\nresult = ocr.extract(image)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#deskew-rotate","title":"Deskew (Rotate)","text":"<pre><code>from PIL import Image\nimport numpy as np\n\n# For skewed documents, rotate to horizontal\nimage = Image.open(\"skewed_document.png\")\n\n# Simple 90-degree rotations\nimage = image.rotate(90, expand=True)\n\n# For arbitrary angles (requires deskew library)\nfrom deskew import determine_skew\nangle = determine_skew(np.array(image))\nif angle:\n    image = image.rotate(angle, expand=True)\n\nresult = ocr.extract(image)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#upscale-small-text","title":"Upscale Small Text","text":"<pre><code>from PIL import Image\n\nimage = Image.open(\"document.png\")\n\n# If text is very small, upscale\nif image.size[0] &lt; 1000:\n    scale = 2\n    new_size = (image.size[0] * scale, image.size[1] * scale)\n    image = image.resize(new_size, Image.Resampling.LANCZOS)\n\nresult = ocr.extract(image)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#complete-preprocessing-pipeline","title":"Complete Preprocessing Pipeline","text":"<pre><code>from PIL import Image, ImageEnhance, ImageFilter\nimport numpy as np\n\ndef preprocess_image(image_path):\n    img = Image.open(image_path)\n\n    # 1. Convert to grayscale\n    img = img.convert(\"L\")\n\n    # 2. Enhance contrast\n    enhancer = ImageEnhance.Contrast(img)\n    img = enhancer.enhance(1.5)\n\n    # 3. Sharpen\n    img = img.filter(ImageFilter.SHARPEN)\n\n    # 4. Upscale if small\n    if img.size[0] &lt; 1000:\n        new_size = (img.size[0] * 2, img.size[1] * 2)\n        img = img.resize(new_size, Image.Resampling.LANCZOS)\n\n    return img\n\n# Use in OCR\npreprocessed = preprocess_image(\"low_quality.png\")\nresult = ocr.extract(preprocessed)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#performance-accuracy","title":"Performance &amp; Accuracy","text":""},{"location":"models/ocr-extraction/tesseract/#speed-characteristics","title":"Speed Characteristics","text":"Setup Image Size Speed Device Single-threaded 1024x768 2-3s CPU 4-threaded 1024x768 0.5-1s CPU (4 cores) GPU-accelerated 1024x768 0.2-0.5s GPU (if compiled)"},{"location":"models/ocr-extraction/tesseract/#accuracy-by-document-type","title":"Accuracy by Document Type","text":"Document Type Quality Accuracy Notes Printed text High 95-99% Best case scenario Scanned PDF Medium 85-95% Needs preprocessing Handwriting High 30-60% Poor, use Surya instead Low contrast Low 20-50% Needs enhancement Multiple languages Medium 80-92% OEM 2 recommended"},{"location":"models/ocr-extraction/tesseract/#language-accuracy","title":"Language Accuracy","text":"Language Accuracy Notes English (Latin) 95-99% Excellent European languages 92-98% Very good Asian languages 80-90% Good (requires language pack) Mixed script 75-85% Challenging"},{"location":"models/ocr-extraction/tesseract/#troubleshooting","title":"Troubleshooting","text":""},{"location":"models/ocr-extraction/tesseract/#installation-issues","title":"Installation Issues","text":"<p>Symptom: <code>ModuleNotFoundError: No module named 'tesseract'</code></p> <p>Solution:</p> <pre><code># Install system Tesseract first (OS-specific)\n# macOS\nbrew install tesseract\n\n# Then install Python package\npip install pytesseract\n</code></pre> <p>Symptom: Python can't find Tesseract binary</p> <p>Solution:</p> <pre><code>import pytesseract\nfrom pathlib import Path\n\n# Option 1: Specify path in code\npytesseract.pytesseract.pytesseract_cmd = r'/usr/local/bin/tesseract'\n\n# Option 2: Configure in TesseractOCRConfig\nconfig = TesseractOCRConfig(\n    tessdata_dir=\"/path/to/tessdata\",\n)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#language-pack-issues","title":"Language Pack Issues","text":"<p>Symptom: Language not found when trying to use non-English</p> <p>Solution:</p> <pre><code># Check installed languages\ntesseract --list-langs\n\n# Install additional language data (macOS)\nbrew install tesseract-lang\n\n# Verify after installation\ntesseract --list-langs | grep fra  # Check for French\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#poor-ocr-quality","title":"Poor OCR Quality","text":"<p>Symptom: Garbled or incomplete text output</p> <p>Solutions (in order of likelihood):</p> <pre><code># 1. Preprocess image (most common fix)\nimage = image.convert(\"L\")  # Grayscale\nenhancer = ImageEnhance.Contrast(image)\nimage = enhancer.enhance(1.5)\n\n# 2. Try different PSM\nconfig = TesseractOCRConfig(psm=6)  # Uniform block\n\n# 3. Try different OEM\nconfig = TesseractOCRConfig(oem=1)  # LSTM only\n\n# 4. Upscale image\nimage = image.resize(\n    (image.size[0] * 2, image.size[1] * 2),\n    Image.Resampling.LANCZOS\n)\n\n# 5. Try line-level (may preserve structure)\nresult = ocr.extract_lines(image)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#performance-issues-slow","title":"Performance Issues (Slow)","text":"<p>Symptom: OCR takes 5+ seconds per page</p> <p>Solutions:</p> <pre><code># 1. Use simpler PSM (fewer segmentation steps)\nconfig = TesseractOCRConfig(psm=6)  # Faster\n\n# 2. Reduce image size\nimage.thumbnail((2048, 2048))\n\n# 3. Use faster OEM\nconfig = TesseractOCRConfig(oem=0)  # Legacy (faster)\n\n# 4. Process on machine with more CPU cores\n# (Tesseract can use multiple cores)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#tesseract-vs-other-ocr-models","title":"Tesseract vs Other OCR Models","text":"Feature Tesseract EasyOCR PaddleOCR Surya Speed Medium Fast Very Fast Medium Language Support 100+ 80+ 80+ Multi Handwriting Poor Medium Medium Excellent GPU Required No Yes Yes Yes Setup System install Python only Python only Python only Cost Free Free Free Free Best For Printed docs General Asian languages Handwriting <p>Choose Tesseract if: - You need CPU-only processing - Processing printed text in 100+ languages - Want zero GPU dependency</p> <p>Not ideal for: - Handwritten documents (use Surya) - Real-time processing (use PaddleOCR) - Asian documents only (use PaddleOCR)</p>"},{"location":"models/ocr-extraction/tesseract/#api-reference","title":"API Reference","text":""},{"location":"models/ocr-extraction/tesseract/#tesseractocrextract","title":"TesseractOCR.extract()","text":"<pre><code>def extract(image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run word-level OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with word-level text blocks\n    \"\"\"\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#tesseractocrextract_lines","title":"TesseractOCR.extract_lines()","text":"<pre><code>def extract_lines(image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run line-level OCR on an image.\n\n    Groups words into lines based on Tesseract's line detection.\n\n    Args:\n        image: Input image\n\n    Returns:\n        OCROutput with line-level text blocks\n    \"\"\"\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#ocroutput-properties","title":"OCROutput Properties","text":"<pre><code>result = ocr.extract(image)\n\n# Text content\nresult.full_text            # Complete text (word-separated)\nresult.text_blocks          # List[TextBlock] objects\nresult.model_name           # \"tesseract\"\nresult.languages_detected   # Languages used\n\n# Image info\nresult.image_width          # Source width in pixels\nresult.image_height         # Source height in pixels\n\n# Statistics\nlen(result.text_blocks)     # Number of detected words/lines\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#textblock-properties","title":"TextBlock Properties","text":"<pre><code>for block in result.text_blocks:\n    block.text              # Word or line text\n    block.bbox              # BoundingBox object\n    block.confidence        # float (0-1)\n    block.granularity       # WORD or LINE\n    block.language          # Detected language code\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#boundingbox-methods","title":"BoundingBox Methods","text":"<pre><code>bbox = block.bbox\n\n# Access coordinates\nbbox.x1, bbox.y1           # Top-left corner\nbbox.x2, bbox.y2           # Bottom-right corner\nbbox.width                 # Width\nbbox.height                # Height\n\n# Convert formats\nbbox.to_list()             # [x1, y1, x2, y2]\nbbox.to_xyxy()             # (x1, y1, x2, y2)\nbbox.to_xywh()             # (x, y, width, height)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"models/ocr-extraction/tesseract/#custom-tesseract-parameters","title":"Custom Tesseract Parameters","text":"<pre><code># Additional config parameters\nconfig = TesseractOCRConfig(\n    languages=[\"eng\"],\n    config_params={\n        # Whitelist specific characters\n        \"tessedit_char_whitelist\": \"0123456789ABCDEFabcdef\",\n\n        # Ignore words shorter than N characters\n        \"min_characters_to_try\": 3,\n\n        # Set segmentation to all caps\n        \"tessedit_create_pdf\": 0,  # Don't create PDF\n    },\n)\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#parallel-processing","title":"Parallel Processing","text":"<pre><code>from concurrent.futures import ThreadPoolExecutor\nfrom pathlib import Path\n\ndef process_image(image_path):\n    image = Image.open(image_path)\n    return ocr.extract(image)\n\n# Process multiple images in parallel\ndoc_dir = Path(\"documents/\")\nimages = list(doc_dir.glob(\"*.png\"))\n\nwith ThreadPoolExecutor(max_workers=4) as executor:\n    results = list(executor.map(process_image, images))\n\nprint(f\"Processed {len(results)} images\")\n</code></pre>"},{"location":"models/ocr-extraction/tesseract/#see-also","title":"See Also","text":"<ul> <li>EasyOCR - GPU-based OCR</li> <li>PaddleOCR - Fast multilingual OCR</li> <li>Surya OCR - Excellent for handwriting</li> <li>OCR Comparison - Model selection matrix</li> <li>Tesseract Docs</li> </ul>"},{"location":"models/text-extraction/dotsocr/","title":"DotsOCR Text Extraction","text":""},{"location":"models/text-extraction/dotsocr/#model-overview","title":"Model Overview","text":"<p>DotsOCR (Deep Object Text Segmentation OCR) is a specialized Vision-Language Model designed specifically for document understanding with built-in layout analysis. Unlike general-purpose VLMs, DotsOCR outputs structured information about document layout while extracting text content.</p> <p>Model ID: rednote-hilab/dots.ocr Repository: DotsOCR on HuggingFace Architecture: Vision Encoder + Language Model Training Focus: Academic papers, technical documents, PDFs</p>"},{"location":"models/text-extraction/dotsocr/#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Layout-Aware Extraction: Detects 11 document element categories with bounding boxes</li> <li>Multi-Format Text: Different formats per category (Markdown, LaTeX, HTML)</li> <li>Fast Inference: 50-100% faster than general-purpose VLMs</li> <li>Normalized Coordinates: All bboxes in 0-1024 range (scale-independent)</li> <li>Reading Order: Maintains document reading order</li> <li>Format-Specific Output:</li> <li>Text/Title/Section-header: Markdown</li> <li>Formula: LaTeX</li> <li>Table: HTML</li> <li>Picture: Bounding box only (no text)</li> </ul>"},{"location":"models/text-extraction/dotsocr/#limitations","title":"Limitations","text":"<ul> <li>PyTorch and VLLM backends only (no MLX, no API)</li> <li>Optimized for academic/technical documents (less good for forms, invoices)</li> <li>Fixed layout categories (cannot add custom categories)</li> <li>Requires GPU (minimum 16GB VRAM for 8B variant)</li> <li>Output is JSON-focused (not raw markdown like Qwen)</li> </ul>"},{"location":"models/text-extraction/dotsocr/#supported-backends","title":"Supported Backends","text":"<p>DotsOCR supports 2 inference backends:</p> Backend Use Case Performance Setup PyTorch Single document, development 50-100 tok/s Simple GPU setup VLLM Batch processing, production 150-300 tok/s Multi-GPU cluster <p>No MLX or API backends available.</p>"},{"location":"models/text-extraction/dotsocr/#installation-configuration","title":"Installation &amp; Configuration","text":""},{"location":"models/text-extraction/dotsocr/#basic-installation","title":"Basic Installation","text":"<pre><code># Install with PyTorch backend\npip install omnidocs[pytorch]\n\n# Or with VLLM for batching\npip install omnidocs[vllm]\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#pytorch-backend-configuration","title":"PyTorch Backend Configuration","text":"<pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nconfig = DotsOCRPyTorchConfig(\n    model=\"rednote-hilab/dots.ocr\",\n    device=\"cuda\",\n    torch_dtype=\"bfloat16\",\n    trust_remote_code=True,\n    device_map=\"auto\",\n    attn_implementation=\"flash_attention_2\",  # Recommended\n)\n\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre> <p>PyTorch Config Parameters:</p> Parameter Type Default Description <code>model</code> str \"rednote-hilab/dots.ocr\" HuggingFace model ID <code>device</code> str \"cuda\" Device: \"cuda\", \"mps\", \"cpu\" <code>torch_dtype</code> str \"bfloat16\" Data type: \"float16\", \"bfloat16\", \"float32\" <code>trust_remote_code</code> bool True Allow custom model code from HuggingFace <code>device_map</code> str \"auto\" Model parallelism: \"auto\", \"balanced\", \"sequential\" <code>attn_implementation</code> str \"flash_attention_2\" Attention type: \"eager\", \"flash_attention_2\", \"sdpa\""},{"location":"models/text-extraction/dotsocr/#vllm-backend-configuration","title":"VLLM Backend Configuration","text":"<pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nconfig = DotsOCRVLLMConfig(\n    model=\"rednote-hilab/dots.ocr\",\n    tensor_parallel_size=1,  # Use 2+ for large models\n    gpu_memory_utilization=0.85,\n    max_model_len=4096,\n)\n\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre> <p>VLLM Config Parameters:</p> Parameter Type Default Description <code>model</code> str Required HuggingFace model ID <code>tensor_parallel_size</code> int 1 Number of GPUs for parallelism <code>gpu_memory_utilization</code> float 0.85 GPU memory usage (0.1-1.0) <code>max_model_len</code> int None Max context length in tokens"},{"location":"models/text-extraction/dotsocr/#layout-categories-11-fixed","title":"Layout Categories (11 Fixed)","text":"<p>DotsOCR recognizes exactly 11 layout element categories:</p> Category Description Text Format Typical Content Title Document/section title Markdown \"Introduction\", \"Chapter 2\" Section-header Subsection heading Markdown \"3.1 Method Overview\" Text Body paragraph Markdown Main content paragraphs List-item Bulleted/numbered item Markdown \"1. First point\", \"\u2022 Item\" Table Tabular data HTML <code>&lt;table&gt;&lt;tr&gt;&lt;td&gt;...&lt;/td&gt;...</code> Formula Mathematical equation LaTeX <code>$E=mc^2$</code> or display math Figure Image/figure/diagram None Bounding box only Caption Figure/table caption Markdown \"Fig 1: System Overview\" Footnote Footer note Markdown Explanatory footnotes Page-header Page header text Markdown Page number, document title Page-footer Page footer text Markdown Page number, author name"},{"location":"models/text-extraction/dotsocr/#usage-examples","title":"Usage Examples","text":""},{"location":"models/text-extraction/dotsocr/#basic-layout-aware-extraction","title":"Basic Layout-Aware Extraction","text":"<pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\nfrom PIL import Image\n\n# Initialize extractor\nconfig = DotsOCRPyTorchConfig(\n    model=\"rednote-hilab/dots.ocr\",\n    device=\"cuda\",\n)\nextractor = DotsOCRTextExtractor(backend=config)\n\n# Load document\nimage = Image.open(\"paper.png\")\n\n# Extract with layout information\nresult = extractor.extract(\n    image,\n    include_layout=True,  # Returns DotsOCRTextOutput\n)\n\n# Access layout elements\nprint(f\"Found {result.num_layout_elements} layout elements\")\nfor elem in result.layout:\n    print(f\"  {elem.category} @ {elem.bbox}: {elem.text[:50]}...\")\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#output-format-examples","title":"Output Format Examples","text":"<pre><code># Default: DotsOCRTextOutput with layout\nresult = extractor.extract(image)\n\n# Access structured layout\nfor elem in result.layout:\n    category = elem.category  # \"Title\", \"Text\", \"Table\", etc.\n    bbox = elem.bbox          # [x1, y1, x2, y2] (0-1024 normalized)\n    text = elem.text          # Content (formatted per category)\n    confidence = elem.confidence  # Detection confidence\n\nprint(result.content)        # Full text (Markdown)\nprint(result.format)         # \"markdown\" (fixed)\nprint(result.has_layout)     # True\nprint(result.content_length) # Total character count\nprint(result.image_width)    # Source image width\nprint(result.image_height)   # Source image height\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#category-specific-processing","title":"Category-Specific Processing","text":"<pre><code># Extract only formulas (as LaTeX)\nformulas = [\n    elem for elem in result.layout\n    if elem.category == \"Formula\"\n]\n\nfor formula in formulas:\n    print(f\"Formula @ {formula.bbox}:\")\n    print(formula.text)  # LaTeX format\n    print()\n\n# Extract tables (as HTML)\ntables = [\n    elem for elem in result.layout\n    if elem.category == \"Table\"\n]\n\nfor table in tables:\n    print(f\"Table @ {table.bbox}:\")\n    print(table.text)  # HTML table\n    print()\n\n# Extract all text content (non-figure)\ntext_elements = [\n    elem for elem in result.layout\n    if elem.category not in [\"Figure\", \"Page-header\", \"Page-footer\"]\n]\n\nfull_text = \"\\n\".join(elem.text for elem in text_elements)\nprint(full_text)  # Cleaned text without layout markers\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#bounding-box-operations","title":"Bounding Box Operations","text":"<pre><code># Access normalized bounding boxes (0-1024 scale)\nfor elem in result.layout:\n    x1, y1, x2, y2 = elem.bbox\n    width = x2 - x1\n    height = y2 - y1\n    area = width * height\n\n    print(f\"{elem.category}: {width}x{height} at ({x1}, {y1})\")\n\n# Filter elements by region (e.g., top half of page)\ntop_half = [\n    elem for elem in result.layout\n    if elem.bbox[1] &lt; 512  # y1 &lt; midpoint\n]\n\n# Filter by size\nlarge_elements = [\n    elem for elem in result.layout\n    if (elem.bbox[2] - elem.bbox[0]) * (elem.bbox[3] - elem.bbox[1]) &gt; 102400\n]\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#batch-processing-with-vllm","title":"Batch Processing with VLLM","text":"<pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\nfrom PIL import Image\nimport json\n\n# Initialize with VLLM\nconfig = DotsOCRVLLMConfig(\n    model=\"rednote-hilab/dots.ocr\",\n    tensor_parallel_size=2,\n    gpu_memory_utilization=0.8,\n)\nextractor = DotsOCRTextExtractor(backend=config)\n\n# Process multiple documents\ndocuments = [\"doc1.png\", \"doc2.png\", \"doc3.png\"]\nresults = []\n\nfor doc_path in documents:\n    image = Image.open(doc_path)\n    result = extractor.extract(image, include_layout=True)\n\n    results.append({\n        \"file\": doc_path,\n        \"elements\": len(result.layout),\n        \"content_length\": result.content_length,\n        \"layout\": [\n            {\n                \"category\": elem.category,\n                \"bbox\": elem.bbox,\n                \"text_length\": len(elem.text) if elem.text else 0,\n            }\n            for elem in result.layout\n        ]\n    })\n\n# Save results\nwith open(\"extraction_results.json\", \"w\") as f:\n    json.dump(results, f, indent=2)\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"models/text-extraction/dotsocr/#memory-requirements","title":"Memory Requirements","text":"Model Framework VRAM Batch Size DotsOCR PyTorch 16 GB 1 (single doc) DotsOCR VLLM 20 GB 2-4 DotsOCR VLLM (2-GPU) 20 GB (per GPU) 6-10"},{"location":"models/text-extraction/dotsocr/#inference-speed","title":"Inference Speed","text":"Setup Speed Throughput PyTorch (single A10) 50-80 tok/s ~400-600 chars/s VLLM (single A10) 150-200 tok/s ~1200-1600 chars/s VLLM (2x A10) 250-350 tok/s ~2000-2800 chars/s"},{"location":"models/text-extraction/dotsocr/#typical-processing-times","title":"Typical Processing Times","text":"Document Tokens Time (PyTorch) Time (VLLM) Single page 1000-2000 12-25s 5-10s 5 pages 5000-10000 60-130s 20-40s 10 pages 10000-20000 130-260s 40-80s"},{"location":"models/text-extraction/dotsocr/#troubleshooting","title":"Troubleshooting","text":""},{"location":"models/text-extraction/dotsocr/#memory-errors","title":"Memory Errors","text":"<p>Symptom: <code>RuntimeError: CUDA out of memory</code></p> <p>Solutions:</p> <pre><code># 1. Use CPU (slow but works)\nconfig = DotsOCRPyTorchConfig(device=\"cpu\")\n\n# 2. Reduce image size before processing\nfrom PIL import Image\nimage = Image.open(\"document.png\")\nimage.thumbnail((2048, 2048))  # Resize if larger\n\n# 3. Use VLLM with memory management\nconfig = DotsOCRVLLMConfig(\n    gpu_memory_utilization=0.7,  # Reduced from 0.85\n    max_model_len=2048,  # Reduced from 4096\n)\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#layout-parsing-errors","title":"Layout Parsing Errors","text":"<p>Symptom: <code>ValueError: Invalid layout JSON structure</code></p> <p>Solution:</p> <pre><code># Check raw output for issues\nresult = extractor.extract(image, include_layout=True)\n\nif result.error:\n    print(f\"Extraction error: {result.error}\")\n    print(f\"Raw output: {result.raw_output[:500]}...\")\n\n# Ensure image is valid\nif image.size[0] &lt; 256 or image.size[1] &lt; 256:\n    print(\"Image too small for reliable layout detection\")\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#missing-layout-categories","title":"Missing Layout Categories","text":"<p>Symptom: Some expected elements not detected</p> <p>Solutions:</p> <pre><code># Check what was detected\ndetected_categories = set(\n    elem.category for elem in result.layout\n)\nprint(f\"Found: {detected_categories}\")\n\n# Element may be below confidence threshold\n# Access raw output to see low-confidence detections\nprint(result.raw_output)\n\n# Try with different preprocessing\nfrom PIL import ImageEnhance\nimage = Image.open(\"document.png\")\nenhancer = ImageEnhance.Contrast(image)\nimage = enhancer.enhance(1.3)\nresult = extractor.extract(image)\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#dotsocr-vs-other-models","title":"DotsOCR vs Other Models","text":""},{"location":"models/text-extraction/dotsocr/#dotsocr-vs-qwen3-vl","title":"DotsOCR vs Qwen3-VL","text":"Feature DotsOCR Qwen3-VL Layout Info Detailed (11 cats) Basic Output Format JSON + Markdown Markdown/HTML Speed Fast Medium Text Quality Good Excellent Multilingual Limited Excellent (25+ langs) Backends PyTorch, VLLM PyTorch, VLLM, MLX, API Best For Layout analysis Text quality <p>Choose DotsOCR if: You need precise layout information for post-processing Choose Qwen if: You need high-quality text in multiple languages</p>"},{"location":"models/text-extraction/dotsocr/#when-to-use-dotsocr","title":"When to Use DotsOCR","text":"<p>Ideal scenarios: - Academic papers with structured layouts - Technical documents with formulas and tables - Batch processing with layout analysis - When you need bounding boxes for each element</p> <p>Not ideal for: - Handwritten documents (use Surya) - Forms with complex fields (use specialized form parser) - Real-time single-document processing (overhead &gt; benefit) - Custom layout categories needed</p>"},{"location":"models/text-extraction/dotsocr/#api-reference","title":"API Reference","text":""},{"location":"models/text-extraction/dotsocr/#dotsocrtextextractorextract","title":"DotsOCRTextExtractor.extract()","text":"<pre><code>def extract(\n    image: Union[Image.Image, np.ndarray, str, Path],\n    include_layout: bool = True,\n    output_format: str = \"markdown\",\n) -&gt; DotsOCRTextOutput:\n    \"\"\"\n    Extract text with layout from document image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n        include_layout: Include layout elements with bboxes (default: True)\n        output_format: \"markdown\" or \"json\" (fixed)\n\n    Returns:\n        DotsOCRTextOutput with layout elements and text\n    \"\"\"\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#dotsocrtextoutput-properties","title":"DotsOCRTextOutput Properties","text":"<pre><code>result = extractor.extract(image)\n\n# Layout information\nresult.layout                   # List[LayoutElement]\nresult.has_layout              # True\nresult.num_layout_elements     # int\n\n# Text content\nresult.content                 # Full text (Markdown)\nresult.format                  # \"markdown\"\nresult.content_length          # Characters\n\n# Element categories\nresult.layout_categories       # List of 11 categories\n\n# Metadata\nresult.image_width            # Source image width\nresult.image_height           # Source image height\nresult.truncated              # Output hit max tokens\nresult.error                  # Error message if any\nresult.raw_output             # Raw model JSON\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#layoutelement-properties","title":"LayoutElement Properties","text":"<pre><code>for elem in result.layout:\n    elem.category        # \"Title\", \"Text\", \"Table\", etc.\n    elem.bbox            # [x1, y1, x2, y2] (0-1024)\n    elem.text            # Content (Markdown/LaTeX/HTML)\n    elem.confidence      # float (0-1) - detection confidence\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#advanced-usage","title":"Advanced Usage","text":""},{"location":"models/text-extraction/dotsocr/#post-processing-extract-figures","title":"Post-Processing: Extract Figures","text":"<pre><code># Get all figures with their captions\nfigures = {}\nfor elem in result.layout:\n    if elem.category == \"Figure\":\n        bbox = elem.bbox\n        figures[str(bbox)] = {\n            \"bbox\": bbox,\n            \"caption\": None,\n        }\n    elif elem.category == \"Caption\":\n        # Find nearest figure\n        # (could implement spatial matching here)\n        pass\n\nfor fig_bbox, fig_data in figures.items():\n    print(f\"Figure @ {fig_data['bbox']}\")\n    print(f\"  Caption: {fig_data['caption']}\")\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#export-to-structured-format","title":"Export to Structured Format","text":"<pre><code>import json\nfrom dataclasses import asdict\n\n# Convert to JSON-serializable format\noutput_data = {\n    \"document\": {\n        \"width\": result.image_width,\n        \"height\": result.image_height,\n    },\n    \"elements\": [\n        {\n            \"category\": elem.category,\n            \"bbox\": {\n                \"x1\": elem.bbox[0],\n                \"y1\": elem.bbox[1],\n                \"x2\": elem.bbox[2],\n                \"y2\": elem.bbox[3],\n            },\n            \"text\": elem.text,\n            \"confidence\": elem.confidence,\n        }\n        for elem in result.layout\n    ]\n}\n\n# Save\nwith open(\"layout_analysis.json\", \"w\") as f:\n    json.dump(output_data, f, indent=2)\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#visualization-with-bounding-boxes","title":"Visualization with Bounding Boxes","text":"<pre><code>from PIL import Image, ImageDraw\n\n# Load original image\nimage = Image.open(\"document.png\")\nimg_w, img_h = image.size\n\n# Create visualization\nviz = image.copy()\ndraw = ImageDraw.Draw(viz)\n\n# Color map for categories\ncolors = {\n    \"Title\": \"red\",\n    \"Text\": \"blue\",\n    \"Table\": \"orange\",\n    \"Formula\": \"purple\",\n    \"Figure\": \"green\",\n}\n\n# Draw bounding boxes\nfor elem in result.layout:\n    # Convert from 0-1024 to pixel coordinates\n    bbox = [\n        (elem.bbox[0] / 1024) * img_w,\n        (elem.bbox[1] / 1024) * img_h,\n        (elem.bbox[2] / 1024) * img_w,\n        (elem.bbox[3] / 1024) * img_h,\n    ]\n\n    color = colors.get(elem.category, \"gray\")\n    draw.rectangle(bbox, outline=color, width=3)\n    draw.text((bbox[0], bbox[1] - 15), elem.category, fill=color)\n\n# Save\nviz.save(\"layout_visualization.png\")\n</code></pre>"},{"location":"models/text-extraction/dotsocr/#see-also","title":"See Also","text":"<ul> <li>Qwen3-VL Text Extraction - For pure text quality</li> <li>DotsOCR Repository</li> <li>Comparison Guide - Model selection matrix</li> </ul>"},{"location":"models/text-extraction/qwen/","title":"Qwen3-VL Text Extraction","text":""},{"location":"models/text-extraction/qwen/#model-overview","title":"Model Overview","text":"<p>Qwen3-VL is an advanced Vision-Language Model optimized for document understanding and text extraction. It excels at producing high-quality markdown and HTML output while maintaining document layout and semantic structure.</p> <p>Model Family: Qwen3-VL-2B, Qwen3-VL-4B, Qwen3-VL-8B, Qwen3-VL-32B Repository: Qwen/Qwen3-VL Recommended Variant: Qwen3-VL-8B-Instruct (best balance of quality and speed)</p>"},{"location":"models/text-extraction/qwen/#key-capabilities","title":"Key Capabilities","text":"<ul> <li>Multi-format Output: Markdown, HTML, or custom formats</li> <li>Layout-Aware: Preserves document structure and semantic relationships</li> <li>Multilingual: Supports 25+ languages with native quality</li> <li>Document Types: PDFs, academic papers, technical docs, web pages, presentations</li> <li>Scale Support: Handles documents from single-page images to 16k+ token outputs</li> <li>Custom Prompts: Flexible prompt engineering for specialized extraction tasks</li> </ul>"},{"location":"models/text-extraction/qwen/#limitations","title":"Limitations","text":"<ul> <li>Requires GPU for inference (2B variant: 4GB VRAM, 8B: 16GB, 32B: 40GB+)</li> <li>Slower than single-task models (100-300 tokens/sec depending on backend)</li> <li>Can struggle with highly stylized or unusual layouts</li> <li>No inherent language detection (specify language in config if needed)</li> </ul>"},{"location":"models/text-extraction/qwen/#supported-backends","title":"Supported Backends","text":"<p>Qwen3-VL supports 4 inference backends, allowing you to choose the right deployment method:</p> Backend Use Case Performance Setup PyTorch Local GPU inference 50-150 tokens/sec Easy, single GPU VLLM High-throughput batching 200-400 tokens/sec Requires GPU cluster MLX Apple Silicon (native) 20-50 tokens/sec macOS M1/M2/M3+ only API Hosted inference Variable Cloud provider"},{"location":"models/text-extraction/qwen/#installation-configuration","title":"Installation &amp; Configuration","text":""},{"location":"models/text-extraction/qwen/#basic-installation","title":"Basic Installation","text":"<pre><code># Install with PyTorch backend (most common)\npip install omnidocs[pytorch]\n\n# Or install with VLLM for high throughput\npip install omnidocs[vllm]\n\n# Or install with all backends\npip install omnidocs[all]\n</code></pre>"},{"location":"models/text-extraction/qwen/#pytorch-backend-configuration","title":"PyTorch Backend Configuration","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cuda\",\n    torch_dtype=\"bfloat16\",\n    device_map=\"auto\",\n    trust_remote_code=True,\n    use_flash_attention=False,  # Set to True if flash-attn installed\n    max_new_tokens=8192,\n    temperature=0.1,\n)\n\nextractor = QwenTextExtractor(backend=config)\n</code></pre> <p>PyTorch Config Parameters:</p> Parameter Type Default Description <code>model</code> str \"Qwen/Qwen3-VL-8B-Instruct\" HuggingFace model ID <code>device</code> str \"cuda\" Device: \"cuda\", \"mps\", \"cpu\" <code>torch_dtype</code> str \"auto\" Data type: \"float16\", \"bfloat16\", \"float32\", \"auto\" <code>device_map</code> str \"auto\" Model parallelism: \"auto\", \"balanced\", \"sequential\", None <code>trust_remote_code</code> bool True Allow custom model code from HuggingFace <code>use_flash_attention</code> bool False Use Flash Attention 2 (faster, requires flash-attn) <code>max_new_tokens</code> int 8192 Max tokens to generate (256-32768) <code>temperature</code> float 0.1 Sampling temperature (0.0-2.0, lower = deterministic)"},{"location":"models/text-extraction/qwen/#vllm-backend-configuration","title":"VLLM Backend Configuration","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\n\nconfig = QwenTextVLLMConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    tensor_parallel_size=1,  # Use 2+ for large models\n    gpu_memory_utilization=0.9,\n    max_model_len=8192,\n)\n\nextractor = QwenTextExtractor(backend=config)\n</code></pre> <p>VLLM Config Parameters:</p> Parameter Type Default Description <code>model</code> str Required HuggingFace model ID <code>tensor_parallel_size</code> int 1 Number of GPUs for parallelism <code>gpu_memory_utilization</code> float 0.9 GPU memory usage (0.1-1.0) <code>max_model_len</code> int None Max context length in tokens"},{"location":"models/text-extraction/qwen/#mlx-backend-configuration-apple-silicon","title":"MLX Backend Configuration (Apple Silicon)","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextMLXConfig\n\nconfig = QwenTextMLXConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    quantization=\"4bit\",  # or \"8bit\", \"none\"\n    max_tokens=8192,\n)\n\nextractor = QwenTextExtractor(backend=config)\n</code></pre>"},{"location":"models/text-extraction/qwen/#api-backend-configuration","title":"API Backend Configuration","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextAPIConfig\n\nconfig = QwenTextAPIConfig(\n    model=\"qwen3-vl-8b\",\n    api_key=\"your-api-key\",\n    base_url=\"https://api.provider.com/v1\",\n    rate_limit=10,  # Requests per second\n)\n\nextractor = QwenTextExtractor(backend=config)\n</code></pre>"},{"location":"models/text-extraction/qwen/#usage-examples","title":"Usage Examples","text":""},{"location":"models/text-extraction/qwen/#basic-text-extraction-markdown","title":"Basic Text Extraction (Markdown)","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nfrom omnidocs import Document\nfrom PIL import Image\n\n# Initialize extractor\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cuda\",\n)\nextractor = QwenTextExtractor(backend=config)\n\n# Load document\nimage = Image.open(\"document.png\")\n\n# Extract text in markdown\nresult = extractor.extract(\n    image,\n    output_format=\"markdown\",\n)\n\nprint(result.content)  # Clean markdown\nprint(result.word_count)  # Approximate word count\n</code></pre>"},{"location":"models/text-extraction/qwen/#multi-format-extraction","title":"Multi-Format Extraction","text":"<pre><code># HTML output (preserves more layout semantics)\nresult_html = extractor.extract(\n    image,\n    output_format=\"html\",\n)\n\n# Custom prompt for specialized extraction\ncustom_prompt = \"\"\"Extract all text as JSON with structure:\n{\n    \"title\": \"...\",\n    \"sections\": [{\"heading\": \"...\", \"content\": \"...\"}],\n    \"tables\": [...]\n}\n\"\"\"\n\nresult_custom = extractor.extract(\n    image,\n    output_format=\"markdown\",\n    custom_prompt=custom_prompt,\n)\n</code></pre>"},{"location":"models/text-extraction/qwen/#batch-processing-with-vllm","title":"Batch Processing with VLLM","text":"<pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\nfrom PIL import Image\nimport time\n\n# Initialize with VLLM for high throughput\nconfig = QwenTextVLLMConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    tensor_parallel_size=2,  # Use 2 GPUs\n    gpu_memory_utilization=0.8,\n)\nextractor = QwenTextExtractor(backend=config)\n\n# Load multiple documents\nimages = [\n    Image.open(f\"doc_{i}.png\") for i in range(10)\n]\n\n# Process with streaming\nresults = []\nstart = time.time()\n\nfor i, image in enumerate(images):\n    result = extractor.extract(image, output_format=\"markdown\")\n    results.append(result)\n    elapsed = time.time() - start\n    throughput = (i + 1) / elapsed * 1000  # chars/sec\n    print(f\"[{i+1}/10] {result.content_length} chars - {throughput:.0f} chars/sec\")\n\nprint(f\"\\nTotal time: {time.time() - start:.1f}s\")\nprint(f\"Avg length: {sum(r.content_length for r in results) / len(results):.0f} chars\")\n</code></pre>"},{"location":"models/text-extraction/qwen/#layout-aware-extraction","title":"Layout-Aware Extraction","text":"<pre><code># Include layout information\nresult = extractor.extract(\n    image,\n    output_format=\"markdown\",\n    include_layout=True,\n)\n\n# Access raw output with bounding boxes\nprint(result.raw_output)  # Contains bbox annotations\n</code></pre>"},{"location":"models/text-extraction/qwen/#api-based-extraction-cloud","title":"API-Based Extraction (Cloud)","text":"<pre><code>import os\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextAPIConfig\nfrom PIL import Image\n\n# Configure API backend\nconfig = QwenTextAPIConfig(\n    model=\"qwen3-vl-8b\",\n    api_key=os.getenv(\"QWEN_API_KEY\"),\n    base_url=\"https://api.together.xyz/v1\",\n    rate_limit=5,\n)\n\nextractor = QwenTextExtractor(backend=config)\n\n# Extract from image\nimage = Image.open(\"document.png\")\nresult = extractor.extract(\n    image,\n    output_format=\"markdown\",\n)\n\nprint(result.content)\n</code></pre>"},{"location":"models/text-extraction/qwen/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"models/text-extraction/qwen/#memory-requirements-by-variant","title":"Memory Requirements by Variant","text":"Model Min VRAM Optimal VRAM Batch Size (VLLM) Qwen3-VL-2B 4 GB 8 GB 8-16 Qwen3-VL-4B 8 GB 12 GB 4-8 Qwen3-VL-8B 16 GB 24 GB 2-4 Qwen3-VL-32B 40 GB 80 GB 1"},{"location":"models/text-extraction/qwen/#inference-speed-single-document","title":"Inference Speed (Single Document)","text":"Backend Model Speed Device PyTorch 8B 50-100 tok/s Single A10 GPU VLLM 8B 200-300 tok/s 2x A10 GPU (tensor parallel) MLX 8B-quantized 20-40 tok/s M3 Max (48GB) API 8B Variable Cloud (depends on provider)"},{"location":"models/text-extraction/qwen/#typical-output-sizes","title":"Typical Output Sizes","text":"Document Type Tokens Characters Single-page document 500-2000 3-12 KB Academic paper page 1000-4000 6-24 KB Multi-page scanned doc 2000-8000 12-48 KB"},{"location":"models/text-extraction/qwen/#troubleshooting","title":"Troubleshooting","text":""},{"location":"models/text-extraction/qwen/#out-of-memory-oom","title":"Out of Memory (OOM)","text":"<p>Symptom: <code>RuntimeError: CUDA out of memory</code></p> <p>Solutions:</p> <pre><code># 1. Reduce max_new_tokens\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    max_new_tokens=4096,  # Reduced from 8192\n)\n\n# 2. Use smaller model variant\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-4B-Instruct\",  # Smaller variant\n)\n\n# 3. Enable quantization\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    load_in_4bit=True,  # Requires bitsandbytes\n)\n\n# 4. Use CPU\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cpu\",  # Slower but works with limited VRAM\n)\n</code></pre>"},{"location":"models/text-extraction/qwen/#slow-inference","title":"Slow Inference","text":"<p>Symptom: Processing takes 30+ seconds per document</p> <p>Solutions:</p> <pre><code># 1. Enable Flash Attention (requires flash-attn package)\nconfig = QwenTextPyTorchConfig(\n    use_flash_attention=True,\n)\n\n# 2. Use VLLM for batching\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextVLLMConfig\nconfig = QwenTextVLLMConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    tensor_parallel_size=2,\n)\n\n# 3. Use smaller model\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-4B-Instruct\",  # 2x faster\n)\n\n# 4. Reduce image size\nfrom PIL import Image\nimage = Image.open(\"document.png\")\nimage.thumbnail((1024, 1024))  # Resize to 1024x1024 max\n</code></pre>"},{"location":"models/text-extraction/qwen/#poor-quality-output","title":"Poor Quality Output","text":"<p>Symptom: Garbled or incomplete text extraction</p> <p>Solutions:</p> <pre><code># 1. Lower temperature for more deterministic output\nconfig = QwenTextPyTorchConfig(\n    temperature=0.01,  # Very low for consistency\n)\n\n# 2. Use larger model variant\nconfig = QwenTextPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-32B-Instruct\",  # Better quality\n)\n\n# 3. Pre-process image (enhance contrast, de-skew)\nfrom PIL import ImageEnhance\nimage = Image.open(\"document.png\")\nenhancer = ImageEnhance.Contrast(image)\nimage = enhancer.enhance(1.5)  # Increase contrast\n\n# 4. Custom prompt for better guidance\ncustom_prompt = \"\"\"Extract all text exactly as it appears.\nPreserve formatting, structure, and special characters.\"\"\"\nresult = extractor.extract(image, custom_prompt=custom_prompt)\n</code></pre>"},{"location":"models/text-extraction/qwen/#api-rate-limiting","title":"API Rate Limiting","text":"<p>Symptom: <code>429 Too Many Requests</code> errors</p> <p>Solutions:</p> <pre><code># Reduce rate limit\nconfig = QwenTextAPIConfig(\n    model=\"qwen3-vl-8b\",\n    api_key=\"...\",\n    rate_limit=2,  # Reduced from 10\n)\n\n# Implement retry logic\nimport time\nmax_retries = 3\nfor attempt in range(max_retries):\n    try:\n        result = extractor.extract(image)\n        break\n    except Exception as e:\n        if attempt &lt; max_retries - 1:\n            wait_time = 2 ** attempt\n            print(f\"Rate limited, waiting {wait_time}s...\")\n            time.sleep(wait_time)\n        else:\n            raise\n</code></pre>"},{"location":"models/text-extraction/qwen/#model-download-issues","title":"Model Download Issues","text":"<p>Symptom: <code>ConnectionError</code> or timeout during model loading</p> <p>Solutions:</p> <pre><code># Set HuggingFace cache directory\nimport os\nos.environ[\"HF_HOME\"] = \"/path/to/cache\"\n\n# Pre-download model\nfrom huggingface_hub import snapshot_download\nsnapshot_download(\"Qwen/Qwen3-VL-8B-Instruct\")\n\n# Use local model path\nconfig = QwenTextPyTorchConfig(\n    model=\"/local/path/to/model\",\n)\n</code></pre>"},{"location":"models/text-extraction/qwen/#model-selection-guide","title":"Model Selection Guide","text":""},{"location":"models/text-extraction/qwen/#when-to-use-qwen3-vl","title":"When to Use Qwen3-VL","text":"<p>Best for: - High-quality document extraction (academic papers, technical docs) - Multilingual documents - Complex layouts with mixed content types - Production systems needing consistent quality</p> <p>Not ideal for: - Real-time processing (see: Nanonuts OCR for speed) - Handwritten documents (see: Surya OCR) - Fixed-label layout detection (see: DocLayout-YOLO)</p>"},{"location":"models/text-extraction/qwen/#qwen-vs-dotsocr-comparison","title":"Qwen vs DotsOCR Comparison","text":"Feature Qwen3-VL DotsOCR Output Quality Excellent Very Good Layout Info Basic Detailed (11 categories) Speed Medium Fast Memory High Medium Multilingual Yes (25+ langs) Limited Model Size Options 2B-32B Single <p>Choose Qwen3-VL if: You need high-quality text and multilingual support Choose DotsOCR if: You need detailed layout information with good performance</p>"},{"location":"models/text-extraction/qwen/#api-reference","title":"API Reference","text":""},{"location":"models/text-extraction/qwen/#qwentextextractorextract","title":"QwenTextExtractor.extract()","text":"<pre><code>def extract(\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: str = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from image using Qwen3-VL.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n        output_format: \"markdown\" or \"html\"\n        include_layout: Include layout information in raw output\n        custom_prompt: Override default extraction prompt\n\n    Returns:\n        TextOutput with extracted content\n    \"\"\"\n</code></pre>"},{"location":"models/text-extraction/qwen/#textoutput-properties","title":"TextOutput Properties","text":"<pre><code>result = extractor.extract(image)\n\n# Access extracted content\nprint(result.content)        # Formatted text (markdown/html)\nprint(result.format)         # Output format\nprint(result.plain_text)     # Plain text without formatting\nprint(result.content_length) # Character count\nprint(result.word_count)     # Approximate word count\nprint(result.image_width)    # Source image width\nprint(result.image_height)   # Source image height\nprint(result.model_name)     # Model used\nprint(result.raw_output)     # Raw model output (with artifacts)\n</code></pre>"},{"location":"models/text-extraction/qwen/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"models/text-extraction/qwen/#device-map-strategies","title":"Device Map Strategies","text":"<pre><code># Auto device mapping (recommended)\ndevice_map = \"auto\"\n\n# Balanced distribution across GPUs\ndevice_map = \"balanced\"\n\n# Sequential loading (one GPU at a time)\ndevice_map = \"sequential\"\n\n# Manual: First layer on GPU0, rest on CPU\ndevice_map = {\n    \"model.layers.0\": 0,\n    \"model.layers.1-31\": \"cpu\",\n}\n</code></pre>"},{"location":"models/text-extraction/qwen/#data-type-selection","title":"Data Type Selection","text":"<pre><code># float32: Full precision (slower, more VRAM)\ntorch_dtype = \"float32\"\n\n# float16: Half precision (faster, less VRAM, less accurate)\ntorch_dtype = \"float16\"\n\n# bfloat16: Brain float (recommended for stability)\ntorch_dtype = \"bfloat16\"\n\n# auto: Let model choose based on hardware\ntorch_dtype = \"auto\"\n</code></pre>"},{"location":"models/text-extraction/qwen/#see-also","title":"See Also","text":"<ul> <li>Qwen HuggingFace Model Card</li> <li>DotsOCR Documentation - For layout-aware extraction</li> <li>Qwen Layout Detection - For layout analysis</li> <li>Comparison Guide - Model selection matrix</li> </ul>"},{"location":"reference/","title":"API Reference","text":"<p>Auto-generated documentation for the OmniDocs package.</p>"},{"location":"reference/#package-structure","title":"Package Structure","text":"<pre><code>omnidocs/\n\u251c\u2500\u2500 document.py          # Core document handling\n\u251c\u2500\u2500 tasks/               # Document processing tasks\n\u2502   \u251c\u2500\u2500 layout_analysis/ # Detect document structure\n\u2502   \u251c\u2500\u2500 text_extraction/ # Extract text (Markdown/HTML)\n\u2502   \u2514\u2500\u2500 ocr_extraction/  # Extract text with bboxes\n\u251c\u2500\u2500 inference/           # Backend implementations\n\u2502   \u251c\u2500\u2500 pytorch.py       # PyTorch/HuggingFace\n\u2502   \u251c\u2500\u2500 vllm.py          # High-throughput VLLM\n\u2502   \u251c\u2500\u2500 mlx.py           # Apple Silicon\n\u2502   \u2514\u2500\u2500 api.py           # API-based inference\n\u2514\u2500\u2500 utils/               # Utility functions\n</code></pre>"},{"location":"reference/#quick-start","title":"Quick Start","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\n# Load document\ndoc = Document.from_pdf(\"document.pdf\")\n\n# Initialize extractor\nextractor = DotsOCRTextExtractor(\n    backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n)\n\n# Extract text\nresult = extractor.extract(doc.get_page(0), output_format=\"markdown\")\nprint(result.content)\n</code></pre> <p>Browse the sections in the sidebar to explore the full API.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>Batch</li> <li>Document</li> <li>Tasks<ul> <li>Overview</li> <li>Layout Extraction<ul> <li>Overview</li> <li>Base</li> <li>Doc Layout YOLO</li> <li>Models</li> <li>Qwen<ul> <li>Overview</li> <li>API</li> <li>Detector</li> <li>MLX</li> <li>PyTorch</li> <li>VLLM</li> </ul> </li> <li>Rtdetr</li> </ul> </li> <li>OCR Extraction<ul> <li>Overview</li> <li>Base</li> <li>EasyOCR</li> <li>Models</li> <li>PaddleOCR</li> <li>Tesseract</li> </ul> </li> <li>Reading Order<ul> <li>Overview</li> <li>Base</li> <li>Models</li> <li>Rule Based<ul> <li>Overview</li> <li>Predictor</li> </ul> </li> </ul> </li> <li>Table Extraction<ul> <li>Overview</li> <li>Base</li> <li>Models</li> <li>Tableformer<ul> <li>Overview</li> <li>Config</li> <li>PyTorch</li> </ul> </li> </ul> </li> <li>Text Extraction<ul> <li>Overview</li> <li>Base</li> <li>Dots OCR<ul> <li>Overview</li> <li>API</li> <li>Extractor</li> <li>PyTorch</li> <li>VLLM</li> </ul> </li> <li>Granitedocling<ul> <li>Overview</li> <li>API</li> <li>Extractor</li> <li>MLX</li> <li>PyTorch</li> <li>VLLM</li> </ul> </li> <li>Models</li> <li>Nanonets<ul> <li>Overview</li> <li>Extractor</li> <li>MLX</li> <li>PyTorch</li> <li>VLLM</li> </ul> </li> <li>Qwen<ul> <li>Overview</li> <li>API</li> <li>Extractor</li> <li>MLX</li> <li>PyTorch</li> <li>VLLM</li> </ul> </li> </ul> </li> </ul> </li> <li>Utils<ul> <li>Overview</li> <li>Aggregation</li> </ul> </li> </ul>"},{"location":"reference/batch/","title":"Batch","text":"<p>OmniDocs Batch Processing Utilities.</p> <p>Provides utilities for processing multiple documents efficiently: - DocumentBatch: Load and iterate over multiple PDFs - process_directory: Convenience function for batch processing - process_document: Process all pages of a single document</p>"},{"location":"reference/batch/#omnidocs.batch.DocumentBatch","title":"DocumentBatch","text":"<pre><code>DocumentBatch(\n    paths: List[Path],\n    dpi: int = 150,\n    page_range: Optional[tuple] = None,\n)\n</code></pre> <p>Batch document loader for processing multiple PDFs.</p> <p>Features: - Lazy loading (documents loaded on iteration) - Memory efficient (processes one document at a time) - Glob pattern support - Progress callbacks</p> <p>Examples:</p> <pre><code># Load from directory\nbatch = DocumentBatch.from_directory(\"pdfs/\")\n\n# Load from list\nbatch = DocumentBatch.from_paths([\"doc1.pdf\", \"doc2.pdf\"])\n\n# Iterate\nfor doc in batch:\n    for page in doc.iter_pages():\n        result = extractor.extract(page)\n</code></pre> <p>Initialize DocumentBatch.</p> PARAMETER DESCRIPTION <code>paths</code> <p>List of PDF file paths</p> <p> TYPE: <code>List[Path]</code> </p> <code>dpi</code> <p>Resolution for page rendering (default: 150)</p> <p> TYPE: <code>int</code> DEFAULT: <code>150</code> </p> <code>page_range</code> <p>Optional (start, end) tuple for page range (applied to all docs)</p> <p> TYPE: <code>Optional[tuple]</code> DEFAULT: <code>None</code> </p> Source code in <code>omnidocs/batch.py</code> <pre><code>def __init__(\n    self,\n    paths: List[Path],\n    dpi: int = 150,\n    page_range: Optional[tuple] = None,\n):\n    \"\"\"\n    Initialize DocumentBatch.\n\n    Args:\n        paths: List of PDF file paths\n        dpi: Resolution for page rendering (default: 150)\n        page_range: Optional (start, end) tuple for page range (applied to all docs)\n    \"\"\"\n    self._paths = paths\n    self._dpi = dpi\n    self._page_range = page_range\n</code></pre>"},{"location":"reference/batch/#omnidocs.batch.DocumentBatch.count","title":"count  <code>property</code>","text":"<pre><code>count: int\n</code></pre> <p>Number of documents in batch.</p>"},{"location":"reference/batch/#omnidocs.batch.DocumentBatch.paths","title":"paths  <code>property</code>","text":"<pre><code>paths: List[Path]\n</code></pre> <p>List of document paths.</p>"},{"location":"reference/batch/#omnidocs.batch.DocumentBatch.from_directory","title":"from_directory  <code>classmethod</code>","text":"<pre><code>from_directory(\n    directory: str,\n    pattern: str = \"*.pdf\",\n    recursive: bool = False,\n    dpi: int = 150,\n    page_range: Optional[tuple] = None,\n) -&gt; DocumentBatch\n</code></pre> <p>Load all PDFs from directory.</p> PARAMETER DESCRIPTION <code>directory</code> <p>Path to directory</p> <p> TYPE: <code>str</code> </p> <code>pattern</code> <p>Glob pattern (default: \"*.pdf\")</p> <p> TYPE: <code>str</code> DEFAULT: <code>'*.pdf'</code> </p> <code>recursive</code> <p>Search subdirectories</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dpi</code> <p>Resolution for rendering</p> <p> TYPE: <code>int</code> DEFAULT: <code>150</code> </p> <code>page_range</code> <p>Optional page range for all documents</p> <p> TYPE: <code>Optional[tuple]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DocumentBatch</code> <p>DocumentBatch instance</p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If directory doesn't exist</p> <p>Examples:</p> <pre><code>batch = DocumentBatch.from_directory(\"pdfs/\")\nbatch = DocumentBatch.from_directory(\"docs/\", pattern=\"*.pdf\", recursive=True)\n</code></pre> Source code in <code>omnidocs/batch.py</code> <pre><code>@classmethod\ndef from_directory(\n    cls,\n    directory: str,\n    pattern: str = \"*.pdf\",\n    recursive: bool = False,\n    dpi: int = 150,\n    page_range: Optional[tuple] = None,\n) -&gt; \"DocumentBatch\":\n    \"\"\"\n    Load all PDFs from directory.\n\n    Args:\n        directory: Path to directory\n        pattern: Glob pattern (default: \"*.pdf\")\n        recursive: Search subdirectories\n        dpi: Resolution for rendering\n        page_range: Optional page range for all documents\n\n    Returns:\n        DocumentBatch instance\n\n    Raises:\n        FileNotFoundError: If directory doesn't exist\n\n    Examples:\n        ```python\n        batch = DocumentBatch.from_directory(\"pdfs/\")\n        batch = DocumentBatch.from_directory(\"docs/\", pattern=\"*.pdf\", recursive=True)\n        ```\n    \"\"\"\n    dir_path = Path(directory)\n    if not dir_path.exists():\n        raise FileNotFoundError(f\"Directory not found: {directory}\")\n\n    if recursive:\n        paths = list(dir_path.rglob(pattern))\n    else:\n        paths = list(dir_path.glob(pattern))\n\n    paths = sorted(paths)  # Consistent ordering\n\n    return cls(paths=paths, dpi=dpi, page_range=page_range)\n</code></pre>"},{"location":"reference/batch/#omnidocs.batch.DocumentBatch.from_paths","title":"from_paths  <code>classmethod</code>","text":"<pre><code>from_paths(\n    paths: List[str],\n    dpi: int = 150,\n    page_range: Optional[tuple] = None,\n) -&gt; DocumentBatch\n</code></pre> <p>Load documents from explicit list of paths.</p> PARAMETER DESCRIPTION <code>paths</code> <p>List of PDF paths</p> <p> TYPE: <code>List[str]</code> </p> <code>dpi</code> <p>Resolution for rendering</p> <p> TYPE: <code>int</code> DEFAULT: <code>150</code> </p> <code>page_range</code> <p>Optional page range for all documents</p> <p> TYPE: <code>Optional[tuple]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>DocumentBatch</code> <p>DocumentBatch instance</p> <p>Examples:</p> <pre><code>batch = DocumentBatch.from_paths([\"doc1.pdf\", \"doc2.pdf\"])\n</code></pre> Source code in <code>omnidocs/batch.py</code> <pre><code>@classmethod\ndef from_paths(\n    cls,\n    paths: List[str],\n    dpi: int = 150,\n    page_range: Optional[tuple] = None,\n) -&gt; \"DocumentBatch\":\n    \"\"\"\n    Load documents from explicit list of paths.\n\n    Args:\n        paths: List of PDF paths\n        dpi: Resolution for rendering\n        page_range: Optional page range for all documents\n\n    Returns:\n        DocumentBatch instance\n\n    Examples:\n        ```python\n        batch = DocumentBatch.from_paths([\"doc1.pdf\", \"doc2.pdf\"])\n        ```\n    \"\"\"\n    return cls(\n        paths=[Path(p) for p in paths],\n        dpi=dpi,\n        page_range=page_range,\n    )\n</code></pre>"},{"location":"reference/batch/#omnidocs.batch.DocumentBatch.iter_with_progress","title":"iter_with_progress","text":"<pre><code>iter_with_progress(\n    callback: Callable[[int, int, str], None],\n) -&gt; Iterator[Document]\n</code></pre> <p>Iterate with progress callback.</p> PARAMETER DESCRIPTION <code>callback</code> <p>Function(current, total, filename) called for each document</p> <p> TYPE: <code>Callable[[int, int, str], None]</code> </p> YIELDS DESCRIPTION <code>Document</code> <p>Document instances</p> <p>Examples:</p> <pre><code>def progress(current, total, filename):\n    print(f\"[{current}/{total}] {filename}\")\n\nfor doc in batch.iter_with_progress(progress):\n    # Process document...\n</code></pre> Source code in <code>omnidocs/batch.py</code> <pre><code>def iter_with_progress(\n    self,\n    callback: Callable[[int, int, str], None],\n) -&gt; Iterator[Document]:\n    \"\"\"\n    Iterate with progress callback.\n\n    Args:\n        callback: Function(current, total, filename) called for each document\n\n    Yields:\n        Document instances\n\n    Examples:\n        ```python\n        def progress(current, total, filename):\n            print(f\"[{current}/{total}] {filename}\")\n\n        for doc in batch.iter_with_progress(progress):\n            # Process document...\n        ```\n    \"\"\"\n    total = len(self._paths)\n    for i, path in enumerate(self._paths):\n        callback(i + 1, total, path.name)\n        doc = Document.from_pdf(\n            str(path),\n            page_range=self._page_range,\n            dpi=self._dpi,\n        )\n        yield doc\n</code></pre>"},{"location":"reference/batch/#omnidocs.batch.DocumentBatch.iter_all_pages","title":"iter_all_pages","text":"<pre><code>iter_all_pages() -&gt; Iterator[tuple]\n</code></pre> <p>Iterate over all pages from all documents.</p> <p>Memory efficient - loads one document at a time.</p> YIELDS DESCRIPTION <code>tuple</code> <p>Tuples of (doc_index, page_index, page_image, doc_path)</p> <p>Examples:</p> <pre><code>for doc_idx, page_idx, page_img, doc_path in batch.iter_all_pages():\n    result = extractor.extract(page_img)\n</code></pre> Source code in <code>omnidocs/batch.py</code> <pre><code>def iter_all_pages(self) -&gt; Iterator[tuple]:\n    \"\"\"\n    Iterate over all pages from all documents.\n\n    Memory efficient - loads one document at a time.\n\n    Yields:\n        Tuples of (doc_index, page_index, page_image, doc_path)\n\n    Examples:\n        ```python\n        for doc_idx, page_idx, page_img, doc_path in batch.iter_all_pages():\n            result = extractor.extract(page_img)\n        ```\n    \"\"\"\n    for doc_idx, path in enumerate(self._paths):\n        doc = Document.from_pdf(\n            str(path),\n            page_range=self._page_range,\n            dpi=self._dpi,\n        )\n        for page_idx in range(doc.page_count):\n            yield (doc_idx, page_idx, doc.get_page(page_idx), path)\n        doc.close()\n</code></pre>"},{"location":"reference/batch/#omnidocs.batch.process_document","title":"process_document","text":"<pre><code>process_document(\n    document: Document,\n    extractor: Any,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n    **extract_kwargs,\n) -&gt; DocumentResult\n</code></pre> <p>Process all pages of a single document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>extractor</code> <p>Initialized extractor (any type)</p> <p> TYPE: <code>Any</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> <code>**extract_kwargs</code> <p>Passed to extractor.extract()</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>DocumentResult</code> <p>DocumentResult with page results</p> <p>Examples:</p> <pre><code>from omnidocs import Document\nfrom omnidocs.batch import process_document\n\ndoc = Document.from_pdf(\"paper.pdf\")\nresult = process_document(doc, extractor, output_format=\"markdown\")\nresult.save_json(\"output.json\")\n</code></pre> Source code in <code>omnidocs/batch.py</code> <pre><code>def process_document(\n    document: Document,\n    extractor: Any,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n    **extract_kwargs,\n) -&gt; \"DocumentResult\":\n    \"\"\"\n    Process all pages of a single document.\n\n    Args:\n        document: Document instance\n        extractor: Initialized extractor (any type)\n        progress_callback: Optional function(current, total) for progress\n        **extract_kwargs: Passed to extractor.extract()\n\n    Returns:\n        DocumentResult with page results\n\n    Examples:\n        ```python\n        from omnidocs import Document\n        from omnidocs.batch import process_document\n\n        doc = Document.from_pdf(\"paper.pdf\")\n        result = process_document(doc, extractor, output_format=\"markdown\")\n        result.save_json(\"output.json\")\n        ```\n    \"\"\"\n    from .utils.aggregation import DocumentResult\n\n    doc_result = DocumentResult(\n        source_path=document.metadata.source_path,\n        page_count=document.page_count,\n    )\n\n    for page_idx in range(document.page_count):\n        if progress_callback:\n            progress_callback(page_idx + 1, document.page_count)\n\n        page = document.get_page(page_idx)\n        result = extractor.extract(page, **extract_kwargs)\n        doc_result.add_page_result(page_idx, result)\n\n    return doc_result\n</code></pre>"},{"location":"reference/batch/#omnidocs.batch.process_directory","title":"process_directory","text":"<pre><code>process_directory(\n    directory: str,\n    extractor: Any,\n    output_dir: Optional[str] = None,\n    pattern: str = \"*.pdf\",\n    recursive: bool = False,\n    dpi: int = 150,\n    progress_callback: Optional[\n        Callable[[str, int, int], None]\n    ] = None,\n    **extract_kwargs,\n) -&gt; BatchResult\n</code></pre> <p>Process all PDFs in a directory.</p> <p>Convenience function for common batch processing pattern.</p> PARAMETER DESCRIPTION <code>directory</code> <p>Path to directory with PDFs</p> <p> TYPE: <code>str</code> </p> <code>extractor</code> <p>Initialized extractor instance</p> <p> TYPE: <code>Any</code> </p> <code>output_dir</code> <p>Optional directory to save results as JSON</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>pattern</code> <p>Glob pattern for files (default: \"*.pdf\")</p> <p> TYPE: <code>str</code> DEFAULT: <code>'*.pdf'</code> </p> <code>recursive</code> <p>Search subdirectories</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>dpi</code> <p>Resolution for page rendering</p> <p> TYPE: <code>int</code> DEFAULT: <code>150</code> </p> <code>progress_callback</code> <p>Function(filename, current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[str, int, int], None]]</code> DEFAULT: <code>None</code> </p> <code>**extract_kwargs</code> <p>Passed to extractor.extract()</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>BatchResult</code> <p>BatchResult with all document results</p> <p>Examples:</p> <pre><code>from omnidocs.batch import process_directory\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nextractor = QwenTextExtractor(\n    backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen2-VL-7B\")\n)\n\nresults = process_directory(\n    \"pdfs/\",\n    extractor,\n    output_dir=\"results/\",\n    output_format=\"markdown\",\n)\n</code></pre> Source code in <code>omnidocs/batch.py</code> <pre><code>def process_directory(\n    directory: str,\n    extractor: Any,\n    output_dir: Optional[str] = None,\n    pattern: str = \"*.pdf\",\n    recursive: bool = False,\n    dpi: int = 150,\n    progress_callback: Optional[Callable[[str, int, int], None]] = None,\n    **extract_kwargs,\n) -&gt; \"BatchResult\":\n    \"\"\"\n    Process all PDFs in a directory.\n\n    Convenience function for common batch processing pattern.\n\n    Args:\n        directory: Path to directory with PDFs\n        extractor: Initialized extractor instance\n        output_dir: Optional directory to save results as JSON\n        pattern: Glob pattern for files (default: \"*.pdf\")\n        recursive: Search subdirectories\n        dpi: Resolution for page rendering\n        progress_callback: Function(filename, current, total) for progress\n        **extract_kwargs: Passed to extractor.extract()\n\n    Returns:\n        BatchResult with all document results\n\n    Examples:\n        ```python\n        from omnidocs.batch import process_directory\n        from omnidocs.tasks.text_extraction import QwenTextExtractor\n        from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n        extractor = QwenTextExtractor(\n            backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen2-VL-7B\")\n        )\n\n        results = process_directory(\n            \"pdfs/\",\n            extractor,\n            output_dir=\"results/\",\n            output_format=\"markdown\",\n        )\n        ```\n    \"\"\"\n    from .utils.aggregation import BatchResult, DocumentResult\n\n    batch = DocumentBatch.from_directory(\n        directory,\n        pattern=pattern,\n        recursive=recursive,\n        dpi=dpi,\n    )\n\n    batch_result = BatchResult()\n\n    for i, (doc, path) in enumerate(zip(batch, batch.paths)):\n        if progress_callback:\n            progress_callback(path.name, i + 1, batch.count)\n\n        doc_result = DocumentResult(\n            source_path=str(path),\n            page_count=doc.page_count,\n        )\n\n        for page_idx in range(doc.page_count):\n            page = doc.get_page(page_idx)\n            result = extractor.extract(page, **extract_kwargs)\n            doc_result.add_page_result(page_idx, result)\n\n        batch_result.add_document_result(path.stem, doc_result)\n\n        # Save individual result if output_dir specified\n        if output_dir:\n            out_path = Path(output_dir) / f\"{path.stem}.json\"\n            out_path.parent.mkdir(parents=True, exist_ok=True)\n            doc_result.save_json(str(out_path))\n\n        doc.close()\n\n    return batch_result\n</code></pre>"},{"location":"reference/document/","title":"Document","text":"<p>OmniDocs Document Loader</p> <p>Stateless document container for loading and accessing PDF/image data. Uses pypdfium2 (Apache 2.0) for PDF rendering and pdfplumber (MIT) for text extraction.</p>"},{"location":"reference/document/#omnidocs.document.DocumentLoadError","title":"DocumentLoadError","text":"<p>               Bases: <code>Exception</code></p> <p>Failed to load document.</p>"},{"location":"reference/document/#omnidocs.document.URLDownloadError","title":"URLDownloadError","text":"<p>               Bases: <code>Exception</code></p> <p>Failed to download from URL.</p>"},{"location":"reference/document/#omnidocs.document.PageRangeError","title":"PageRangeError","text":"<p>               Bases: <code>Exception</code></p> <p>Invalid page range.</p>"},{"location":"reference/document/#omnidocs.document.UnsupportedFormatError","title":"UnsupportedFormatError","text":"<p>               Bases: <code>Exception</code></p> <p>Unsupported file format.</p>"},{"location":"reference/document/#omnidocs.document.DocumentMetadata","title":"DocumentMetadata","text":"<p>               Bases: <code>BaseModel</code></p> <p>Metadata container for documents.</p>"},{"location":"reference/document/#omnidocs.document.LazyPage","title":"LazyPage","text":"<pre><code>LazyPage(\n    pdf_doc: PdfDocument, page_index: int, dpi: int = 150\n)\n</code></pre> <p>Lazy page wrapper - renders only when accessed.</p> <p>This avoids loading all pages into memory upfront for large PDFs.</p> Source code in <code>omnidocs/document.py</code> <pre><code>def __init__(self, pdf_doc: pdfium.PdfDocument, page_index: int, dpi: int = 150):\n    self._pdf_doc = pdf_doc\n    self._page_index = page_index\n    self._dpi = dpi\n    self._cached_image: Optional[Image.Image] = None\n    self._cached_text: Optional[str] = None\n</code></pre>"},{"location":"reference/document/#omnidocs.document.LazyPage.image","title":"image  <code>property</code>","text":"<pre><code>image: Image\n</code></pre> <p>Render page to PIL Image (cached after first access).</p>"},{"location":"reference/document/#omnidocs.document.LazyPage.text","title":"text  <code>property</code>","text":"<pre><code>text: str\n</code></pre> <p>Extract text from page using pypdfium2 (cached).</p>"},{"location":"reference/document/#omnidocs.document.LazyPage.size","title":"size  <code>property</code>","text":"<pre><code>size: tuple\n</code></pre> <p>Get page dimensions without full render (fast).</p>"},{"location":"reference/document/#omnidocs.document.LazyPage.clear_cache","title":"clear_cache","text":"<pre><code>clear_cache()\n</code></pre> <p>Clear cached image to free memory.</p> Source code in <code>omnidocs/document.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear cached image to free memory.\"\"\"\n    self._cached_image = None\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document","title":"Document","text":"<pre><code>Document(\n    pdf_doc: Optional[PdfDocument],\n    pdf_bytes: Optional[bytes],\n    metadata: DocumentMetadata,\n    dpi: int = 150,\n    page_range: Optional[tuple] = None,\n    preloaded_images: Optional[List[Image]] = None,\n)\n</code></pre> <p>Stateless document container for OmniDocs.</p> <p>Features: - Lazy page rendering (pages only rendered when accessed) - Page caching (rendered pages cached to avoid re-rendering) - Multiple source support (PDF file, URL, bytes, images) - Text extraction with pypdfium2 first, pdfplumber fallback - Memory efficient for large documents</p> <p>Design: - Document is SOURCE DATA only - does NOT store task results - Users manage their own analysis results and caching strategy</p> <p>Examples:</p> <pre><code># Load from file\ndoc = Document.from_pdf(\"paper.pdf\")\n\n# Access pages\npage = doc.get_page(0)  # 0-indexed\ntext = doc.get_page_text(1)  # 1-indexed for compatibility\n\n# Iterate efficiently\nfor page in doc.iter_pages():\n        result = layout.extract(page)\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>def __init__(\n    self,\n    pdf_doc: Optional[pdfium.PdfDocument],\n    pdf_bytes: Optional[bytes],\n    metadata: DocumentMetadata,\n    dpi: int = 150,\n    page_range: Optional[tuple] = None,\n    preloaded_images: Optional[List[Image.Image]] = None,\n):\n    self._pdf_doc = pdf_doc\n    self._pdf_bytes = pdf_bytes\n    self._metadata = metadata\n    self._dpi = dpi\n    self._page_range = page_range\n\n    # For image-based documents (no PDF)\n    self._preloaded_images = preloaded_images\n\n    # Lazy page wrappers\n    self._lazy_pages: Optional[List[LazyPage]] = None\n    if pdf_doc is not None:\n        start = page_range[0] if page_range else 0\n        end = page_range[1] if page_range else len(pdf_doc) - 1\n        self._lazy_pages = [LazyPage(pdf_doc, i, dpi) for i in range(start, end + 1)]\n\n    # Full text cache\n    self._full_text_cache: Optional[str] = None\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.page_count","title":"page_count  <code>property</code>","text":"<pre><code>page_count: int\n</code></pre> <p>Number of pages in document.</p>"},{"location":"reference/document/#omnidocs.document.Document.metadata","title":"metadata  <code>property</code>","text":"<pre><code>metadata: DocumentMetadata\n</code></pre> <p>Document metadata.</p>"},{"location":"reference/document/#omnidocs.document.Document.pages","title":"pages  <code>property</code>","text":"<pre><code>pages: List[Image]\n</code></pre> <p>List of all page images.</p> <p>Note: This renders ALL pages. For large documents, use get_page() or iter_pages() instead.</p> RETURNS DESCRIPTION <code>List[Image]</code> <p>List of PIL Images</p>"},{"location":"reference/document/#omnidocs.document.Document.text","title":"text  <code>property</code>","text":"<pre><code>text: str\n</code></pre> <p>Full document text (lazy, cached).</p> <p>Uses pypdfium2 first (fast), falls back to pdfplumber if needed.</p> RETURNS DESCRIPTION <code>str</code> <p>Full document text</p>"},{"location":"reference/document/#omnidocs.document.Document.from_pdf","title":"from_pdf  <code>classmethod</code>","text":"<pre><code>from_pdf(\n    path: str,\n    page_range: Optional[tuple] = None,\n    dpi: int = 150,\n) -&gt; Document\n</code></pre> <p>Load document from PDF file (lazy - pages not rendered yet).</p> PARAMETER DESCRIPTION <code>path</code> <p>Path to PDF file</p> <p> TYPE: <code>str</code> </p> <code>page_range</code> <p>Optional (start, end) tuple for page range (0-indexed, inclusive)</p> <p> TYPE: <code>Optional[tuple]</code> DEFAULT: <code>None</code> </p> <code>dpi</code> <p>Resolution for page rendering (default: 150)</p> <p> TYPE: <code>int</code> DEFAULT: <code>150</code> </p> RETURNS DESCRIPTION <code>Document</code> <p>Document instance</p> RAISES DESCRIPTION <code>DocumentLoadError</code> <p>If file not found</p> <code>UnsupportedFormatError</code> <p>If not a PDF file</p> <code>PageRangeError</code> <p>If page range is invalid</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\ndoc = Document.from_pdf(\"paper.pdf\", page_range=(0, 4))\ndoc = Document.from_pdf(\"paper.pdf\", dpi=300)\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>@classmethod\ndef from_pdf(\n    cls,\n    path: str,\n    page_range: Optional[tuple] = None,\n    dpi: int = 150,\n) -&gt; \"Document\":\n    \"\"\"\n    Load document from PDF file (lazy - pages not rendered yet).\n\n    Args:\n        path: Path to PDF file\n        page_range: Optional (start, end) tuple for page range (0-indexed, inclusive)\n        dpi: Resolution for page rendering (default: 150)\n\n    Returns:\n        Document instance\n\n    Raises:\n        DocumentLoadError: If file not found\n        UnsupportedFormatError: If not a PDF file\n        PageRangeError: If page range is invalid\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        doc = Document.from_pdf(\"paper.pdf\", page_range=(0, 4))\n        doc = Document.from_pdf(\"paper.pdf\", dpi=300)\n        ```\n    \"\"\"\n    path = Path(path)\n\n    if not path.exists():\n        raise DocumentLoadError(f\"File not found: {path}\")\n\n    if path.suffix.lower() != \".pdf\":\n        raise UnsupportedFormatError(f\"Expected PDF file, got: {path.suffix}\")\n\n    # Read file bytes\n    pdf_bytes = path.read_bytes()\n    pdf_doc = pdfium.PdfDocument(pdf_bytes)\n    total_pages = len(pdf_doc)\n\n    # Validate page range\n    if page_range:\n        start, end = page_range\n        if start &lt; 0 or end &gt;= total_pages or start &gt; end:\n            raise PageRangeError(f\"Invalid page range ({start}, {end}) for {total_pages} pages\")\n\n    # Extract metadata (fast, no rendering)\n    pdf_meta = {}\n    try:\n        meta = pdf_doc.get_metadata_dict()\n        if meta:\n            pdf_meta = {k: v for k, v in meta.items() if v}\n    except Exception:\n        pass\n\n    actual_pages = (page_range[1] - page_range[0] + 1) if page_range else total_pages\n\n    metadata = DocumentMetadata(\n        source_type=\"file\",\n        source_path=str(path.absolute()),\n        file_name=path.name,\n        file_size=len(pdf_bytes),\n        pdf_metadata=pdf_meta or None,\n        page_count=actual_pages,\n        format=\"pdf\",\n        image_dpi=dpi,\n    )\n\n    return cls(\n        pdf_doc=pdf_doc,\n        pdf_bytes=pdf_bytes,\n        metadata=metadata,\n        dpi=dpi,\n        page_range=page_range,\n    )\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.from_url","title":"from_url  <code>classmethod</code>","text":"<pre><code>from_url(\n    url: str,\n    page_range: Optional[tuple] = None,\n    dpi: int = 150,\n    timeout: int = 30,\n) -&gt; Document\n</code></pre> <p>Download and load document from URL (lazy).</p> PARAMETER DESCRIPTION <code>url</code> <p>URL to PDF file</p> <p> TYPE: <code>str</code> </p> <code>page_range</code> <p>Optional (start, end) tuple for page range</p> <p> TYPE: <code>Optional[tuple]</code> DEFAULT: <code>None</code> </p> <code>dpi</code> <p>Resolution for page rendering</p> <p> TYPE: <code>int</code> DEFAULT: <code>150</code> </p> <code>timeout</code> <p>Download timeout in seconds</p> <p> TYPE: <code>int</code> DEFAULT: <code>30</code> </p> RETURNS DESCRIPTION <code>Document</code> <p>Document instance</p> RAISES DESCRIPTION <code>URLDownloadError</code> <p>If download fails</p> <code>PageRangeError</code> <p>If page range is invalid</p> <p>Examples:</p> <pre><code>doc = Document.from_url(\"https://example.com/doc.pdf\")\ndoc = Document.from_url(\"https://example.com/doc.pdf\", timeout=60)\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>@classmethod\ndef from_url(\n    cls,\n    url: str,\n    page_range: Optional[tuple] = None,\n    dpi: int = 150,\n    timeout: int = 30,\n) -&gt; \"Document\":\n    \"\"\"\n    Download and load document from URL (lazy).\n\n    Args:\n        url: URL to PDF file\n        page_range: Optional (start, end) tuple for page range\n        dpi: Resolution for page rendering\n        timeout: Download timeout in seconds\n\n    Returns:\n        Document instance\n\n    Raises:\n        URLDownloadError: If download fails\n        PageRangeError: If page range is invalid\n\n    Examples:\n        ```python\n        doc = Document.from_url(\"https://example.com/doc.pdf\")\n        doc = Document.from_url(\"https://example.com/doc.pdf\", timeout=60)\n        ```\n    \"\"\"\n    try:\n        import requests\n    except ImportError:\n        raise ImportError(\"requests is required for URL downloads. Install with: pip install requests\")\n\n    try:\n        response = requests.get(url, timeout=timeout)\n        response.raise_for_status()\n    except requests.RequestException as e:\n        raise URLDownloadError(f\"Failed to download: {e}\")\n\n    pdf_bytes = response.content\n    pdf_doc = pdfium.PdfDocument(pdf_bytes)\n    total_pages = len(pdf_doc)\n\n    if page_range:\n        start, end = page_range\n        if start &lt; 0 or end &gt;= total_pages or start &gt; end:\n            raise PageRangeError(\"Invalid page range\")\n\n    pdf_meta = {}\n    try:\n        meta = pdf_doc.get_metadata_dict()\n        if meta:\n            pdf_meta = {k: v for k, v in meta.items() if v}\n    except Exception:\n        pass\n\n    file_name = url.split(\"/\")[-1].split(\"?\")[0]\n    if not file_name.endswith(\".pdf\"):\n        file_name = \"downloaded.pdf\"\n\n    actual_pages = (page_range[1] - page_range[0] + 1) if page_range else total_pages\n\n    metadata = DocumentMetadata(\n        source_type=\"url\",\n        source_path=url,\n        file_name=file_name,\n        file_size=len(pdf_bytes),\n        pdf_metadata=pdf_meta or None,\n        page_count=actual_pages,\n        format=\"pdf\",\n        image_dpi=dpi,\n    )\n\n    return cls(\n        pdf_doc=pdf_doc,\n        pdf_bytes=pdf_bytes,\n        metadata=metadata,\n        dpi=dpi,\n        page_range=page_range,\n    )\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.from_bytes","title":"from_bytes  <code>classmethod</code>","text":"<pre><code>from_bytes(\n    data: bytes,\n    filename: Optional[str] = None,\n    page_range: Optional[tuple] = None,\n    dpi: int = 150,\n) -&gt; Document\n</code></pre> <p>Load document from PDF bytes (lazy).</p> PARAMETER DESCRIPTION <code>data</code> <p>PDF file bytes</p> <p> TYPE: <code>bytes</code> </p> <code>filename</code> <p>Optional filename for metadata</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>page_range</code> <p>Optional (start, end) tuple for page range</p> <p> TYPE: <code>Optional[tuple]</code> DEFAULT: <code>None</code> </p> <code>dpi</code> <p>Resolution for page rendering</p> <p> TYPE: <code>int</code> DEFAULT: <code>150</code> </p> RETURNS DESCRIPTION <code>Document</code> <p>Document instance</p> RAISES DESCRIPTION <code>PageRangeError</code> <p>If page range is invalid</p> <p>Examples:</p> <pre><code>with open(\"doc.pdf\", \"rb\") as f:\n        doc = Document.from_bytes(f.read())\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>@classmethod\ndef from_bytes(\n    cls,\n    data: bytes,\n    filename: Optional[str] = None,\n    page_range: Optional[tuple] = None,\n    dpi: int = 150,\n) -&gt; \"Document\":\n    \"\"\"\n    Load document from PDF bytes (lazy).\n\n    Args:\n        data: PDF file bytes\n        filename: Optional filename for metadata\n        page_range: Optional (start, end) tuple for page range\n        dpi: Resolution for page rendering\n\n    Returns:\n        Document instance\n\n    Raises:\n        PageRangeError: If page range is invalid\n\n    Examples:\n        ```python\n        with open(\"doc.pdf\", \"rb\") as f:\n                doc = Document.from_bytes(f.read())\n        ```\n    \"\"\"\n    pdf_doc = pdfium.PdfDocument(data)\n    total_pages = len(pdf_doc)\n\n    if page_range:\n        start, end = page_range\n        if start &lt; 0 or end &gt;= total_pages or start &gt; end:\n            raise PageRangeError(\"Invalid page range\")\n\n    pdf_meta = {}\n    try:\n        meta = pdf_doc.get_metadata_dict()\n        if meta:\n            pdf_meta = {k: v for k, v in meta.items() if v}\n    except Exception:\n        pass\n\n    actual_pages = (page_range[1] - page_range[0] + 1) if page_range else total_pages\n\n    metadata = DocumentMetadata(\n        source_type=\"bytes\",\n        file_name=filename or \"document.pdf\",\n        file_size=len(data),\n        pdf_metadata=pdf_meta or None,\n        page_count=actual_pages,\n        format=\"pdf\",\n        image_dpi=dpi,\n    )\n\n    return cls(\n        pdf_doc=pdf_doc,\n        pdf_bytes=data,\n        metadata=metadata,\n        dpi=dpi,\n        page_range=page_range,\n    )\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.from_image","title":"from_image  <code>classmethod</code>","text":"<pre><code>from_image(path: str) -&gt; Document\n</code></pre> <p>Load document from single image file.</p> PARAMETER DESCRIPTION <code>path</code> <p>Path to image file</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Document</code> <p>Document instance</p> RAISES DESCRIPTION <code>DocumentLoadError</code> <p>If file not found</p> <p>Examples:</p> <pre><code>doc = Document.from_image(\"page.png\")\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>@classmethod\ndef from_image(cls, path: str) -&gt; \"Document\":\n    \"\"\"\n    Load document from single image file.\n\n    Args:\n        path: Path to image file\n\n    Returns:\n        Document instance\n\n    Raises:\n        DocumentLoadError: If file not found\n\n    Examples:\n        ```python\n        doc = Document.from_image(\"page.png\")\n        ```\n    \"\"\"\n    path = Path(path)\n    if not path.exists():\n        raise DocumentLoadError(f\"File not found: {path}\")\n\n    img = Image.open(path).convert(\"RGB\")\n\n    metadata = DocumentMetadata(\n        source_type=\"image\",\n        source_path=str(path.absolute()),\n        file_name=path.name,\n        file_size=path.stat().st_size,\n        page_count=1,\n        format=path.suffix.lower().replace(\".\", \"\"),\n    )\n\n    return cls(\n        pdf_doc=None,\n        pdf_bytes=None,\n        metadata=metadata,\n        preloaded_images=[img],\n    )\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.from_images","title":"from_images  <code>classmethod</code>","text":"<pre><code>from_images(paths: List[str]) -&gt; Document\n</code></pre> <p>Load document from multiple images (multi-page).</p> PARAMETER DESCRIPTION <code>paths</code> <p>List of paths to image files</p> <p> TYPE: <code>List[str]</code> </p> RETURNS DESCRIPTION <code>Document</code> <p>Document instance</p> RAISES DESCRIPTION <code>DocumentLoadError</code> <p>If any file not found</p> <p>Examples:</p> <pre><code>doc = Document.from_images([\"page1.png\", \"page2.png\"])\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>@classmethod\ndef from_images(cls, paths: List[str]) -&gt; \"Document\":\n    \"\"\"\n    Load document from multiple images (multi-page).\n\n    Args:\n        paths: List of paths to image files\n\n    Returns:\n        Document instance\n\n    Raises:\n        DocumentLoadError: If any file not found\n\n    Examples:\n        ```python\n        doc = Document.from_images([\"page1.png\", \"page2.png\"])\n        ```\n    \"\"\"\n    images = []\n    total_size = 0\n\n    for p in paths:\n        path = Path(p)\n        if not path.exists():\n            raise DocumentLoadError(f\"File not found: {path}\")\n        images.append(Image.open(path).convert(\"RGB\"))\n        total_size += path.stat().st_size\n\n    metadata = DocumentMetadata(\n        source_type=\"image\",\n        file_name=f\"{len(paths)}_images\",\n        file_size=total_size,\n        page_count=len(images),\n        format=\"images\",\n    )\n\n    return cls(\n        pdf_doc=None,\n        pdf_bytes=None,\n        metadata=metadata,\n        preloaded_images=images,\n    )\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.get_page","title":"get_page","text":"<pre><code>get_page(page_num: int) -&gt; Image.Image\n</code></pre> <p>Get single page image (0-indexed).</p> <p>More memory efficient than accessing .pages for large documents.</p> PARAMETER DESCRIPTION <code>page_num</code> <p>Page number (0-indexed)</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image</p> RAISES DESCRIPTION <code>PageRangeError</code> <p>If page number out of range</p> <p>Examples:</p> <pre><code>page = doc.get_page(0)  # First page\npage = doc.get_page(doc.page_count - 1)  # Last page\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>def get_page(self, page_num: int) -&gt; Image.Image:\n    \"\"\"\n    Get single page image (0-indexed).\n\n    More memory efficient than accessing .pages for large documents.\n\n    Args:\n        page_num: Page number (0-indexed)\n\n    Returns:\n        PIL Image\n\n    Raises:\n        PageRangeError: If page number out of range\n\n    Examples:\n        ```python\n        page = doc.get_page(0)  # First page\n        page = doc.get_page(doc.page_count - 1)  # Last page\n        ```\n    \"\"\"\n    if self._preloaded_images:\n        if page_num &lt; 0 or page_num &gt;= len(self._preloaded_images):\n            raise PageRangeError(f\"Page {page_num} out of range (0-{len(self._preloaded_images) - 1})\")\n        return self._preloaded_images[page_num]\n\n    if self._lazy_pages:\n        if page_num &lt; 0 or page_num &gt;= len(self._lazy_pages):\n            raise PageRangeError(f\"Page {page_num} out of range (0-{len(self._lazy_pages) - 1})\")\n        return self._lazy_pages[page_num].image\n\n    raise PageRangeError(\"No pages available\")\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.get_page_text","title":"get_page_text","text":"<pre><code>get_page_text(page_num: int) -&gt; str\n</code></pre> <p>Get text for specific page (1-indexed for compatibility with PDF page numbers).</p> PARAMETER DESCRIPTION <code>page_num</code> <p>Page number (1-indexed, like PDF viewers)</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Page text</p> RAISES DESCRIPTION <code>PageRangeError</code> <p>If page number out of range</p> <p>Examples:</p> <pre><code>text = doc.get_page_text(1)  # First page\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>def get_page_text(self, page_num: int) -&gt; str:\n    \"\"\"\n    Get text for specific page (1-indexed for compatibility with PDF page numbers).\n\n    Args:\n        page_num: Page number (1-indexed, like PDF viewers)\n\n    Returns:\n        Page text\n\n    Raises:\n        PageRangeError: If page number out of range\n\n    Examples:\n        ```python\n        text = doc.get_page_text(1)  # First page\n        ```\n    \"\"\"\n    idx = page_num - 1  # Convert to 0-based\n\n    if self._lazy_pages:\n        if idx &lt; 0 or idx &gt;= len(self._lazy_pages):\n            raise PageRangeError(f\"Page {page_num} out of range (1-{len(self._lazy_pages)})\")\n        return self._lazy_pages[idx].text\n\n    return \"\"\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.get_page_size","title":"get_page_size","text":"<pre><code>get_page_size(page_num: int) -&gt; tuple\n</code></pre> <p>Get page dimensions without rendering (fast).</p> PARAMETER DESCRIPTION <code>page_num</code> <p>Page number (0-indexed)</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>tuple</code> <p>Tuple of (width, height) in pixels</p> <p>Examples:</p> <pre><code>width, height = doc.get_page_size(0)\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>def get_page_size(self, page_num: int) -&gt; tuple:\n    \"\"\"\n    Get page dimensions without rendering (fast).\n\n    Args:\n        page_num: Page number (0-indexed)\n\n    Returns:\n        Tuple of (width, height) in pixels\n\n    Examples:\n        ```python\n        width, height = doc.get_page_size(0)\n        ```\n    \"\"\"\n    if self._lazy_pages:\n        if page_num &lt; 0 or page_num &gt;= len(self._lazy_pages):\n            raise PageRangeError(f\"Page {page_num} out of range\")\n        return self._lazy_pages[page_num].size\n\n    if self._preloaded_images:\n        if page_num &lt; 0 or page_num &gt;= len(self._preloaded_images):\n            raise PageRangeError(f\"Page {page_num} out of range\")\n        return self._preloaded_images[page_num].size\n\n    raise PageRangeError(\"No pages available\")\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.iter_pages","title":"iter_pages","text":"<pre><code>iter_pages() -&gt; Iterator[Image.Image]\n</code></pre> <p>Iterate over pages one at a time (memory efficient).</p> <p>Use this for large documents instead of .pages property.</p> YIELDS DESCRIPTION <code>Image</code> <p>PIL Images</p> <p>Examples:</p> <pre><code>for page in doc.iter_pages():\n        result = layout.extract(page)\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>def iter_pages(self) -&gt; Iterator[Image.Image]:\n    \"\"\"\n    Iterate over pages one at a time (memory efficient).\n\n    Use this for large documents instead of .pages property.\n\n    Yields:\n        PIL Images\n\n    Examples:\n        ```python\n        for page in doc.iter_pages():\n                result = layout.extract(page)\n        ```\n    \"\"\"\n    for i in range(self.page_count):\n        yield self.get_page(i)\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.clear_cache","title":"clear_cache","text":"<pre><code>clear_cache(page_num: Optional[int] = None)\n</code></pre> <p>Clear cached page images to free memory.</p> PARAMETER DESCRIPTION <code>page_num</code> <p>Specific page to clear, or None for all pages</p> <p> TYPE: <code>Optional[int]</code> DEFAULT: <code>None</code> </p> <p>Examples:</p> <pre><code>doc.clear_cache()  # Clear all\ndoc.clear_cache(0)  # Clear just first page\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>def clear_cache(self, page_num: Optional[int] = None):\n    \"\"\"\n    Clear cached page images to free memory.\n\n    Args:\n        page_num: Specific page to clear, or None for all pages\n\n    Examples:\n        ```python\n        doc.clear_cache()  # Clear all\n        doc.clear_cache(0)  # Clear just first page\n        ```\n    \"\"\"\n    if self._lazy_pages:\n        if page_num is not None:\n            if 0 &lt;= page_num &lt; len(self._lazy_pages):\n                self._lazy_pages[page_num].clear_cache()\n        else:\n            for lp in self._lazy_pages:\n                lp.clear_cache()\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.save_images","title":"save_images","text":"<pre><code>save_images(\n    output_dir: str,\n    prefix: str = \"page\",\n    format: str = \"PNG\",\n) -&gt; List[Path]\n</code></pre> <p>Save all pages as individual image files.</p> PARAMETER DESCRIPTION <code>output_dir</code> <p>Output directory path</p> <p> TYPE: <code>str</code> </p> <code>prefix</code> <p>Filename prefix (default: \"page\")</p> <p> TYPE: <code>str</code> DEFAULT: <code>'page'</code> </p> <code>format</code> <p>Image format (default: \"PNG\")</p> <p> TYPE: <code>str</code> DEFAULT: <code>'PNG'</code> </p> RETURNS DESCRIPTION <code>List[Path]</code> <p>List of saved file paths</p> <p>Examples:</p> <pre><code>paths = doc.save_images(\"output/\", prefix=\"doc\", format=\"PNG\")\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>def save_images(\n    self,\n    output_dir: str,\n    prefix: str = \"page\",\n    format: str = \"PNG\",\n) -&gt; List[Path]:\n    \"\"\"\n    Save all pages as individual image files.\n\n    Args:\n        output_dir: Output directory path\n        prefix: Filename prefix (default: \"page\")\n        format: Image format (default: \"PNG\")\n\n    Returns:\n        List of saved file paths\n\n    Examples:\n        ```python\n        paths = doc.save_images(\"output/\", prefix=\"doc\", format=\"PNG\")\n        ```\n    \"\"\"\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    saved = []\n    for i in range(self.page_count):\n        img = self.get_page(i)\n        file_path = output_path / f\"{prefix}_{i + 1:03d}.{format.lower()}\"\n        img.save(file_path, format=format)\n        saved.append(file_path)\n\n    return saved\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert document metadata to dictionary.</p> RETURNS DESCRIPTION <code>dict</code> <p>Dictionary of metadata</p> <p>Examples:</p> <pre><code>data = doc.to_dict()\nprint(data['page_count'])\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"\n    Convert document metadata to dictionary.\n\n    Returns:\n        Dictionary of metadata\n\n    Examples:\n        ```python\n        data = doc.to_dict()\n        print(data['page_count'])\n        ```\n    \"\"\"\n    return self._metadata.model_dump()\n</code></pre>"},{"location":"reference/document/#omnidocs.document.Document.close","title":"close","text":"<pre><code>close()\n</code></pre> <p>Close PDF document and free resources.</p> <p>Examples:</p> <pre><code>doc.close()\n</code></pre> Source code in <code>omnidocs/document.py</code> <pre><code>def close(self):\n    \"\"\"\n    Close PDF document and free resources.\n\n    Examples:\n        ```python\n        doc.close()\n        ```\n    \"\"\"\n    if self._pdf_doc:\n        self._pdf_doc.close()\n        self._pdf_doc = None\n    self._lazy_pages = None\n    self._pdf_bytes = None\n</code></pre>"},{"location":"reference/tasks/overview/","title":"Overview","text":"<p>OmniDocs Task Modules.</p> <p>Each task module provides extractors for specific document processing tasks.</p> Available task modules <ul> <li>layout_extraction: Detect document structure (titles, tables, figures, etc.)</li> <li>ocr_extraction: Extract text with bounding boxes from images</li> <li>text_extraction: Convert document images to HTML/Markdown</li> <li>table_extraction: Extract table structure and content</li> <li>reading_order: Determine logical reading sequence of document elements</li> </ul>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction","title":"layout_extraction","text":"<p>Layout Extraction Module.</p> <p>Provides extractors for detecting document layout elements such as titles, text blocks, figures, tables, formulas, and captions.</p> Available Extractors <ul> <li>DocLayoutYOLO: YOLO-based layout detector (fast, accurate)</li> <li>RTDETRLayoutExtractor: Transformer-based detector (more categories)</li> <li>QwenLayoutDetector: VLM-based detector with custom label support (multi-backend)</li> </ul> Example <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\nextractor = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n# VLM-based detection with custom labels\nfrom omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BaseLayoutExtractor","title":"BaseLayoutExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for layout extractors.</p> <p>All layout extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyLayoutExtractor(BaseLayoutExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Load model weights\n            pass\n\n        def extract(self, image):\n            # Run extraction\n            return LayoutOutput(...)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BaseLayoutExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput containing detected layout boxes with standardized labels</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format is not supported</p> <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/layout_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n\n    Returns:\n        LayoutOutput containing detected layout boxes with standardized labels\n\n    Raises:\n        ValueError: If image format is not supported\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BaseLayoutExtractor.batch_extract","title":"batch_extract","text":"<pre><code>batch_extract(\n    images: List[Union[Image, ndarray, str, Path]],\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[LayoutOutput]\n</code></pre> <p>Run layout extraction on multiple images.</p> <p>Default implementation loops over extract(). Subclasses can override for optimized batching.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of images in any supported format</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[LayoutOutput]</code> <p>List of LayoutOutput in same order as input</p> <p>Examples:</p> <pre><code>images = [doc.get_page(i) for i in range(doc.page_count)]\nresults = extractor.batch_extract(images)\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/base.py</code> <pre><code>def batch_extract(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[LayoutOutput]:\n    \"\"\"\n    Run layout extraction on multiple images.\n\n    Default implementation loops over extract(). Subclasses can override\n    for optimized batching.\n\n    Args:\n        images: List of images in any supported format\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of LayoutOutput in same order as input\n\n    Examples:\n        ```python\n        images = [doc.get_page(i) for i in range(doc.page_count)]\n        results = extractor.batch_extract(images)\n        ```\n    \"\"\"\n    results = []\n    total = len(images)\n\n    for i, image in enumerate(images):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(image)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BaseLayoutExtractor.extract_document","title":"extract_document","text":"<pre><code>extract_document(\n    document: Document,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[LayoutOutput]\n</code></pre> <p>Run layout extraction on all pages of a document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[LayoutOutput]</code> <p>List of LayoutOutput, one per page</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresults = extractor.extract_document(doc)\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/base.py</code> <pre><code>def extract_document(\n    self,\n    document: \"Document\",\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[LayoutOutput]:\n    \"\"\"\n    Run layout extraction on all pages of a document.\n\n    Args:\n        document: Document instance\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of LayoutOutput, one per page\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        results = extractor.extract_document(doc)\n        ```\n    \"\"\"\n    results = []\n    total = document.page_count\n\n    for i, page in enumerate(document.iter_pages()):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(page)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.DocLayoutYOLO","title":"DocLayoutYOLO","text":"<pre><code>DocLayoutYOLO(config: DocLayoutYOLOConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>DocLayout-YOLO layout extractor.</p> <p>A YOLO-based model optimized for document layout detection. Detects: title, text, figure, table, formula, captions, etc.</p> <p>This is a single-backend model (PyTorch only).</p> Example <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\nextractor = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre> <p>Initialize DocLayout-YOLO extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object with device, model_path, etc.</p> <p> TYPE: <code>DocLayoutYOLOConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/doc_layout_yolo.py</code> <pre><code>def __init__(self, config: DocLayoutYOLOConfig):\n    \"\"\"\n    Initialize DocLayout-YOLO extractor.\n\n    Args:\n        config: Configuration object with device, model_path, etc.\n    \"\"\"\n    self.config = config\n    self._model = None\n    self._device = self._resolve_device(config.device)\n    self._model_path = self._resolve_model_path(config.model_path)\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.DocLayoutYOLO.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> Source code in <code>omnidocs/tasks/layout_extraction/doc_layout_yolo.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n    if self._model is None:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    img_width, img_height = pil_image.size\n\n    # Run inference\n    results = self._model.predict(\n        pil_image,\n        imgsz=self.config.img_size,\n        conf=self.config.confidence,\n        device=self._device,\n    )\n\n    result = results[0]\n\n    # Parse detections\n    layout_boxes = []\n\n    if hasattr(result, \"boxes\") and result.boxes is not None:\n        boxes = result.boxes\n\n        for i in range(len(boxes)):\n            # Get coordinates\n            bbox_coords = boxes.xyxy[i].cpu().numpy().tolist()\n\n            # Get class and confidence\n            class_id = int(boxes.cls[i].item())\n            confidence = float(boxes.conf[i].item())\n\n            # Get original label from class names\n            original_label = DOCLAYOUT_YOLO_CLASS_NAMES.get(class_id, f\"class_{class_id}\")\n\n            # Map to standardized label\n            standard_label = DOCLAYOUT_YOLO_MAPPING.to_standard(original_label)\n\n            layout_boxes.append(\n                LayoutBox(\n                    label=standard_label,\n                    bbox=BoundingBox.from_list(bbox_coords),\n                    confidence=confidence,\n                    class_id=class_id,\n                    original_label=original_label,\n                )\n            )\n\n    # Sort by y-coordinate (top to bottom reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=img_width,\n        image_height=img_height,\n        model_name=\"DocLayout-YOLO\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.DocLayoutYOLOConfig","title":"DocLayoutYOLOConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for DocLayout-YOLO layout extractor.</p> <p>This is a single-backend model (PyTorch only).</p> Example <pre><code>config = DocLayoutYOLOConfig(device=\"cuda\", confidence=0.3)\nextractor = DocLayoutYOLO(config=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box coordinates in pixel space.</p> <p>Coordinates follow the convention: (x1, y1) is top-left, (x2, y2) is bottom-right.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x, y, width, height) format.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_xywh(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x, y, width, height) format.\"\"\"\n    return (self.x1, self.y1, self.width, self.height)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> <p>Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas. This provides consistent coordinates regardless of original image size.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Example <pre><code>bbox = BoundingBox(x1=100, y1=50, x2=500, y2=300)\nnormalized = bbox.to_normalized(1000, 800)\n# x: 100/1000*1024 = 102.4, y: 50/800*1024 = 64\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas.\n    This provides consistent coordinates regardless of original image size.\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n\n    Example:\n        ```python\n        bbox = BoundingBox(x1=100, y1=50, x2=500, y2=300)\n        normalized = bbox.to_normalized(1000, 800)\n        # x: 100/1000*1024 = 102.4, y: 50/800*1024 = 64\n        ```\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.BoundingBox.to_absolute","title":"to_absolute","text":"<pre><code>to_absolute(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert from normalized (0-1024) to absolute pixel coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Target image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Target image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with absolute pixel coordinates</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_absolute(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert from normalized (0-1024) to absolute pixel coordinates.\n\n    Args:\n        image_width: Target image width in pixels\n        image_height: Target image height in pixels\n\n    Returns:\n        New BoundingBox with absolute pixel coordinates\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / NORMALIZED_SIZE * image_width,\n        y1=self.y1 / NORMALIZED_SIZE * image_height,\n        x2=self.x2 / NORMALIZED_SIZE * image_width,\n        y2=self.y2 / NORMALIZED_SIZE * image_height,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.CustomLabel","title":"CustomLabel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Type-safe custom layout label definition for VLM-based models.</p> <p>VLM models like Qwen3-VL support flexible custom labels beyond the standard LayoutLabel enum. Use this class to define custom labels with validation.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import CustomLabel\n\n# Simple custom label\ncode_block = CustomLabel(name=\"code_block\")\n\n# With metadata\nsidebar = CustomLabel(\n        name=\"sidebar\",\n        description=\"Secondary content panel\",\n        color=\"#9B59B6\",\n    )\n\n# Use with QwenLayoutDetector\nresult = detector.extract(image, custom_labels=[code_block, sidebar])\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LabelMapping","title":"LabelMapping","text":"<pre><code>LabelMapping(mapping: Dict[str, LayoutLabel])\n</code></pre> <p>Base class for model-specific label mappings.</p> <p>Each model maps its native labels to standardized LayoutLabel values.</p> <p>Initialize label mapping.</p> PARAMETER DESCRIPTION <code>mapping</code> <p>Dict mapping model-specific labels to LayoutLabel enum values</p> <p> TYPE: <code>Dict[str, LayoutLabel]</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def __init__(self, mapping: Dict[str, LayoutLabel]):\n    \"\"\"\n    Initialize label mapping.\n\n    Args:\n        mapping: Dict mapping model-specific labels to LayoutLabel enum values\n    \"\"\"\n    self._mapping = {k.lower(): v for k, v in mapping.items()}\n    self._reverse_mapping = {v: k for k, v in mapping.items()}\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LabelMapping.supported_labels","title":"supported_labels  <code>property</code>","text":"<pre><code>supported_labels: List[str]\n</code></pre> <p>Get list of supported model-specific labels.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LabelMapping.standard_labels","title":"standard_labels  <code>property</code>","text":"<pre><code>standard_labels: List[LayoutLabel]\n</code></pre> <p>Get list of standard labels this mapping produces.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LabelMapping.to_standard","title":"to_standard","text":"<pre><code>to_standard(model_label: str) -&gt; LayoutLabel\n</code></pre> <p>Convert model-specific label to standardized LayoutLabel.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_standard(self, model_label: str) -&gt; LayoutLabel:\n    \"\"\"Convert model-specific label to standardized LayoutLabel.\"\"\"\n    return self._mapping.get(model_label.lower(), LayoutLabel.UNKNOWN)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LabelMapping.from_standard","title":"from_standard","text":"<pre><code>from_standard(standard_label: LayoutLabel) -&gt; Optional[str]\n</code></pre> <p>Convert standardized LayoutLabel to model-specific label.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def from_standard(self, standard_label: LayoutLabel) -&gt; Optional[str]:\n    \"\"\"Convert standardized LayoutLabel to model-specific label.\"\"\"\n    return self._reverse_mapping.get(standard_label)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutBox","title":"LayoutBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single detected layout element with label, bounding box, and confidence.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutBox.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"label\": self.label.value,\n        \"bbox\": self.bbox.to_list(),\n        \"confidence\": self.confidence,\n        \"class_id\": self.class_id,\n        \"original_label\": self.original_label,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutBox.get_normalized_bbox","title":"get_normalized_bbox","text":"<pre><code>get_normalized_bbox(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Get bounding box in normalized (0-1024) coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox with normalized coordinates</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def get_normalized_bbox(self, image_width: int, image_height: int) -&gt; BoundingBox:\n    \"\"\"\n    Get bounding box in normalized (0-1024) coordinates.\n\n    Args:\n        image_width: Original image width\n        image_height: Original image height\n\n    Returns:\n        BoundingBox with normalized coordinates\n    \"\"\"\n    return self.bbox.to_normalized(image_width, image_height)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutLabel","title":"LayoutLabel","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Standardized layout labels used across all layout extractors.</p> <p>These provide a consistent vocabulary regardless of which model is used.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput","title":"LayoutOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete layout extraction results for a single image.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.element_count","title":"element_count  <code>property</code>","text":"<pre><code>element_count: int\n</code></pre> <p>Number of detected elements.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.labels_found","title":"labels_found  <code>property</code>","text":"<pre><code>labels_found: List[str]\n</code></pre> <p>Unique labels found in detections.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.filter_by_label","title":"filter_by_label","text":"<pre><code>filter_by_label(label: LayoutLabel) -&gt; List[LayoutBox]\n</code></pre> <p>Filter boxes by label.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def filter_by_label(self, label: LayoutLabel) -&gt; List[LayoutBox]:\n    \"\"\"Filter boxes by label.\"\"\"\n    return [box for box in self.bboxes if box.label == label]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(\n    min_confidence: float,\n) -&gt; List[LayoutBox]\n</code></pre> <p>Filter boxes by minimum confidence.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def filter_by_confidence(self, min_confidence: float) -&gt; List[LayoutBox]:\n    \"\"\"Filter boxes by minimum confidence.\"\"\"\n    return [box for box in self.bboxes if box.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"bboxes\": [box.to_dict() for box in self.bboxes],\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"element_count\": self.element_count,\n        \"labels_found\": self.labels_found,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.sort_by_position","title":"sort_by_position","text":"<pre><code>sort_by_position(\n    top_to_bottom: bool = True,\n) -&gt; LayoutOutput\n</code></pre> <p>Return a new LayoutOutput with boxes sorted by position.</p> PARAMETER DESCRIPTION <code>top_to_bottom</code> <p>If True, sort by y-coordinate (reading order)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def sort_by_position(self, top_to_bottom: bool = True) -&gt; \"LayoutOutput\":\n    \"\"\"\n    Return a new LayoutOutput with boxes sorted by position.\n\n    Args:\n        top_to_bottom: If True, sort by y-coordinate (reading order)\n    \"\"\"\n    sorted_boxes = sorted(self.bboxes, key=lambda b: (b.bbox.y1, b.bbox.x1), reverse=not top_to_bottom)\n    return LayoutOutput(\n        bboxes=sorted_boxes,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        model_name=self.model_name,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.get_normalized_bboxes","title":"get_normalized_bboxes","text":"<pre><code>get_normalized_bboxes() -&gt; List[Dict]\n</code></pre> <p>Get all bounding boxes in normalized (0-1024) coordinates.</p> RETURNS DESCRIPTION <code>List[Dict]</code> <p>List of dicts with normalized bbox coordinates and metadata.</p> Example <pre><code>result = extractor.extract(image)\nnormalized = result.get_normalized_bboxes()\nfor box in normalized:\n        print(f\"{box['label']}: {box['bbox']}\")  # coords in 0-1024 range\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def get_normalized_bboxes(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all bounding boxes in normalized (0-1024) coordinates.\n\n    Returns:\n        List of dicts with normalized bbox coordinates and metadata.\n\n    Example:\n        ```python\n        result = extractor.extract(image)\n        normalized = result.get_normalized_bboxes()\n        for box in normalized:\n                print(f\"{box['label']}: {box['bbox']}\")  # coords in 0-1024 range\n        ```\n    \"\"\"\n    normalized = []\n    for box in self.bboxes:\n        norm_bbox = box.bbox.to_normalized(self.image_width, self.image_height)\n        normalized.append(\n            {\n                \"label\": box.label.value,\n                \"bbox\": norm_bbox.to_list(),\n                \"confidence\": box.confidence,\n                \"class_id\": box.class_id,\n                \"original_label\": box.original_label,\n            }\n        )\n    return normalized\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.visualize","title":"visualize","text":"<pre><code>visualize(\n    image: Image,\n    output_path: Optional[Union[str, Path]] = None,\n    show_labels: bool = True,\n    show_confidence: bool = True,\n    line_width: int = 3,\n    font_size: int = 12,\n) -&gt; Image.Image\n</code></pre> <p>Visualize layout detection results on the image.</p> <p>Draws bounding boxes with labels and confidence scores on the image. Each layout category has a distinct color for easy identification.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to draw on (will be copied, not modified)</p> <p> TYPE: <code>Image</code> </p> <code>output_path</code> <p>Optional path to save the visualization</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> <code>show_labels</code> <p>Whether to show label text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_confidence</code> <p>Whether to show confidence scores</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>line_width</code> <p>Width of bounding box lines</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>font_size</code> <p>Size of label text (note: uses default font)</p> <p> TYPE: <code>int</code> DEFAULT: <code>12</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image with visualizations drawn</p> Example <pre><code>result = extractor.extract(image)\nviz = result.visualize(image, output_path=\"layout_viz.png\")\nviz.show()  # Display in notebook/viewer\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def visualize(\n    self,\n    image: \"Image.Image\",\n    output_path: Optional[Union[str, Path]] = None,\n    show_labels: bool = True,\n    show_confidence: bool = True,\n    line_width: int = 3,\n    font_size: int = 12,\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Visualize layout detection results on the image.\n\n    Draws bounding boxes with labels and confidence scores on the image.\n    Each layout category has a distinct color for easy identification.\n\n    Args:\n        image: PIL Image to draw on (will be copied, not modified)\n        output_path: Optional path to save the visualization\n        show_labels: Whether to show label text\n        show_confidence: Whether to show confidence scores\n        line_width: Width of bounding box lines\n        font_size: Size of label text (note: uses default font)\n\n    Returns:\n        PIL Image with visualizations drawn\n\n    Example:\n        ```python\n        result = extractor.extract(image)\n        viz = result.visualize(image, output_path=\"layout_viz.png\")\n        viz.show()  # Display in notebook/viewer\n        ```\n    \"\"\"\n    from PIL import ImageDraw\n\n    # Copy image to avoid modifying original\n    viz_image = image.copy().convert(\"RGB\")\n    draw = ImageDraw.Draw(viz_image)\n\n    for box in self.bboxes:\n        # Get color for this label\n        color = LABEL_COLORS.get(box.label, \"#95A5A6\")\n\n        # Draw bounding box\n        coords = box.bbox.to_xyxy()\n        draw.rectangle(coords, outline=color, width=line_width)\n\n        # Build label text\n        if show_labels or show_confidence:\n            label_parts = []\n            if show_labels:\n                label_parts.append(box.label.value)\n            if show_confidence:\n                label_parts.append(f\"{box.confidence:.2f}\")\n            label_text = \" \".join(label_parts)\n\n            # Draw label background\n            text_bbox = draw.textbbox((coords[0], coords[1] - 20), label_text)\n            draw.rectangle(text_bbox, fill=color)\n\n            # Draw label text\n            draw.text(\n                (coords[0], coords[1] - 20),\n                label_text,\n                fill=\"white\",\n            )\n\n    # Save if path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        viz_image.save(output_path)\n\n    return viz_image\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; LayoutOutput\n</code></pre> <p>Load a LayoutOutput instance from a JSON file.</p> <p>Reads a JSON file and deserializes its contents into a LayoutOutput object. Uses Pydantic's model_validate_json for proper handling of nested objects.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to JSON file containing serialized LayoutOutput data.       Can be string or pathlib.Path object.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>Deserialized layout output instance from file.</p> <p> TYPE: <code>LayoutOutput</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>UnicodeDecodeError</code> <p>If file cannot be decoded as UTF-8.</p> <code>ValueError</code> <p>If file contents are not valid JSON.</p> <code>ValidationError</code> <p>If JSON data doesn't match LayoutOutput schema.</p> Example <p><pre><code>output = LayoutOutput.load_json('layout_results.json')\nprint(f\"Found {output.element_count} elements\")\n</code></pre> Found 5 elements</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"LayoutOutput\":\n    \"\"\"\n    Load a LayoutOutput instance from a JSON file.\n\n    Reads a JSON file and deserializes its contents into a LayoutOutput object.\n    Uses Pydantic's model_validate_json for proper handling of nested objects.\n\n    Args:\n        file_path: Path to JSON file containing serialized LayoutOutput data.\n                  Can be string or pathlib.Path object.\n\n    Returns:\n        LayoutOutput: Deserialized layout output instance from file.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n        UnicodeDecodeError: If file cannot be decoded as UTF-8.\n        ValueError: If file contents are not valid JSON.\n        ValidationError: If JSON data doesn't match LayoutOutput schema.\n\n    Example:\n        ```python\n        output = LayoutOutput.load_json('layout_results.json')\n        print(f\"Found {output.element_count} elements\")\n        ```\n        Found 5 elements\n    \"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save LayoutOutput instance to a JSON file.</p> <p>Serializes the LayoutOutput object to JSON and writes it to a file. Automatically creates parent directories if they don't exist. Uses UTF-8 encoding for compatibility and proper handling of special characters.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path where JSON file should be saved. Can be string or       pathlib.Path object. Parent directories will be created       if they don't exist.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> RAISES DESCRIPTION <code>OSError</code> <p>If file cannot be written due to permission or disk errors.</p> <code>TypeError</code> <p>If file_path is not a string or Path object.</p> Example <pre><code>output = LayoutOutput(bboxes=[], image_width=800, image_height=600)\noutput.save_json('results/layout_output.json')\n# File is created at results/layout_output.json\n# Parent 'results' directory is created if it didn't exist\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"\n    Save LayoutOutput instance to a JSON file.\n\n    Serializes the LayoutOutput object to JSON and writes it to a file.\n    Automatically creates parent directories if they don't exist. Uses UTF-8\n    encoding for compatibility and proper handling of special characters.\n\n    Args:\n        file_path: Path where JSON file should be saved. Can be string or\n                  pathlib.Path object. Parent directories will be created\n                  if they don't exist.\n\n    Returns:\n        None\n\n    Raises:\n        OSError: If file cannot be written due to permission or disk errors.\n        TypeError: If file_path is not a string or Path object.\n\n    Example:\n        ```python\n        output = LayoutOutput(bboxes=[], image_width=800, image_height=600)\n        output.save_json('results/layout_output.json')\n        # File is created at results/layout_output.json\n        # Parent 'results' directory is created if it didn't exist\n        ```\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.QwenLayoutDetector","title":"QwenLayoutDetector","text":"<pre><code>QwenLayoutDetector(backend: QwenLayoutBackendConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>Qwen3-VL Vision-Language Model layout detector.</p> <p>A flexible VLM-based layout detector that supports custom labels. Unlike fixed-label models (DocLayoutYOLO, RT-DETR), Qwen can detect any document elements specified at runtime.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\n# Initialize with PyTorch backend\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Basic extraction with default labels\nresult = detector.extract(image)\n\n# With custom labels (strings)\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n\n# With typed custom labels\nlabels = [\n        CustomLabel(name=\"code_block\", color=\"#E74C3C\"),\n        CustomLabel(name=\"sidebar\", description=\"Side panel content\"),\n    ]\nresult = detector.extract(image, custom_labels=labels)\n</code></pre> <p>Initialize Qwen layout detector.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend - QwenLayoutVLLMConfig: VLLM high-throughput backend - QwenLayoutMLXConfig: MLX backend for Apple Silicon - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenLayoutBackendConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def __init__(self, backend: QwenLayoutBackendConfig):\n    \"\"\"\n    Initialize Qwen layout detector.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenLayoutVLLMConfig: VLLM high-throughput backend\n            - QwenLayoutMLXConfig: MLX backend for Apple Silicon\n            - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.QwenLayoutDetector.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    custom_labels: Optional[\n        List[Union[str, CustomLabel]]\n    ] = None,\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout detection on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>custom_labels</code> <p>Optional custom labels to detect. Can be: - None: Use default labels (title, text, table, figure, etc.) - List[str]: Simple label names [\"code_block\", \"sidebar\"] - List[CustomLabel]: Typed labels with metadata</p> <p> TYPE: <code>Optional[List[Union[str, CustomLabel]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    custom_labels: Optional[List[Union[str, CustomLabel]]] = None,\n) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout detection on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        custom_labels: Optional custom labels to detect. Can be:\n            - None: Use default labels (title, text, table, figure, etc.)\n            - List[str]: Simple label names [\"code_block\", \"sidebar\"]\n            - List[CustomLabel]: Typed labels with metadata\n\n    Returns:\n        LayoutOutput with detected layout boxes\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Normalize labels\n    label_names = self._normalize_labels(custom_labels)\n\n    # Build prompt\n    prompt = self._build_detection_prompt(label_names)\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenLayoutPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenLayoutVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenLayoutMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenLayoutAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse detections\n    detections = self._parse_json_output(raw_output)\n\n    # Convert to LayoutOutput\n    layout_boxes = self._build_layout_boxes(detections, width, height)\n\n    # Sort by position (reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.RTDETRConfig","title":"RTDETRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for RT-DETR layout extractor.</p> <p>This is a single-backend model (PyTorch/Transformers only).</p> Example <pre><code>config = RTDETRConfig(device=\"cuda\", confidence=0.4)\nextractor = RTDETRLayoutExtractor(config=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.RTDETRLayoutExtractor","title":"RTDETRLayoutExtractor","text":"<pre><code>RTDETRLayoutExtractor(config: RTDETRConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>RT-DETR layout extractor using HuggingFace Transformers.</p> <p>A transformer-based real-time detection model for document layout. Detects: title, text, table, figure, list, formula, captions, headers, footers.</p> <p>This is a single-backend model (PyTorch/Transformers only).</p> Example <pre><code>from omnidocs.tasks.layout_extraction import RTDETRLayoutExtractor, RTDETRConfig\n\nextractor = RTDETRLayoutExtractor(config=RTDETRConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre> <p>Initialize RT-DETR layout extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object with device, model settings, etc.</p> <p> TYPE: <code>RTDETRConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/rtdetr.py</code> <pre><code>def __init__(self, config: RTDETRConfig):\n    \"\"\"\n    Initialize RT-DETR layout extractor.\n\n    Args:\n        config: Configuration object with device, model settings, etc.\n    \"\"\"\n    self.config = config\n    self._model = None\n    self._processor = None\n    self._device = self._resolve_device(config.device)\n    self._model_path = self._resolve_model_path(config.model_path)\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.RTDETRLayoutExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> Source code in <code>omnidocs/tasks/layout_extraction/rtdetr.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n    import torch\n\n    if self._model is None or self._processor is None:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    img_width, img_height = pil_image.size\n\n    # Preprocess\n    inputs = self._processor(\n        images=pil_image,\n        return_tensors=\"pt\",\n        size={\"height\": self.config.image_size, \"width\": self.config.image_size},\n    )\n\n    # Move to device\n    inputs = {k: v.to(self._device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n\n    # Run inference\n    with torch.no_grad():\n        outputs = self._model(**inputs)\n\n    # Post-process results\n    target_sizes = torch.tensor([[img_height, img_width]])\n    results = self._processor.post_process_object_detection(\n        outputs,\n        target_sizes=target_sizes,\n        threshold=self.config.confidence,\n    )[0]\n\n    # Parse detections\n    layout_boxes = []\n\n    for score, label_id, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n        confidence = float(score.item())\n        class_id = int(label_id.item())\n\n        # Get original label from model config\n        # Note: The model outputs 0-indexed class IDs, but id2label has background at index 0,\n        # so we add 1 to map correctly (e.g., model output 8 -&gt; id2label[9] = \"Table\")\n        original_label = self._model.config.id2label.get(class_id + 1, f\"class_{class_id}\")\n\n        # Map to standardized label\n        standard_label = RTDETR_MAPPING.to_standard(original_label)\n\n        # Box coordinates\n        box_coords = box.cpu().tolist()\n\n        layout_boxes.append(\n            LayoutBox(\n                label=standard_label,\n                bbox=BoundingBox.from_list(box_coords),\n                confidence=confidence,\n                class_id=class_id,\n                original_label=original_label,\n            )\n        )\n\n    # Sort by y-coordinate (top to bottom reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=img_width,\n        image_height=img_height,\n        model_name=\"RT-DETR (docling-layout)\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.base","title":"base","text":"<p>Base class for layout extractors.</p> <p>Defines the abstract interface that all layout extractors must implement.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.base.BaseLayoutExtractor","title":"BaseLayoutExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for layout extractors.</p> <p>All layout extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyLayoutExtractor(BaseLayoutExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Load model weights\n            pass\n\n        def extract(self, image):\n            # Run extraction\n            return LayoutOutput(...)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.base.BaseLayoutExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput containing detected layout boxes with standardized labels</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format is not supported</p> <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/layout_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n\n    Returns:\n        LayoutOutput containing detected layout boxes with standardized labels\n\n    Raises:\n        ValueError: If image format is not supported\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.base.BaseLayoutExtractor.batch_extract","title":"batch_extract","text":"<pre><code>batch_extract(\n    images: List[Union[Image, ndarray, str, Path]],\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[LayoutOutput]\n</code></pre> <p>Run layout extraction on multiple images.</p> <p>Default implementation loops over extract(). Subclasses can override for optimized batching.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of images in any supported format</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[LayoutOutput]</code> <p>List of LayoutOutput in same order as input</p> <p>Examples:</p> <pre><code>images = [doc.get_page(i) for i in range(doc.page_count)]\nresults = extractor.batch_extract(images)\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/base.py</code> <pre><code>def batch_extract(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[LayoutOutput]:\n    \"\"\"\n    Run layout extraction on multiple images.\n\n    Default implementation loops over extract(). Subclasses can override\n    for optimized batching.\n\n    Args:\n        images: List of images in any supported format\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of LayoutOutput in same order as input\n\n    Examples:\n        ```python\n        images = [doc.get_page(i) for i in range(doc.page_count)]\n        results = extractor.batch_extract(images)\n        ```\n    \"\"\"\n    results = []\n    total = len(images)\n\n    for i, image in enumerate(images):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(image)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.base.BaseLayoutExtractor.extract_document","title":"extract_document","text":"<pre><code>extract_document(\n    document: Document,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[LayoutOutput]\n</code></pre> <p>Run layout extraction on all pages of a document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[LayoutOutput]</code> <p>List of LayoutOutput, one per page</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresults = extractor.extract_document(doc)\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/base.py</code> <pre><code>def extract_document(\n    self,\n    document: \"Document\",\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[LayoutOutput]:\n    \"\"\"\n    Run layout extraction on all pages of a document.\n\n    Args:\n        document: Document instance\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of LayoutOutput, one per page\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        results = extractor.extract_document(doc)\n        ```\n    \"\"\"\n    results = []\n    total = document.page_count\n\n    for i, page in enumerate(document.iter_pages()):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(page)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.doc_layout_yolo","title":"doc_layout_yolo","text":"<p>DocLayout-YOLO layout extractor.</p> <p>A YOLO-based model for document layout detection, optimized for academic papers and technical documents.</p> <p>Model: juliozhao/DocLayout-YOLO-DocStructBench</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.doc_layout_yolo.DocLayoutYOLOConfig","title":"DocLayoutYOLOConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for DocLayout-YOLO layout extractor.</p> <p>This is a single-backend model (PyTorch only).</p> Example <pre><code>config = DocLayoutYOLOConfig(device=\"cuda\", confidence=0.3)\nextractor = DocLayoutYOLO(config=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.doc_layout_yolo.DocLayoutYOLO","title":"DocLayoutYOLO","text":"<pre><code>DocLayoutYOLO(config: DocLayoutYOLOConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>DocLayout-YOLO layout extractor.</p> <p>A YOLO-based model optimized for document layout detection. Detects: title, text, figure, table, formula, captions, etc.</p> <p>This is a single-backend model (PyTorch only).</p> Example <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\nextractor = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre> <p>Initialize DocLayout-YOLO extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object with device, model_path, etc.</p> <p> TYPE: <code>DocLayoutYOLOConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/doc_layout_yolo.py</code> <pre><code>def __init__(self, config: DocLayoutYOLOConfig):\n    \"\"\"\n    Initialize DocLayout-YOLO extractor.\n\n    Args:\n        config: Configuration object with device, model_path, etc.\n    \"\"\"\n    self.config = config\n    self._model = None\n    self._device = self._resolve_device(config.device)\n    self._model_path = self._resolve_model_path(config.model_path)\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.doc_layout_yolo.DocLayoutYOLO.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> Source code in <code>omnidocs/tasks/layout_extraction/doc_layout_yolo.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n    if self._model is None:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    img_width, img_height = pil_image.size\n\n    # Run inference\n    results = self._model.predict(\n        pil_image,\n        imgsz=self.config.img_size,\n        conf=self.config.confidence,\n        device=self._device,\n    )\n\n    result = results[0]\n\n    # Parse detections\n    layout_boxes = []\n\n    if hasattr(result, \"boxes\") and result.boxes is not None:\n        boxes = result.boxes\n\n        for i in range(len(boxes)):\n            # Get coordinates\n            bbox_coords = boxes.xyxy[i].cpu().numpy().tolist()\n\n            # Get class and confidence\n            class_id = int(boxes.cls[i].item())\n            confidence = float(boxes.conf[i].item())\n\n            # Get original label from class names\n            original_label = DOCLAYOUT_YOLO_CLASS_NAMES.get(class_id, f\"class_{class_id}\")\n\n            # Map to standardized label\n            standard_label = DOCLAYOUT_YOLO_MAPPING.to_standard(original_label)\n\n            layout_boxes.append(\n                LayoutBox(\n                    label=standard_label,\n                    bbox=BoundingBox.from_list(bbox_coords),\n                    confidence=confidence,\n                    class_id=class_id,\n                    original_label=original_label,\n                )\n            )\n\n    # Sort by y-coordinate (top to bottom reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=img_width,\n        image_height=img_height,\n        model_name=\"DocLayout-YOLO\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models","title":"models","text":"<p>Pydantic models for layout extraction outputs.</p> <p>Defines standardized output types and label enums for layout detection.</p> Coordinate Systems <ul> <li>Absolute (default): Coordinates in pixels relative to original image size</li> <li>Normalized (0-1024): Coordinates scaled to 0-1024 range (virtual 1024x1024 canvas)</li> </ul> <p>Use <code>bbox.to_normalized(width, height)</code> or <code>output.get_normalized_bboxes()</code> to convert to normalized coordinates.</p> Example <pre><code>result = extractor.extract(image)  # Returns absolute pixel coordinates\nnormalized = result.get_normalized_bboxes()  # Returns 0-1024 normalized coords\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutLabel","title":"LayoutLabel","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Standardized layout labels used across all layout extractors.</p> <p>These provide a consistent vocabulary regardless of which model is used.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.CustomLabel","title":"CustomLabel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Type-safe custom layout label definition for VLM-based models.</p> <p>VLM models like Qwen3-VL support flexible custom labels beyond the standard LayoutLabel enum. Use this class to define custom labels with validation.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import CustomLabel\n\n# Simple custom label\ncode_block = CustomLabel(name=\"code_block\")\n\n# With metadata\nsidebar = CustomLabel(\n        name=\"sidebar\",\n        description=\"Secondary content panel\",\n        color=\"#9B59B6\",\n    )\n\n# Use with QwenLayoutDetector\nresult = detector.extract(image, custom_labels=[code_block, sidebar])\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LabelMapping","title":"LabelMapping","text":"<pre><code>LabelMapping(mapping: Dict[str, LayoutLabel])\n</code></pre> <p>Base class for model-specific label mappings.</p> <p>Each model maps its native labels to standardized LayoutLabel values.</p> <p>Initialize label mapping.</p> PARAMETER DESCRIPTION <code>mapping</code> <p>Dict mapping model-specific labels to LayoutLabel enum values</p> <p> TYPE: <code>Dict[str, LayoutLabel]</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def __init__(self, mapping: Dict[str, LayoutLabel]):\n    \"\"\"\n    Initialize label mapping.\n\n    Args:\n        mapping: Dict mapping model-specific labels to LayoutLabel enum values\n    \"\"\"\n    self._mapping = {k.lower(): v for k, v in mapping.items()}\n    self._reverse_mapping = {v: k for k, v in mapping.items()}\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LabelMapping.supported_labels","title":"supported_labels  <code>property</code>","text":"<pre><code>supported_labels: List[str]\n</code></pre> <p>Get list of supported model-specific labels.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LabelMapping.standard_labels","title":"standard_labels  <code>property</code>","text":"<pre><code>standard_labels: List[LayoutLabel]\n</code></pre> <p>Get list of standard labels this mapping produces.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LabelMapping.to_standard","title":"to_standard","text":"<pre><code>to_standard(model_label: str) -&gt; LayoutLabel\n</code></pre> <p>Convert model-specific label to standardized LayoutLabel.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_standard(self, model_label: str) -&gt; LayoutLabel:\n    \"\"\"Convert model-specific label to standardized LayoutLabel.\"\"\"\n    return self._mapping.get(model_label.lower(), LayoutLabel.UNKNOWN)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LabelMapping.from_standard","title":"from_standard","text":"<pre><code>from_standard(standard_label: LayoutLabel) -&gt; Optional[str]\n</code></pre> <p>Convert standardized LayoutLabel to model-specific label.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def from_standard(self, standard_label: LayoutLabel) -&gt; Optional[str]:\n    \"\"\"Convert standardized LayoutLabel to model-specific label.\"\"\"\n    return self._reverse_mapping.get(standard_label)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box coordinates in pixel space.</p> <p>Coordinates follow the convention: (x1, y1) is top-left, (x2, y2) is bottom-right.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x, y, width, height) format.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_xywh(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x, y, width, height) format.\"\"\"\n    return (self.x1, self.y1, self.width, self.height)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> <p>Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas. This provides consistent coordinates regardless of original image size.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Example <pre><code>bbox = BoundingBox(x1=100, y1=50, x2=500, y2=300)\nnormalized = bbox.to_normalized(1000, 800)\n# x: 100/1000*1024 = 102.4, y: 50/800*1024 = 64\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas.\n    This provides consistent coordinates regardless of original image size.\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n\n    Example:\n        ```python\n        bbox = BoundingBox(x1=100, y1=50, x2=500, y2=300)\n        normalized = bbox.to_normalized(1000, 800)\n        # x: 100/1000*1024 = 102.4, y: 50/800*1024 = 64\n        ```\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_absolute","title":"to_absolute","text":"<pre><code>to_absolute(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert from normalized (0-1024) to absolute pixel coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Target image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Target image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with absolute pixel coordinates</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_absolute(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert from normalized (0-1024) to absolute pixel coordinates.\n\n    Args:\n        image_width: Target image width in pixels\n        image_height: Target image height in pixels\n\n    Returns:\n        New BoundingBox with absolute pixel coordinates\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / NORMALIZED_SIZE * image_width,\n        y1=self.y1 / NORMALIZED_SIZE * image_height,\n        x2=self.x2 / NORMALIZED_SIZE * image_width,\n        y2=self.y2 / NORMALIZED_SIZE * image_height,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutBox","title":"LayoutBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single detected layout element with label, bounding box, and confidence.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutBox.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"label\": self.label.value,\n        \"bbox\": self.bbox.to_list(),\n        \"confidence\": self.confidence,\n        \"class_id\": self.class_id,\n        \"original_label\": self.original_label,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutBox.get_normalized_bbox","title":"get_normalized_bbox","text":"<pre><code>get_normalized_bbox(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Get bounding box in normalized (0-1024) coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox with normalized coordinates</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def get_normalized_bbox(self, image_width: int, image_height: int) -&gt; BoundingBox:\n    \"\"\"\n    Get bounding box in normalized (0-1024) coordinates.\n\n    Args:\n        image_width: Original image width\n        image_height: Original image height\n\n    Returns:\n        BoundingBox with normalized coordinates\n    \"\"\"\n    return self.bbox.to_normalized(image_width, image_height)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput","title":"LayoutOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete layout extraction results for a single image.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.element_count","title":"element_count  <code>property</code>","text":"<pre><code>element_count: int\n</code></pre> <p>Number of detected elements.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.labels_found","title":"labels_found  <code>property</code>","text":"<pre><code>labels_found: List[str]\n</code></pre> <p>Unique labels found in detections.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.filter_by_label","title":"filter_by_label","text":"<pre><code>filter_by_label(label: LayoutLabel) -&gt; List[LayoutBox]\n</code></pre> <p>Filter boxes by label.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def filter_by_label(self, label: LayoutLabel) -&gt; List[LayoutBox]:\n    \"\"\"Filter boxes by label.\"\"\"\n    return [box for box in self.bboxes if box.label == label]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(\n    min_confidence: float,\n) -&gt; List[LayoutBox]\n</code></pre> <p>Filter boxes by minimum confidence.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def filter_by_confidence(self, min_confidence: float) -&gt; List[LayoutBox]:\n    \"\"\"Filter boxes by minimum confidence.\"\"\"\n    return [box for box in self.bboxes if box.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"bboxes\": [box.to_dict() for box in self.bboxes],\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"element_count\": self.element_count,\n        \"labels_found\": self.labels_found,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.sort_by_position","title":"sort_by_position","text":"<pre><code>sort_by_position(\n    top_to_bottom: bool = True,\n) -&gt; LayoutOutput\n</code></pre> <p>Return a new LayoutOutput with boxes sorted by position.</p> PARAMETER DESCRIPTION <code>top_to_bottom</code> <p>If True, sort by y-coordinate (reading order)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def sort_by_position(self, top_to_bottom: bool = True) -&gt; \"LayoutOutput\":\n    \"\"\"\n    Return a new LayoutOutput with boxes sorted by position.\n\n    Args:\n        top_to_bottom: If True, sort by y-coordinate (reading order)\n    \"\"\"\n    sorted_boxes = sorted(self.bboxes, key=lambda b: (b.bbox.y1, b.bbox.x1), reverse=not top_to_bottom)\n    return LayoutOutput(\n        bboxes=sorted_boxes,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        model_name=self.model_name,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.get_normalized_bboxes","title":"get_normalized_bboxes","text":"<pre><code>get_normalized_bboxes() -&gt; List[Dict]\n</code></pre> <p>Get all bounding boxes in normalized (0-1024) coordinates.</p> RETURNS DESCRIPTION <code>List[Dict]</code> <p>List of dicts with normalized bbox coordinates and metadata.</p> Example <pre><code>result = extractor.extract(image)\nnormalized = result.get_normalized_bboxes()\nfor box in normalized:\n        print(f\"{box['label']}: {box['bbox']}\")  # coords in 0-1024 range\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def get_normalized_bboxes(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all bounding boxes in normalized (0-1024) coordinates.\n\n    Returns:\n        List of dicts with normalized bbox coordinates and metadata.\n\n    Example:\n        ```python\n        result = extractor.extract(image)\n        normalized = result.get_normalized_bboxes()\n        for box in normalized:\n                print(f\"{box['label']}: {box['bbox']}\")  # coords in 0-1024 range\n        ```\n    \"\"\"\n    normalized = []\n    for box in self.bboxes:\n        norm_bbox = box.bbox.to_normalized(self.image_width, self.image_height)\n        normalized.append(\n            {\n                \"label\": box.label.value,\n                \"bbox\": norm_bbox.to_list(),\n                \"confidence\": box.confidence,\n                \"class_id\": box.class_id,\n                \"original_label\": box.original_label,\n            }\n        )\n    return normalized\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.visualize","title":"visualize","text":"<pre><code>visualize(\n    image: Image,\n    output_path: Optional[Union[str, Path]] = None,\n    show_labels: bool = True,\n    show_confidence: bool = True,\n    line_width: int = 3,\n    font_size: int = 12,\n) -&gt; Image.Image\n</code></pre> <p>Visualize layout detection results on the image.</p> <p>Draws bounding boxes with labels and confidence scores on the image. Each layout category has a distinct color for easy identification.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to draw on (will be copied, not modified)</p> <p> TYPE: <code>Image</code> </p> <code>output_path</code> <p>Optional path to save the visualization</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> <code>show_labels</code> <p>Whether to show label text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_confidence</code> <p>Whether to show confidence scores</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>line_width</code> <p>Width of bounding box lines</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>font_size</code> <p>Size of label text (note: uses default font)</p> <p> TYPE: <code>int</code> DEFAULT: <code>12</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image with visualizations drawn</p> Example <pre><code>result = extractor.extract(image)\nviz = result.visualize(image, output_path=\"layout_viz.png\")\nviz.show()  # Display in notebook/viewer\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def visualize(\n    self,\n    image: \"Image.Image\",\n    output_path: Optional[Union[str, Path]] = None,\n    show_labels: bool = True,\n    show_confidence: bool = True,\n    line_width: int = 3,\n    font_size: int = 12,\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Visualize layout detection results on the image.\n\n    Draws bounding boxes with labels and confidence scores on the image.\n    Each layout category has a distinct color for easy identification.\n\n    Args:\n        image: PIL Image to draw on (will be copied, not modified)\n        output_path: Optional path to save the visualization\n        show_labels: Whether to show label text\n        show_confidence: Whether to show confidence scores\n        line_width: Width of bounding box lines\n        font_size: Size of label text (note: uses default font)\n\n    Returns:\n        PIL Image with visualizations drawn\n\n    Example:\n        ```python\n        result = extractor.extract(image)\n        viz = result.visualize(image, output_path=\"layout_viz.png\")\n        viz.show()  # Display in notebook/viewer\n        ```\n    \"\"\"\n    from PIL import ImageDraw\n\n    # Copy image to avoid modifying original\n    viz_image = image.copy().convert(\"RGB\")\n    draw = ImageDraw.Draw(viz_image)\n\n    for box in self.bboxes:\n        # Get color for this label\n        color = LABEL_COLORS.get(box.label, \"#95A5A6\")\n\n        # Draw bounding box\n        coords = box.bbox.to_xyxy()\n        draw.rectangle(coords, outline=color, width=line_width)\n\n        # Build label text\n        if show_labels or show_confidence:\n            label_parts = []\n            if show_labels:\n                label_parts.append(box.label.value)\n            if show_confidence:\n                label_parts.append(f\"{box.confidence:.2f}\")\n            label_text = \" \".join(label_parts)\n\n            # Draw label background\n            text_bbox = draw.textbbox((coords[0], coords[1] - 20), label_text)\n            draw.rectangle(text_bbox, fill=color)\n\n            # Draw label text\n            draw.text(\n                (coords[0], coords[1] - 20),\n                label_text,\n                fill=\"white\",\n            )\n\n    # Save if path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        viz_image.save(output_path)\n\n    return viz_image\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; LayoutOutput\n</code></pre> <p>Load a LayoutOutput instance from a JSON file.</p> <p>Reads a JSON file and deserializes its contents into a LayoutOutput object. Uses Pydantic's model_validate_json for proper handling of nested objects.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to JSON file containing serialized LayoutOutput data.       Can be string or pathlib.Path object.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>Deserialized layout output instance from file.</p> <p> TYPE: <code>LayoutOutput</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>UnicodeDecodeError</code> <p>If file cannot be decoded as UTF-8.</p> <code>ValueError</code> <p>If file contents are not valid JSON.</p> <code>ValidationError</code> <p>If JSON data doesn't match LayoutOutput schema.</p> Example <p><pre><code>output = LayoutOutput.load_json('layout_results.json')\nprint(f\"Found {output.element_count} elements\")\n</code></pre> Found 5 elements</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"LayoutOutput\":\n    \"\"\"\n    Load a LayoutOutput instance from a JSON file.\n\n    Reads a JSON file and deserializes its contents into a LayoutOutput object.\n    Uses Pydantic's model_validate_json for proper handling of nested objects.\n\n    Args:\n        file_path: Path to JSON file containing serialized LayoutOutput data.\n                  Can be string or pathlib.Path object.\n\n    Returns:\n        LayoutOutput: Deserialized layout output instance from file.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n        UnicodeDecodeError: If file cannot be decoded as UTF-8.\n        ValueError: If file contents are not valid JSON.\n        ValidationError: If JSON data doesn't match LayoutOutput schema.\n\n    Example:\n        ```python\n        output = LayoutOutput.load_json('layout_results.json')\n        print(f\"Found {output.element_count} elements\")\n        ```\n        Found 5 elements\n    \"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save LayoutOutput instance to a JSON file.</p> <p>Serializes the LayoutOutput object to JSON and writes it to a file. Automatically creates parent directories if they don't exist. Uses UTF-8 encoding for compatibility and proper handling of special characters.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path where JSON file should be saved. Can be string or       pathlib.Path object. Parent directories will be created       if they don't exist.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> RAISES DESCRIPTION <code>OSError</code> <p>If file cannot be written due to permission or disk errors.</p> <code>TypeError</code> <p>If file_path is not a string or Path object.</p> Example <pre><code>output = LayoutOutput(bboxes=[], image_width=800, image_height=600)\noutput.save_json('results/layout_output.json')\n# File is created at results/layout_output.json\n# Parent 'results' directory is created if it didn't exist\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"\n    Save LayoutOutput instance to a JSON file.\n\n    Serializes the LayoutOutput object to JSON and writes it to a file.\n    Automatically creates parent directories if they don't exist. Uses UTF-8\n    encoding for compatibility and proper handling of special characters.\n\n    Args:\n        file_path: Path where JSON file should be saved. Can be string or\n                  pathlib.Path object. Parent directories will be created\n                  if they don't exist.\n\n    Returns:\n        None\n\n    Raises:\n        OSError: If file cannot be written due to permission or disk errors.\n        TypeError: If file_path is not a string or Path object.\n\n    Example:\n        ```python\n        output = LayoutOutput(bboxes=[], image_width=800, image_height=600)\n        output.save_json('results/layout_output.json')\n        # File is created at results/layout_output.json\n        # Parent 'results' directory is created if it didn't exist\n        ```\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen","title":"qwen","text":"<p>Qwen3-VL backend configurations and detector for layout detection.</p> Available backends <ul> <li>QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend</li> <li>QwenLayoutVLLMConfig: VLLM high-throughput backend</li> <li>QwenLayoutMLXConfig: MLX backend for Apple Silicon</li> <li>QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</li> </ul> Example <pre><code>from omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\nconfig = QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutAPIConfig","title":"QwenLayoutAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen layout detection.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenLayoutAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutDetector","title":"QwenLayoutDetector","text":"<pre><code>QwenLayoutDetector(backend: QwenLayoutBackendConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>Qwen3-VL Vision-Language Model layout detector.</p> <p>A flexible VLM-based layout detector that supports custom labels. Unlike fixed-label models (DocLayoutYOLO, RT-DETR), Qwen can detect any document elements specified at runtime.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\n# Initialize with PyTorch backend\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Basic extraction with default labels\nresult = detector.extract(image)\n\n# With custom labels (strings)\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n\n# With typed custom labels\nlabels = [\n        CustomLabel(name=\"code_block\", color=\"#E74C3C\"),\n        CustomLabel(name=\"sidebar\", description=\"Side panel content\"),\n    ]\nresult = detector.extract(image, custom_labels=labels)\n</code></pre> <p>Initialize Qwen layout detector.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend - QwenLayoutVLLMConfig: VLLM high-throughput backend - QwenLayoutMLXConfig: MLX backend for Apple Silicon - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenLayoutBackendConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def __init__(self, backend: QwenLayoutBackendConfig):\n    \"\"\"\n    Initialize Qwen layout detector.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenLayoutVLLMConfig: VLLM high-throughput backend\n            - QwenLayoutMLXConfig: MLX backend for Apple Silicon\n            - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutDetector.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    custom_labels: Optional[\n        List[Union[str, CustomLabel]]\n    ] = None,\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout detection on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>custom_labels</code> <p>Optional custom labels to detect. Can be: - None: Use default labels (title, text, table, figure, etc.) - List[str]: Simple label names [\"code_block\", \"sidebar\"] - List[CustomLabel]: Typed labels with metadata</p> <p> TYPE: <code>Optional[List[Union[str, CustomLabel]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    custom_labels: Optional[List[Union[str, CustomLabel]]] = None,\n) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout detection on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        custom_labels: Optional custom labels to detect. Can be:\n            - None: Use default labels (title, text, table, figure, etc.)\n            - List[str]: Simple label names [\"code_block\", \"sidebar\"]\n            - List[CustomLabel]: Typed labels with metadata\n\n    Returns:\n        LayoutOutput with detected layout boxes\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Normalize labels\n    label_names = self._normalize_labels(custom_labels)\n\n    # Build prompt\n    prompt = self._build_detection_prompt(label_names)\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenLayoutPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenLayoutVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenLayoutMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenLayoutAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse detections\n    detections = self._parse_json_output(raw_output)\n\n    # Convert to LayoutOutput\n    layout_boxes = self._build_layout_boxes(detections, width, height)\n\n    # Sort by position (reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutMLXConfig","title":"QwenLayoutMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen layout detection.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenLayoutMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutPyTorchConfig","title":"QwenLayoutPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen layout detection.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutVLLMConfig","title":"QwenLayoutVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen layout detection.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.api","title":"api","text":"<p>API backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.api.QwenLayoutAPIConfig","title":"QwenLayoutAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen layout detection.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenLayoutAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.detector","title":"detector","text":"<p>Qwen3-VL layout detector.</p> <p>A Vision-Language Model for flexible layout detection with custom label support. Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = detector.extract(image)\n\n# With custom labels\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.detector.QwenLayoutDetector","title":"QwenLayoutDetector","text":"<pre><code>QwenLayoutDetector(backend: QwenLayoutBackendConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>Qwen3-VL Vision-Language Model layout detector.</p> <p>A flexible VLM-based layout detector that supports custom labels. Unlike fixed-label models (DocLayoutYOLO, RT-DETR), Qwen can detect any document elements specified at runtime.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\n# Initialize with PyTorch backend\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Basic extraction with default labels\nresult = detector.extract(image)\n\n# With custom labels (strings)\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n\n# With typed custom labels\nlabels = [\n        CustomLabel(name=\"code_block\", color=\"#E74C3C\"),\n        CustomLabel(name=\"sidebar\", description=\"Side panel content\"),\n    ]\nresult = detector.extract(image, custom_labels=labels)\n</code></pre> <p>Initialize Qwen layout detector.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend - QwenLayoutVLLMConfig: VLLM high-throughput backend - QwenLayoutMLXConfig: MLX backend for Apple Silicon - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenLayoutBackendConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def __init__(self, backend: QwenLayoutBackendConfig):\n    \"\"\"\n    Initialize Qwen layout detector.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenLayoutVLLMConfig: VLLM high-throughput backend\n            - QwenLayoutMLXConfig: MLX backend for Apple Silicon\n            - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.detector.QwenLayoutDetector.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    custom_labels: Optional[\n        List[Union[str, CustomLabel]]\n    ] = None,\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout detection on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>custom_labels</code> <p>Optional custom labels to detect. Can be: - None: Use default labels (title, text, table, figure, etc.) - List[str]: Simple label names [\"code_block\", \"sidebar\"] - List[CustomLabel]: Typed labels with metadata</p> <p> TYPE: <code>Optional[List[Union[str, CustomLabel]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    custom_labels: Optional[List[Union[str, CustomLabel]]] = None,\n) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout detection on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        custom_labels: Optional custom labels to detect. Can be:\n            - None: Use default labels (title, text, table, figure, etc.)\n            - List[str]: Simple label names [\"code_block\", \"sidebar\"]\n            - List[CustomLabel]: Typed labels with metadata\n\n    Returns:\n        LayoutOutput with detected layout boxes\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Normalize labels\n    label_names = self._normalize_labels(custom_labels)\n\n    # Build prompt\n    prompt = self._build_detection_prompt(label_names)\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenLayoutPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenLayoutVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenLayoutMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenLayoutAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse detections\n    detections = self._parse_json_output(raw_output)\n\n    # Convert to LayoutOutput\n    layout_boxes = self._build_layout_boxes(detections, width, height)\n\n    # Sort by position (reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.mlx","title":"mlx","text":"<p>MLX backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.mlx.QwenLayoutMLXConfig","title":"QwenLayoutMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen layout detection.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenLayoutMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.pytorch","title":"pytorch","text":"<p>PyTorch/HuggingFace backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.pytorch.QwenLayoutPyTorchConfig","title":"QwenLayoutPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen layout detection.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.vllm","title":"vllm","text":"<p>VLLM backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.qwen.vllm.QwenLayoutVLLMConfig","title":"QwenLayoutVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen layout detection.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.rtdetr","title":"rtdetr","text":"<p>RT-DETR layout extractor.</p> <p>A transformer-based real-time detection model for document layout detection. Uses HuggingFace Transformers implementation.</p> <p>Model: HuggingPanda/docling-layout</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.rtdetr.RTDETRConfig","title":"RTDETRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for RT-DETR layout extractor.</p> <p>This is a single-backend model (PyTorch/Transformers only).</p> Example <pre><code>config = RTDETRConfig(device=\"cuda\", confidence=0.4)\nextractor = RTDETRLayoutExtractor(config=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.rtdetr.RTDETRLayoutExtractor","title":"RTDETRLayoutExtractor","text":"<pre><code>RTDETRLayoutExtractor(config: RTDETRConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>RT-DETR layout extractor using HuggingFace Transformers.</p> <p>A transformer-based real-time detection model for document layout. Detects: title, text, table, figure, list, formula, captions, headers, footers.</p> <p>This is a single-backend model (PyTorch/Transformers only).</p> Example <pre><code>from omnidocs.tasks.layout_extraction import RTDETRLayoutExtractor, RTDETRConfig\n\nextractor = RTDETRLayoutExtractor(config=RTDETRConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre> <p>Initialize RT-DETR layout extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object with device, model settings, etc.</p> <p> TYPE: <code>RTDETRConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/rtdetr.py</code> <pre><code>def __init__(self, config: RTDETRConfig):\n    \"\"\"\n    Initialize RT-DETR layout extractor.\n\n    Args:\n        config: Configuration object with device, model settings, etc.\n    \"\"\"\n    self.config = config\n    self._model = None\n    self._processor = None\n    self._device = self._resolve_device(config.device)\n    self._model_path = self._resolve_model_path(config.model_path)\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.layout_extraction.rtdetr.RTDETRLayoutExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> Source code in <code>omnidocs/tasks/layout_extraction/rtdetr.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n    import torch\n\n    if self._model is None or self._processor is None:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    img_width, img_height = pil_image.size\n\n    # Preprocess\n    inputs = self._processor(\n        images=pil_image,\n        return_tensors=\"pt\",\n        size={\"height\": self.config.image_size, \"width\": self.config.image_size},\n    )\n\n    # Move to device\n    inputs = {k: v.to(self._device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n\n    # Run inference\n    with torch.no_grad():\n        outputs = self._model(**inputs)\n\n    # Post-process results\n    target_sizes = torch.tensor([[img_height, img_width]])\n    results = self._processor.post_process_object_detection(\n        outputs,\n        target_sizes=target_sizes,\n        threshold=self.config.confidence,\n    )[0]\n\n    # Parse detections\n    layout_boxes = []\n\n    for score, label_id, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n        confidence = float(score.item())\n        class_id = int(label_id.item())\n\n        # Get original label from model config\n        # Note: The model outputs 0-indexed class IDs, but id2label has background at index 0,\n        # so we add 1 to map correctly (e.g., model output 8 -&gt; id2label[9] = \"Table\")\n        original_label = self._model.config.id2label.get(class_id + 1, f\"class_{class_id}\")\n\n        # Map to standardized label\n        standard_label = RTDETR_MAPPING.to_standard(original_label)\n\n        # Box coordinates\n        box_coords = box.cpu().tolist()\n\n        layout_boxes.append(\n            LayoutBox(\n                label=standard_label,\n                bbox=BoundingBox.from_list(box_coords),\n                confidence=confidence,\n                class_id=class_id,\n                original_label=original_label,\n            )\n        )\n\n    # Sort by y-coordinate (top to bottom reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=img_width,\n        image_height=img_height,\n        model_name=\"RT-DETR (docling-layout)\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction","title":"ocr_extraction","text":"<p>OCR Extraction Module.</p> <p>Provides extractors for detecting text with bounding boxes from document images. Returns text content along with spatial coordinates (unlike Text Extraction which returns formatted Markdown/HTML without coordinates).</p> Available Extractors <ul> <li>TesseractOCR: Open-source OCR (CPU, requires system Tesseract)</li> <li>EasyOCR: PyTorch-based OCR (CPU/GPU, 80+ languages)</li> <li>PaddleOCR: PaddlePaddle-based OCR (CPU/GPU, excellent CJK support)</li> </ul> Key Difference from Text Extraction <ul> <li>OCR Extraction: Text + Bounding Boxes (spatial location)</li> <li>Text Extraction: Markdown/HTML (formatted document export)</li> </ul> Example <pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\n\nocr = TesseractOCR(config=TesseractOCRConfig(languages=[\"eng\"]))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()} (conf: {block.confidence:.2f})\")\n# With EasyOCR\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\nocr = EasyOCR(config=EasyOCRConfig(languages=[\"en\", \"ch_sim\"], gpu=True))\nresult = ocr.extract(image)\n# With PaddleOCR\nfrom omnidocs.tasks.ocr_extraction import PaddleOCR, PaddleOCRConfig\n\nocr = PaddleOCR(config=PaddleOCRConfig(lang=\"ch\", device=\"cpu\"))\nresult = ocr.extract(image)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BaseOCRExtractor","title":"BaseOCRExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for OCR extractors.</p> <p>All OCR extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyOCRExtractor(BaseOCRExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Initialize OCR engine\n            pass\n\n        def extract(self, image):\n            # Run OCR extraction\n            return OCROutput(...)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BaseOCRExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput containing detected text blocks with bounding boxes</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format is not supported</p> <code>RuntimeError</code> <p>If OCR engine is not initialized or extraction fails</p> Source code in <code>omnidocs/tasks/ocr_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR extraction on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n\n    Returns:\n        OCROutput containing detected text blocks with bounding boxes\n\n    Raises:\n        ValueError: If image format is not supported\n        RuntimeError: If OCR engine is not initialized or extraction fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BaseOCRExtractor.batch_extract","title":"batch_extract","text":"<pre><code>batch_extract(\n    images: List[Union[Image, ndarray, str, Path]],\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[OCROutput]\n</code></pre> <p>Run OCR extraction on multiple images.</p> <p>Default implementation loops over extract(). Subclasses can override for optimized batching.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of images in any supported format</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[OCROutput]</code> <p>List of OCROutput in same order as input</p> <p>Examples:</p> <pre><code>images = [doc.get_page(i) for i in range(doc.page_count)]\nresults = extractor.batch_extract(images)\n</code></pre> Source code in <code>omnidocs/tasks/ocr_extraction/base.py</code> <pre><code>def batch_extract(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[OCROutput]:\n    \"\"\"\n    Run OCR extraction on multiple images.\n\n    Default implementation loops over extract(). Subclasses can override\n    for optimized batching.\n\n    Args:\n        images: List of images in any supported format\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of OCROutput in same order as input\n\n    Examples:\n        ```python\n        images = [doc.get_page(i) for i in range(doc.page_count)]\n        results = extractor.batch_extract(images)\n        ```\n    \"\"\"\n    results = []\n    total = len(images)\n\n    for i, image in enumerate(images):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(image)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BaseOCRExtractor.extract_document","title":"extract_document","text":"<pre><code>extract_document(\n    document: Document,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[OCROutput]\n</code></pre> <p>Run OCR extraction on all pages of a document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[OCROutput]</code> <p>List of OCROutput, one per page</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresults = extractor.extract_document(doc)\n</code></pre> Source code in <code>omnidocs/tasks/ocr_extraction/base.py</code> <pre><code>def extract_document(\n    self,\n    document: \"Document\",\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[OCROutput]:\n    \"\"\"\n    Run OCR extraction on all pages of a document.\n\n    Args:\n        document: Document instance\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of OCROutput, one per page\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        results = extractor.extract_document(doc)\n        ```\n    \"\"\"\n    results = []\n    total = document.page_count\n\n    for i, page in enumerate(document.iter_pages()):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(page)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.EasyOCR","title":"EasyOCR","text":"<pre><code>EasyOCR(config: EasyOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>EasyOCR text extractor.</p> <p>Single-backend model (PyTorch - CPU/GPU).</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\nocr = EasyOCR(config=EasyOCRConfig(languages=[\"en\"], gpu=True))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize EasyOCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>EasyOCRConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If easyocr is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def __init__(self, config: EasyOCRConfig):\n    \"\"\"\n    Initialize EasyOCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        ImportError: If easyocr is not installed\n    \"\"\"\n    self.config = config\n    self._reader = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.EasyOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    detail: int = 1,\n    paragraph: bool = False,\n    min_size: int = 10,\n    text_threshold: float = 0.7,\n    low_text: float = 0.4,\n    link_threshold: float = 0.4,\n    canvas_size: int = 2560,\n    mag_ratio: float = 1.0,\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>detail</code> <p>0 = simple output, 1 = detailed with boxes</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>paragraph</code> <p>Combine results into paragraphs</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>min_size</code> <p>Minimum text box size</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>text_threshold</code> <p>Text confidence threshold</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.7</code> </p> <code>low_text</code> <p>Low text bound</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>link_threshold</code> <p>Link threshold for text joining</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>canvas_size</code> <p>Max image dimension for processing</p> <p> TYPE: <code>int</code> DEFAULT: <code>2560</code> </p> <code>mag_ratio</code> <p>Magnification ratio</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks</p> RAISES DESCRIPTION <code>ValueError</code> <p>If detail is not 0 or 1</p> <code>RuntimeError</code> <p>If EasyOCR is not initialized</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    detail: int = 1,\n    paragraph: bool = False,\n    min_size: int = 10,\n    text_threshold: float = 0.7,\n    low_text: float = 0.4,\n    link_threshold: float = 0.4,\n    canvas_size: int = 2560,\n    mag_ratio: float = 1.0,\n) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n        detail: 0 = simple output, 1 = detailed with boxes\n        paragraph: Combine results into paragraphs\n        min_size: Minimum text box size\n        text_threshold: Text confidence threshold\n        low_text: Low text bound\n        link_threshold: Link threshold for text joining\n        canvas_size: Max image dimension for processing\n        mag_ratio: Magnification ratio\n\n    Returns:\n        OCROutput with detected text blocks\n\n    Raises:\n        ValueError: If detail is not 0 or 1\n        RuntimeError: If EasyOCR is not initialized\n    \"\"\"\n    if self._reader is None:\n        raise RuntimeError(\"EasyOCR not initialized. Call _load_model() first.\")\n\n    # Validate detail parameter\n    if detail not in (0, 1):\n        raise ValueError(f\"detail must be 0 or 1, got {detail}\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Convert to numpy array for EasyOCR\n    image_array = np.array(pil_image)\n\n    # Run EasyOCR\n    results = self._reader.readtext(\n        image_array,\n        detail=detail,\n        paragraph=paragraph,\n        min_size=min_size,\n        text_threshold=text_threshold,\n        low_text=low_text,\n        link_threshold=link_threshold,\n        canvas_size=canvas_size,\n        mag_ratio=mag_ratio,\n    )\n\n    # Parse results\n    text_blocks = []\n    full_text_parts = []\n\n    for result in results:\n        if detail == 0:\n            # Simple output: just text\n            text = result\n            confidence = 1.0\n            bbox = BoundingBox(x1=0, y1=0, x2=0, y2=0)\n            polygon = None\n        else:\n            # Detailed output: [polygon, text, confidence]\n            polygon_points, text, confidence = result\n\n            # EasyOCR returns 4 corner points: [[x1,y1], [x2,y1], [x2,y2], [x1,y2]]\n            # Convert to list of lists for storage\n            polygon = [list(p) for p in polygon_points]\n\n            # Convert to axis-aligned bounding box\n            bbox = BoundingBox.from_polygon(polygon)\n\n        if not text.strip():\n            continue\n\n        text_blocks.append(\n            TextBlock(\n                text=text,\n                bbox=bbox,\n                confidence=float(confidence),\n                granularity=(OCRGranularity.LINE if paragraph else OCRGranularity.WORD),\n                polygon=polygon,\n                language=\"+\".join(self.config.languages),\n            )\n        )\n\n        full_text_parts.append(text)\n\n    # Sort by position\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\" \".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.EasyOCR.extract_batch","title":"extract_batch","text":"<pre><code>extract_batch(\n    images: List[Union[Image, ndarray, str, Path]], **kwargs\n) -&gt; List[OCROutput]\n</code></pre> <p>Run OCR on multiple images.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of input images</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>**kwargs</code> <p>Arguments passed to extract()</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>List[OCROutput]</code> <p>List of OCROutput objects</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def extract_batch(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    **kwargs,\n) -&gt; List[OCROutput]:\n    \"\"\"\n    Run OCR on multiple images.\n\n    Args:\n        images: List of input images\n        **kwargs: Arguments passed to extract()\n\n    Returns:\n        List of OCROutput objects\n    \"\"\"\n    results = []\n    for img in images:\n        results.append(self.extract(img, **kwargs))\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.EasyOCRConfig","title":"EasyOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for EasyOCR extractor.</p> <p>This is a single-backend model (PyTorch - CPU/GPU).</p> Example <pre><code>config = EasyOCRConfig(languages=[\"en\", \"ch_sim\"], gpu=True)\nocr = EasyOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box coordinates in pixel space.</p> <p>Coordinates follow the convention: (x1, y1) is top-left, (x2, y2) is bottom-right. For rotated text, use the polygon field in TextBlock instead.</p> Example <pre><code>bbox = BoundingBox(x1=100, y1=50, x2=300, y2=80)\nprint(bbox.width, bbox.height)  # 200, 30\nprint(bbox.center)  # (200.0, 65.0)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x, y, width, height) format.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_xywh(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x, y, width, height) format.\"\"\"\n    return (self.x1, self.y1, self.width, self.height)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.from_polygon","title":"from_polygon  <code>classmethod</code>","text":"<pre><code>from_polygon(polygon: List[List[float]]) -&gt; BoundingBox\n</code></pre> <p>Create axis-aligned bounding box from polygon points.</p> PARAMETER DESCRIPTION <code>polygon</code> <p>List of [x, y] points (usually 4 for quadrilateral)</p> <p> TYPE: <code>List[List[float]]</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox that encloses all polygon points</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef from_polygon(cls, polygon: List[List[float]]) -&gt; \"BoundingBox\":\n    \"\"\"\n    Create axis-aligned bounding box from polygon points.\n\n    Args:\n        polygon: List of [x, y] points (usually 4 for quadrilateral)\n\n    Returns:\n        BoundingBox that encloses all polygon points\n    \"\"\"\n    if not polygon:\n        raise ValueError(\"Polygon cannot be empty\")\n\n    xs = [p[0] for p in polygon]\n    ys = [p[1] for p in polygon]\n    return cls(x1=min(xs), y1=min(ys), x2=max(xs), y2=max(ys))\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> <p>Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas. This provides consistent coordinates regardless of original image size.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas.\n    This provides consistent coordinates regardless of original image size.\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.to_absolute","title":"to_absolute","text":"<pre><code>to_absolute(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert from normalized (0-1024) to absolute pixel coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Target image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Target image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with absolute pixel coordinates</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_absolute(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert from normalized (0-1024) to absolute pixel coordinates.\n\n    Args:\n        image_width: Target image width in pixels\n        image_height: Target image height in pixels\n\n    Returns:\n        New BoundingBox with absolute pixel coordinates\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / NORMALIZED_SIZE * image_width,\n        y1=self.y1 / NORMALIZED_SIZE * image_height,\n        x2=self.x2 / NORMALIZED_SIZE * image_width,\n        y2=self.y2 / NORMALIZED_SIZE * image_height,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCRGranularity","title":"OCRGranularity","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>OCR detection granularity levels.</p> <p>Different OCR engines return results at different granularity levels. This enum standardizes the options across all extractors.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput","title":"OCROutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete OCR extraction results for a single image.</p> <p>Contains all detected text blocks with their bounding boxes, plus metadata about the extraction.</p> Example <pre><code>result = ocr.extract(image)\nprint(f\"Found {result.block_count} blocks\")\nprint(f\"Full text: {result.full_text}\")\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.block_count","title":"block_count  <code>property</code>","text":"<pre><code>block_count: int\n</code></pre> <p>Number of detected text blocks.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.word_count","title":"word_count  <code>property</code>","text":"<pre><code>word_count: int\n</code></pre> <p>Approximate word count from full text.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.average_confidence","title":"average_confidence  <code>property</code>","text":"<pre><code>average_confidence: float\n</code></pre> <p>Average confidence across all text blocks.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(\n    min_confidence: float,\n) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by minimum confidence.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def filter_by_confidence(self, min_confidence: float) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by minimum confidence.\"\"\"\n    return [b for b in self.text_blocks if b.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.filter_by_granularity","title":"filter_by_granularity","text":"<pre><code>filter_by_granularity(\n    granularity: OCRGranularity,\n) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by granularity level.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def filter_by_granularity(self, granularity: OCRGranularity) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by granularity level.\"\"\"\n    return [b for b in self.text_blocks if b.granularity == granularity]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"text_blocks\": [b.to_dict() for b in self.text_blocks],\n        \"full_text\": self.full_text,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"languages_detected\": self.languages_detected,\n        \"block_count\": self.block_count,\n        \"word_count\": self.word_count,\n        \"average_confidence\": self.average_confidence,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.sort_by_position","title":"sort_by_position","text":"<pre><code>sort_by_position(top_to_bottom: bool = True) -&gt; OCROutput\n</code></pre> <p>Return a new OCROutput with blocks sorted by position.</p> PARAMETER DESCRIPTION <code>top_to_bottom</code> <p>If True, sort by y-coordinate (reading order)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>New OCROutput with sorted text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def sort_by_position(self, top_to_bottom: bool = True) -&gt; \"OCROutput\":\n    \"\"\"\n    Return a new OCROutput with blocks sorted by position.\n\n    Args:\n        top_to_bottom: If True, sort by y-coordinate (reading order)\n\n    Returns:\n        New OCROutput with sorted text blocks\n    \"\"\"\n    sorted_blocks = sorted(\n        self.text_blocks,\n        key=lambda b: (b.bbox.y1, b.bbox.x1),\n        reverse=not top_to_bottom,\n    )\n    # Regenerate full_text in sorted order\n    full_text = \" \".join(b.text for b in sorted_blocks)\n\n    return OCROutput(\n        text_blocks=sorted_blocks,\n        full_text=full_text,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        model_name=self.model_name,\n        languages_detected=self.languages_detected,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.get_normalized_blocks","title":"get_normalized_blocks","text":"<pre><code>get_normalized_blocks() -&gt; List[Dict]\n</code></pre> <p>Get all text blocks with normalized (0-1024) coordinates.</p> RETURNS DESCRIPTION <code>List[Dict]</code> <p>List of dicts with normalized bbox coordinates and metadata.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def get_normalized_blocks(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all text blocks with normalized (0-1024) coordinates.\n\n    Returns:\n        List of dicts with normalized bbox coordinates and metadata.\n    \"\"\"\n    normalized = []\n    for block in self.text_blocks:\n        norm_bbox = block.bbox.to_normalized(self.image_width, self.image_height)\n        normalized.append(\n            {\n                \"text\": block.text,\n                \"bbox\": norm_bbox.to_list(),\n                \"confidence\": block.confidence,\n                \"granularity\": block.granularity.value,\n                \"language\": block.language,\n            }\n        )\n    return normalized\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.visualize","title":"visualize","text":"<pre><code>visualize(\n    image: Image,\n    output_path: Optional[Union[str, Path]] = None,\n    show_text: bool = True,\n    show_confidence: bool = False,\n    line_width: int = 2,\n    box_color: str = \"#2ECC71\",\n    text_color: str = \"#000000\",\n) -&gt; Image.Image\n</code></pre> <p>Visualize OCR results on the image.</p> <p>Draws bounding boxes around detected text with optional labels.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to draw on (will be copied, not modified)</p> <p> TYPE: <code>Image</code> </p> <code>output_path</code> <p>Optional path to save the visualization</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> <code>show_text</code> <p>Whether to show detected text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_confidence</code> <p>Whether to show confidence scores</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>line_width</code> <p>Width of bounding box lines</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>box_color</code> <p>Color for bounding boxes (hex)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'#2ECC71'</code> </p> <code>text_color</code> <p>Color for text labels (hex)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'#000000'</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image with visualizations drawn</p> Example <pre><code>result = ocr.extract(image)\nviz = result.visualize(image, output_path=\"ocr_viz.png\")\n</code></pre> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def visualize(\n    self,\n    image: \"Image.Image\",\n    output_path: Optional[Union[str, Path]] = None,\n    show_text: bool = True,\n    show_confidence: bool = False,\n    line_width: int = 2,\n    box_color: str = \"#2ECC71\",\n    text_color: str = \"#000000\",\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Visualize OCR results on the image.\n\n    Draws bounding boxes around detected text with optional labels.\n\n    Args:\n        image: PIL Image to draw on (will be copied, not modified)\n        output_path: Optional path to save the visualization\n        show_text: Whether to show detected text\n        show_confidence: Whether to show confidence scores\n        line_width: Width of bounding box lines\n        box_color: Color for bounding boxes (hex)\n        text_color: Color for text labels (hex)\n\n    Returns:\n        PIL Image with visualizations drawn\n\n    Example:\n        ```python\n        result = ocr.extract(image)\n        viz = result.visualize(image, output_path=\"ocr_viz.png\")\n        ```\n    \"\"\"\n    from PIL import ImageDraw, ImageFont\n\n    # Copy image to avoid modifying original\n    viz_image = image.copy().convert(\"RGB\")\n    draw = ImageDraw.Draw(viz_image)\n\n    # Try to get a font\n    try:\n        font = ImageFont.truetype(\"/System/Library/Fonts/Helvetica.ttc\", 12)\n    except Exception:\n        try:\n            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 12)\n        except Exception:\n            font = ImageFont.load_default()\n\n    for block in self.text_blocks:\n        coords = block.bbox.to_xyxy()\n\n        # Draw polygon if available, otherwise draw rectangle\n        if block.polygon:\n            flat_polygon = [coord for point in block.polygon for coord in point]\n            draw.polygon(flat_polygon, outline=box_color, width=line_width)\n        else:\n            draw.rectangle(coords, outline=box_color, width=line_width)\n\n        # Build label text\n        if show_text or show_confidence:\n            label_parts = []\n            if show_text:\n                # Truncate long text\n                text = block.text[:25] + \"...\" if len(block.text) &gt; 25 else block.text\n                label_parts.append(text)\n            if show_confidence:\n                label_parts.append(f\"{block.confidence:.2f}\")\n            label_text = \" | \".join(label_parts)\n\n            # Position label below the box\n            label_x = coords[0]\n            label_y = coords[3] + 2  # Below bottom edge\n\n            # Draw label with background\n            text_bbox = draw.textbbox((label_x, label_y), label_text, font=font)\n            padding = 2\n            draw.rectangle(\n                [\n                    text_bbox[0] - padding,\n                    text_bbox[1] - padding,\n                    text_bbox[2] + padding,\n                    text_bbox[3] + padding,\n                ],\n                fill=\"#FFFFFF\",\n                outline=box_color,\n            )\n            draw.text((label_x, label_y), label_text, fill=text_color, font=font)\n\n    # Save if path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        viz_image.save(output_path)\n\n    return viz_image\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; OCROutput\n</code></pre> <p>Load an OCROutput instance from a JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to JSON file</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput instance</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"OCROutput\":\n    \"\"\"\n    Load an OCROutput instance from a JSON file.\n\n    Args:\n        file_path: Path to JSON file\n\n    Returns:\n        OCROutput instance\n    \"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.OCROutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save OCROutput instance to a JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path where JSON file should be saved</p> <p> TYPE: <code>Union[str, Path]</code> </p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"\n    Save OCROutput instance to a JSON file.\n\n    Args:\n        file_path: Path where JSON file should be saved\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.TextBlock","title":"TextBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single detected text element with text, bounding box, and confidence.</p> <p>This is the fundamental unit of OCR output - can represent a character, word, line, or block depending on the OCR model and configuration.</p> Example <pre><code>block = TextBlock(\n        text=\"Hello\",\n        bbox=BoundingBox(x1=100, y1=50, x2=200, y2=80),\n        confidence=0.95,\n        granularity=OCRGranularity.WORD,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.TextBlock.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"text\": self.text,\n        \"bbox\": self.bbox.to_list(),\n        \"confidence\": self.confidence,\n        \"granularity\": self.granularity.value,\n        \"polygon\": self.polygon,\n        \"language\": self.language,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.TextBlock.get_normalized_bbox","title":"get_normalized_bbox","text":"<pre><code>get_normalized_bbox(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Get bounding box in normalized (0-1024) coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox with normalized coordinates</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def get_normalized_bbox(self, image_width: int, image_height: int) -&gt; BoundingBox:\n    \"\"\"\n    Get bounding box in normalized (0-1024) coordinates.\n\n    Args:\n        image_width: Original image width\n        image_height: Original image height\n\n    Returns:\n        BoundingBox with normalized coordinates\n    \"\"\"\n    return self.bbox.to_normalized(image_width, image_height)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.PaddleOCR","title":"PaddleOCR","text":"<pre><code>PaddleOCR(config: PaddleOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>PaddleOCR text extractor.</p> <p>Single-backend model (PaddlePaddle - CPU/GPU).</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import PaddleOCR, PaddleOCRConfig\n\nocr = PaddleOCR(config=PaddleOCRConfig(lang=\"en\", device=\"cpu\"))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize PaddleOCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>PaddleOCRConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If paddleocr or paddlepaddle is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/paddleocr.py</code> <pre><code>def __init__(self, config: PaddleOCRConfig):\n    \"\"\"\n    Initialize PaddleOCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        ImportError: If paddleocr or paddlepaddle is not installed\n    \"\"\"\n    self.config = config\n    self._ocr = None\n\n    # Normalize language code\n    self._lang = LANG_CODES.get(config.lang.lower(), config.lang)\n\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.PaddleOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/paddleocr.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with detected text blocks\n    \"\"\"\n    if self._ocr is None:\n        raise RuntimeError(\"PaddleOCR not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Convert to numpy array\n    image_array = np.array(pil_image)\n\n    # Run PaddleOCR v3.x - use predict() method\n    results = self._ocr.predict(image_array)\n\n    # Parse results\n    text_blocks = []\n\n    # PaddleOCR may return None or empty results\n    if results is None or len(results) == 0:\n        return OCROutput(\n            text_blocks=[],\n            full_text=\"\",\n            image_width=image_width,\n            image_height=image_height,\n            model_name=self.MODEL_NAME,\n            languages_detected=[self._lang],\n        )\n\n    # PaddleOCR v3.x returns list of dicts with 'rec_texts', 'rec_scores', 'dt_polys'\n    for result in results:\n        if result is None:\n            continue\n\n        rec_texts = result.get(\"rec_texts\", [])\n        rec_scores = result.get(\"rec_scores\", [])\n        dt_polys = result.get(\"dt_polys\", [])\n\n        for i, text in enumerate(rec_texts):\n            if not text.strip():\n                continue\n\n            confidence = rec_scores[i] if i &lt; len(rec_scores) else 1.0\n\n            # Get polygon and convert to list\n            polygon: Optional[List[List[float]]] = None\n            if i &lt; len(dt_polys) and dt_polys[i] is not None:\n                poly_array = dt_polys[i]\n                # Handle numpy array\n                if hasattr(poly_array, \"tolist\"):\n                    polygon = poly_array.tolist()\n                else:\n                    polygon = list(poly_array)\n\n            # Convert polygon to bbox\n            if polygon:\n                bbox = BoundingBox.from_polygon(polygon)\n            else:\n                bbox = BoundingBox(x1=0, y1=0, x2=0, y2=0)\n\n            text_blocks.append(\n                TextBlock(\n                    text=text,\n                    bbox=bbox,\n                    confidence=float(confidence),\n                    granularity=OCRGranularity.LINE,\n                    polygon=polygon,\n                    language=self._lang,\n                )\n            )\n\n    # Sort by position (top to bottom, left to right)\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    # Build full_text from sorted blocks to ensure reading order\n    full_text = \" \".join(block.text for block in text_blocks)\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=full_text,\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=[self._lang],\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.PaddleOCRConfig","title":"PaddleOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for PaddleOCR extractor.</p> <p>This is a single-backend model (PaddlePaddle - CPU/GPU).</p> Example <pre><code>config = PaddleOCRConfig(lang=\"ch\", device=\"gpu\")\nocr = PaddleOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.TesseractOCR","title":"TesseractOCR","text":"<pre><code>TesseractOCR(config: TesseractOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>Tesseract OCR extractor.</p> <p>Single-backend model (CPU only). Requires system Tesseract installation.</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\n\nocr = TesseractOCR(config=TesseractOCRConfig(languages=[\"eng\"]))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize Tesseract OCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>TesseractOCRConfig</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If Tesseract is not installed</p> <code>ImportError</code> <p>If pytesseract is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def __init__(self, config: TesseractOCRConfig):\n    \"\"\"\n    Initialize Tesseract OCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        RuntimeError: If Tesseract is not installed\n        ImportError: If pytesseract is not installed\n    \"\"\"\n    self.config = config\n    self._pytesseract = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.TesseractOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks at word level</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with detected text blocks at word level\n    \"\"\"\n    if self._pytesseract is None:\n        raise RuntimeError(\"Tesseract not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Build config string\n    config = f\"--oem {self.config.oem} --psm {self.config.psm}\"\n    if self.config.config_params:\n        for key, value in self.config.config_params.items():\n            config += f\" -c {key}={value}\"\n\n    # Language string\n    lang_str = \"+\".join(self.config.languages)\n\n    # Get detailed data (word-level boxes)\n    data = self._pytesseract.image_to_data(\n        pil_image,\n        lang=lang_str,\n        config=config,\n        output_type=self._pytesseract.Output.DICT,\n    )\n\n    # Parse results into TextBlocks\n    text_blocks = []\n    full_text_parts = []\n\n    n_boxes = len(data[\"text\"])\n    for i in range(n_boxes):\n        text = data[\"text\"][i].strip()\n        # Safely convert conf to float (handles string values from some Tesseract versions)\n        try:\n            conf = float(data[\"conf\"][i])\n        except (ValueError, TypeError):\n            conf = -1\n\n        # Skip empty text or low confidence (-1 means no confidence)\n        if not text or conf == -1:\n            continue\n\n        # Tesseract returns confidence as 0-100, normalize to 0-1\n        confidence = conf / 100.0\n\n        # Get bounding box\n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n\n        bbox = BoundingBox(\n            x1=float(x),\n            y1=float(y),\n            x2=float(x + w),\n            y2=float(y + h),\n        )\n\n        text_blocks.append(\n            TextBlock(\n                text=text,\n                bbox=bbox,\n                confidence=confidence,\n                granularity=OCRGranularity.WORD,\n                language=lang_str,\n            )\n        )\n\n        full_text_parts.append(text)\n\n    # Sort by position (top to bottom, left to right)\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\" \".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.TesseractOCR.extract_lines","title":"extract_lines","text":"<pre><code>extract_lines(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR and return line-level blocks.</p> <p>Groups words into lines based on Tesseract's line detection.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with line-level text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def extract_lines(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR and return line-level blocks.\n\n    Groups words into lines based on Tesseract's line detection.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with line-level text blocks\n    \"\"\"\n    if self._pytesseract is None:\n        raise RuntimeError(\"Tesseract not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Build config string (including config_params like extract method)\n    config = f\"--oem {self.config.oem} --psm {self.config.psm}\"\n    if self.config.config_params:\n        for key, value in self.config.config_params.items():\n            config += f\" -c {key}={value}\"\n\n    # Language string\n    lang_str = \"+\".join(self.config.languages)\n\n    # Get detailed data\n    data = self._pytesseract.image_to_data(\n        pil_image,\n        lang=lang_str,\n        config=config,\n        output_type=self._pytesseract.Output.DICT,\n    )\n\n    # Group words into lines\n    lines: Dict[tuple, Dict] = {}\n    n_boxes = len(data[\"text\"])\n\n    for i in range(n_boxes):\n        text = data[\"text\"][i].strip()\n        # Safely convert conf to float (handles string values from some Tesseract versions)\n        try:\n            conf = float(data[\"conf\"][i])\n        except (ValueError, TypeError):\n            conf = -1\n\n        if not text or conf == -1:\n            continue\n\n        # Tesseract provides block_num, par_num, line_num\n        line_key = (data[\"block_num\"][i], data[\"par_num\"][i], data[\"line_num\"][i])\n\n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n\n        if line_key not in lines:\n            lines[line_key] = {\n                \"words\": [],\n                \"confidences\": [],\n                \"x1\": x,\n                \"y1\": y,\n                \"x2\": x + w,\n                \"y2\": y + h,\n            }\n\n        lines[line_key][\"words\"].append(text)\n        lines[line_key][\"confidences\"].append(conf / 100.0)\n        lines[line_key][\"x1\"] = min(lines[line_key][\"x1\"], x)\n        lines[line_key][\"y1\"] = min(lines[line_key][\"y1\"], y)\n        lines[line_key][\"x2\"] = max(lines[line_key][\"x2\"], x + w)\n        lines[line_key][\"y2\"] = max(lines[line_key][\"y2\"], y + h)\n\n    # Convert to TextBlocks\n    text_blocks = []\n    full_text_parts = []\n\n    for line_key in sorted(lines.keys()):\n        line = lines[line_key]\n        line_text = \" \".join(line[\"words\"])\n        avg_conf = sum(line[\"confidences\"]) / len(line[\"confidences\"])\n\n        bbox = BoundingBox(\n            x1=float(line[\"x1\"]),\n            y1=float(line[\"y1\"]),\n            x2=float(line[\"x2\"]),\n            y2=float(line[\"y2\"]),\n        )\n\n        text_blocks.append(\n            TextBlock(\n                text=line_text,\n                bbox=bbox,\n                confidence=avg_conf,\n                granularity=OCRGranularity.LINE,\n                language=lang_str,\n            )\n        )\n\n        full_text_parts.append(line_text)\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\"\\n\".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.TesseractOCRConfig","title":"TesseractOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Tesseract OCR extractor.</p> <p>This is a single-backend model (CPU only, requires system Tesseract).</p> Example <pre><code>config = TesseractOCRConfig(languages=[\"eng\", \"fra\"], psm=3)\nocr = TesseractOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.base","title":"base","text":"<p>Base class for OCR extractors.</p> <p>Defines the abstract interface that all OCR extractors must implement.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor","title":"BaseOCRExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for OCR extractors.</p> <p>All OCR extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyOCRExtractor(BaseOCRExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Initialize OCR engine\n            pass\n\n        def extract(self, image):\n            # Run OCR extraction\n            return OCROutput(...)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput containing detected text blocks with bounding boxes</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format is not supported</p> <code>RuntimeError</code> <p>If OCR engine is not initialized or extraction fails</p> Source code in <code>omnidocs/tasks/ocr_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR extraction on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n\n    Returns:\n        OCROutput containing detected text blocks with bounding boxes\n\n    Raises:\n        ValueError: If image format is not supported\n        RuntimeError: If OCR engine is not initialized or extraction fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.batch_extract","title":"batch_extract","text":"<pre><code>batch_extract(\n    images: List[Union[Image, ndarray, str, Path]],\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[OCROutput]\n</code></pre> <p>Run OCR extraction on multiple images.</p> <p>Default implementation loops over extract(). Subclasses can override for optimized batching.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of images in any supported format</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[OCROutput]</code> <p>List of OCROutput in same order as input</p> <p>Examples:</p> <pre><code>images = [doc.get_page(i) for i in range(doc.page_count)]\nresults = extractor.batch_extract(images)\n</code></pre> Source code in <code>omnidocs/tasks/ocr_extraction/base.py</code> <pre><code>def batch_extract(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[OCROutput]:\n    \"\"\"\n    Run OCR extraction on multiple images.\n\n    Default implementation loops over extract(). Subclasses can override\n    for optimized batching.\n\n    Args:\n        images: List of images in any supported format\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of OCROutput in same order as input\n\n    Examples:\n        ```python\n        images = [doc.get_page(i) for i in range(doc.page_count)]\n        results = extractor.batch_extract(images)\n        ```\n    \"\"\"\n    results = []\n    total = len(images)\n\n    for i, image in enumerate(images):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(image)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.extract_document","title":"extract_document","text":"<pre><code>extract_document(\n    document: Document,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[OCROutput]\n</code></pre> <p>Run OCR extraction on all pages of a document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[OCROutput]</code> <p>List of OCROutput, one per page</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresults = extractor.extract_document(doc)\n</code></pre> Source code in <code>omnidocs/tasks/ocr_extraction/base.py</code> <pre><code>def extract_document(\n    self,\n    document: \"Document\",\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[OCROutput]:\n    \"\"\"\n    Run OCR extraction on all pages of a document.\n\n    Args:\n        document: Document instance\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of OCROutput, one per page\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        results = extractor.extract_document(doc)\n        ```\n    \"\"\"\n    results = []\n    total = document.page_count\n\n    for i, page in enumerate(document.iter_pages()):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(page)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.easyocr","title":"easyocr","text":"<p>EasyOCR extractor.</p> <p>EasyOCR is a PyTorch-based OCR engine with excellent multi-language support. - GPU accelerated (optional) - Supports 80+ languages - Good for scene text and printed documents</p> Python Package <p>pip install easyocr</p> Model Download Location <p>By default, EasyOCR downloads models to ~/.EasyOCR/ Can be overridden with model_storage_directory parameter</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCRConfig","title":"EasyOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for EasyOCR extractor.</p> <p>This is a single-backend model (PyTorch - CPU/GPU).</p> Example <pre><code>config = EasyOCRConfig(languages=[\"en\", \"ch_sim\"], gpu=True)\nocr = EasyOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCR","title":"EasyOCR","text":"<pre><code>EasyOCR(config: EasyOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>EasyOCR text extractor.</p> <p>Single-backend model (PyTorch - CPU/GPU).</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\nocr = EasyOCR(config=EasyOCRConfig(languages=[\"en\"], gpu=True))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize EasyOCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>EasyOCRConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If easyocr is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def __init__(self, config: EasyOCRConfig):\n    \"\"\"\n    Initialize EasyOCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        ImportError: If easyocr is not installed\n    \"\"\"\n    self.config = config\n    self._reader = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    detail: int = 1,\n    paragraph: bool = False,\n    min_size: int = 10,\n    text_threshold: float = 0.7,\n    low_text: float = 0.4,\n    link_threshold: float = 0.4,\n    canvas_size: int = 2560,\n    mag_ratio: float = 1.0,\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>detail</code> <p>0 = simple output, 1 = detailed with boxes</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>paragraph</code> <p>Combine results into paragraphs</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>min_size</code> <p>Minimum text box size</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>text_threshold</code> <p>Text confidence threshold</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.7</code> </p> <code>low_text</code> <p>Low text bound</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>link_threshold</code> <p>Link threshold for text joining</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>canvas_size</code> <p>Max image dimension for processing</p> <p> TYPE: <code>int</code> DEFAULT: <code>2560</code> </p> <code>mag_ratio</code> <p>Magnification ratio</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks</p> RAISES DESCRIPTION <code>ValueError</code> <p>If detail is not 0 or 1</p> <code>RuntimeError</code> <p>If EasyOCR is not initialized</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    detail: int = 1,\n    paragraph: bool = False,\n    min_size: int = 10,\n    text_threshold: float = 0.7,\n    low_text: float = 0.4,\n    link_threshold: float = 0.4,\n    canvas_size: int = 2560,\n    mag_ratio: float = 1.0,\n) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n        detail: 0 = simple output, 1 = detailed with boxes\n        paragraph: Combine results into paragraphs\n        min_size: Minimum text box size\n        text_threshold: Text confidence threshold\n        low_text: Low text bound\n        link_threshold: Link threshold for text joining\n        canvas_size: Max image dimension for processing\n        mag_ratio: Magnification ratio\n\n    Returns:\n        OCROutput with detected text blocks\n\n    Raises:\n        ValueError: If detail is not 0 or 1\n        RuntimeError: If EasyOCR is not initialized\n    \"\"\"\n    if self._reader is None:\n        raise RuntimeError(\"EasyOCR not initialized. Call _load_model() first.\")\n\n    # Validate detail parameter\n    if detail not in (0, 1):\n        raise ValueError(f\"detail must be 0 or 1, got {detail}\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Convert to numpy array for EasyOCR\n    image_array = np.array(pil_image)\n\n    # Run EasyOCR\n    results = self._reader.readtext(\n        image_array,\n        detail=detail,\n        paragraph=paragraph,\n        min_size=min_size,\n        text_threshold=text_threshold,\n        low_text=low_text,\n        link_threshold=link_threshold,\n        canvas_size=canvas_size,\n        mag_ratio=mag_ratio,\n    )\n\n    # Parse results\n    text_blocks = []\n    full_text_parts = []\n\n    for result in results:\n        if detail == 0:\n            # Simple output: just text\n            text = result\n            confidence = 1.0\n            bbox = BoundingBox(x1=0, y1=0, x2=0, y2=0)\n            polygon = None\n        else:\n            # Detailed output: [polygon, text, confidence]\n            polygon_points, text, confidence = result\n\n            # EasyOCR returns 4 corner points: [[x1,y1], [x2,y1], [x2,y2], [x1,y2]]\n            # Convert to list of lists for storage\n            polygon = [list(p) for p in polygon_points]\n\n            # Convert to axis-aligned bounding box\n            bbox = BoundingBox.from_polygon(polygon)\n\n        if not text.strip():\n            continue\n\n        text_blocks.append(\n            TextBlock(\n                text=text,\n                bbox=bbox,\n                confidence=float(confidence),\n                granularity=(OCRGranularity.LINE if paragraph else OCRGranularity.WORD),\n                polygon=polygon,\n                language=\"+\".join(self.config.languages),\n            )\n        )\n\n        full_text_parts.append(text)\n\n    # Sort by position\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\" \".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCR.extract_batch","title":"extract_batch","text":"<pre><code>extract_batch(\n    images: List[Union[Image, ndarray, str, Path]], **kwargs\n) -&gt; List[OCROutput]\n</code></pre> <p>Run OCR on multiple images.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of input images</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>**kwargs</code> <p>Arguments passed to extract()</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>List[OCROutput]</code> <p>List of OCROutput objects</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def extract_batch(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    **kwargs,\n) -&gt; List[OCROutput]:\n    \"\"\"\n    Run OCR on multiple images.\n\n    Args:\n        images: List of input images\n        **kwargs: Arguments passed to extract()\n\n    Returns:\n        List of OCROutput objects\n    \"\"\"\n    results = []\n    for img in images:\n        results.append(self.extract(img, **kwargs))\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models","title":"models","text":"<p>Pydantic models for OCR extraction outputs.</p> <p>Defines standardized output types for OCR detection including text blocks with bounding boxes, confidence scores, and granularity levels.</p> <p>Key difference from Text Extraction: - OCR returns text WITH bounding boxes (word/line/character level) - Text Extraction returns formatted text (MD/HTML) WITHOUT bboxes</p> Coordinate Systems <ul> <li>Absolute (default): Coordinates in pixels relative to original image size</li> <li>Normalized (0-1024): Coordinates scaled to 0-1024 range (virtual 1024x1024 canvas)</li> </ul> <p>Use <code>bbox.to_normalized(width, height)</code> or <code>output.get_normalized_blocks()</code> to convert to normalized coordinates.</p> Example <pre><code>result = ocr.extract(image)  # Returns absolute pixel coordinates\nnormalized = result.get_normalized_blocks()  # Returns 0-1024 normalized coords\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCRGranularity","title":"OCRGranularity","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>OCR detection granularity levels.</p> <p>Different OCR engines return results at different granularity levels. This enum standardizes the options across all extractors.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box coordinates in pixel space.</p> <p>Coordinates follow the convention: (x1, y1) is top-left, (x2, y2) is bottom-right. For rotated text, use the polygon field in TextBlock instead.</p> Example <pre><code>bbox = BoundingBox(x1=100, y1=50, x2=300, y2=80)\nprint(bbox.width, bbox.height)  # 200, 30\nprint(bbox.center)  # (200.0, 65.0)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x, y, width, height) format.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_xywh(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x, y, width, height) format.\"\"\"\n    return (self.x1, self.y1, self.width, self.height)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.from_polygon","title":"from_polygon  <code>classmethod</code>","text":"<pre><code>from_polygon(polygon: List[List[float]]) -&gt; BoundingBox\n</code></pre> <p>Create axis-aligned bounding box from polygon points.</p> PARAMETER DESCRIPTION <code>polygon</code> <p>List of [x, y] points (usually 4 for quadrilateral)</p> <p> TYPE: <code>List[List[float]]</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox that encloses all polygon points</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef from_polygon(cls, polygon: List[List[float]]) -&gt; \"BoundingBox\":\n    \"\"\"\n    Create axis-aligned bounding box from polygon points.\n\n    Args:\n        polygon: List of [x, y] points (usually 4 for quadrilateral)\n\n    Returns:\n        BoundingBox that encloses all polygon points\n    \"\"\"\n    if not polygon:\n        raise ValueError(\"Polygon cannot be empty\")\n\n    xs = [p[0] for p in polygon]\n    ys = [p[1] for p in polygon]\n    return cls(x1=min(xs), y1=min(ys), x2=max(xs), y2=max(ys))\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> <p>Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas. This provides consistent coordinates regardless of original image size.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas.\n    This provides consistent coordinates regardless of original image size.\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_absolute","title":"to_absolute","text":"<pre><code>to_absolute(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert from normalized (0-1024) to absolute pixel coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Target image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Target image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with absolute pixel coordinates</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_absolute(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert from normalized (0-1024) to absolute pixel coordinates.\n\n    Args:\n        image_width: Target image width in pixels\n        image_height: Target image height in pixels\n\n    Returns:\n        New BoundingBox with absolute pixel coordinates\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / NORMALIZED_SIZE * image_width,\n        y1=self.y1 / NORMALIZED_SIZE * image_height,\n        x2=self.x2 / NORMALIZED_SIZE * image_width,\n        y2=self.y2 / NORMALIZED_SIZE * image_height,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.TextBlock","title":"TextBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single detected text element with text, bounding box, and confidence.</p> <p>This is the fundamental unit of OCR output - can represent a character, word, line, or block depending on the OCR model and configuration.</p> Example <pre><code>block = TextBlock(\n        text=\"Hello\",\n        bbox=BoundingBox(x1=100, y1=50, x2=200, y2=80),\n        confidence=0.95,\n        granularity=OCRGranularity.WORD,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.TextBlock.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"text\": self.text,\n        \"bbox\": self.bbox.to_list(),\n        \"confidence\": self.confidence,\n        \"granularity\": self.granularity.value,\n        \"polygon\": self.polygon,\n        \"language\": self.language,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.TextBlock.get_normalized_bbox","title":"get_normalized_bbox","text":"<pre><code>get_normalized_bbox(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Get bounding box in normalized (0-1024) coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox with normalized coordinates</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def get_normalized_bbox(self, image_width: int, image_height: int) -&gt; BoundingBox:\n    \"\"\"\n    Get bounding box in normalized (0-1024) coordinates.\n\n    Args:\n        image_width: Original image width\n        image_height: Original image height\n\n    Returns:\n        BoundingBox with normalized coordinates\n    \"\"\"\n    return self.bbox.to_normalized(image_width, image_height)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput","title":"OCROutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete OCR extraction results for a single image.</p> <p>Contains all detected text blocks with their bounding boxes, plus metadata about the extraction.</p> Example <pre><code>result = ocr.extract(image)\nprint(f\"Found {result.block_count} blocks\")\nprint(f\"Full text: {result.full_text}\")\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.block_count","title":"block_count  <code>property</code>","text":"<pre><code>block_count: int\n</code></pre> <p>Number of detected text blocks.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.word_count","title":"word_count  <code>property</code>","text":"<pre><code>word_count: int\n</code></pre> <p>Approximate word count from full text.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.average_confidence","title":"average_confidence  <code>property</code>","text":"<pre><code>average_confidence: float\n</code></pre> <p>Average confidence across all text blocks.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(\n    min_confidence: float,\n) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by minimum confidence.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def filter_by_confidence(self, min_confidence: float) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by minimum confidence.\"\"\"\n    return [b for b in self.text_blocks if b.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.filter_by_granularity","title":"filter_by_granularity","text":"<pre><code>filter_by_granularity(\n    granularity: OCRGranularity,\n) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by granularity level.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def filter_by_granularity(self, granularity: OCRGranularity) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by granularity level.\"\"\"\n    return [b for b in self.text_blocks if b.granularity == granularity]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"text_blocks\": [b.to_dict() for b in self.text_blocks],\n        \"full_text\": self.full_text,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"languages_detected\": self.languages_detected,\n        \"block_count\": self.block_count,\n        \"word_count\": self.word_count,\n        \"average_confidence\": self.average_confidence,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.sort_by_position","title":"sort_by_position","text":"<pre><code>sort_by_position(top_to_bottom: bool = True) -&gt; OCROutput\n</code></pre> <p>Return a new OCROutput with blocks sorted by position.</p> PARAMETER DESCRIPTION <code>top_to_bottom</code> <p>If True, sort by y-coordinate (reading order)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>New OCROutput with sorted text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def sort_by_position(self, top_to_bottom: bool = True) -&gt; \"OCROutput\":\n    \"\"\"\n    Return a new OCROutput with blocks sorted by position.\n\n    Args:\n        top_to_bottom: If True, sort by y-coordinate (reading order)\n\n    Returns:\n        New OCROutput with sorted text blocks\n    \"\"\"\n    sorted_blocks = sorted(\n        self.text_blocks,\n        key=lambda b: (b.bbox.y1, b.bbox.x1),\n        reverse=not top_to_bottom,\n    )\n    # Regenerate full_text in sorted order\n    full_text = \" \".join(b.text for b in sorted_blocks)\n\n    return OCROutput(\n        text_blocks=sorted_blocks,\n        full_text=full_text,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        model_name=self.model_name,\n        languages_detected=self.languages_detected,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.get_normalized_blocks","title":"get_normalized_blocks","text":"<pre><code>get_normalized_blocks() -&gt; List[Dict]\n</code></pre> <p>Get all text blocks with normalized (0-1024) coordinates.</p> RETURNS DESCRIPTION <code>List[Dict]</code> <p>List of dicts with normalized bbox coordinates and metadata.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def get_normalized_blocks(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all text blocks with normalized (0-1024) coordinates.\n\n    Returns:\n        List of dicts with normalized bbox coordinates and metadata.\n    \"\"\"\n    normalized = []\n    for block in self.text_blocks:\n        norm_bbox = block.bbox.to_normalized(self.image_width, self.image_height)\n        normalized.append(\n            {\n                \"text\": block.text,\n                \"bbox\": norm_bbox.to_list(),\n                \"confidence\": block.confidence,\n                \"granularity\": block.granularity.value,\n                \"language\": block.language,\n            }\n        )\n    return normalized\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.visualize","title":"visualize","text":"<pre><code>visualize(\n    image: Image,\n    output_path: Optional[Union[str, Path]] = None,\n    show_text: bool = True,\n    show_confidence: bool = False,\n    line_width: int = 2,\n    box_color: str = \"#2ECC71\",\n    text_color: str = \"#000000\",\n) -&gt; Image.Image\n</code></pre> <p>Visualize OCR results on the image.</p> <p>Draws bounding boxes around detected text with optional labels.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to draw on (will be copied, not modified)</p> <p> TYPE: <code>Image</code> </p> <code>output_path</code> <p>Optional path to save the visualization</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> <code>show_text</code> <p>Whether to show detected text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_confidence</code> <p>Whether to show confidence scores</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>line_width</code> <p>Width of bounding box lines</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>box_color</code> <p>Color for bounding boxes (hex)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'#2ECC71'</code> </p> <code>text_color</code> <p>Color for text labels (hex)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'#000000'</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image with visualizations drawn</p> Example <pre><code>result = ocr.extract(image)\nviz = result.visualize(image, output_path=\"ocr_viz.png\")\n</code></pre> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def visualize(\n    self,\n    image: \"Image.Image\",\n    output_path: Optional[Union[str, Path]] = None,\n    show_text: bool = True,\n    show_confidence: bool = False,\n    line_width: int = 2,\n    box_color: str = \"#2ECC71\",\n    text_color: str = \"#000000\",\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Visualize OCR results on the image.\n\n    Draws bounding boxes around detected text with optional labels.\n\n    Args:\n        image: PIL Image to draw on (will be copied, not modified)\n        output_path: Optional path to save the visualization\n        show_text: Whether to show detected text\n        show_confidence: Whether to show confidence scores\n        line_width: Width of bounding box lines\n        box_color: Color for bounding boxes (hex)\n        text_color: Color for text labels (hex)\n\n    Returns:\n        PIL Image with visualizations drawn\n\n    Example:\n        ```python\n        result = ocr.extract(image)\n        viz = result.visualize(image, output_path=\"ocr_viz.png\")\n        ```\n    \"\"\"\n    from PIL import ImageDraw, ImageFont\n\n    # Copy image to avoid modifying original\n    viz_image = image.copy().convert(\"RGB\")\n    draw = ImageDraw.Draw(viz_image)\n\n    # Try to get a font\n    try:\n        font = ImageFont.truetype(\"/System/Library/Fonts/Helvetica.ttc\", 12)\n    except Exception:\n        try:\n            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 12)\n        except Exception:\n            font = ImageFont.load_default()\n\n    for block in self.text_blocks:\n        coords = block.bbox.to_xyxy()\n\n        # Draw polygon if available, otherwise draw rectangle\n        if block.polygon:\n            flat_polygon = [coord for point in block.polygon for coord in point]\n            draw.polygon(flat_polygon, outline=box_color, width=line_width)\n        else:\n            draw.rectangle(coords, outline=box_color, width=line_width)\n\n        # Build label text\n        if show_text or show_confidence:\n            label_parts = []\n            if show_text:\n                # Truncate long text\n                text = block.text[:25] + \"...\" if len(block.text) &gt; 25 else block.text\n                label_parts.append(text)\n            if show_confidence:\n                label_parts.append(f\"{block.confidence:.2f}\")\n            label_text = \" | \".join(label_parts)\n\n            # Position label below the box\n            label_x = coords[0]\n            label_y = coords[3] + 2  # Below bottom edge\n\n            # Draw label with background\n            text_bbox = draw.textbbox((label_x, label_y), label_text, font=font)\n            padding = 2\n            draw.rectangle(\n                [\n                    text_bbox[0] - padding,\n                    text_bbox[1] - padding,\n                    text_bbox[2] + padding,\n                    text_bbox[3] + padding,\n                ],\n                fill=\"#FFFFFF\",\n                outline=box_color,\n            )\n            draw.text((label_x, label_y), label_text, fill=text_color, font=font)\n\n    # Save if path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        viz_image.save(output_path)\n\n    return viz_image\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; OCROutput\n</code></pre> <p>Load an OCROutput instance from a JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to JSON file</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput instance</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"OCROutput\":\n    \"\"\"\n    Load an OCROutput instance from a JSON file.\n\n    Args:\n        file_path: Path to JSON file\n\n    Returns:\n        OCROutput instance\n    \"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save OCROutput instance to a JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path where JSON file should be saved</p> <p> TYPE: <code>Union[str, Path]</code> </p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"\n    Save OCROutput instance to a JSON file.\n\n    Args:\n        file_path: Path where JSON file should be saved\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.paddleocr","title":"paddleocr","text":"<p>PaddleOCR extractor.</p> <p>PaddleOCR is an OCR toolkit developed by Baidu/PaddlePaddle. - Excellent for CJK languages (Chinese, Japanese, Korean) - GPU accelerated - Supports layout analysis + OCR</p> Python Package <p>pip install paddleocr paddlepaddle  # CPU version pip install paddleocr paddlepaddle-gpu  # GPU version</p> Model Download Location <p>By default, PaddleOCR downloads models to ~/.paddleocr/</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.paddleocr.PaddleOCRConfig","title":"PaddleOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for PaddleOCR extractor.</p> <p>This is a single-backend model (PaddlePaddle - CPU/GPU).</p> Example <pre><code>config = PaddleOCRConfig(lang=\"ch\", device=\"gpu\")\nocr = PaddleOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.paddleocr.PaddleOCR","title":"PaddleOCR","text":"<pre><code>PaddleOCR(config: PaddleOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>PaddleOCR text extractor.</p> <p>Single-backend model (PaddlePaddle - CPU/GPU).</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import PaddleOCR, PaddleOCRConfig\n\nocr = PaddleOCR(config=PaddleOCRConfig(lang=\"en\", device=\"cpu\"))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize PaddleOCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>PaddleOCRConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If paddleocr or paddlepaddle is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/paddleocr.py</code> <pre><code>def __init__(self, config: PaddleOCRConfig):\n    \"\"\"\n    Initialize PaddleOCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        ImportError: If paddleocr or paddlepaddle is not installed\n    \"\"\"\n    self.config = config\n    self._ocr = None\n\n    # Normalize language code\n    self._lang = LANG_CODES.get(config.lang.lower(), config.lang)\n\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.paddleocr.PaddleOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/paddleocr.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with detected text blocks\n    \"\"\"\n    if self._ocr is None:\n        raise RuntimeError(\"PaddleOCR not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Convert to numpy array\n    image_array = np.array(pil_image)\n\n    # Run PaddleOCR v3.x - use predict() method\n    results = self._ocr.predict(image_array)\n\n    # Parse results\n    text_blocks = []\n\n    # PaddleOCR may return None or empty results\n    if results is None or len(results) == 0:\n        return OCROutput(\n            text_blocks=[],\n            full_text=\"\",\n            image_width=image_width,\n            image_height=image_height,\n            model_name=self.MODEL_NAME,\n            languages_detected=[self._lang],\n        )\n\n    # PaddleOCR v3.x returns list of dicts with 'rec_texts', 'rec_scores', 'dt_polys'\n    for result in results:\n        if result is None:\n            continue\n\n        rec_texts = result.get(\"rec_texts\", [])\n        rec_scores = result.get(\"rec_scores\", [])\n        dt_polys = result.get(\"dt_polys\", [])\n\n        for i, text in enumerate(rec_texts):\n            if not text.strip():\n                continue\n\n            confidence = rec_scores[i] if i &lt; len(rec_scores) else 1.0\n\n            # Get polygon and convert to list\n            polygon: Optional[List[List[float]]] = None\n            if i &lt; len(dt_polys) and dt_polys[i] is not None:\n                poly_array = dt_polys[i]\n                # Handle numpy array\n                if hasattr(poly_array, \"tolist\"):\n                    polygon = poly_array.tolist()\n                else:\n                    polygon = list(poly_array)\n\n            # Convert polygon to bbox\n            if polygon:\n                bbox = BoundingBox.from_polygon(polygon)\n            else:\n                bbox = BoundingBox(x1=0, y1=0, x2=0, y2=0)\n\n            text_blocks.append(\n                TextBlock(\n                    text=text,\n                    bbox=bbox,\n                    confidence=float(confidence),\n                    granularity=OCRGranularity.LINE,\n                    polygon=polygon,\n                    language=self._lang,\n                )\n            )\n\n    # Sort by position (top to bottom, left to right)\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    # Build full_text from sorted blocks to ensure reading order\n    full_text = \" \".join(block.text for block in text_blocks)\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=full_text,\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=[self._lang],\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.tesseract","title":"tesseract","text":"<p>Tesseract OCR extractor.</p> <p>Tesseract is an open-source OCR engine maintained by Google. - CPU-based (no GPU required) - Requires system installation of Tesseract - Good for printed text, supports 100+ languages</p> System Requirements <p>macOS: brew install tesseract Ubuntu: sudo apt-get install tesseract-ocr Windows: Download from https://github.com/UB-Mannheim/tesseract/wiki</p> Python Package <p>pip install pytesseract</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCRConfig","title":"TesseractOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Tesseract OCR extractor.</p> <p>This is a single-backend model (CPU only, requires system Tesseract).</p> Example <pre><code>config = TesseractOCRConfig(languages=[\"eng\", \"fra\"], psm=3)\nocr = TesseractOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCR","title":"TesseractOCR","text":"<pre><code>TesseractOCR(config: TesseractOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>Tesseract OCR extractor.</p> <p>Single-backend model (CPU only). Requires system Tesseract installation.</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\n\nocr = TesseractOCR(config=TesseractOCRConfig(languages=[\"eng\"]))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize Tesseract OCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>TesseractOCRConfig</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If Tesseract is not installed</p> <code>ImportError</code> <p>If pytesseract is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def __init__(self, config: TesseractOCRConfig):\n    \"\"\"\n    Initialize Tesseract OCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        RuntimeError: If Tesseract is not installed\n        ImportError: If pytesseract is not installed\n    \"\"\"\n    self.config = config\n    self._pytesseract = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks at word level</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with detected text blocks at word level\n    \"\"\"\n    if self._pytesseract is None:\n        raise RuntimeError(\"Tesseract not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Build config string\n    config = f\"--oem {self.config.oem} --psm {self.config.psm}\"\n    if self.config.config_params:\n        for key, value in self.config.config_params.items():\n            config += f\" -c {key}={value}\"\n\n    # Language string\n    lang_str = \"+\".join(self.config.languages)\n\n    # Get detailed data (word-level boxes)\n    data = self._pytesseract.image_to_data(\n        pil_image,\n        lang=lang_str,\n        config=config,\n        output_type=self._pytesseract.Output.DICT,\n    )\n\n    # Parse results into TextBlocks\n    text_blocks = []\n    full_text_parts = []\n\n    n_boxes = len(data[\"text\"])\n    for i in range(n_boxes):\n        text = data[\"text\"][i].strip()\n        # Safely convert conf to float (handles string values from some Tesseract versions)\n        try:\n            conf = float(data[\"conf\"][i])\n        except (ValueError, TypeError):\n            conf = -1\n\n        # Skip empty text or low confidence (-1 means no confidence)\n        if not text or conf == -1:\n            continue\n\n        # Tesseract returns confidence as 0-100, normalize to 0-1\n        confidence = conf / 100.0\n\n        # Get bounding box\n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n\n        bbox = BoundingBox(\n            x1=float(x),\n            y1=float(y),\n            x2=float(x + w),\n            y2=float(y + h),\n        )\n\n        text_blocks.append(\n            TextBlock(\n                text=text,\n                bbox=bbox,\n                confidence=confidence,\n                granularity=OCRGranularity.WORD,\n                language=lang_str,\n            )\n        )\n\n        full_text_parts.append(text)\n\n    # Sort by position (top to bottom, left to right)\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\" \".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCR.extract_lines","title":"extract_lines","text":"<pre><code>extract_lines(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR and return line-level blocks.</p> <p>Groups words into lines based on Tesseract's line detection.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with line-level text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def extract_lines(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR and return line-level blocks.\n\n    Groups words into lines based on Tesseract's line detection.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with line-level text blocks\n    \"\"\"\n    if self._pytesseract is None:\n        raise RuntimeError(\"Tesseract not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Build config string (including config_params like extract method)\n    config = f\"--oem {self.config.oem} --psm {self.config.psm}\"\n    if self.config.config_params:\n        for key, value in self.config.config_params.items():\n            config += f\" -c {key}={value}\"\n\n    # Language string\n    lang_str = \"+\".join(self.config.languages)\n\n    # Get detailed data\n    data = self._pytesseract.image_to_data(\n        pil_image,\n        lang=lang_str,\n        config=config,\n        output_type=self._pytesseract.Output.DICT,\n    )\n\n    # Group words into lines\n    lines: Dict[tuple, Dict] = {}\n    n_boxes = len(data[\"text\"])\n\n    for i in range(n_boxes):\n        text = data[\"text\"][i].strip()\n        # Safely convert conf to float (handles string values from some Tesseract versions)\n        try:\n            conf = float(data[\"conf\"][i])\n        except (ValueError, TypeError):\n            conf = -1\n\n        if not text or conf == -1:\n            continue\n\n        # Tesseract provides block_num, par_num, line_num\n        line_key = (data[\"block_num\"][i], data[\"par_num\"][i], data[\"line_num\"][i])\n\n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n\n        if line_key not in lines:\n            lines[line_key] = {\n                \"words\": [],\n                \"confidences\": [],\n                \"x1\": x,\n                \"y1\": y,\n                \"x2\": x + w,\n                \"y2\": y + h,\n            }\n\n        lines[line_key][\"words\"].append(text)\n        lines[line_key][\"confidences\"].append(conf / 100.0)\n        lines[line_key][\"x1\"] = min(lines[line_key][\"x1\"], x)\n        lines[line_key][\"y1\"] = min(lines[line_key][\"y1\"], y)\n        lines[line_key][\"x2\"] = max(lines[line_key][\"x2\"], x + w)\n        lines[line_key][\"y2\"] = max(lines[line_key][\"y2\"], y + h)\n\n    # Convert to TextBlocks\n    text_blocks = []\n    full_text_parts = []\n\n    for line_key in sorted(lines.keys()):\n        line = lines[line_key]\n        line_text = \" \".join(line[\"words\"])\n        avg_conf = sum(line[\"confidences\"]) / len(line[\"confidences\"])\n\n        bbox = BoundingBox(\n            x1=float(line[\"x1\"]),\n            y1=float(line[\"y1\"]),\n            x2=float(line[\"x2\"]),\n            y2=float(line[\"y2\"]),\n        )\n\n        text_blocks.append(\n            TextBlock(\n                text=line_text,\n                bbox=bbox,\n                confidence=avg_conf,\n                granularity=OCRGranularity.LINE,\n                language=lang_str,\n            )\n        )\n\n        full_text_parts.append(line_text)\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\"\\n\".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order","title":"reading_order","text":"<p>Reading Order Module.</p> <p>Provides predictors for determining the logical reading sequence of document elements based on layout detection and spatial analysis.</p> Available Predictors <ul> <li>RuleBasedReadingOrderPredictor: Rule-based predictor using R-tree indexing</li> </ul> Example <pre><code>from omnidocs.tasks.reading_order import RuleBasedReadingOrderPredictor\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\n# Initialize components\nlayout_extractor = DocLayoutYOLO(config=DocLayoutYOLOConfig())\nocr = EasyOCR(config=EasyOCRConfig())\npredictor = RuleBasedReadingOrderPredictor()\n\n# Process document\nlayout = layout_extractor.extract(image)\nocr_result = ocr.extract(image)\nreading_order = predictor.predict(layout, ocr_result)\n\n# Get text in reading order\ntext = reading_order.get_full_text()\n\n# Get elements by type\ntables = reading_order.get_elements_by_type(ElementType.TABLE)\n\n# Get caption associations\nfor elem in reading_order.ordered_elements:\n    if elem.element_type == ElementType.FIGURE:\n        captions = reading_order.get_captions_for(elem.original_id)\n        print(f\"Figure {elem.original_id} captions: {[c.text for c in captions]}\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.BaseReadingOrderPredictor","title":"BaseReadingOrderPredictor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for reading order predictors.</p> <p>Reading order predictors take layout detection and OCR results and produce a properly ordered sequence of document elements.</p> Example <pre><code>predictor = RuleBasedReadingOrderPredictor()\n\n# Get layout and OCR\nlayout = layout_extractor.extract(image)\nocr = ocr_extractor.extract(image)\n\n# Predict reading order\nresult = predictor.predict(layout, ocr)\n\n# Or with multiple pages\nresults = predictor.predict_multi_page(layouts, ocrs)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.BaseReadingOrderPredictor.predict","title":"predict  <code>abstractmethod</code>","text":"<pre><code>predict(\n    layout: LayoutOutput,\n    ocr: Optional[OCROutput] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput\n</code></pre> <p>Predict reading order for a single page.</p> PARAMETER DESCRIPTION <code>layout</code> <p>Layout detection results with bounding boxes</p> <p> TYPE: <code>LayoutOutput</code> </p> <code>ocr</code> <p>Optional OCR results. If provided, text will be  matched to layout elements by bbox overlap.</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> <code>page_no</code> <p>Page number (for multi-page documents)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ReadingOrderOutput</code> <p>ReadingOrderOutput with ordered elements and associations</p> Example <pre><code>layout = layout_extractor.extract(page_image)\nocr = ocr_extractor.extract(page_image)\norder = predictor.predict(layout, ocr, page_no=0)\n</code></pre> Source code in <code>omnidocs/tasks/reading_order/base.py</code> <pre><code>@abstractmethod\ndef predict(\n    self,\n    layout: \"LayoutOutput\",\n    ocr: Optional[\"OCROutput\"] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput:\n    \"\"\"\n    Predict reading order for a single page.\n\n    Args:\n        layout: Layout detection results with bounding boxes\n        ocr: Optional OCR results. If provided, text will be\n             matched to layout elements by bbox overlap.\n        page_no: Page number (for multi-page documents)\n\n    Returns:\n        ReadingOrderOutput with ordered elements and associations\n\n    Example:\n        ```python\n        layout = layout_extractor.extract(page_image)\n        ocr = ocr_extractor.extract(page_image)\n        order = predictor.predict(layout, ocr, page_no=0)\n        ```\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.BaseReadingOrderPredictor.predict_multi_page","title":"predict_multi_page","text":"<pre><code>predict_multi_page(\n    layouts: List[LayoutOutput],\n    ocrs: Optional[List[OCROutput]] = None,\n) -&gt; List[ReadingOrderOutput]\n</code></pre> <p>Predict reading order for multiple pages.</p> PARAMETER DESCRIPTION <code>layouts</code> <p>List of layout results, one per page</p> <p> TYPE: <code>List[LayoutOutput]</code> </p> <code>ocrs</code> <p>Optional list of OCR results, one per page</p> <p> TYPE: <code>Optional[List[OCROutput]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[ReadingOrderOutput]</code> <p>List of ReadingOrderOutput, one per page</p> Source code in <code>omnidocs/tasks/reading_order/base.py</code> <pre><code>def predict_multi_page(\n    self,\n    layouts: List[\"LayoutOutput\"],\n    ocrs: Optional[List[\"OCROutput\"]] = None,\n) -&gt; List[ReadingOrderOutput]:\n    \"\"\"\n    Predict reading order for multiple pages.\n\n    Args:\n        layouts: List of layout results, one per page\n        ocrs: Optional list of OCR results, one per page\n\n    Returns:\n        List of ReadingOrderOutput, one per page\n    \"\"\"\n    results = []\n\n    for i, layout in enumerate(layouts):\n        ocr = ocrs[i] if ocrs else None\n        result = self.predict(layout, ocr, page_no=i)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box in pixel coordinates.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.ElementType","title":"ElementType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of document element for reading order.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.OrderedElement","title":"OrderedElement","text":"<p>               Bases: <code>BaseModel</code></p> <p>A document element with its reading order position.</p> <p>Combines layout detection results with OCR text and assigns a reading order index.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.OrderedElement.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"index\": self.index,\n        \"element_type\": self.element_type.value,\n        \"bbox\": self.bbox.to_list(),\n        \"text\": self.text,\n        \"confidence\": self.confidence,\n        \"page_no\": self.page_no,\n        \"original_id\": self.original_id,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.ReadingOrderOutput","title":"ReadingOrderOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete reading order prediction result.</p> <p>Provides: - Ordered list of document elements - Caption-to-element associations - Footnote-to-element associations - Merge suggestions for split elements</p> Example <pre><code>result = predictor.predict(layout, ocr)\n\n# Get full text in reading order\nfull_text = result.get_full_text()\n\n# Get elements by type\ntables = result.get_elements_by_type(ElementType.TABLE)\n\n# Find caption for a figure\ncaptions = result.get_captions_for(figure_element.original_id)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.ReadingOrderOutput.element_count","title":"element_count  <code>property</code>","text":"<pre><code>element_count: int\n</code></pre> <p>Total number of ordered elements.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.ReadingOrderOutput.get_full_text","title":"get_full_text","text":"<pre><code>get_full_text(separator: str = '\\n\\n') -&gt; str\n</code></pre> <p>Get concatenated text in reading order.</p> <p>Excludes page headers, footers, captions, and footnotes from main text flow.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def get_full_text(self, separator: str = \"\\n\\n\") -&gt; str:\n    \"\"\"\n    Get concatenated text in reading order.\n\n    Excludes page headers, footers, captions, and footnotes\n    from main text flow.\n    \"\"\"\n    main_elements = [\n        e\n        for e in self.ordered_elements\n        if e.element_type\n        not in (\n            ElementType.PAGE_HEADER,\n            ElementType.PAGE_FOOTER,\n            ElementType.CAPTION,\n            ElementType.FOOTNOTE,\n        )\n    ]\n    return separator.join(e.text for e in main_elements if e.text)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.ReadingOrderOutput.get_elements_by_type","title":"get_elements_by_type","text":"<pre><code>get_elements_by_type(\n    element_type: ElementType,\n) -&gt; List[OrderedElement]\n</code></pre> <p>Filter elements by type.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def get_elements_by_type(self, element_type: ElementType) -&gt; List[OrderedElement]:\n    \"\"\"Filter elements by type.\"\"\"\n    return [e for e in self.ordered_elements if e.element_type == element_type]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.ReadingOrderOutput.get_captions_for","title":"get_captions_for","text":"<pre><code>get_captions_for(element_id: int) -&gt; List[OrderedElement]\n</code></pre> <p>Get caption elements for a given element ID.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def get_captions_for(self, element_id: int) -&gt; List[OrderedElement]:\n    \"\"\"Get caption elements for a given element ID.\"\"\"\n    caption_ids = self.caption_map.get(element_id, [])\n    return [e for e in self.ordered_elements if e.original_id in caption_ids]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.ReadingOrderOutput.get_footnotes_for","title":"get_footnotes_for","text":"<pre><code>get_footnotes_for(element_id: int) -&gt; List[OrderedElement]\n</code></pre> <p>Get footnote elements for a given element ID.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def get_footnotes_for(self, element_id: int) -&gt; List[OrderedElement]:\n    \"\"\"Get footnote elements for a given element ID.\"\"\"\n    footnote_ids = self.footnote_map.get(element_id, [])\n    return [e for e in self.ordered_elements if e.original_id in footnote_ids]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.ReadingOrderOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"ordered_elements\": [e.to_dict() for e in self.ordered_elements],\n        \"caption_map\": self.caption_map,\n        \"footnote_map\": self.footnote_map,\n        \"merge_map\": self.merge_map,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"element_count\": self.element_count,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.ReadingOrderOutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save to JSON file.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"Save to JSON file.\"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.ReadingOrderOutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(\n    file_path: Union[str, Path],\n) -&gt; ReadingOrderOutput\n</code></pre> <p>Load from JSON file.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"ReadingOrderOutput\":\n    \"\"\"Load from JSON file.\"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.RuleBasedReadingOrderPredictor","title":"RuleBasedReadingOrderPredictor","text":"<pre><code>RuleBasedReadingOrderPredictor()\n</code></pre> <p>               Bases: <code>BaseReadingOrderPredictor</code></p> <p>Rule-based reading order predictor using spatial analysis.</p> <p>Uses R-tree spatial indexing and rule-based algorithms to determine the logical reading sequence of document elements. This is a CPU-only implementation that doesn't require GPU resources.</p> <p>Features: - Multi-column layout detection - Header/footer separation - Caption-to-figure/table association - Footnote linking - Element merge suggestions</p> Example <pre><code>from omnidocs.tasks.reading_order import RuleBasedReadingOrderPredictor\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\n# Initialize components\nlayout_extractor = DocLayoutYOLO(config=DocLayoutYOLOConfig())\nocr = EasyOCR(config=EasyOCRConfig())\npredictor = RuleBasedReadingOrderPredictor()\n\n# Process document\nlayout = layout_extractor.extract(image)\nocr_result = ocr.extract(image)\nreading_order = predictor.predict(layout, ocr_result)\n\n# Get text in reading order\ntext = reading_order.get_full_text()\n</code></pre> <p>Initialize the reading order predictor.</p> Source code in <code>omnidocs/tasks/reading_order/rule_based/predictor.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the reading order predictor.\"\"\"\n    self.dilated_page_element = True\n    # Apply horizontal dilation only if less than this page-width normalized threshold\n    self._horizontal_dilation_threshold_norm = 0.15\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.RuleBasedReadingOrderPredictor.predict","title":"predict","text":"<pre><code>predict(\n    layout: LayoutOutput,\n    ocr: Optional[OCROutput] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput\n</code></pre> <p>Predict reading order for a single page.</p> PARAMETER DESCRIPTION <code>layout</code> <p>Layout detection results with bounding boxes</p> <p> TYPE: <code>LayoutOutput</code> </p> <code>ocr</code> <p>Optional OCR results for text content</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> <code>page_no</code> <p>Page number (for multi-page documents)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ReadingOrderOutput</code> <p>ReadingOrderOutput with ordered elements and associations</p> Source code in <code>omnidocs/tasks/reading_order/rule_based/predictor.py</code> <pre><code>def predict(\n    self,\n    layout: \"LayoutOutput\",\n    ocr: Optional[\"OCROutput\"] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput:\n    \"\"\"\n    Predict reading order for a single page.\n\n    Args:\n        layout: Layout detection results with bounding boxes\n        ocr: Optional OCR results for text content\n        page_no: Page number (for multi-page documents)\n\n    Returns:\n        ReadingOrderOutput with ordered elements and associations\n    \"\"\"\n    page_width = layout.image_width\n    page_height = layout.image_height\n\n    # Build text map from OCR if available\n    text_map: Dict[int, str] = {}\n    if ocr:\n        text_map = self._build_text_map(layout, ocr)\n\n    # Convert layout boxes to internal PageElements\n    page_elements: List[_PageElement] = []\n    for i, box in enumerate(layout.bboxes):\n        label_str = box.label.value.lower()\n        element_type = LABEL_TO_ELEMENT_TYPE.get(label_str, ElementType.OTHER)\n\n        # Convert from top-left origin to bottom-left origin\n        elem = _PageElement(\n            cid=i,\n            text=text_map.get(i, \"\"),\n            page_no=page_no,\n            page_width=page_width,\n            page_height=page_height,\n            label=element_type,\n            left=box.bbox.x1,\n            bottom=page_height - box.bbox.y2,  # Convert y2 to bottom\n            right=box.bbox.x2,\n            top=page_height - box.bbox.y1,  # Convert y1 to top\n        )\n        page_elements.append(elem)\n\n    # Run reading order prediction\n    sorted_elements = self._predict_reading_order(page_elements)\n\n    # Get caption associations\n    caption_map = self._find_to_captions(sorted_elements)\n\n    # Get footnote associations\n    footnote_map = self._find_to_footnotes(sorted_elements)\n\n    # Get merge suggestions\n    merge_map = self._predict_merges(sorted_elements)\n\n    # Convert to OrderedElements\n    ordered_elements: List[OrderedElement] = []\n    for idx, elem in enumerate(sorted_elements):\n        # Convert back from bottom-left to top-left origin\n        bbox = BoundingBox(\n            x1=elem.left,\n            y1=page_height - elem.top,\n            x2=elem.right,\n            y2=page_height - elem.bottom,\n        )\n\n        confidence = 1.0\n        if elem.cid &lt; len(layout.bboxes):\n            confidence = layout.bboxes[elem.cid].confidence\n\n        ordered_elem = OrderedElement(\n            index=idx,\n            element_type=elem.label,\n            bbox=bbox,\n            text=elem.text,\n            confidence=confidence,\n            page_no=page_no,\n            original_id=elem.cid,\n        )\n        ordered_elements.append(ordered_elem)\n\n    return ReadingOrderOutput(\n        ordered_elements=ordered_elements,\n        caption_map=caption_map,\n        footnote_map=footnote_map,\n        merge_map=merge_map,\n        image_width=page_width,\n        image_height=page_height,\n        model_name=\"RuleBasedReadingOrderPredictor\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.base","title":"base","text":"<p>Base class for reading order predictors.</p> <p>Defines the abstract interface that all reading order predictors must implement.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.base.BaseReadingOrderPredictor","title":"BaseReadingOrderPredictor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for reading order predictors.</p> <p>Reading order predictors take layout detection and OCR results and produce a properly ordered sequence of document elements.</p> Example <pre><code>predictor = RuleBasedReadingOrderPredictor()\n\n# Get layout and OCR\nlayout = layout_extractor.extract(image)\nocr = ocr_extractor.extract(image)\n\n# Predict reading order\nresult = predictor.predict(layout, ocr)\n\n# Or with multiple pages\nresults = predictor.predict_multi_page(layouts, ocrs)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.base.BaseReadingOrderPredictor.predict","title":"predict  <code>abstractmethod</code>","text":"<pre><code>predict(\n    layout: LayoutOutput,\n    ocr: Optional[OCROutput] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput\n</code></pre> <p>Predict reading order for a single page.</p> PARAMETER DESCRIPTION <code>layout</code> <p>Layout detection results with bounding boxes</p> <p> TYPE: <code>LayoutOutput</code> </p> <code>ocr</code> <p>Optional OCR results. If provided, text will be  matched to layout elements by bbox overlap.</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> <code>page_no</code> <p>Page number (for multi-page documents)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ReadingOrderOutput</code> <p>ReadingOrderOutput with ordered elements and associations</p> Example <pre><code>layout = layout_extractor.extract(page_image)\nocr = ocr_extractor.extract(page_image)\norder = predictor.predict(layout, ocr, page_no=0)\n</code></pre> Source code in <code>omnidocs/tasks/reading_order/base.py</code> <pre><code>@abstractmethod\ndef predict(\n    self,\n    layout: \"LayoutOutput\",\n    ocr: Optional[\"OCROutput\"] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput:\n    \"\"\"\n    Predict reading order for a single page.\n\n    Args:\n        layout: Layout detection results with bounding boxes\n        ocr: Optional OCR results. If provided, text will be\n             matched to layout elements by bbox overlap.\n        page_no: Page number (for multi-page documents)\n\n    Returns:\n        ReadingOrderOutput with ordered elements and associations\n\n    Example:\n        ```python\n        layout = layout_extractor.extract(page_image)\n        ocr = ocr_extractor.extract(page_image)\n        order = predictor.predict(layout, ocr, page_no=0)\n        ```\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.base.BaseReadingOrderPredictor.predict_multi_page","title":"predict_multi_page","text":"<pre><code>predict_multi_page(\n    layouts: List[LayoutOutput],\n    ocrs: Optional[List[OCROutput]] = None,\n) -&gt; List[ReadingOrderOutput]\n</code></pre> <p>Predict reading order for multiple pages.</p> PARAMETER DESCRIPTION <code>layouts</code> <p>List of layout results, one per page</p> <p> TYPE: <code>List[LayoutOutput]</code> </p> <code>ocrs</code> <p>Optional list of OCR results, one per page</p> <p> TYPE: <code>Optional[List[OCROutput]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[ReadingOrderOutput]</code> <p>List of ReadingOrderOutput, one per page</p> Source code in <code>omnidocs/tasks/reading_order/base.py</code> <pre><code>def predict_multi_page(\n    self,\n    layouts: List[\"LayoutOutput\"],\n    ocrs: Optional[List[\"OCROutput\"]] = None,\n) -&gt; List[ReadingOrderOutput]:\n    \"\"\"\n    Predict reading order for multiple pages.\n\n    Args:\n        layouts: List of layout results, one per page\n        ocrs: Optional list of OCR results, one per page\n\n    Returns:\n        List of ReadingOrderOutput, one per page\n    \"\"\"\n    results = []\n\n    for i, layout in enumerate(layouts):\n        ocr = ocrs[i] if ocrs else None\n        result = self.predict(layout, ocr, page_no=i)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.models","title":"models","text":"<p>Pydantic models for reading order prediction.</p> <p>Takes layout detection and OCR results, produces ordered element sequence with caption and footnote associations.</p> Example <pre><code># Get layout and OCR\nlayout = layout_extractor.extract(image)\nocr = ocr_extractor.extract(image)\n\n# Predict reading order\nreading_order = predictor.predict(layout, ocr)\n\n# Iterate in reading order\nfor element in reading_order.ordered_elements:\n    print(f\"{element.index}: [{element.element_type}] {element.text[:50]}...\")\n\n# Get caption associations\nfor fig_id, caption_ids in reading_order.caption_map.items():\n    print(f\"Figure {fig_id} has captions: {caption_ids}\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.models.ElementType","title":"ElementType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of document element for reading order.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.models.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box in pixel coordinates.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.models.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.models.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.models.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.models.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.models.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.models.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.models.OrderedElement","title":"OrderedElement","text":"<p>               Bases: <code>BaseModel</code></p> <p>A document element with its reading order position.</p> <p>Combines layout detection results with OCR text and assigns a reading order index.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.models.OrderedElement.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"index\": self.index,\n        \"element_type\": self.element_type.value,\n        \"bbox\": self.bbox.to_list(),\n        \"text\": self.text,\n        \"confidence\": self.confidence,\n        \"page_no\": self.page_no,\n        \"original_id\": self.original_id,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.models.ReadingOrderOutput","title":"ReadingOrderOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete reading order prediction result.</p> <p>Provides: - Ordered list of document elements - Caption-to-element associations - Footnote-to-element associations - Merge suggestions for split elements</p> Example <pre><code>result = predictor.predict(layout, ocr)\n\n# Get full text in reading order\nfull_text = result.get_full_text()\n\n# Get elements by type\ntables = result.get_elements_by_type(ElementType.TABLE)\n\n# Find caption for a figure\ncaptions = result.get_captions_for(figure_element.original_id)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.element_count","title":"element_count  <code>property</code>","text":"<pre><code>element_count: int\n</code></pre> <p>Total number of ordered elements.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.get_full_text","title":"get_full_text","text":"<pre><code>get_full_text(separator: str = '\\n\\n') -&gt; str\n</code></pre> <p>Get concatenated text in reading order.</p> <p>Excludes page headers, footers, captions, and footnotes from main text flow.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def get_full_text(self, separator: str = \"\\n\\n\") -&gt; str:\n    \"\"\"\n    Get concatenated text in reading order.\n\n    Excludes page headers, footers, captions, and footnotes\n    from main text flow.\n    \"\"\"\n    main_elements = [\n        e\n        for e in self.ordered_elements\n        if e.element_type\n        not in (\n            ElementType.PAGE_HEADER,\n            ElementType.PAGE_FOOTER,\n            ElementType.CAPTION,\n            ElementType.FOOTNOTE,\n        )\n    ]\n    return separator.join(e.text for e in main_elements if e.text)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.get_elements_by_type","title":"get_elements_by_type","text":"<pre><code>get_elements_by_type(\n    element_type: ElementType,\n) -&gt; List[OrderedElement]\n</code></pre> <p>Filter elements by type.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def get_elements_by_type(self, element_type: ElementType) -&gt; List[OrderedElement]:\n    \"\"\"Filter elements by type.\"\"\"\n    return [e for e in self.ordered_elements if e.element_type == element_type]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.get_captions_for","title":"get_captions_for","text":"<pre><code>get_captions_for(element_id: int) -&gt; List[OrderedElement]\n</code></pre> <p>Get caption elements for a given element ID.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def get_captions_for(self, element_id: int) -&gt; List[OrderedElement]:\n    \"\"\"Get caption elements for a given element ID.\"\"\"\n    caption_ids = self.caption_map.get(element_id, [])\n    return [e for e in self.ordered_elements if e.original_id in caption_ids]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.get_footnotes_for","title":"get_footnotes_for","text":"<pre><code>get_footnotes_for(element_id: int) -&gt; List[OrderedElement]\n</code></pre> <p>Get footnote elements for a given element ID.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def get_footnotes_for(self, element_id: int) -&gt; List[OrderedElement]:\n    \"\"\"Get footnote elements for a given element ID.\"\"\"\n    footnote_ids = self.footnote_map.get(element_id, [])\n    return [e for e in self.ordered_elements if e.original_id in footnote_ids]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"ordered_elements\": [e.to_dict() for e in self.ordered_elements],\n        \"caption_map\": self.caption_map,\n        \"footnote_map\": self.footnote_map,\n        \"merge_map\": self.merge_map,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"element_count\": self.element_count,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save to JSON file.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"Save to JSON file.\"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(\n    file_path: Union[str, Path],\n) -&gt; ReadingOrderOutput\n</code></pre> <p>Load from JSON file.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"ReadingOrderOutput\":\n    \"\"\"Load from JSON file.\"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.rule_based","title":"rule_based","text":"<p>Rule-based reading order predictor module.</p> <p>Provides rule-based reading order prediction using spatial analysis.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.rule_based.RuleBasedReadingOrderPredictor","title":"RuleBasedReadingOrderPredictor","text":"<pre><code>RuleBasedReadingOrderPredictor()\n</code></pre> <p>               Bases: <code>BaseReadingOrderPredictor</code></p> <p>Rule-based reading order predictor using spatial analysis.</p> <p>Uses R-tree spatial indexing and rule-based algorithms to determine the logical reading sequence of document elements. This is a CPU-only implementation that doesn't require GPU resources.</p> <p>Features: - Multi-column layout detection - Header/footer separation - Caption-to-figure/table association - Footnote linking - Element merge suggestions</p> Example <pre><code>from omnidocs.tasks.reading_order import RuleBasedReadingOrderPredictor\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\n# Initialize components\nlayout_extractor = DocLayoutYOLO(config=DocLayoutYOLOConfig())\nocr = EasyOCR(config=EasyOCRConfig())\npredictor = RuleBasedReadingOrderPredictor()\n\n# Process document\nlayout = layout_extractor.extract(image)\nocr_result = ocr.extract(image)\nreading_order = predictor.predict(layout, ocr_result)\n\n# Get text in reading order\ntext = reading_order.get_full_text()\n</code></pre> <p>Initialize the reading order predictor.</p> Source code in <code>omnidocs/tasks/reading_order/rule_based/predictor.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the reading order predictor.\"\"\"\n    self.dilated_page_element = True\n    # Apply horizontal dilation only if less than this page-width normalized threshold\n    self._horizontal_dilation_threshold_norm = 0.15\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.rule_based.RuleBasedReadingOrderPredictor.predict","title":"predict","text":"<pre><code>predict(\n    layout: LayoutOutput,\n    ocr: Optional[OCROutput] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput\n</code></pre> <p>Predict reading order for a single page.</p> PARAMETER DESCRIPTION <code>layout</code> <p>Layout detection results with bounding boxes</p> <p> TYPE: <code>LayoutOutput</code> </p> <code>ocr</code> <p>Optional OCR results for text content</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> <code>page_no</code> <p>Page number (for multi-page documents)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ReadingOrderOutput</code> <p>ReadingOrderOutput with ordered elements and associations</p> Source code in <code>omnidocs/tasks/reading_order/rule_based/predictor.py</code> <pre><code>def predict(\n    self,\n    layout: \"LayoutOutput\",\n    ocr: Optional[\"OCROutput\"] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput:\n    \"\"\"\n    Predict reading order for a single page.\n\n    Args:\n        layout: Layout detection results with bounding boxes\n        ocr: Optional OCR results for text content\n        page_no: Page number (for multi-page documents)\n\n    Returns:\n        ReadingOrderOutput with ordered elements and associations\n    \"\"\"\n    page_width = layout.image_width\n    page_height = layout.image_height\n\n    # Build text map from OCR if available\n    text_map: Dict[int, str] = {}\n    if ocr:\n        text_map = self._build_text_map(layout, ocr)\n\n    # Convert layout boxes to internal PageElements\n    page_elements: List[_PageElement] = []\n    for i, box in enumerate(layout.bboxes):\n        label_str = box.label.value.lower()\n        element_type = LABEL_TO_ELEMENT_TYPE.get(label_str, ElementType.OTHER)\n\n        # Convert from top-left origin to bottom-left origin\n        elem = _PageElement(\n            cid=i,\n            text=text_map.get(i, \"\"),\n            page_no=page_no,\n            page_width=page_width,\n            page_height=page_height,\n            label=element_type,\n            left=box.bbox.x1,\n            bottom=page_height - box.bbox.y2,  # Convert y2 to bottom\n            right=box.bbox.x2,\n            top=page_height - box.bbox.y1,  # Convert y1 to top\n        )\n        page_elements.append(elem)\n\n    # Run reading order prediction\n    sorted_elements = self._predict_reading_order(page_elements)\n\n    # Get caption associations\n    caption_map = self._find_to_captions(sorted_elements)\n\n    # Get footnote associations\n    footnote_map = self._find_to_footnotes(sorted_elements)\n\n    # Get merge suggestions\n    merge_map = self._predict_merges(sorted_elements)\n\n    # Convert to OrderedElements\n    ordered_elements: List[OrderedElement] = []\n    for idx, elem in enumerate(sorted_elements):\n        # Convert back from bottom-left to top-left origin\n        bbox = BoundingBox(\n            x1=elem.left,\n            y1=page_height - elem.top,\n            x2=elem.right,\n            y2=page_height - elem.bottom,\n        )\n\n        confidence = 1.0\n        if elem.cid &lt; len(layout.bboxes):\n            confidence = layout.bboxes[elem.cid].confidence\n\n        ordered_elem = OrderedElement(\n            index=idx,\n            element_type=elem.label,\n            bbox=bbox,\n            text=elem.text,\n            confidence=confidence,\n            page_no=page_no,\n            original_id=elem.cid,\n        )\n        ordered_elements.append(ordered_elem)\n\n    return ReadingOrderOutput(\n        ordered_elements=ordered_elements,\n        caption_map=caption_map,\n        footnote_map=footnote_map,\n        merge_map=merge_map,\n        image_width=page_width,\n        image_height=page_height,\n        model_name=\"RuleBasedReadingOrderPredictor\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.rule_based.predictor","title":"predictor","text":"<p>Rule-based reading order predictor.</p> <p>Uses spatial analysis and R-tree indexing to determine the logical reading sequence of document elements. Self-contained implementation without external dependencies on docling-ibm-models.</p> <p>Based on the algorithm from docling-ibm-models, adapted for omnidocs.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.rule_based.predictor.RuleBasedReadingOrderPredictor","title":"RuleBasedReadingOrderPredictor","text":"<pre><code>RuleBasedReadingOrderPredictor()\n</code></pre> <p>               Bases: <code>BaseReadingOrderPredictor</code></p> <p>Rule-based reading order predictor using spatial analysis.</p> <p>Uses R-tree spatial indexing and rule-based algorithms to determine the logical reading sequence of document elements. This is a CPU-only implementation that doesn't require GPU resources.</p> <p>Features: - Multi-column layout detection - Header/footer separation - Caption-to-figure/table association - Footnote linking - Element merge suggestions</p> Example <pre><code>from omnidocs.tasks.reading_order import RuleBasedReadingOrderPredictor\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\n# Initialize components\nlayout_extractor = DocLayoutYOLO(config=DocLayoutYOLOConfig())\nocr = EasyOCR(config=EasyOCRConfig())\npredictor = RuleBasedReadingOrderPredictor()\n\n# Process document\nlayout = layout_extractor.extract(image)\nocr_result = ocr.extract(image)\nreading_order = predictor.predict(layout, ocr_result)\n\n# Get text in reading order\ntext = reading_order.get_full_text()\n</code></pre> <p>Initialize the reading order predictor.</p> Source code in <code>omnidocs/tasks/reading_order/rule_based/predictor.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the reading order predictor.\"\"\"\n    self.dilated_page_element = True\n    # Apply horizontal dilation only if less than this page-width normalized threshold\n    self._horizontal_dilation_threshold_norm = 0.15\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.reading_order.rule_based.predictor.RuleBasedReadingOrderPredictor.predict","title":"predict","text":"<pre><code>predict(\n    layout: LayoutOutput,\n    ocr: Optional[OCROutput] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput\n</code></pre> <p>Predict reading order for a single page.</p> PARAMETER DESCRIPTION <code>layout</code> <p>Layout detection results with bounding boxes</p> <p> TYPE: <code>LayoutOutput</code> </p> <code>ocr</code> <p>Optional OCR results for text content</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> <code>page_no</code> <p>Page number (for multi-page documents)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ReadingOrderOutput</code> <p>ReadingOrderOutput with ordered elements and associations</p> Source code in <code>omnidocs/tasks/reading_order/rule_based/predictor.py</code> <pre><code>def predict(\n    self,\n    layout: \"LayoutOutput\",\n    ocr: Optional[\"OCROutput\"] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput:\n    \"\"\"\n    Predict reading order for a single page.\n\n    Args:\n        layout: Layout detection results with bounding boxes\n        ocr: Optional OCR results for text content\n        page_no: Page number (for multi-page documents)\n\n    Returns:\n        ReadingOrderOutput with ordered elements and associations\n    \"\"\"\n    page_width = layout.image_width\n    page_height = layout.image_height\n\n    # Build text map from OCR if available\n    text_map: Dict[int, str] = {}\n    if ocr:\n        text_map = self._build_text_map(layout, ocr)\n\n    # Convert layout boxes to internal PageElements\n    page_elements: List[_PageElement] = []\n    for i, box in enumerate(layout.bboxes):\n        label_str = box.label.value.lower()\n        element_type = LABEL_TO_ELEMENT_TYPE.get(label_str, ElementType.OTHER)\n\n        # Convert from top-left origin to bottom-left origin\n        elem = _PageElement(\n            cid=i,\n            text=text_map.get(i, \"\"),\n            page_no=page_no,\n            page_width=page_width,\n            page_height=page_height,\n            label=element_type,\n            left=box.bbox.x1,\n            bottom=page_height - box.bbox.y2,  # Convert y2 to bottom\n            right=box.bbox.x2,\n            top=page_height - box.bbox.y1,  # Convert y1 to top\n        )\n        page_elements.append(elem)\n\n    # Run reading order prediction\n    sorted_elements = self._predict_reading_order(page_elements)\n\n    # Get caption associations\n    caption_map = self._find_to_captions(sorted_elements)\n\n    # Get footnote associations\n    footnote_map = self._find_to_footnotes(sorted_elements)\n\n    # Get merge suggestions\n    merge_map = self._predict_merges(sorted_elements)\n\n    # Convert to OrderedElements\n    ordered_elements: List[OrderedElement] = []\n    for idx, elem in enumerate(sorted_elements):\n        # Convert back from bottom-left to top-left origin\n        bbox = BoundingBox(\n            x1=elem.left,\n            y1=page_height - elem.top,\n            x2=elem.right,\n            y2=page_height - elem.bottom,\n        )\n\n        confidence = 1.0\n        if elem.cid &lt; len(layout.bboxes):\n            confidence = layout.bboxes[elem.cid].confidence\n\n        ordered_elem = OrderedElement(\n            index=idx,\n            element_type=elem.label,\n            bbox=bbox,\n            text=elem.text,\n            confidence=confidence,\n            page_no=page_no,\n            original_id=elem.cid,\n        )\n        ordered_elements.append(ordered_elem)\n\n    return ReadingOrderOutput(\n        ordered_elements=ordered_elements,\n        caption_map=caption_map,\n        footnote_map=footnote_map,\n        merge_map=merge_map,\n        image_width=page_width,\n        image_height=page_height,\n        model_name=\"RuleBasedReadingOrderPredictor\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction","title":"table_extraction","text":"<p>Table Extraction Module.</p> <p>Provides extractors for detecting and extracting table structure from document images. Outputs structured table data with cells, spans, and multiple export formats (HTML, Markdown, Pandas DataFrame).</p> Available Extractors <ul> <li>TableFormerExtractor: Transformer-based table structure extractor</li> </ul> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Initialize extractor\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(mode=\"fast\", device=\"cuda\")\n)\n\n# Extract table structure\nresult = extractor.extract(table_image)\n\n# Get HTML output\nhtml = result.to_html()\n\n# Get DataFrame\ndf = result.to_dataframe()\n\n# Get Markdown\nmd = result.to_markdown()\n\n# Access cells\nfor cell in result.cells:\n    print(f\"[{cell.row},{cell.col}] {cell.text}\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.BaseTableExtractor","title":"BaseTableExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for table structure extractors.</p> <p>Table extractors analyze table images to detect cell structure, identify headers, and extract text content.</p> Example <pre><code>class MyTableExtractor(BaseTableExtractor):\n    def __init__(self, config: MyConfig):\n        self.config = config\n        self._load_model()\n\n    def _load_model(self):\n        # Load model weights\n        pass\n\n    def extract(self, image):\n        # Run extraction\n        return TableOutput(...)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.BaseTableExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    ocr_output: Optional[OCROutput] = None,\n) -&gt; TableOutput\n</code></pre> <p>Extract table structure from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Table image (should be cropped to table region)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>ocr_output</code> <p>Optional OCR results for cell text matching.        If not provided, model will attempt to extract text.</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TableOutput</code> <p>TableOutput with cells, structure, and export methods</p> Example <pre><code># Without OCR (model extracts text)\nresult = extractor.extract(table_image)\n\n# With OCR (better text quality)\nocr = some_ocr.extract(table_image)\nresult = extractor.extract(table_image, ocr_output=ocr)\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    ocr_output: Optional[\"OCROutput\"] = None,\n) -&gt; TableOutput:\n    \"\"\"\n    Extract table structure from an image.\n\n    Args:\n        image: Table image (should be cropped to table region)\n        ocr_output: Optional OCR results for cell text matching.\n                   If not provided, model will attempt to extract text.\n\n    Returns:\n        TableOutput with cells, structure, and export methods\n\n    Example:\n        ```python\n        # Without OCR (model extracts text)\n        result = extractor.extract(table_image)\n\n        # With OCR (better text quality)\n        ocr = some_ocr.extract(table_image)\n        result = extractor.extract(table_image, ocr_output=ocr)\n        ```\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.BaseTableExtractor.batch_extract","title":"batch_extract","text":"<pre><code>batch_extract(\n    images: List[Union[Image, ndarray, str, Path]],\n    ocr_outputs: Optional[List[OCROutput]] = None,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[TableOutput]\n</code></pre> <p>Extract tables from multiple images.</p> <p>Default implementation loops over extract(). Subclasses can override for optimized batching.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of table images</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>ocr_outputs</code> <p>Optional list of OCR results (same length as images)</p> <p> TYPE: <code>Optional[List[OCROutput]]</code> DEFAULT: <code>None</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TableOutput]</code> <p>List of TableOutput in same order as input</p> <p>Examples:</p> <pre><code>results = extractor.batch_extract(table_images)\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def batch_extract(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    ocr_outputs: Optional[List[\"OCROutput\"]] = None,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[TableOutput]:\n    \"\"\"\n    Extract tables from multiple images.\n\n    Default implementation loops over extract(). Subclasses can override\n    for optimized batching.\n\n    Args:\n        images: List of table images\n        ocr_outputs: Optional list of OCR results (same length as images)\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of TableOutput in same order as input\n\n    Examples:\n        ```python\n        results = extractor.batch_extract(table_images)\n        ```\n    \"\"\"\n    results = []\n    total = len(images)\n\n    for i, image in enumerate(images):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        ocr = ocr_outputs[i] if ocr_outputs else None\n        result = self.extract(image, ocr_output=ocr)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.BaseTableExtractor.extract_document","title":"extract_document","text":"<pre><code>extract_document(\n    document: Document,\n    table_bboxes: Optional[List[List[float]]] = None,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[TableOutput]\n</code></pre> <p>Extract tables from all pages of a document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>table_bboxes</code> <p>Optional list of table bounding boxes per page.          Each element should be a list of [x1, y1, x2, y2] coords.</p> <p> TYPE: <code>Optional[List[List[float]]]</code> DEFAULT: <code>None</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TableOutput]</code> <p>List of TableOutput, one per detected table</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresults = extractor.extract_document(doc)\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def extract_document(\n    self,\n    document: \"Document\",\n    table_bboxes: Optional[List[List[float]]] = None,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[TableOutput]:\n    \"\"\"\n    Extract tables from all pages of a document.\n\n    Args:\n        document: Document instance\n        table_bboxes: Optional list of table bounding boxes per page.\n                     Each element should be a list of [x1, y1, x2, y2] coords.\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of TableOutput, one per detected table\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        results = extractor.extract_document(doc)\n        ```\n    \"\"\"\n    results = []\n    total = document.page_count\n\n    for i, page in enumerate(document.iter_pages()):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        # If no bboxes provided, process entire page\n        if table_bboxes is None:\n            result = self.extract(page)\n            results.append(result)\n        else:\n            # Crop and process each table region\n            for bbox in table_bboxes:\n                x1, y1, x2, y2 = bbox\n                table_region = page.crop((x1, y1, x2, y2))\n                result = self.extract(table_region)\n                results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box in pixel coordinates.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.BoundingBox.from_ltrb","title":"from_ltrb  <code>classmethod</code>","text":"<pre><code>from_ltrb(\n    left: float, top: float, right: float, bottom: float\n) -&gt; BoundingBox\n</code></pre> <p>Create from left, top, right, bottom coordinates.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>@classmethod\ndef from_ltrb(cls, left: float, top: float, right: float, bottom: float) -&gt; \"BoundingBox\":\n    \"\"\"Create from left, top, right, bottom coordinates.\"\"\"\n    return cls(x1=left, y1=top, x2=right, y2=bottom)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.CellType","title":"CellType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of table cell.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableCell","title":"TableCell","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single table cell with position, span, and content.</p> <p>The cell position uses 0-indexed row/column indices. Spans indicate how many rows/columns the cell occupies.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableCell.end_row","title":"end_row  <code>property</code>","text":"<pre><code>end_row: int\n</code></pre> <p>Ending row index (exclusive).</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableCell.end_col","title":"end_col  <code>property</code>","text":"<pre><code>end_col: int\n</code></pre> <p>Ending column index (exclusive).</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableCell.is_header","title":"is_header  <code>property</code>","text":"<pre><code>is_header: bool\n</code></pre> <p>Check if cell is any type of header.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableCell.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"row\": self.row,\n        \"col\": self.col,\n        \"row_span\": self.row_span,\n        \"col_span\": self.col_span,\n        \"text\": self.text,\n        \"cell_type\": self.cell_type.value,\n        \"bbox\": self.bbox.to_list() if self.bbox else None,\n        \"confidence\": self.confidence,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableOutput","title":"TableOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete table extraction result.</p> <p>Provides multiple export formats and utility methods for working with extracted table data.</p> Example <pre><code>result = extractor.extract(table_image)\n\n# Basic info\nprint(f\"Table: {result.num_rows}x{result.num_cols}\")\n\n# Export to HTML\nhtml = result.to_html()\n\n# Export to Pandas\ndf = result.to_dataframe()\n\n# Export to Markdown\nmd = result.to_markdown()\n\n# Access specific cell\ncell = result.get_cell(row=0, col=0)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableOutput.cell_count","title":"cell_count  <code>property</code>","text":"<pre><code>cell_count: int\n</code></pre> <p>Number of cells in the table.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableOutput.has_headers","title":"has_headers  <code>property</code>","text":"<pre><code>has_headers: bool\n</code></pre> <p>Check if table has header cells.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableOutput.get_cell","title":"get_cell","text":"<pre><code>get_cell(row: int, col: int) -&gt; Optional[TableCell]\n</code></pre> <p>Get cell at specific position.</p> <p>Handles merged cells by returning the cell that covers the position.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def get_cell(self, row: int, col: int) -&gt; Optional[TableCell]:\n    \"\"\"\n    Get cell at specific position.\n\n    Handles merged cells by returning the cell that covers the position.\n    \"\"\"\n    for cell in self.cells:\n        if cell.row &lt;= row &lt; cell.end_row and cell.col &lt;= col &lt; cell.end_col:\n            return cell\n    return None\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableOutput.get_row","title":"get_row","text":"<pre><code>get_row(row: int) -&gt; List[TableCell]\n</code></pre> <p>Get all cells in a specific row.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def get_row(self, row: int) -&gt; List[TableCell]:\n    \"\"\"Get all cells in a specific row.\"\"\"\n    return [c for c in self.cells if c.row == row]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableOutput.get_column","title":"get_column","text":"<pre><code>get_column(col: int) -&gt; List[TableCell]\n</code></pre> <p>Get all cells in a specific column.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def get_column(self, col: int) -&gt; List[TableCell]:\n    \"\"\"Get all cells in a specific column.\"\"\"\n    return [c for c in self.cells if c.col == col]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableOutput.to_html","title":"to_html","text":"<pre><code>to_html(include_styles: bool = True) -&gt; str\n</code></pre> <p>Convert table to HTML string.</p> PARAMETER DESCRIPTION <code>include_styles</code> <p>Whether to include basic CSS styling</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>str</code> <p>HTML table string</p> Example <pre><code>html = result.to_html()\nwith open(\"table.html\", \"w\") as f:\n    f.write(html)\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_html(self, include_styles: bool = True) -&gt; str:\n    \"\"\"\n    Convert table to HTML string.\n\n    Args:\n        include_styles: Whether to include basic CSS styling\n\n    Returns:\n        HTML table string\n\n    Example:\n        ```python\n        html = result.to_html()\n        with open(\"table.html\", \"w\") as f:\n            f.write(html)\n        ```\n    \"\"\"\n    # Build 2D grid accounting for spans\n    grid: List[List[Optional[TableCell]]] = [[None for _ in range(self.num_cols)] for _ in range(self.num_rows)]\n\n    for cell in self.cells:\n        for r in range(cell.row, cell.end_row):\n            for c in range(cell.col, cell.end_col):\n                if r &lt; self.num_rows and c &lt; self.num_cols:\n                    grid[r][c] = cell\n\n    # Generate HTML\n    lines = []\n\n    if include_styles:\n        lines.append('&lt;table style=\"border-collapse: collapse; width: 100%;\"&gt;')\n    else:\n        lines.append(\"&lt;table&gt;\")\n\n    processed: set[Tuple[int, int]] = set()  # Track cells we've already output\n\n    for row_idx in range(self.num_rows):\n        lines.append(\"  &lt;tr&gt;\")\n\n        for col_idx in range(self.num_cols):\n            cell = grid[row_idx][col_idx]\n\n            if cell is None:\n                lines.append(\"    &lt;td&gt;&lt;/td&gt;\")\n                continue\n\n            # Skip if this cell was already output (merged cell)\n            cell_id = (cell.row, cell.col)\n            if cell_id in processed:\n                continue\n            processed.add(cell_id)\n\n            # Determine tag based on cell type\n            tag = \"th\" if cell.is_header else \"td\"\n\n            # Build attributes\n            attrs = []\n            if cell.row_span &gt; 1:\n                attrs.append(f'rowspan=\"{cell.row_span}\"')\n            if cell.col_span &gt; 1:\n                attrs.append(f'colspan=\"{cell.col_span}\"')\n            if include_styles:\n                attrs.append('style=\"border: 1px solid #ddd; padding: 8px;\"')\n\n            attr_str = \" \" + \" \".join(attrs) if attrs else \"\"\n\n            # Escape HTML in text\n            text = (cell.text or \"\").replace(\"&amp;\", \"&amp;amp;\").replace(\"&lt;\", \"&amp;lt;\").replace(\"&gt;\", \"&amp;gt;\")\n\n            lines.append(f\"    &lt;{tag}{attr_str}&gt;{text}&lt;/{tag}&gt;\")\n\n        lines.append(\"  &lt;/tr&gt;\")\n\n    lines.append(\"&lt;/table&gt;\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableOutput.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe()\n</code></pre> <p>Convert table to Pandas DataFrame.</p> RETURNS DESCRIPTION <p>pandas.DataFrame with table data</p> RAISES DESCRIPTION <code>ImportError</code> <p>If pandas is not installed</p> Example <pre><code>df = result.to_dataframe()\nprint(df.head())\ndf.to_csv(\"table.csv\")\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_dataframe(self):\n    \"\"\"\n    Convert table to Pandas DataFrame.\n\n    Returns:\n        pandas.DataFrame with table data\n\n    Raises:\n        ImportError: If pandas is not installed\n\n    Example:\n        ```python\n        df = result.to_dataframe()\n        print(df.head())\n        df.to_csv(\"table.csv\")\n        ```\n    \"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\"pandas is required for to_dataframe(). Install with: pip install pandas\")\n\n    # Build 2D array\n    data: List[List[Optional[str]]] = [[None for _ in range(self.num_cols)] for _ in range(self.num_rows)]\n\n    for cell in self.cells:\n        # For merged cells, put value in top-left position\n        if cell.row &lt; self.num_rows and cell.col &lt; self.num_cols:\n            data[cell.row][cell.col] = cell.text\n\n    # Determine if first row is header\n    first_row_cells = self.get_row(0)\n    use_header = all(c.cell_type == CellType.COLUMN_HEADER for c in first_row_cells) if first_row_cells else False\n\n    if use_header and self.num_rows &gt; 1:\n        headers = data[0]\n        data = data[1:]\n        return pd.DataFrame(data, columns=headers)\n    else:\n        return pd.DataFrame(data)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableOutput.to_markdown","title":"to_markdown","text":"<pre><code>to_markdown() -&gt; str\n</code></pre> <p>Convert table to Markdown format.</p> <p>Note: Markdown tables don't support merged cells, so spans are ignored and only the top-left cell value is used.</p> RETURNS DESCRIPTION <code>str</code> <p>Markdown table string</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_markdown(self) -&gt; str:\n    \"\"\"\n    Convert table to Markdown format.\n\n    Note: Markdown tables don't support merged cells, so spans\n    are ignored and only the top-left cell value is used.\n\n    Returns:\n        Markdown table string\n    \"\"\"\n    if self.num_rows == 0 or self.num_cols == 0:\n        return \"\"\n\n    # Build 2D grid\n    grid: List[List[str]] = [[\"\" for _ in range(self.num_cols)] for _ in range(self.num_rows)]\n\n    for cell in self.cells:\n        if cell.row &lt; self.num_rows and cell.col &lt; self.num_cols:\n            grid[cell.row][cell.col] = cell.text or \"\"\n\n    lines = []\n\n    # Header row\n    lines.append(\"| \" + \" | \".join(grid[0]) + \" |\")\n\n    # Separator\n    lines.append(\"| \" + \" | \".join([\"---\"] * self.num_cols) + \" |\")\n\n    # Data rows\n    for row in grid[1:]:\n        lines.append(\"| \" + \" | \".join(row) + \" |\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"cells\": [c.to_dict() for c in self.cells],\n        \"num_rows\": self.num_rows,\n        \"num_cols\": self.num_cols,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"html\": self.to_html(include_styles=False),\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableOutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save to JSON file.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"Save to JSON file.\"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableOutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; TableOutput\n</code></pre> <p>Load from JSON file.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"TableOutput\":\n    \"\"\"Load from JSON file.\"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableFormerConfig","title":"TableFormerConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for TableFormer table structure extractor.</p> <p>TableFormer is a transformer-based model that predicts table structure using OTSL (Optimal Table Structure Language) tags and cell bounding boxes.</p> ATTRIBUTE DESCRIPTION <code>mode</code> <p>Inference mode - \"fast\" or \"accurate\"</p> <p> TYPE: <code>TableFormerMode</code> </p> <code>device</code> <p>Device for inference - \"cpu\", \"cuda\", \"mps\", or \"auto\"</p> <p> TYPE: <code>Literal['cpu', 'cuda', 'mps', 'auto']</code> </p> <code>num_threads</code> <p>Number of CPU threads for inference</p> <p> TYPE: <code>int</code> </p> <code>do_cell_matching</code> <p>Whether to match predicted cells with OCR text cells</p> <p> TYPE: <code>bool</code> </p> <code>artifacts_path</code> <p>Path to pre-downloaded model artifacts</p> <p> TYPE: <code>Optional[str]</code> </p> <code>repo_id</code> <p>HuggingFace model repository</p> <p> TYPE: <code>str</code> </p> <code>revision</code> <p>Model revision/tag</p> <p> TYPE: <code>str</code> </p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Fast mode\nextractor = TableFormerExtractor(config=TableFormerConfig(mode=\"fast\"))\n\n# Accurate mode with GPU\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(\n        mode=\"accurate\",\n        device=\"cuda\",\n        do_cell_matching=True,\n    )\n)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableFormerExtractor","title":"TableFormerExtractor","text":"<pre><code>TableFormerExtractor(config: TableFormerConfig)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>Table structure extractor using TableFormer model.</p> <p>TableFormer is a transformer-based model that predicts table structure using OTSL (Optimal Table Structure Language) tags. It can detect: - Cell boundaries (bounding boxes) - Row and column spans - Header cells (column and row headers) - Section rows</p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Initialize extractor\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(mode=\"fast\", device=\"cuda\")\n)\n\n# Extract table structure\nresult = extractor.extract(table_image)\n\n# Get HTML output\nhtml = result.to_html()\n\n# Get DataFrame\ndf = result.to_dataframe()\n</code></pre> <p>Initialize TableFormer extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>TableFormerConfig with model settings</p> <p> TYPE: <code>TableFormerConfig</code> </p> Source code in <code>omnidocs/tasks/table_extraction/tableformer/pytorch.py</code> <pre><code>def __init__(self, config: TableFormerConfig):\n    \"\"\"\n    Initialize TableFormer extractor.\n\n    Args:\n        config: TableFormerConfig with model settings\n    \"\"\"\n    self.config = config\n    self._device = _resolve_device(config.device)\n    self._predictor = None\n    self._model_config: Optional[Dict] = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableFormerExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    ocr_output: Optional[OCROutput] = None,\n) -&gt; TableOutput\n</code></pre> <p>Extract table structure from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Table image (should be cropped to table region)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>ocr_output</code> <p>Optional OCR results for cell text matching</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TableOutput</code> <p>TableOutput with cells, structure, and export methods</p> Example <pre><code>result = extractor.extract(table_image)\nprint(f\"Table: {result.num_rows}x{result.num_cols}\")\nhtml = result.to_html()\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/tableformer/pytorch.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    ocr_output: Optional[\"OCROutput\"] = None,\n) -&gt; TableOutput:\n    \"\"\"\n    Extract table structure from an image.\n\n    Args:\n        image: Table image (should be cropped to table region)\n        ocr_output: Optional OCR results for cell text matching\n\n    Returns:\n        TableOutput with cells, structure, and export methods\n\n    Example:\n        ```python\n        result = extractor.extract(table_image)\n        print(f\"Table: {result.num_rows}x{result.num_cols}\")\n        html = result.to_html()\n        ```\n    \"\"\"\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Convert to OpenCV format (required by TFPredictor)\n    try:\n        import cv2\n    except ImportError:\n        raise ImportError(\n            \"opencv-python is required for TableFormerExtractor. Install with: pip install opencv-python-headless\"\n        )\n\n    cv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n\n    # Build iOCR page data\n    tokens = self._build_tokens_from_ocr(ocr_output) if ocr_output else []\n    iocr_page = {\n        \"width\": width,\n        \"height\": height,\n        \"image\": cv_image,\n        \"tokens\": tokens,\n    }\n\n    # Table bbox is the entire image\n    table_bbox = [0, 0, width, height]\n\n    # Run prediction\n    results = self._predictor.multi_table_predict(\n        iocr_page=iocr_page,\n        table_bboxes=[table_bbox],\n        do_matching=self.config.do_cell_matching,\n        correct_overlapping_cells=self.config.correct_overlapping_cells,\n        sort_row_col_indexes=self.config.sort_row_col_indexes,\n    )\n\n    # Convert results to TableOutput\n    return self._convert_results(results, width, height)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.TableFormerMode","title":"TableFormerMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>TableFormer inference mode.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.base","title":"base","text":"<p>Base class for table extractors.</p> <p>Defines the abstract interface that all table extractors must implement.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.base.BaseTableExtractor","title":"BaseTableExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for table structure extractors.</p> <p>Table extractors analyze table images to detect cell structure, identify headers, and extract text content.</p> Example <pre><code>class MyTableExtractor(BaseTableExtractor):\n    def __init__(self, config: MyConfig):\n        self.config = config\n        self._load_model()\n\n    def _load_model(self):\n        # Load model weights\n        pass\n\n    def extract(self, image):\n        # Run extraction\n        return TableOutput(...)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.base.BaseTableExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    ocr_output: Optional[OCROutput] = None,\n) -&gt; TableOutput\n</code></pre> <p>Extract table structure from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Table image (should be cropped to table region)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>ocr_output</code> <p>Optional OCR results for cell text matching.        If not provided, model will attempt to extract text.</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TableOutput</code> <p>TableOutput with cells, structure, and export methods</p> Example <pre><code># Without OCR (model extracts text)\nresult = extractor.extract(table_image)\n\n# With OCR (better text quality)\nocr = some_ocr.extract(table_image)\nresult = extractor.extract(table_image, ocr_output=ocr)\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    ocr_output: Optional[\"OCROutput\"] = None,\n) -&gt; TableOutput:\n    \"\"\"\n    Extract table structure from an image.\n\n    Args:\n        image: Table image (should be cropped to table region)\n        ocr_output: Optional OCR results for cell text matching.\n                   If not provided, model will attempt to extract text.\n\n    Returns:\n        TableOutput with cells, structure, and export methods\n\n    Example:\n        ```python\n        # Without OCR (model extracts text)\n        result = extractor.extract(table_image)\n\n        # With OCR (better text quality)\n        ocr = some_ocr.extract(table_image)\n        result = extractor.extract(table_image, ocr_output=ocr)\n        ```\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.base.BaseTableExtractor.batch_extract","title":"batch_extract","text":"<pre><code>batch_extract(\n    images: List[Union[Image, ndarray, str, Path]],\n    ocr_outputs: Optional[List[OCROutput]] = None,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[TableOutput]\n</code></pre> <p>Extract tables from multiple images.</p> <p>Default implementation loops over extract(). Subclasses can override for optimized batching.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of table images</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>ocr_outputs</code> <p>Optional list of OCR results (same length as images)</p> <p> TYPE: <code>Optional[List[OCROutput]]</code> DEFAULT: <code>None</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TableOutput]</code> <p>List of TableOutput in same order as input</p> <p>Examples:</p> <pre><code>results = extractor.batch_extract(table_images)\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def batch_extract(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    ocr_outputs: Optional[List[\"OCROutput\"]] = None,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[TableOutput]:\n    \"\"\"\n    Extract tables from multiple images.\n\n    Default implementation loops over extract(). Subclasses can override\n    for optimized batching.\n\n    Args:\n        images: List of table images\n        ocr_outputs: Optional list of OCR results (same length as images)\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of TableOutput in same order as input\n\n    Examples:\n        ```python\n        results = extractor.batch_extract(table_images)\n        ```\n    \"\"\"\n    results = []\n    total = len(images)\n\n    for i, image in enumerate(images):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        ocr = ocr_outputs[i] if ocr_outputs else None\n        result = self.extract(image, ocr_output=ocr)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.base.BaseTableExtractor.extract_document","title":"extract_document","text":"<pre><code>extract_document(\n    document: Document,\n    table_bboxes: Optional[List[List[float]]] = None,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[TableOutput]\n</code></pre> <p>Extract tables from all pages of a document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>table_bboxes</code> <p>Optional list of table bounding boxes per page.          Each element should be a list of [x1, y1, x2, y2] coords.</p> <p> TYPE: <code>Optional[List[List[float]]]</code> DEFAULT: <code>None</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TableOutput]</code> <p>List of TableOutput, one per detected table</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresults = extractor.extract_document(doc)\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def extract_document(\n    self,\n    document: \"Document\",\n    table_bboxes: Optional[List[List[float]]] = None,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[TableOutput]:\n    \"\"\"\n    Extract tables from all pages of a document.\n\n    Args:\n        document: Document instance\n        table_bboxes: Optional list of table bounding boxes per page.\n                     Each element should be a list of [x1, y1, x2, y2] coords.\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of TableOutput, one per detected table\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        results = extractor.extract_document(doc)\n        ```\n    \"\"\"\n    results = []\n    total = document.page_count\n\n    for i, page in enumerate(document.iter_pages()):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        # If no bboxes provided, process entire page\n        if table_bboxes is None:\n            result = self.extract(page)\n            results.append(result)\n        else:\n            # Crop and process each table region\n            for bbox in table_bboxes:\n                x1, y1, x2, y2 = bbox\n                table_region = page.crop((x1, y1, x2, y2))\n                result = self.extract(table_region)\n                results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models","title":"models","text":"<p>Pydantic models for table extraction outputs.</p> <p>Provides structured table data with cells, spans, and multiple export formats including HTML, Markdown, and Pandas DataFrame conversion.</p> Example <pre><code>result = extractor.extract(table_image)\n\n# Get HTML\nhtml = result.to_html()\n\n# Get Pandas DataFrame\ndf = result.to_dataframe()\n\n# Access cells\nfor cell in result.cells:\n    print(f\"[{cell.row},{cell.col}] {cell.text}\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.CellType","title":"CellType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of table cell.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box in pixel coordinates.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.BoundingBox.from_ltrb","title":"from_ltrb  <code>classmethod</code>","text":"<pre><code>from_ltrb(\n    left: float, top: float, right: float, bottom: float\n) -&gt; BoundingBox\n</code></pre> <p>Create from left, top, right, bottom coordinates.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>@classmethod\ndef from_ltrb(cls, left: float, top: float, right: float, bottom: float) -&gt; \"BoundingBox\":\n    \"\"\"Create from left, top, right, bottom coordinates.\"\"\"\n    return cls(x1=left, y1=top, x2=right, y2=bottom)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.TableCell","title":"TableCell","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single table cell with position, span, and content.</p> <p>The cell position uses 0-indexed row/column indices. Spans indicate how many rows/columns the cell occupies.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.TableCell.end_row","title":"end_row  <code>property</code>","text":"<pre><code>end_row: int\n</code></pre> <p>Ending row index (exclusive).</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.TableCell.end_col","title":"end_col  <code>property</code>","text":"<pre><code>end_col: int\n</code></pre> <p>Ending column index (exclusive).</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.TableCell.is_header","title":"is_header  <code>property</code>","text":"<pre><code>is_header: bool\n</code></pre> <p>Check if cell is any type of header.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.TableCell.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"row\": self.row,\n        \"col\": self.col,\n        \"row_span\": self.row_span,\n        \"col_span\": self.col_span,\n        \"text\": self.text,\n        \"cell_type\": self.cell_type.value,\n        \"bbox\": self.bbox.to_list() if self.bbox else None,\n        \"confidence\": self.confidence,\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.TableOutput","title":"TableOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete table extraction result.</p> <p>Provides multiple export formats and utility methods for working with extracted table data.</p> Example <pre><code>result = extractor.extract(table_image)\n\n# Basic info\nprint(f\"Table: {result.num_rows}x{result.num_cols}\")\n\n# Export to HTML\nhtml = result.to_html()\n\n# Export to Pandas\ndf = result.to_dataframe()\n\n# Export to Markdown\nmd = result.to_markdown()\n\n# Access specific cell\ncell = result.get_cell(row=0, col=0)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.TableOutput.cell_count","title":"cell_count  <code>property</code>","text":"<pre><code>cell_count: int\n</code></pre> <p>Number of cells in the table.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.TableOutput.has_headers","title":"has_headers  <code>property</code>","text":"<pre><code>has_headers: bool\n</code></pre> <p>Check if table has header cells.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.TableOutput.get_cell","title":"get_cell","text":"<pre><code>get_cell(row: int, col: int) -&gt; Optional[TableCell]\n</code></pre> <p>Get cell at specific position.</p> <p>Handles merged cells by returning the cell that covers the position.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def get_cell(self, row: int, col: int) -&gt; Optional[TableCell]:\n    \"\"\"\n    Get cell at specific position.\n\n    Handles merged cells by returning the cell that covers the position.\n    \"\"\"\n    for cell in self.cells:\n        if cell.row &lt;= row &lt; cell.end_row and cell.col &lt;= col &lt; cell.end_col:\n            return cell\n    return None\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.TableOutput.get_row","title":"get_row","text":"<pre><code>get_row(row: int) -&gt; List[TableCell]\n</code></pre> <p>Get all cells in a specific row.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def get_row(self, row: int) -&gt; List[TableCell]:\n    \"\"\"Get all cells in a specific row.\"\"\"\n    return [c for c in self.cells if c.row == row]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.TableOutput.get_column","title":"get_column","text":"<pre><code>get_column(col: int) -&gt; List[TableCell]\n</code></pre> <p>Get all cells in a specific column.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def get_column(self, col: int) -&gt; List[TableCell]:\n    \"\"\"Get all cells in a specific column.\"\"\"\n    return [c for c in self.cells if c.col == col]\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.TableOutput.to_html","title":"to_html","text":"<pre><code>to_html(include_styles: bool = True) -&gt; str\n</code></pre> <p>Convert table to HTML string.</p> PARAMETER DESCRIPTION <code>include_styles</code> <p>Whether to include basic CSS styling</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>str</code> <p>HTML table string</p> Example <pre><code>html = result.to_html()\nwith open(\"table.html\", \"w\") as f:\n    f.write(html)\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_html(self, include_styles: bool = True) -&gt; str:\n    \"\"\"\n    Convert table to HTML string.\n\n    Args:\n        include_styles: Whether to include basic CSS styling\n\n    Returns:\n        HTML table string\n\n    Example:\n        ```python\n        html = result.to_html()\n        with open(\"table.html\", \"w\") as f:\n            f.write(html)\n        ```\n    \"\"\"\n    # Build 2D grid accounting for spans\n    grid: List[List[Optional[TableCell]]] = [[None for _ in range(self.num_cols)] for _ in range(self.num_rows)]\n\n    for cell in self.cells:\n        for r in range(cell.row, cell.end_row):\n            for c in range(cell.col, cell.end_col):\n                if r &lt; self.num_rows and c &lt; self.num_cols:\n                    grid[r][c] = cell\n\n    # Generate HTML\n    lines = []\n\n    if include_styles:\n        lines.append('&lt;table style=\"border-collapse: collapse; width: 100%;\"&gt;')\n    else:\n        lines.append(\"&lt;table&gt;\")\n\n    processed: set[Tuple[int, int]] = set()  # Track cells we've already output\n\n    for row_idx in range(self.num_rows):\n        lines.append(\"  &lt;tr&gt;\")\n\n        for col_idx in range(self.num_cols):\n            cell = grid[row_idx][col_idx]\n\n            if cell is None:\n                lines.append(\"    &lt;td&gt;&lt;/td&gt;\")\n                continue\n\n            # Skip if this cell was already output (merged cell)\n            cell_id = (cell.row, cell.col)\n            if cell_id in processed:\n                continue\n            processed.add(cell_id)\n\n            # Determine tag based on cell type\n            tag = \"th\" if cell.is_header else \"td\"\n\n            # Build attributes\n            attrs = []\n            if cell.row_span &gt; 1:\n                attrs.append(f'rowspan=\"{cell.row_span}\"')\n            if cell.col_span &gt; 1:\n                attrs.append(f'colspan=\"{cell.col_span}\"')\n            if include_styles:\n                attrs.append('style=\"border: 1px solid #ddd; padding: 8px;\"')\n\n            attr_str = \" \" + \" \".join(attrs) if attrs else \"\"\n\n            # Escape HTML in text\n            text = (cell.text or \"\").replace(\"&amp;\", \"&amp;amp;\").replace(\"&lt;\", \"&amp;lt;\").replace(\"&gt;\", \"&amp;gt;\")\n\n            lines.append(f\"    &lt;{tag}{attr_str}&gt;{text}&lt;/{tag}&gt;\")\n\n        lines.append(\"  &lt;/tr&gt;\")\n\n    lines.append(\"&lt;/table&gt;\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.TableOutput.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe()\n</code></pre> <p>Convert table to Pandas DataFrame.</p> RETURNS DESCRIPTION <p>pandas.DataFrame with table data</p> RAISES DESCRIPTION <code>ImportError</code> <p>If pandas is not installed</p> Example <pre><code>df = result.to_dataframe()\nprint(df.head())\ndf.to_csv(\"table.csv\")\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_dataframe(self):\n    \"\"\"\n    Convert table to Pandas DataFrame.\n\n    Returns:\n        pandas.DataFrame with table data\n\n    Raises:\n        ImportError: If pandas is not installed\n\n    Example:\n        ```python\n        df = result.to_dataframe()\n        print(df.head())\n        df.to_csv(\"table.csv\")\n        ```\n    \"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\"pandas is required for to_dataframe(). Install with: pip install pandas\")\n\n    # Build 2D array\n    data: List[List[Optional[str]]] = [[None for _ in range(self.num_cols)] for _ in range(self.num_rows)]\n\n    for cell in self.cells:\n        # For merged cells, put value in top-left position\n        if cell.row &lt; self.num_rows and cell.col &lt; self.num_cols:\n            data[cell.row][cell.col] = cell.text\n\n    # Determine if first row is header\n    first_row_cells = self.get_row(0)\n    use_header = all(c.cell_type == CellType.COLUMN_HEADER for c in first_row_cells) if first_row_cells else False\n\n    if use_header and self.num_rows &gt; 1:\n        headers = data[0]\n        data = data[1:]\n        return pd.DataFrame(data, columns=headers)\n    else:\n        return pd.DataFrame(data)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.TableOutput.to_markdown","title":"to_markdown","text":"<pre><code>to_markdown() -&gt; str\n</code></pre> <p>Convert table to Markdown format.</p> <p>Note: Markdown tables don't support merged cells, so spans are ignored and only the top-left cell value is used.</p> RETURNS DESCRIPTION <code>str</code> <p>Markdown table string</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_markdown(self) -&gt; str:\n    \"\"\"\n    Convert table to Markdown format.\n\n    Note: Markdown tables don't support merged cells, so spans\n    are ignored and only the top-left cell value is used.\n\n    Returns:\n        Markdown table string\n    \"\"\"\n    if self.num_rows == 0 or self.num_cols == 0:\n        return \"\"\n\n    # Build 2D grid\n    grid: List[List[str]] = [[\"\" for _ in range(self.num_cols)] for _ in range(self.num_rows)]\n\n    for cell in self.cells:\n        if cell.row &lt; self.num_rows and cell.col &lt; self.num_cols:\n            grid[cell.row][cell.col] = cell.text or \"\"\n\n    lines = []\n\n    # Header row\n    lines.append(\"| \" + \" | \".join(grid[0]) + \" |\")\n\n    # Separator\n    lines.append(\"| \" + \" | \".join([\"---\"] * self.num_cols) + \" |\")\n\n    # Data rows\n    for row in grid[1:]:\n        lines.append(\"| \" + \" | \".join(row) + \" |\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.TableOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"cells\": [c.to_dict() for c in self.cells],\n        \"num_rows\": self.num_rows,\n        \"num_cols\": self.num_cols,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"html\": self.to_html(include_styles=False),\n    }\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.TableOutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save to JSON file.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"Save to JSON file.\"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.models.TableOutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; TableOutput\n</code></pre> <p>Load from JSON file.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"TableOutput\":\n    \"\"\"Load from JSON file.\"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.tableformer","title":"tableformer","text":"<p>TableFormer module for table structure extraction.</p> <p>Provides the TableFormer-based table structure extractor.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.tableformer.TableFormerConfig","title":"TableFormerConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for TableFormer table structure extractor.</p> <p>TableFormer is a transformer-based model that predicts table structure using OTSL (Optimal Table Structure Language) tags and cell bounding boxes.</p> ATTRIBUTE DESCRIPTION <code>mode</code> <p>Inference mode - \"fast\" or \"accurate\"</p> <p> TYPE: <code>TableFormerMode</code> </p> <code>device</code> <p>Device for inference - \"cpu\", \"cuda\", \"mps\", or \"auto\"</p> <p> TYPE: <code>Literal['cpu', 'cuda', 'mps', 'auto']</code> </p> <code>num_threads</code> <p>Number of CPU threads for inference</p> <p> TYPE: <code>int</code> </p> <code>do_cell_matching</code> <p>Whether to match predicted cells with OCR text cells</p> <p> TYPE: <code>bool</code> </p> <code>artifacts_path</code> <p>Path to pre-downloaded model artifacts</p> <p> TYPE: <code>Optional[str]</code> </p> <code>repo_id</code> <p>HuggingFace model repository</p> <p> TYPE: <code>str</code> </p> <code>revision</code> <p>Model revision/tag</p> <p> TYPE: <code>str</code> </p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Fast mode\nextractor = TableFormerExtractor(config=TableFormerConfig(mode=\"fast\"))\n\n# Accurate mode with GPU\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(\n        mode=\"accurate\",\n        device=\"cuda\",\n        do_cell_matching=True,\n    )\n)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.tableformer.TableFormerMode","title":"TableFormerMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>TableFormer inference mode.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.tableformer.TableFormerExtractor","title":"TableFormerExtractor","text":"<pre><code>TableFormerExtractor(config: TableFormerConfig)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>Table structure extractor using TableFormer model.</p> <p>TableFormer is a transformer-based model that predicts table structure using OTSL (Optimal Table Structure Language) tags. It can detect: - Cell boundaries (bounding boxes) - Row and column spans - Header cells (column and row headers) - Section rows</p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Initialize extractor\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(mode=\"fast\", device=\"cuda\")\n)\n\n# Extract table structure\nresult = extractor.extract(table_image)\n\n# Get HTML output\nhtml = result.to_html()\n\n# Get DataFrame\ndf = result.to_dataframe()\n</code></pre> <p>Initialize TableFormer extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>TableFormerConfig with model settings</p> <p> TYPE: <code>TableFormerConfig</code> </p> Source code in <code>omnidocs/tasks/table_extraction/tableformer/pytorch.py</code> <pre><code>def __init__(self, config: TableFormerConfig):\n    \"\"\"\n    Initialize TableFormer extractor.\n\n    Args:\n        config: TableFormerConfig with model settings\n    \"\"\"\n    self.config = config\n    self._device = _resolve_device(config.device)\n    self._predictor = None\n    self._model_config: Optional[Dict] = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.tableformer.TableFormerExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    ocr_output: Optional[OCROutput] = None,\n) -&gt; TableOutput\n</code></pre> <p>Extract table structure from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Table image (should be cropped to table region)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>ocr_output</code> <p>Optional OCR results for cell text matching</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TableOutput</code> <p>TableOutput with cells, structure, and export methods</p> Example <pre><code>result = extractor.extract(table_image)\nprint(f\"Table: {result.num_rows}x{result.num_cols}\")\nhtml = result.to_html()\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/tableformer/pytorch.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    ocr_output: Optional[\"OCROutput\"] = None,\n) -&gt; TableOutput:\n    \"\"\"\n    Extract table structure from an image.\n\n    Args:\n        image: Table image (should be cropped to table region)\n        ocr_output: Optional OCR results for cell text matching\n\n    Returns:\n        TableOutput with cells, structure, and export methods\n\n    Example:\n        ```python\n        result = extractor.extract(table_image)\n        print(f\"Table: {result.num_rows}x{result.num_cols}\")\n        html = result.to_html()\n        ```\n    \"\"\"\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Convert to OpenCV format (required by TFPredictor)\n    try:\n        import cv2\n    except ImportError:\n        raise ImportError(\n            \"opencv-python is required for TableFormerExtractor. Install with: pip install opencv-python-headless\"\n        )\n\n    cv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n\n    # Build iOCR page data\n    tokens = self._build_tokens_from_ocr(ocr_output) if ocr_output else []\n    iocr_page = {\n        \"width\": width,\n        \"height\": height,\n        \"image\": cv_image,\n        \"tokens\": tokens,\n    }\n\n    # Table bbox is the entire image\n    table_bbox = [0, 0, width, height]\n\n    # Run prediction\n    results = self._predictor.multi_table_predict(\n        iocr_page=iocr_page,\n        table_bboxes=[table_bbox],\n        do_matching=self.config.do_cell_matching,\n        correct_overlapping_cells=self.config.correct_overlapping_cells,\n        sort_row_col_indexes=self.config.sort_row_col_indexes,\n    )\n\n    # Convert results to TableOutput\n    return self._convert_results(results, width, height)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.tableformer.config","title":"config","text":"<p>Configuration for TableFormer table structure extractor.</p> <p>TableFormer uses a dual-decoder transformer architecture with OTSL+ support for recognizing table structure from images.</p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Fast mode (default)\nextractor = TableFormerExtractor(config=TableFormerConfig())\n\n# Accurate mode with GPU\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(\n        mode=\"accurate\",\n        device=\"cuda\",\n        do_cell_matching=True,\n    )\n)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.tableformer.config.TableFormerMode","title":"TableFormerMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>TableFormer inference mode.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.tableformer.config.TableFormerConfig","title":"TableFormerConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for TableFormer table structure extractor.</p> <p>TableFormer is a transformer-based model that predicts table structure using OTSL (Optimal Table Structure Language) tags and cell bounding boxes.</p> ATTRIBUTE DESCRIPTION <code>mode</code> <p>Inference mode - \"fast\" or \"accurate\"</p> <p> TYPE: <code>TableFormerMode</code> </p> <code>device</code> <p>Device for inference - \"cpu\", \"cuda\", \"mps\", or \"auto\"</p> <p> TYPE: <code>Literal['cpu', 'cuda', 'mps', 'auto']</code> </p> <code>num_threads</code> <p>Number of CPU threads for inference</p> <p> TYPE: <code>int</code> </p> <code>do_cell_matching</code> <p>Whether to match predicted cells with OCR text cells</p> <p> TYPE: <code>bool</code> </p> <code>artifacts_path</code> <p>Path to pre-downloaded model artifacts</p> <p> TYPE: <code>Optional[str]</code> </p> <code>repo_id</code> <p>HuggingFace model repository</p> <p> TYPE: <code>str</code> </p> <code>revision</code> <p>Model revision/tag</p> <p> TYPE: <code>str</code> </p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Fast mode\nextractor = TableFormerExtractor(config=TableFormerConfig(mode=\"fast\"))\n\n# Accurate mode with GPU\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(\n        mode=\"accurate\",\n        device=\"cuda\",\n        do_cell_matching=True,\n    )\n)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.tableformer.pytorch","title":"pytorch","text":"<p>TableFormer extractor implementation using PyTorch backend.</p> <p>Uses the TFPredictor from docling-ibm-models for table structure recognition.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.tableformer.pytorch.TableFormerExtractor","title":"TableFormerExtractor","text":"<pre><code>TableFormerExtractor(config: TableFormerConfig)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>Table structure extractor using TableFormer model.</p> <p>TableFormer is a transformer-based model that predicts table structure using OTSL (Optimal Table Structure Language) tags. It can detect: - Cell boundaries (bounding boxes) - Row and column spans - Header cells (column and row headers) - Section rows</p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Initialize extractor\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(mode=\"fast\", device=\"cuda\")\n)\n\n# Extract table structure\nresult = extractor.extract(table_image)\n\n# Get HTML output\nhtml = result.to_html()\n\n# Get DataFrame\ndf = result.to_dataframe()\n</code></pre> <p>Initialize TableFormer extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>TableFormerConfig with model settings</p> <p> TYPE: <code>TableFormerConfig</code> </p> Source code in <code>omnidocs/tasks/table_extraction/tableformer/pytorch.py</code> <pre><code>def __init__(self, config: TableFormerConfig):\n    \"\"\"\n    Initialize TableFormer extractor.\n\n    Args:\n        config: TableFormerConfig with model settings\n    \"\"\"\n    self.config = config\n    self._device = _resolve_device(config.device)\n    self._predictor = None\n    self._model_config: Optional[Dict] = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.table_extraction.tableformer.pytorch.TableFormerExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    ocr_output: Optional[OCROutput] = None,\n) -&gt; TableOutput\n</code></pre> <p>Extract table structure from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Table image (should be cropped to table region)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>ocr_output</code> <p>Optional OCR results for cell text matching</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TableOutput</code> <p>TableOutput with cells, structure, and export methods</p> Example <pre><code>result = extractor.extract(table_image)\nprint(f\"Table: {result.num_rows}x{result.num_cols}\")\nhtml = result.to_html()\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/tableformer/pytorch.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    ocr_output: Optional[\"OCROutput\"] = None,\n) -&gt; TableOutput:\n    \"\"\"\n    Extract table structure from an image.\n\n    Args:\n        image: Table image (should be cropped to table region)\n        ocr_output: Optional OCR results for cell text matching\n\n    Returns:\n        TableOutput with cells, structure, and export methods\n\n    Example:\n        ```python\n        result = extractor.extract(table_image)\n        print(f\"Table: {result.num_rows}x{result.num_cols}\")\n        html = result.to_html()\n        ```\n    \"\"\"\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Convert to OpenCV format (required by TFPredictor)\n    try:\n        import cv2\n    except ImportError:\n        raise ImportError(\n            \"opencv-python is required for TableFormerExtractor. Install with: pip install opencv-python-headless\"\n        )\n\n    cv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n\n    # Build iOCR page data\n    tokens = self._build_tokens_from_ocr(ocr_output) if ocr_output else []\n    iocr_page = {\n        \"width\": width,\n        \"height\": height,\n        \"image\": cv_image,\n        \"tokens\": tokens,\n    }\n\n    # Table bbox is the entire image\n    table_bbox = [0, 0, width, height]\n\n    # Run prediction\n    results = self._predictor.multi_table_predict(\n        iocr_page=iocr_page,\n        table_bboxes=[table_bbox],\n        do_matching=self.config.do_cell_matching,\n        correct_overlapping_cells=self.config.correct_overlapping_cells,\n        sort_row_col_indexes=self.config.sort_row_col_indexes,\n    )\n\n    # Convert results to TableOutput\n    return self._convert_results(results, width, height)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction","title":"text_extraction","text":"<p>Text Extraction Module.</p> <p>Provides extractors for converting document images to structured text formats (HTML, Markdown, JSON). Uses Vision-Language Models for accurate text extraction with formatting preservation and optional layout detection.</p> Available Extractors <ul> <li>QwenTextExtractor: Qwen3-VL based extractor (multi-backend)</li> <li>DotsOCRTextExtractor: Dots OCR with layout-aware extraction (PyTorch/VLLM/API)</li> <li>NanonetsTextExtractor: Nanonets OCR2-3B for text extraction (PyTorch/VLLM)</li> <li>GraniteDoclingTextExtractor: IBM Granite Docling for document conversion (multi-backend)</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.BaseTextExtractor","title":"BaseTextExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for text extractors.</p> <p>All text extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyTextExtractor(BaseTextExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Load model weights\n            pass\n\n        def extract(self, image, output_format=\"markdown\"):\n            # Run extraction\n            return TextOutput(...)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.BaseTextExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format or output_format is not supported</p> <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        ValueError: If image format or output_format is not supported\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.BaseTextExtractor.batch_extract","title":"batch_extract","text":"<pre><code>batch_extract(\n    images: List[Union[Image, ndarray, str, Path]],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[TextOutput]\n</code></pre> <p>Extract text from multiple images.</p> <p>Default implementation loops over extract(). Subclasses can override for optimized batching (e.g., VLLM).</p> PARAMETER DESCRIPTION <code>images</code> <p>List of images in any supported format</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>output_format</code> <p>Desired output format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TextOutput]</code> <p>List of TextOutput in same order as input</p> <p>Examples:</p> <pre><code>images = [doc.get_page(i) for i in range(doc.page_count)]\nresults = extractor.batch_extract(images, output_format=\"markdown\")\n</code></pre> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def batch_extract(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[TextOutput]:\n    \"\"\"\n    Extract text from multiple images.\n\n    Default implementation loops over extract(). Subclasses can override\n    for optimized batching (e.g., VLLM).\n\n    Args:\n        images: List of images in any supported format\n        output_format: Desired output format\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of TextOutput in same order as input\n\n    Examples:\n        ```python\n        images = [doc.get_page(i) for i in range(doc.page_count)]\n        results = extractor.batch_extract(images, output_format=\"markdown\")\n        ```\n    \"\"\"\n    results = []\n    total = len(images)\n\n    for i, image in enumerate(images):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(image, output_format=output_format)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.BaseTextExtractor.extract_document","title":"extract_document","text":"<pre><code>extract_document(\n    document: Document,\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[TextOutput]\n</code></pre> <p>Extract text from all pages of a document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>output_format</code> <p>Desired output format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TextOutput]</code> <p>List of TextOutput, one per page</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresults = extractor.extract_document(doc, output_format=\"markdown\")\n</code></pre> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def extract_document(\n    self,\n    document: \"Document\",\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[TextOutput]:\n    \"\"\"\n    Extract text from all pages of a document.\n\n    Args:\n        document: Document instance\n        output_format: Desired output format\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of TextOutput, one per page\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        results = extractor.extract_document(doc, output_format=\"markdown\")\n        ```\n    \"\"\"\n    results = []\n    total = document.page_count\n\n    for i, page in enumerate(document.iter_pages()):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(page, output_format=output_format)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.DotsOCRTextExtractor","title":"DotsOCRTextExtractor","text":"<pre><code>DotsOCRTextExtractor(backend: DotsOCRBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Dots OCR Vision-Language Model text extractor with layout detection.</p> <p>Extracts text from document images with layout information including: - 11 layout categories (Caption, Footnote, Formula, List-item, etc.) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown, LaTeX, HTML) - Reading order preservation</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\n\n# Extract with layout\nresult = extractor.extract(image, include_layout=True)\nprint(f\"Found {result.num_layout_elements} elements\")\nprint(result.content)\n</code></pre> <p>Initialize Dots OCR text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend - DotsOCRVLLMConfig: VLLM high-throughput backend - DotsOCRAPIConfig: API backend (online VLLM server)</p> <p> TYPE: <code>DotsOCRBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def __init__(self, backend: DotsOCRBackendConfig):\n    \"\"\"\n    Initialize Dots OCR text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend\n            - DotsOCRVLLMConfig: VLLM high-throughput backend\n            - DotsOCRAPIConfig: API backend (online VLLM server)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._model: Any = None\n    self._loaded = False\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.DotsOCRTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\n        \"markdown\", \"html\", \"json\"\n    ] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput\n</code></pre> <p>Extract text from image using Dots OCR.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\", \"html\", or \"json\")</p> <p> TYPE: <code>Literal['markdown', 'html', 'json']</code> DEFAULT: <code>'markdown'</code> </p> <code>include_layout</code> <p>Include layout bounding boxes in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>custom_prompt</code> <p>Override default extraction prompt</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_tokens</code> <p>Maximum tokens for generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>8192</code> </p> RETURNS DESCRIPTION <code>DotsOCRTextOutput</code> <p>DotsOCRTextOutput with extracted content and optional layout</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"markdown\", \"html\", \"json\"] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput:\n    \"\"\"\n    Extract text from image using Dots OCR.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\", \"html\", or \"json\")\n        include_layout: Include layout bounding boxes in output\n        custom_prompt: Override default extraction prompt\n        max_tokens: Maximum tokens for generation\n\n    Returns:\n        DotsOCRTextOutput with extracted content and optional layout\n\n    Raises:\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    img = self._prepare_image(image)\n\n    # Get prompt\n    prompt = custom_prompt or DOTS_OCR_PROMPT\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"DotsOCRPyTorchConfig\":\n        raw_output = self._infer_pytorch(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRVLLMConfig\":\n        raw_output = self._infer_vllm(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRAPIConfig\":\n        raw_output = self._infer_api(img, prompt, max_tokens)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse output\n    return self._parse_output(\n        raw_output,\n        img.size,\n        output_format,\n        include_layout,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.GraniteDoclingTextExtractor","title":"GraniteDoclingTextExtractor","text":"<pre><code>GraniteDoclingTextExtractor(\n    backend: GraniteDoclingTextBackendConfig,\n)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Granite Docling text extractor supporting PyTorch, VLLM, MLX, and API backends.</p> <p>Granite Docling is IBM's compact vision-language model optimized for document conversion. It outputs DocTags format which is converted to Markdown using the docling_core library.</p> Example <p>from omnidocs.tasks.text_extraction.granitedocling import ( ...     GraniteDoclingTextExtractor, ...     GraniteDoclingTextPyTorchConfig, ... ) config = GraniteDoclingTextPyTorchConfig(device=\"cuda\") extractor = GraniteDoclingTextExtractor(backend=config) result = extractor.extract(image, output_format=\"markdown\") print(result.content)</p> <p>Initialize Granite Docling extractor with backend configuration.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration (PyTorch, VLLM, MLX, or API config)</p> <p> TYPE: <code>GraniteDoclingTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/granitedocling/extractor.py</code> <pre><code>def __init__(self, backend: GraniteDoclingTextBackendConfig):\n    \"\"\"\n    Initialize Granite Docling extractor with backend configuration.\n\n    Args:\n        backend: Backend configuration (PyTorch, VLLM, MLX, or API config)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded: bool = False\n\n    # Backend-specific helpers\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n    self._sampling_params_class: Any = None\n    self._device: str = \"cpu\"\n\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.GraniteDoclingTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image using Granite Docling.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\" or \"html\")</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput with extracted content</p> Source code in <code>omnidocs/tasks/text_extraction/granitedocling/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image using Granite Docling.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\" or \"html\")\n\n    Returns:\n        TextOutput with extracted content\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}\")\n\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Dispatch to backend-specific inference\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"GraniteDoclingTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image)\n    elif config_type == \"GraniteDoclingTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image)\n    elif config_type == \"GraniteDoclingTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image)\n    elif config_type == \"GraniteDoclingTextAPIConfig\":\n        raw_output = self._infer_api(pil_image)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Convert DocTags to Markdown\n    markdown_output = self._convert_doctags_to_markdown(raw_output, pil_image)\n\n    # For HTML output, wrap in basic HTML structure\n    if output_format == \"html\":\n        content = f\"&lt;html&gt;&lt;body&gt;\\n{markdown_output}\\n&lt;/body&gt;&lt;/html&gt;\"\n    else:\n        content = markdown_output\n\n    return TextOutput(\n        content=content,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=self._extract_plain_text(markdown_output),\n        image_width=width,\n        image_height=height,\n        model_name=f\"Granite-Docling-258M ({config_type.replace('Config', '')})\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.DotsOCRTextOutput","title":"DotsOCRTextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text extraction output from Dots OCR with layout information.</p> <p>Dots OCR provides structured output with: - Layout detection (11 categories) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown/LaTeX/HTML) - Reading order preservation</p> Layout Categories <p>Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title</p> Text Formatting <ul> <li>Text/Title/Section-header: Markdown</li> <li>Formula: LaTeX</li> <li>Table: HTML</li> <li>Picture: (text omitted)</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nresult = extractor.extract(image, include_layout=True)\nprint(result.content)  # Full text with formatting\nfor elem in result.layout:\n        print(f\"{elem.category}: {elem.bbox}\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.DotsOCRTextOutput.num_layout_elements","title":"num_layout_elements  <code>property</code>","text":"<pre><code>num_layout_elements: int\n</code></pre> <p>Number of detected layout elements.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.DotsOCRTextOutput.content_length","title":"content_length  <code>property</code>","text":"<pre><code>content_length: int\n</code></pre> <p>Length of extracted content in characters.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.LayoutElement","title":"LayoutElement","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single layout element from document layout detection.</p> <p>Represents a detected region in the document with its bounding box, category label, and extracted text content.</p> ATTRIBUTE DESCRIPTION <code>bbox</code> <p>Bounding box coordinates [x1, y1, x2, y2] (normalized to 0-1024)</p> <p> TYPE: <code>List[int]</code> </p> <code>category</code> <p>Layout category (e.g., \"Text\", \"Title\", \"Table\", \"Formula\")</p> <p> TYPE: <code>str</code> </p> <code>text</code> <p>Extracted text content (None for pictures)</p> <p> TYPE: <code>Optional[str]</code> </p> <code>confidence</code> <p>Detection confidence score (optional)</p> <p> TYPE: <code>Optional[float]</code> </p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.OutputFormat","title":"OutputFormat","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported text extraction output formats.</p> Each format has different characteristics <ul> <li>HTML: Structured with div elements, preserves layout semantics</li> <li>MARKDOWN: Portable, human-readable, good for documentation</li> <li>JSON: Structured data with layout information (Dots OCR)</li> </ul>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.TextOutput","title":"TextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text extraction output from a document image.</p> <p>Contains the extracted text content in the requested format, along with optional raw output and plain text versions.</p> Example <pre><code>result = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)  # Clean markdown\nprint(result.plain_text)  # Plain text without formatting\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.TextOutput.content_length","title":"content_length  <code>property</code>","text":"<pre><code>content_length: int\n</code></pre> <p>Length of the extracted content in characters.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.TextOutput.word_count","title":"word_count  <code>property</code>","text":"<pre><code>word_count: int\n</code></pre> <p>Approximate word count of the plain text.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.NanonetsTextExtractor","title":"NanonetsTextExtractor","text":"<pre><code>NanonetsTextExtractor(backend: NanonetsTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Nanonets OCR2-3B Vision-Language Model text extractor.</p> <p>Extracts text from document images with support for: - Tables (output as HTML) - Equations (output as LaTeX) - Image captions (wrapped in  tags) - Watermarks (wrapped in  tags) - Page numbers (wrapped in  tags) - Checkboxes (using \u2610 and \u2611 symbols)</p> <p>Supports PyTorch, VLLM, and MLX backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import NanonetsTextExtractor\nfrom omnidocs.tasks.text_extraction.nanonets import NanonetsTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = NanonetsTextExtractor(\n        backend=NanonetsTextPyTorchConfig()\n    )\n\n# Extract text\nresult = extractor.extract(image)\nprint(result.content)\n</code></pre> <p>Initialize Nanonets text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend - NanonetsTextVLLMConfig: VLLM high-throughput backend - NanonetsTextMLXConfig: MLX backend for Apple Silicon</p> <p> TYPE: <code>NanonetsTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/nanonets/extractor.py</code> <pre><code>def __init__(self, backend: NanonetsTextBackendConfig):\n    \"\"\"\n    Initialize Nanonets text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend\n            - NanonetsTextVLLMConfig: VLLM high-throughput backend\n            - NanonetsTextMLXConfig: MLX backend for Apple Silicon\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._device: str = \"cpu\"\n\n    # MLX-specific helpers\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.NanonetsTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> <p>Note: Nanonets OCR2 produces a unified output format that includes tables as HTML and equations as LaTeX inline. The output_format parameter is accepted for API compatibility but does not change the output structure.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Accepted for API compatibility (default: \"markdown\")</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/nanonets/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Note: Nanonets OCR2 produces a unified output format that includes\n    tables as HTML and equations as LaTeX inline. The output_format\n    parameter is accepted for API compatibility but does not change\n    the output structure.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Accepted for API compatibility (default: \"markdown\")\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"NanonetsTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image)\n    elif config_type == \"NanonetsTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image)\n    elif config_type == \"NanonetsTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    cleaned_output = raw_output.replace(\"&lt;|im_end|&gt;\", \"\").strip()\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=cleaned_output,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Nanonets-OCR2-3B ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.QwenTextExtractor","title":"QwenTextExtractor","text":"<pre><code>QwenTextExtractor(backend: QwenTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Qwen3-VL Vision-Language Model text extractor.</p> <p>Extracts text from document images and outputs as structured HTML or Markdown. Uses Qwen3-VL's built-in document parsing prompts.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Extract as Markdown\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n\n# Extract as HTML\nresult = extractor.extract(image, output_format=\"html\")\nprint(result.content)\n</code></pre> <p>Initialize Qwen text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenTextPyTorchConfig: PyTorch/HuggingFace backend - QwenTextVLLMConfig: VLLM high-throughput backend - QwenTextMLXConfig: MLX backend for Apple Silicon - QwenTextAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def __init__(self, backend: QwenTextBackendConfig):\n    \"\"\"\n    Initialize Qwen text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenTextPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenTextVLLMConfig: VLLM high-throughput backend\n            - QwenTextMLXConfig: MLX backend for Apple Silicon\n            - QwenTextAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.QwenTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML with div elements - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format or output_format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML with div elements\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format or output_format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}. Expected 'html' or 'markdown'.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Get prompt for output format\n    prompt = QWEN_PROMPTS[output_format]\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenTextAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    if output_format == \"html\":\n        cleaned_output = _clean_html_output(raw_output)\n    else:\n        cleaned_output = _clean_markdown_output(raw_output)\n\n    # Extract plain text\n    plain_text = _extract_plain_text(raw_output, output_format)\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=plain_text,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.base","title":"base","text":"<p>Base class for text extractors.</p> <p>Defines the abstract interface that all text extractors must implement.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.base.BaseTextExtractor","title":"BaseTextExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for text extractors.</p> <p>All text extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyTextExtractor(BaseTextExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Load model weights\n            pass\n\n        def extract(self, image, output_format=\"markdown\"):\n            # Run extraction\n            return TextOutput(...)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.base.BaseTextExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format or output_format is not supported</p> <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        ValueError: If image format or output_format is not supported\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.base.BaseTextExtractor.batch_extract","title":"batch_extract","text":"<pre><code>batch_extract(\n    images: List[Union[Image, ndarray, str, Path]],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[TextOutput]\n</code></pre> <p>Extract text from multiple images.</p> <p>Default implementation loops over extract(). Subclasses can override for optimized batching (e.g., VLLM).</p> PARAMETER DESCRIPTION <code>images</code> <p>List of images in any supported format</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>output_format</code> <p>Desired output format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TextOutput]</code> <p>List of TextOutput in same order as input</p> <p>Examples:</p> <pre><code>images = [doc.get_page(i) for i in range(doc.page_count)]\nresults = extractor.batch_extract(images, output_format=\"markdown\")\n</code></pre> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def batch_extract(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[TextOutput]:\n    \"\"\"\n    Extract text from multiple images.\n\n    Default implementation loops over extract(). Subclasses can override\n    for optimized batching (e.g., VLLM).\n\n    Args:\n        images: List of images in any supported format\n        output_format: Desired output format\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of TextOutput in same order as input\n\n    Examples:\n        ```python\n        images = [doc.get_page(i) for i in range(doc.page_count)]\n        results = extractor.batch_extract(images, output_format=\"markdown\")\n        ```\n    \"\"\"\n    results = []\n    total = len(images)\n\n    for i, image in enumerate(images):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(image, output_format=output_format)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.base.BaseTextExtractor.extract_document","title":"extract_document","text":"<pre><code>extract_document(\n    document: Document,\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[TextOutput]\n</code></pre> <p>Extract text from all pages of a document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>output_format</code> <p>Desired output format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TextOutput]</code> <p>List of TextOutput, one per page</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresults = extractor.extract_document(doc, output_format=\"markdown\")\n</code></pre> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def extract_document(\n    self,\n    document: \"Document\",\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[TextOutput]:\n    \"\"\"\n    Extract text from all pages of a document.\n\n    Args:\n        document: Document instance\n        output_format: Desired output format\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of TextOutput, one per page\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        results = extractor.extract_document(doc, output_format=\"markdown\")\n        ```\n    \"\"\"\n    results = []\n    total = document.page_count\n\n    for i, page in enumerate(document.iter_pages()):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(page, output_format=output_format)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr","title":"dotsocr","text":"<p>Dots OCR text extractor and backend configurations.</p> <p>Available backends: - PyTorch: DotsOCRPyTorchConfig (local GPU inference) - VLLM: DotsOCRVLLMConfig (offline batch inference) - API: DotsOCRAPIConfig (online VLLM server via OpenAI-compatible API)</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRAPIConfig","title":"DotsOCRAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Dots OCR.</p> <p>This config is for accessing a deployed VLLM server via OpenAI-compatible API. Typically used with modal_dotsocr_vllm_online.py deployment.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRAPIConfig\n\nconfig = DotsOCRAPIConfig(\n        model=\"dotsocr\",\n        api_base=\"https://your-modal-app.modal.run/v1\",\n        api_key=\"optional-key\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRTextExtractor","title":"DotsOCRTextExtractor","text":"<pre><code>DotsOCRTextExtractor(backend: DotsOCRBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Dots OCR Vision-Language Model text extractor with layout detection.</p> <p>Extracts text from document images with layout information including: - 11 layout categories (Caption, Footnote, Formula, List-item, etc.) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown, LaTeX, HTML) - Reading order preservation</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\n\n# Extract with layout\nresult = extractor.extract(image, include_layout=True)\nprint(f\"Found {result.num_layout_elements} elements\")\nprint(result.content)\n</code></pre> <p>Initialize Dots OCR text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend - DotsOCRVLLMConfig: VLLM high-throughput backend - DotsOCRAPIConfig: API backend (online VLLM server)</p> <p> TYPE: <code>DotsOCRBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def __init__(self, backend: DotsOCRBackendConfig):\n    \"\"\"\n    Initialize Dots OCR text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend\n            - DotsOCRVLLMConfig: VLLM high-throughput backend\n            - DotsOCRAPIConfig: API backend (online VLLM server)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._model: Any = None\n    self._loaded = False\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\n        \"markdown\", \"html\", \"json\"\n    ] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput\n</code></pre> <p>Extract text from image using Dots OCR.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\", \"html\", or \"json\")</p> <p> TYPE: <code>Literal['markdown', 'html', 'json']</code> DEFAULT: <code>'markdown'</code> </p> <code>include_layout</code> <p>Include layout bounding boxes in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>custom_prompt</code> <p>Override default extraction prompt</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_tokens</code> <p>Maximum tokens for generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>8192</code> </p> RETURNS DESCRIPTION <code>DotsOCRTextOutput</code> <p>DotsOCRTextOutput with extracted content and optional layout</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"markdown\", \"html\", \"json\"] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput:\n    \"\"\"\n    Extract text from image using Dots OCR.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\", \"html\", or \"json\")\n        include_layout: Include layout bounding boxes in output\n        custom_prompt: Override default extraction prompt\n        max_tokens: Maximum tokens for generation\n\n    Returns:\n        DotsOCRTextOutput with extracted content and optional layout\n\n    Raises:\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    img = self._prepare_image(image)\n\n    # Get prompt\n    prompt = custom_prompt or DOTS_OCR_PROMPT\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"DotsOCRPyTorchConfig\":\n        raw_output = self._infer_pytorch(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRVLLMConfig\":\n        raw_output = self._infer_vllm(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRAPIConfig\":\n        raw_output = self._infer_api(img, prompt, max_tokens)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse output\n    return self._parse_output(\n        raw_output,\n        img.size,\n        output_format,\n        include_layout,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRPyTorchConfig","title":"DotsOCRPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Dots OCR.</p> <p>Dots OCR provides layout-aware text extraction with 11 predefined layout categories (Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title).</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nconfig = DotsOCRPyTorchConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRVLLMConfig","title":"DotsOCRVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Dots OCR.</p> <p>VLLM provides high-throughput inference with optimizations like: - PagedAttention for efficient KV cache management - Continuous batching for higher throughput - Optimized CUDA kernels</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nconfig = DotsOCRVLLMConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.api","title":"api","text":"<p>API backend configuration for Dots OCR (VLLM online server).</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.api.DotsOCRAPIConfig","title":"DotsOCRAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Dots OCR.</p> <p>This config is for accessing a deployed VLLM server via OpenAI-compatible API. Typically used with modal_dotsocr_vllm_online.py deployment.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRAPIConfig\n\nconfig = DotsOCRAPIConfig(\n        model=\"dotsocr\",\n        api_base=\"https://your-modal-app.modal.run/v1\",\n        api_key=\"optional-key\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.extractor","title":"extractor","text":"<p>Dots OCR text extractor with layout-aware extraction.</p> <p>A Vision-Language Model optimized for document OCR with structured output containing layout information, bounding boxes, and multi-format text.</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\nresult = extractor.extract(image, include_layout=True)\nprint(result.content)\nfor elem in result.layout:\n        print(f\"{elem.category}: {elem.bbox}\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.extractor.DotsOCRTextExtractor","title":"DotsOCRTextExtractor","text":"<pre><code>DotsOCRTextExtractor(backend: DotsOCRBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Dots OCR Vision-Language Model text extractor with layout detection.</p> <p>Extracts text from document images with layout information including: - 11 layout categories (Caption, Footnote, Formula, List-item, etc.) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown, LaTeX, HTML) - Reading order preservation</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\n\n# Extract with layout\nresult = extractor.extract(image, include_layout=True)\nprint(f\"Found {result.num_layout_elements} elements\")\nprint(result.content)\n</code></pre> <p>Initialize Dots OCR text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend - DotsOCRVLLMConfig: VLLM high-throughput backend - DotsOCRAPIConfig: API backend (online VLLM server)</p> <p> TYPE: <code>DotsOCRBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def __init__(self, backend: DotsOCRBackendConfig):\n    \"\"\"\n    Initialize Dots OCR text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend\n            - DotsOCRVLLMConfig: VLLM high-throughput backend\n            - DotsOCRAPIConfig: API backend (online VLLM server)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._model: Any = None\n    self._loaded = False\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.extractor.DotsOCRTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\n        \"markdown\", \"html\", \"json\"\n    ] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput\n</code></pre> <p>Extract text from image using Dots OCR.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\", \"html\", or \"json\")</p> <p> TYPE: <code>Literal['markdown', 'html', 'json']</code> DEFAULT: <code>'markdown'</code> </p> <code>include_layout</code> <p>Include layout bounding boxes in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>custom_prompt</code> <p>Override default extraction prompt</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_tokens</code> <p>Maximum tokens for generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>8192</code> </p> RETURNS DESCRIPTION <code>DotsOCRTextOutput</code> <p>DotsOCRTextOutput with extracted content and optional layout</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"markdown\", \"html\", \"json\"] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput:\n    \"\"\"\n    Extract text from image using Dots OCR.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\", \"html\", or \"json\")\n        include_layout: Include layout bounding boxes in output\n        custom_prompt: Override default extraction prompt\n        max_tokens: Maximum tokens for generation\n\n    Returns:\n        DotsOCRTextOutput with extracted content and optional layout\n\n    Raises:\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    img = self._prepare_image(image)\n\n    # Get prompt\n    prompt = custom_prompt or DOTS_OCR_PROMPT\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"DotsOCRPyTorchConfig\":\n        raw_output = self._infer_pytorch(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRVLLMConfig\":\n        raw_output = self._infer_vllm(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRAPIConfig\":\n        raw_output = self._infer_api(img, prompt, max_tokens)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse output\n    return self._parse_output(\n        raw_output,\n        img.size,\n        output_format,\n        include_layout,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.pytorch","title":"pytorch","text":"<p>PyTorch backend configuration for Dots OCR.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.pytorch.DotsOCRPyTorchConfig","title":"DotsOCRPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Dots OCR.</p> <p>Dots OCR provides layout-aware text extraction with 11 predefined layout categories (Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title).</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nconfig = DotsOCRPyTorchConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.vllm","title":"vllm","text":"<p>VLLM backend configuration for Dots OCR.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.dotsocr.vllm.DotsOCRVLLMConfig","title":"DotsOCRVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Dots OCR.</p> <p>VLLM provides high-throughput inference with optimizations like: - PagedAttention for efficient KV cache management - Continuous batching for higher throughput - Optimized CUDA kernels</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nconfig = DotsOCRVLLMConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.granitedocling","title":"granitedocling","text":"<p>Granite Docling text extraction with multi-backend support.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.granitedocling.GraniteDoclingTextAPIConfig","title":"GraniteDoclingTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction via API.</p> <p>Uses OpenAI-compatible API endpoints (LiteLLM, OpenRouter, etc.).</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.granitedocling.GraniteDoclingTextExtractor","title":"GraniteDoclingTextExtractor","text":"<pre><code>GraniteDoclingTextExtractor(\n    backend: GraniteDoclingTextBackendConfig,\n)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Granite Docling text extractor supporting PyTorch, VLLM, MLX, and API backends.</p> <p>Granite Docling is IBM's compact vision-language model optimized for document conversion. It outputs DocTags format which is converted to Markdown using the docling_core library.</p> Example <p>from omnidocs.tasks.text_extraction.granitedocling import ( ...     GraniteDoclingTextExtractor, ...     GraniteDoclingTextPyTorchConfig, ... ) config = GraniteDoclingTextPyTorchConfig(device=\"cuda\") extractor = GraniteDoclingTextExtractor(backend=config) result = extractor.extract(image, output_format=\"markdown\") print(result.content)</p> <p>Initialize Granite Docling extractor with backend configuration.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration (PyTorch, VLLM, MLX, or API config)</p> <p> TYPE: <code>GraniteDoclingTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/granitedocling/extractor.py</code> <pre><code>def __init__(self, backend: GraniteDoclingTextBackendConfig):\n    \"\"\"\n    Initialize Granite Docling extractor with backend configuration.\n\n    Args:\n        backend: Backend configuration (PyTorch, VLLM, MLX, or API config)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded: bool = False\n\n    # Backend-specific helpers\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n    self._sampling_params_class: Any = None\n    self._device: str = \"cpu\"\n\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.granitedocling.GraniteDoclingTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image using Granite Docling.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\" or \"html\")</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput with extracted content</p> Source code in <code>omnidocs/tasks/text_extraction/granitedocling/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image using Granite Docling.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\" or \"html\")\n\n    Returns:\n        TextOutput with extracted content\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}\")\n\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Dispatch to backend-specific inference\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"GraniteDoclingTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image)\n    elif config_type == \"GraniteDoclingTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image)\n    elif config_type == \"GraniteDoclingTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image)\n    elif config_type == \"GraniteDoclingTextAPIConfig\":\n        raw_output = self._infer_api(pil_image)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Convert DocTags to Markdown\n    markdown_output = self._convert_doctags_to_markdown(raw_output, pil_image)\n\n    # For HTML output, wrap in basic HTML structure\n    if output_format == \"html\":\n        content = f\"&lt;html&gt;&lt;body&gt;\\n{markdown_output}\\n&lt;/body&gt;&lt;/html&gt;\"\n    else:\n        content = markdown_output\n\n    return TextOutput(\n        content=content,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=self._extract_plain_text(markdown_output),\n        image_width=width,\n        image_height=height,\n        model_name=f\"Granite-Docling-258M ({config_type.replace('Config', '')})\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.granitedocling.GraniteDoclingTextMLXConfig","title":"GraniteDoclingTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with MLX backend.</p> <p>This backend is optimized for Apple Silicon Macs (M1/M2/M3/M4). Uses the MLX-optimized model variant.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.granitedocling.GraniteDoclingTextPyTorchConfig","title":"GraniteDoclingTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with PyTorch backend.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.granitedocling.GraniteDoclingTextVLLMConfig","title":"GraniteDoclingTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with VLLM backend.</p> <p>IMPORTANT: This config uses revision=\"untied\" by default, which is required for VLLM compatibility with Granite Docling's tied weights.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.granitedocling.api","title":"api","text":"<p>API backend configuration for Granite Docling text extraction.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.granitedocling.api.GraniteDoclingTextAPIConfig","title":"GraniteDoclingTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction via API.</p> <p>Uses OpenAI-compatible API endpoints (LiteLLM, OpenRouter, etc.).</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.granitedocling.extractor","title":"extractor","text":"<p>Granite Docling text extractor with multi-backend support.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.granitedocling.extractor.GraniteDoclingTextExtractor","title":"GraniteDoclingTextExtractor","text":"<pre><code>GraniteDoclingTextExtractor(\n    backend: GraniteDoclingTextBackendConfig,\n)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Granite Docling text extractor supporting PyTorch, VLLM, MLX, and API backends.</p> <p>Granite Docling is IBM's compact vision-language model optimized for document conversion. It outputs DocTags format which is converted to Markdown using the docling_core library.</p> Example <p>from omnidocs.tasks.text_extraction.granitedocling import ( ...     GraniteDoclingTextExtractor, ...     GraniteDoclingTextPyTorchConfig, ... ) config = GraniteDoclingTextPyTorchConfig(device=\"cuda\") extractor = GraniteDoclingTextExtractor(backend=config) result = extractor.extract(image, output_format=\"markdown\") print(result.content)</p> <p>Initialize Granite Docling extractor with backend configuration.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration (PyTorch, VLLM, MLX, or API config)</p> <p> TYPE: <code>GraniteDoclingTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/granitedocling/extractor.py</code> <pre><code>def __init__(self, backend: GraniteDoclingTextBackendConfig):\n    \"\"\"\n    Initialize Granite Docling extractor with backend configuration.\n\n    Args:\n        backend: Backend configuration (PyTorch, VLLM, MLX, or API config)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded: bool = False\n\n    # Backend-specific helpers\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n    self._sampling_params_class: Any = None\n    self._device: str = \"cpu\"\n\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.granitedocling.extractor.GraniteDoclingTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image using Granite Docling.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\" or \"html\")</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput with extracted content</p> Source code in <code>omnidocs/tasks/text_extraction/granitedocling/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image using Granite Docling.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\" or \"html\")\n\n    Returns:\n        TextOutput with extracted content\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}\")\n\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Dispatch to backend-specific inference\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"GraniteDoclingTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image)\n    elif config_type == \"GraniteDoclingTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image)\n    elif config_type == \"GraniteDoclingTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image)\n    elif config_type == \"GraniteDoclingTextAPIConfig\":\n        raw_output = self._infer_api(pil_image)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Convert DocTags to Markdown\n    markdown_output = self._convert_doctags_to_markdown(raw_output, pil_image)\n\n    # For HTML output, wrap in basic HTML structure\n    if output_format == \"html\":\n        content = f\"&lt;html&gt;&lt;body&gt;\\n{markdown_output}\\n&lt;/body&gt;&lt;/html&gt;\"\n    else:\n        content = markdown_output\n\n    return TextOutput(\n        content=content,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=self._extract_plain_text(markdown_output),\n        image_width=width,\n        image_height=height,\n        model_name=f\"Granite-Docling-258M ({config_type.replace('Config', '')})\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.granitedocling.mlx","title":"mlx","text":"<p>MLX backend configuration for Granite Docling text extraction (Apple Silicon).</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.granitedocling.mlx.GraniteDoclingTextMLXConfig","title":"GraniteDoclingTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with MLX backend.</p> <p>This backend is optimized for Apple Silicon Macs (M1/M2/M3/M4). Uses the MLX-optimized model variant.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.granitedocling.pytorch","title":"pytorch","text":"<p>PyTorch backend configuration for Granite Docling text extraction.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.granitedocling.pytorch.GraniteDoclingTextPyTorchConfig","title":"GraniteDoclingTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with PyTorch backend.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.granitedocling.vllm","title":"vllm","text":"<p>VLLM backend configuration for Granite Docling text extraction.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.granitedocling.vllm.GraniteDoclingTextVLLMConfig","title":"GraniteDoclingTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with VLLM backend.</p> <p>IMPORTANT: This config uses revision=\"untied\" by default, which is required for VLLM compatibility with Granite Docling's tied weights.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.models","title":"models","text":"<p>Pydantic models for text extraction outputs.</p> <p>Defines output types and format enums for text extraction.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.models.OutputFormat","title":"OutputFormat","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported text extraction output formats.</p> Each format has different characteristics <ul> <li>HTML: Structured with div elements, preserves layout semantics</li> <li>MARKDOWN: Portable, human-readable, good for documentation</li> <li>JSON: Structured data with layout information (Dots OCR)</li> </ul>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.models.TextOutput","title":"TextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text extraction output from a document image.</p> <p>Contains the extracted text content in the requested format, along with optional raw output and plain text versions.</p> Example <pre><code>result = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)  # Clean markdown\nprint(result.plain_text)  # Plain text without formatting\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.models.TextOutput.content_length","title":"content_length  <code>property</code>","text":"<pre><code>content_length: int\n</code></pre> <p>Length of the extracted content in characters.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.models.TextOutput.word_count","title":"word_count  <code>property</code>","text":"<pre><code>word_count: int\n</code></pre> <p>Approximate word count of the plain text.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.models.LayoutElement","title":"LayoutElement","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single layout element from document layout detection.</p> <p>Represents a detected region in the document with its bounding box, category label, and extracted text content.</p> ATTRIBUTE DESCRIPTION <code>bbox</code> <p>Bounding box coordinates [x1, y1, x2, y2] (normalized to 0-1024)</p> <p> TYPE: <code>List[int]</code> </p> <code>category</code> <p>Layout category (e.g., \"Text\", \"Title\", \"Table\", \"Formula\")</p> <p> TYPE: <code>str</code> </p> <code>text</code> <p>Extracted text content (None for pictures)</p> <p> TYPE: <code>Optional[str]</code> </p> <code>confidence</code> <p>Detection confidence score (optional)</p> <p> TYPE: <code>Optional[float]</code> </p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.models.DotsOCRTextOutput","title":"DotsOCRTextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text extraction output from Dots OCR with layout information.</p> <p>Dots OCR provides structured output with: - Layout detection (11 categories) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown/LaTeX/HTML) - Reading order preservation</p> Layout Categories <p>Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title</p> Text Formatting <ul> <li>Text/Title/Section-header: Markdown</li> <li>Formula: LaTeX</li> <li>Table: HTML</li> <li>Picture: (text omitted)</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nresult = extractor.extract(image, include_layout=True)\nprint(result.content)  # Full text with formatting\nfor elem in result.layout:\n        print(f\"{elem.category}: {elem.bbox}\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.models.DotsOCRTextOutput.num_layout_elements","title":"num_layout_elements  <code>property</code>","text":"<pre><code>num_layout_elements: int\n</code></pre> <p>Number of detected layout elements.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.models.DotsOCRTextOutput.content_length","title":"content_length  <code>property</code>","text":"<pre><code>content_length: int\n</code></pre> <p>Length of extracted content in characters.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.nanonets","title":"nanonets","text":"<p>Nanonets OCR2-3B backend configurations and extractor for text extraction.</p> Available backends <ul> <li>NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend</li> <li>NanonetsTextVLLMConfig: VLLM high-throughput backend</li> <li>NanonetsTextMLXConfig: MLX backend for Apple Silicon</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction.nanonets import NanonetsTextPyTorchConfig\nconfig = NanonetsTextPyTorchConfig()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.nanonets.NanonetsTextExtractor","title":"NanonetsTextExtractor","text":"<pre><code>NanonetsTextExtractor(backend: NanonetsTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Nanonets OCR2-3B Vision-Language Model text extractor.</p> <p>Extracts text from document images with support for: - Tables (output as HTML) - Equations (output as LaTeX) - Image captions (wrapped in  tags) - Watermarks (wrapped in  tags) - Page numbers (wrapped in  tags) - Checkboxes (using \u2610 and \u2611 symbols)</p> <p>Supports PyTorch, VLLM, and MLX backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import NanonetsTextExtractor\nfrom omnidocs.tasks.text_extraction.nanonets import NanonetsTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = NanonetsTextExtractor(\n        backend=NanonetsTextPyTorchConfig()\n    )\n\n# Extract text\nresult = extractor.extract(image)\nprint(result.content)\n</code></pre> <p>Initialize Nanonets text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend - NanonetsTextVLLMConfig: VLLM high-throughput backend - NanonetsTextMLXConfig: MLX backend for Apple Silicon</p> <p> TYPE: <code>NanonetsTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/nanonets/extractor.py</code> <pre><code>def __init__(self, backend: NanonetsTextBackendConfig):\n    \"\"\"\n    Initialize Nanonets text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend\n            - NanonetsTextVLLMConfig: VLLM high-throughput backend\n            - NanonetsTextMLXConfig: MLX backend for Apple Silicon\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._device: str = \"cpu\"\n\n    # MLX-specific helpers\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.nanonets.NanonetsTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> <p>Note: Nanonets OCR2 produces a unified output format that includes tables as HTML and equations as LaTeX inline. The output_format parameter is accepted for API compatibility but does not change the output structure.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Accepted for API compatibility (default: \"markdown\")</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/nanonets/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Note: Nanonets OCR2 produces a unified output format that includes\n    tables as HTML and equations as LaTeX inline. The output_format\n    parameter is accepted for API compatibility but does not change\n    the output structure.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Accepted for API compatibility (default: \"markdown\")\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"NanonetsTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image)\n    elif config_type == \"NanonetsTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image)\n    elif config_type == \"NanonetsTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    cleaned_output = raw_output.replace(\"&lt;|im_end|&gt;\", \"\").strip()\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=cleaned_output,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Nanonets-OCR2-3B ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.nanonets.NanonetsTextMLXConfig","title":"NanonetsTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3/M4+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = NanonetsTextMLXConfig(\n        model=\"mlx-community/Nanonets-OCR2-3B-bf16\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.nanonets.NanonetsTextPyTorchConfig","title":"NanonetsTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate</p> Example <pre><code>config = NanonetsTextPyTorchConfig(\n        device=\"cuda\",\n        torch_dtype=\"float16\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.nanonets.NanonetsTextVLLMConfig","title":"NanonetsTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = NanonetsTextVLLMConfig(\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.85,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.nanonets.extractor","title":"extractor","text":"<p>Nanonets OCR2-3B text extractor.</p> <p>A Vision-Language Model for extracting text from document images with support for tables (HTML), equations (LaTeX), and image captions.</p> <p>Supports PyTorch and VLLM backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import NanonetsTextExtractor\nfrom omnidocs.tasks.text_extraction.nanonets import NanonetsTextPyTorchConfig\n\nextractor = NanonetsTextExtractor(\n        backend=NanonetsTextPyTorchConfig()\n    )\nresult = extractor.extract(image)\nprint(result.content)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.nanonets.extractor.NanonetsTextExtractor","title":"NanonetsTextExtractor","text":"<pre><code>NanonetsTextExtractor(backend: NanonetsTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Nanonets OCR2-3B Vision-Language Model text extractor.</p> <p>Extracts text from document images with support for: - Tables (output as HTML) - Equations (output as LaTeX) - Image captions (wrapped in  tags) - Watermarks (wrapped in  tags) - Page numbers (wrapped in  tags) - Checkboxes (using \u2610 and \u2611 symbols)</p> <p>Supports PyTorch, VLLM, and MLX backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import NanonetsTextExtractor\nfrom omnidocs.tasks.text_extraction.nanonets import NanonetsTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = NanonetsTextExtractor(\n        backend=NanonetsTextPyTorchConfig()\n    )\n\n# Extract text\nresult = extractor.extract(image)\nprint(result.content)\n</code></pre> <p>Initialize Nanonets text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend - NanonetsTextVLLMConfig: VLLM high-throughput backend - NanonetsTextMLXConfig: MLX backend for Apple Silicon</p> <p> TYPE: <code>NanonetsTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/nanonets/extractor.py</code> <pre><code>def __init__(self, backend: NanonetsTextBackendConfig):\n    \"\"\"\n    Initialize Nanonets text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend\n            - NanonetsTextVLLMConfig: VLLM high-throughput backend\n            - NanonetsTextMLXConfig: MLX backend for Apple Silicon\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._device: str = \"cpu\"\n\n    # MLX-specific helpers\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.nanonets.extractor.NanonetsTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> <p>Note: Nanonets OCR2 produces a unified output format that includes tables as HTML and equations as LaTeX inline. The output_format parameter is accepted for API compatibility but does not change the output structure.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Accepted for API compatibility (default: \"markdown\")</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/nanonets/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Note: Nanonets OCR2 produces a unified output format that includes\n    tables as HTML and equations as LaTeX inline. The output_format\n    parameter is accepted for API compatibility but does not change\n    the output structure.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Accepted for API compatibility (default: \"markdown\")\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"NanonetsTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image)\n    elif config_type == \"NanonetsTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image)\n    elif config_type == \"NanonetsTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    cleaned_output = raw_output.replace(\"&lt;|im_end|&gt;\", \"\").strip()\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=cleaned_output,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Nanonets-OCR2-3B ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.nanonets.mlx","title":"mlx","text":"<p>MLX backend configuration for Nanonets OCR2-3B text extraction.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.nanonets.mlx.NanonetsTextMLXConfig","title":"NanonetsTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3/M4+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = NanonetsTextMLXConfig(\n        model=\"mlx-community/Nanonets-OCR2-3B-bf16\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.nanonets.pytorch","title":"pytorch","text":"<p>PyTorch/HuggingFace backend configuration for Nanonets OCR2-3B text extraction.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.nanonets.pytorch.NanonetsTextPyTorchConfig","title":"NanonetsTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate</p> Example <pre><code>config = NanonetsTextPyTorchConfig(\n        device=\"cuda\",\n        torch_dtype=\"float16\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.nanonets.vllm","title":"vllm","text":"<p>VLLM backend configuration for Nanonets OCR2-3B text extraction.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.nanonets.vllm.NanonetsTextVLLMConfig","title":"NanonetsTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = NanonetsTextVLLMConfig(\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.85,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen","title":"qwen","text":"<p>Qwen3-VL backend configurations and extractor for text extraction.</p> Available backends <ul> <li>QwenTextPyTorchConfig: PyTorch/HuggingFace backend</li> <li>QwenTextVLLMConfig: VLLM high-throughput backend</li> <li>QwenTextMLXConfig: MLX backend for Apple Silicon</li> <li>QwenTextAPIConfig: API backend (OpenRouter, etc.)</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nconfig = QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextAPIConfig","title":"QwenTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen text extraction.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenTextAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextExtractor","title":"QwenTextExtractor","text":"<pre><code>QwenTextExtractor(backend: QwenTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Qwen3-VL Vision-Language Model text extractor.</p> <p>Extracts text from document images and outputs as structured HTML or Markdown. Uses Qwen3-VL's built-in document parsing prompts.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Extract as Markdown\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n\n# Extract as HTML\nresult = extractor.extract(image, output_format=\"html\")\nprint(result.content)\n</code></pre> <p>Initialize Qwen text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenTextPyTorchConfig: PyTorch/HuggingFace backend - QwenTextVLLMConfig: VLLM high-throughput backend - QwenTextMLXConfig: MLX backend for Apple Silicon - QwenTextAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def __init__(self, backend: QwenTextBackendConfig):\n    \"\"\"\n    Initialize Qwen text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenTextPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenTextVLLMConfig: VLLM high-throughput backend\n            - QwenTextMLXConfig: MLX backend for Apple Silicon\n            - QwenTextAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML with div elements - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format or output_format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML with div elements\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format or output_format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}. Expected 'html' or 'markdown'.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Get prompt for output format\n    prompt = QWEN_PROMPTS[output_format]\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenTextAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    if output_format == \"html\":\n        cleaned_output = _clean_html_output(raw_output)\n    else:\n        cleaned_output = _clean_markdown_output(raw_output)\n\n    # Extract plain text\n    plain_text = _extract_plain_text(raw_output, output_format)\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=plain_text,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextMLXConfig","title":"QwenTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenTextMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextPyTorchConfig","title":"QwenTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextVLLMConfig","title":"QwenTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenTextVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.api","title":"api","text":"<p>API backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.api.QwenTextAPIConfig","title":"QwenTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen text extraction.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenTextAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.extractor","title":"extractor","text":"<p>Qwen3-VL text extractor.</p> <p>A Vision-Language Model for extracting text from document images as structured HTML or Markdown.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.extractor.QwenTextExtractor","title":"QwenTextExtractor","text":"<pre><code>QwenTextExtractor(backend: QwenTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Qwen3-VL Vision-Language Model text extractor.</p> <p>Extracts text from document images and outputs as structured HTML or Markdown. Uses Qwen3-VL's built-in document parsing prompts.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Extract as Markdown\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n\n# Extract as HTML\nresult = extractor.extract(image, output_format=\"html\")\nprint(result.content)\n</code></pre> <p>Initialize Qwen text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenTextPyTorchConfig: PyTorch/HuggingFace backend - QwenTextVLLMConfig: VLLM high-throughput backend - QwenTextMLXConfig: MLX backend for Apple Silicon - QwenTextAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def __init__(self, backend: QwenTextBackendConfig):\n    \"\"\"\n    Initialize Qwen text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenTextPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenTextVLLMConfig: VLLM high-throughput backend\n            - QwenTextMLXConfig: MLX backend for Apple Silicon\n            - QwenTextAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.extractor.QwenTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML with div elements - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format or output_format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML with div elements\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format or output_format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}. Expected 'html' or 'markdown'.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Get prompt for output format\n    prompt = QWEN_PROMPTS[output_format]\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenTextAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    if output_format == \"html\":\n        cleaned_output = _clean_html_output(raw_output)\n    else:\n        cleaned_output = _clean_markdown_output(raw_output)\n\n    # Extract plain text\n    plain_text = _extract_plain_text(raw_output, output_format)\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=plain_text,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.mlx","title":"mlx","text":"<p>MLX backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.mlx.QwenTextMLXConfig","title":"QwenTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenTextMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.pytorch","title":"pytorch","text":"<p>PyTorch/HuggingFace backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.pytorch.QwenTextPyTorchConfig","title":"QwenTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.vllm","title":"vllm","text":"<p>VLLM backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/overview/#omnidocs.tasks.text_extraction.qwen.vllm.QwenTextVLLMConfig","title":"QwenTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenTextVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/base/","title":"Base","text":"<p>Base class for layout extractors.</p> <p>Defines the abstract interface that all layout extractors must implement.</p>"},{"location":"reference/tasks/layout_extraction/base/#omnidocs.tasks.layout_extraction.base.BaseLayoutExtractor","title":"BaseLayoutExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for layout extractors.</p> <p>All layout extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyLayoutExtractor(BaseLayoutExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Load model weights\n            pass\n\n        def extract(self, image):\n            # Run extraction\n            return LayoutOutput(...)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/base/#omnidocs.tasks.layout_extraction.base.BaseLayoutExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput containing detected layout boxes with standardized labels</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format is not supported</p> <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/layout_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n\n    Returns:\n        LayoutOutput containing detected layout boxes with standardized labels\n\n    Raises:\n        ValueError: If image format is not supported\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/layout_extraction/base/#omnidocs.tasks.layout_extraction.base.BaseLayoutExtractor.batch_extract","title":"batch_extract","text":"<pre><code>batch_extract(\n    images: List[Union[Image, ndarray, str, Path]],\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[LayoutOutput]\n</code></pre> <p>Run layout extraction on multiple images.</p> <p>Default implementation loops over extract(). Subclasses can override for optimized batching.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of images in any supported format</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[LayoutOutput]</code> <p>List of LayoutOutput in same order as input</p> <p>Examples:</p> <pre><code>images = [doc.get_page(i) for i in range(doc.page_count)]\nresults = extractor.batch_extract(images)\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/base.py</code> <pre><code>def batch_extract(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[LayoutOutput]:\n    \"\"\"\n    Run layout extraction on multiple images.\n\n    Default implementation loops over extract(). Subclasses can override\n    for optimized batching.\n\n    Args:\n        images: List of images in any supported format\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of LayoutOutput in same order as input\n\n    Examples:\n        ```python\n        images = [doc.get_page(i) for i in range(doc.page_count)]\n        results = extractor.batch_extract(images)\n        ```\n    \"\"\"\n    results = []\n    total = len(images)\n\n    for i, image in enumerate(images):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(image)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/layout_extraction/base/#omnidocs.tasks.layout_extraction.base.BaseLayoutExtractor.extract_document","title":"extract_document","text":"<pre><code>extract_document(\n    document: Document,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[LayoutOutput]\n</code></pre> <p>Run layout extraction on all pages of a document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[LayoutOutput]</code> <p>List of LayoutOutput, one per page</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresults = extractor.extract_document(doc)\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/base.py</code> <pre><code>def extract_document(\n    self,\n    document: \"Document\",\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[LayoutOutput]:\n    \"\"\"\n    Run layout extraction on all pages of a document.\n\n    Args:\n        document: Document instance\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of LayoutOutput, one per page\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        results = extractor.extract_document(doc)\n        ```\n    \"\"\"\n    results = []\n    total = document.page_count\n\n    for i, page in enumerate(document.iter_pages()):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(page)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/layout_extraction/doc_layout_yolo/","title":"Doc Layout YOLO","text":"<p>DocLayout-YOLO layout extractor.</p> <p>A YOLO-based model for document layout detection, optimized for academic papers and technical documents.</p> <p>Model: juliozhao/DocLayout-YOLO-DocStructBench</p>"},{"location":"reference/tasks/layout_extraction/doc_layout_yolo/#omnidocs.tasks.layout_extraction.doc_layout_yolo.DocLayoutYOLOConfig","title":"DocLayoutYOLOConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for DocLayout-YOLO layout extractor.</p> <p>This is a single-backend model (PyTorch only).</p> Example <pre><code>config = DocLayoutYOLOConfig(device=\"cuda\", confidence=0.3)\nextractor = DocLayoutYOLO(config=config)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/doc_layout_yolo/#omnidocs.tasks.layout_extraction.doc_layout_yolo.DocLayoutYOLO","title":"DocLayoutYOLO","text":"<pre><code>DocLayoutYOLO(config: DocLayoutYOLOConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>DocLayout-YOLO layout extractor.</p> <p>A YOLO-based model optimized for document layout detection. Detects: title, text, figure, table, formula, captions, etc.</p> <p>This is a single-backend model (PyTorch only).</p> Example <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\nextractor = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre> <p>Initialize DocLayout-YOLO extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object with device, model_path, etc.</p> <p> TYPE: <code>DocLayoutYOLOConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/doc_layout_yolo.py</code> <pre><code>def __init__(self, config: DocLayoutYOLOConfig):\n    \"\"\"\n    Initialize DocLayout-YOLO extractor.\n\n    Args:\n        config: Configuration object with device, model_path, etc.\n    \"\"\"\n    self.config = config\n    self._model = None\n    self._device = self._resolve_device(config.device)\n    self._model_path = self._resolve_model_path(config.model_path)\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/doc_layout_yolo/#omnidocs.tasks.layout_extraction.doc_layout_yolo.DocLayoutYOLO.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> Source code in <code>omnidocs/tasks/layout_extraction/doc_layout_yolo.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n    if self._model is None:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    img_width, img_height = pil_image.size\n\n    # Run inference\n    results = self._model.predict(\n        pil_image,\n        imgsz=self.config.img_size,\n        conf=self.config.confidence,\n        device=self._device,\n    )\n\n    result = results[0]\n\n    # Parse detections\n    layout_boxes = []\n\n    if hasattr(result, \"boxes\") and result.boxes is not None:\n        boxes = result.boxes\n\n        for i in range(len(boxes)):\n            # Get coordinates\n            bbox_coords = boxes.xyxy[i].cpu().numpy().tolist()\n\n            # Get class and confidence\n            class_id = int(boxes.cls[i].item())\n            confidence = float(boxes.conf[i].item())\n\n            # Get original label from class names\n            original_label = DOCLAYOUT_YOLO_CLASS_NAMES.get(class_id, f\"class_{class_id}\")\n\n            # Map to standardized label\n            standard_label = DOCLAYOUT_YOLO_MAPPING.to_standard(original_label)\n\n            layout_boxes.append(\n                LayoutBox(\n                    label=standard_label,\n                    bbox=BoundingBox.from_list(bbox_coords),\n                    confidence=confidence,\n                    class_id=class_id,\n                    original_label=original_label,\n                )\n            )\n\n    # Sort by y-coordinate (top to bottom reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=img_width,\n        image_height=img_height,\n        model_name=\"DocLayout-YOLO\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/","title":"Models","text":"<p>Pydantic models for layout extraction outputs.</p> <p>Defines standardized output types and label enums for layout detection.</p> Coordinate Systems <ul> <li>Absolute (default): Coordinates in pixels relative to original image size</li> <li>Normalized (0-1024): Coordinates scaled to 0-1024 range (virtual 1024x1024 canvas)</li> </ul> <p>Use <code>bbox.to_normalized(width, height)</code> or <code>output.get_normalized_bboxes()</code> to convert to normalized coordinates.</p> Example <pre><code>result = extractor.extract(image)  # Returns absolute pixel coordinates\nnormalized = result.get_normalized_bboxes()  # Returns 0-1024 normalized coords\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutLabel","title":"LayoutLabel","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Standardized layout labels used across all layout extractors.</p> <p>These provide a consistent vocabulary regardless of which model is used.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.CustomLabel","title":"CustomLabel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Type-safe custom layout label definition for VLM-based models.</p> <p>VLM models like Qwen3-VL support flexible custom labels beyond the standard LayoutLabel enum. Use this class to define custom labels with validation.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import CustomLabel\n\n# Simple custom label\ncode_block = CustomLabel(name=\"code_block\")\n\n# With metadata\nsidebar = CustomLabel(\n        name=\"sidebar\",\n        description=\"Secondary content panel\",\n        color=\"#9B59B6\",\n    )\n\n# Use with QwenLayoutDetector\nresult = detector.extract(image, custom_labels=[code_block, sidebar])\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LabelMapping","title":"LabelMapping","text":"<pre><code>LabelMapping(mapping: Dict[str, LayoutLabel])\n</code></pre> <p>Base class for model-specific label mappings.</p> <p>Each model maps its native labels to standardized LayoutLabel values.</p> <p>Initialize label mapping.</p> PARAMETER DESCRIPTION <code>mapping</code> <p>Dict mapping model-specific labels to LayoutLabel enum values</p> <p> TYPE: <code>Dict[str, LayoutLabel]</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def __init__(self, mapping: Dict[str, LayoutLabel]):\n    \"\"\"\n    Initialize label mapping.\n\n    Args:\n        mapping: Dict mapping model-specific labels to LayoutLabel enum values\n    \"\"\"\n    self._mapping = {k.lower(): v for k, v in mapping.items()}\n    self._reverse_mapping = {v: k for k, v in mapping.items()}\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LabelMapping.supported_labels","title":"supported_labels  <code>property</code>","text":"<pre><code>supported_labels: List[str]\n</code></pre> <p>Get list of supported model-specific labels.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LabelMapping.standard_labels","title":"standard_labels  <code>property</code>","text":"<pre><code>standard_labels: List[LayoutLabel]\n</code></pre> <p>Get list of standard labels this mapping produces.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LabelMapping.to_standard","title":"to_standard","text":"<pre><code>to_standard(model_label: str) -&gt; LayoutLabel\n</code></pre> <p>Convert model-specific label to standardized LayoutLabel.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_standard(self, model_label: str) -&gt; LayoutLabel:\n    \"\"\"Convert model-specific label to standardized LayoutLabel.\"\"\"\n    return self._mapping.get(model_label.lower(), LayoutLabel.UNKNOWN)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LabelMapping.from_standard","title":"from_standard","text":"<pre><code>from_standard(standard_label: LayoutLabel) -&gt; Optional[str]\n</code></pre> <p>Convert standardized LayoutLabel to model-specific label.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def from_standard(self, standard_label: LayoutLabel) -&gt; Optional[str]:\n    \"\"\"Convert standardized LayoutLabel to model-specific label.\"\"\"\n    return self._reverse_mapping.get(standard_label)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box coordinates in pixel space.</p> <p>Coordinates follow the convention: (x1, y1) is top-left, (x2, y2) is bottom-right.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x, y, width, height) format.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_xywh(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x, y, width, height) format.\"\"\"\n    return (self.x1, self.y1, self.width, self.height)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> <p>Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas. This provides consistent coordinates regardless of original image size.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Example <pre><code>bbox = BoundingBox(x1=100, y1=50, x2=500, y2=300)\nnormalized = bbox.to_normalized(1000, 800)\n# x: 100/1000*1024 = 102.4, y: 50/800*1024 = 64\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas.\n    This provides consistent coordinates regardless of original image size.\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n\n    Example:\n        ```python\n        bbox = BoundingBox(x1=100, y1=50, x2=500, y2=300)\n        normalized = bbox.to_normalized(1000, 800)\n        # x: 100/1000*1024 = 102.4, y: 50/800*1024 = 64\n        ```\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_absolute","title":"to_absolute","text":"<pre><code>to_absolute(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert from normalized (0-1024) to absolute pixel coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Target image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Target image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with absolute pixel coordinates</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_absolute(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert from normalized (0-1024) to absolute pixel coordinates.\n\n    Args:\n        image_width: Target image width in pixels\n        image_height: Target image height in pixels\n\n    Returns:\n        New BoundingBox with absolute pixel coordinates\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / NORMALIZED_SIZE * image_width,\n        y1=self.y1 / NORMALIZED_SIZE * image_height,\n        x2=self.x2 / NORMALIZED_SIZE * image_width,\n        y2=self.y2 / NORMALIZED_SIZE * image_height,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutBox","title":"LayoutBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single detected layout element with label, bounding box, and confidence.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutBox.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"label\": self.label.value,\n        \"bbox\": self.bbox.to_list(),\n        \"confidence\": self.confidence,\n        \"class_id\": self.class_id,\n        \"original_label\": self.original_label,\n    }\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutBox.get_normalized_bbox","title":"get_normalized_bbox","text":"<pre><code>get_normalized_bbox(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Get bounding box in normalized (0-1024) coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox with normalized coordinates</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def get_normalized_bbox(self, image_width: int, image_height: int) -&gt; BoundingBox:\n    \"\"\"\n    Get bounding box in normalized (0-1024) coordinates.\n\n    Args:\n        image_width: Original image width\n        image_height: Original image height\n\n    Returns:\n        BoundingBox with normalized coordinates\n    \"\"\"\n    return self.bbox.to_normalized(image_width, image_height)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput","title":"LayoutOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete layout extraction results for a single image.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput.element_count","title":"element_count  <code>property</code>","text":"<pre><code>element_count: int\n</code></pre> <p>Number of detected elements.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput.labels_found","title":"labels_found  <code>property</code>","text":"<pre><code>labels_found: List[str]\n</code></pre> <p>Unique labels found in detections.</p>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput.filter_by_label","title":"filter_by_label","text":"<pre><code>filter_by_label(label: LayoutLabel) -&gt; List[LayoutBox]\n</code></pre> <p>Filter boxes by label.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def filter_by_label(self, label: LayoutLabel) -&gt; List[LayoutBox]:\n    \"\"\"Filter boxes by label.\"\"\"\n    return [box for box in self.bboxes if box.label == label]\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(\n    min_confidence: float,\n) -&gt; List[LayoutBox]\n</code></pre> <p>Filter boxes by minimum confidence.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def filter_by_confidence(self, min_confidence: float) -&gt; List[LayoutBox]:\n    \"\"\"Filter boxes by minimum confidence.\"\"\"\n    return [box for box in self.bboxes if box.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"bboxes\": [box.to_dict() for box in self.bboxes],\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"element_count\": self.element_count,\n        \"labels_found\": self.labels_found,\n    }\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput.sort_by_position","title":"sort_by_position","text":"<pre><code>sort_by_position(\n    top_to_bottom: bool = True,\n) -&gt; LayoutOutput\n</code></pre> <p>Return a new LayoutOutput with boxes sorted by position.</p> PARAMETER DESCRIPTION <code>top_to_bottom</code> <p>If True, sort by y-coordinate (reading order)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def sort_by_position(self, top_to_bottom: bool = True) -&gt; \"LayoutOutput\":\n    \"\"\"\n    Return a new LayoutOutput with boxes sorted by position.\n\n    Args:\n        top_to_bottom: If True, sort by y-coordinate (reading order)\n    \"\"\"\n    sorted_boxes = sorted(self.bboxes, key=lambda b: (b.bbox.y1, b.bbox.x1), reverse=not top_to_bottom)\n    return LayoutOutput(\n        bboxes=sorted_boxes,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        model_name=self.model_name,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput.get_normalized_bboxes","title":"get_normalized_bboxes","text":"<pre><code>get_normalized_bboxes() -&gt; List[Dict]\n</code></pre> <p>Get all bounding boxes in normalized (0-1024) coordinates.</p> RETURNS DESCRIPTION <code>List[Dict]</code> <p>List of dicts with normalized bbox coordinates and metadata.</p> Example <pre><code>result = extractor.extract(image)\nnormalized = result.get_normalized_bboxes()\nfor box in normalized:\n        print(f\"{box['label']}: {box['bbox']}\")  # coords in 0-1024 range\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def get_normalized_bboxes(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all bounding boxes in normalized (0-1024) coordinates.\n\n    Returns:\n        List of dicts with normalized bbox coordinates and metadata.\n\n    Example:\n        ```python\n        result = extractor.extract(image)\n        normalized = result.get_normalized_bboxes()\n        for box in normalized:\n                print(f\"{box['label']}: {box['bbox']}\")  # coords in 0-1024 range\n        ```\n    \"\"\"\n    normalized = []\n    for box in self.bboxes:\n        norm_bbox = box.bbox.to_normalized(self.image_width, self.image_height)\n        normalized.append(\n            {\n                \"label\": box.label.value,\n                \"bbox\": norm_bbox.to_list(),\n                \"confidence\": box.confidence,\n                \"class_id\": box.class_id,\n                \"original_label\": box.original_label,\n            }\n        )\n    return normalized\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput.visualize","title":"visualize","text":"<pre><code>visualize(\n    image: Image,\n    output_path: Optional[Union[str, Path]] = None,\n    show_labels: bool = True,\n    show_confidence: bool = True,\n    line_width: int = 3,\n    font_size: int = 12,\n) -&gt; Image.Image\n</code></pre> <p>Visualize layout detection results on the image.</p> <p>Draws bounding boxes with labels and confidence scores on the image. Each layout category has a distinct color for easy identification.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to draw on (will be copied, not modified)</p> <p> TYPE: <code>Image</code> </p> <code>output_path</code> <p>Optional path to save the visualization</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> <code>show_labels</code> <p>Whether to show label text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_confidence</code> <p>Whether to show confidence scores</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>line_width</code> <p>Width of bounding box lines</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>font_size</code> <p>Size of label text (note: uses default font)</p> <p> TYPE: <code>int</code> DEFAULT: <code>12</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image with visualizations drawn</p> Example <pre><code>result = extractor.extract(image)\nviz = result.visualize(image, output_path=\"layout_viz.png\")\nviz.show()  # Display in notebook/viewer\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def visualize(\n    self,\n    image: \"Image.Image\",\n    output_path: Optional[Union[str, Path]] = None,\n    show_labels: bool = True,\n    show_confidence: bool = True,\n    line_width: int = 3,\n    font_size: int = 12,\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Visualize layout detection results on the image.\n\n    Draws bounding boxes with labels and confidence scores on the image.\n    Each layout category has a distinct color for easy identification.\n\n    Args:\n        image: PIL Image to draw on (will be copied, not modified)\n        output_path: Optional path to save the visualization\n        show_labels: Whether to show label text\n        show_confidence: Whether to show confidence scores\n        line_width: Width of bounding box lines\n        font_size: Size of label text (note: uses default font)\n\n    Returns:\n        PIL Image with visualizations drawn\n\n    Example:\n        ```python\n        result = extractor.extract(image)\n        viz = result.visualize(image, output_path=\"layout_viz.png\")\n        viz.show()  # Display in notebook/viewer\n        ```\n    \"\"\"\n    from PIL import ImageDraw\n\n    # Copy image to avoid modifying original\n    viz_image = image.copy().convert(\"RGB\")\n    draw = ImageDraw.Draw(viz_image)\n\n    for box in self.bboxes:\n        # Get color for this label\n        color = LABEL_COLORS.get(box.label, \"#95A5A6\")\n\n        # Draw bounding box\n        coords = box.bbox.to_xyxy()\n        draw.rectangle(coords, outline=color, width=line_width)\n\n        # Build label text\n        if show_labels or show_confidence:\n            label_parts = []\n            if show_labels:\n                label_parts.append(box.label.value)\n            if show_confidence:\n                label_parts.append(f\"{box.confidence:.2f}\")\n            label_text = \" \".join(label_parts)\n\n            # Draw label background\n            text_bbox = draw.textbbox((coords[0], coords[1] - 20), label_text)\n            draw.rectangle(text_bbox, fill=color)\n\n            # Draw label text\n            draw.text(\n                (coords[0], coords[1] - 20),\n                label_text,\n                fill=\"white\",\n            )\n\n    # Save if path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        viz_image.save(output_path)\n\n    return viz_image\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; LayoutOutput\n</code></pre> <p>Load a LayoutOutput instance from a JSON file.</p> <p>Reads a JSON file and deserializes its contents into a LayoutOutput object. Uses Pydantic's model_validate_json for proper handling of nested objects.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to JSON file containing serialized LayoutOutput data.       Can be string or pathlib.Path object.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>Deserialized layout output instance from file.</p> <p> TYPE: <code>LayoutOutput</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>UnicodeDecodeError</code> <p>If file cannot be decoded as UTF-8.</p> <code>ValueError</code> <p>If file contents are not valid JSON.</p> <code>ValidationError</code> <p>If JSON data doesn't match LayoutOutput schema.</p> Example <p><pre><code>output = LayoutOutput.load_json('layout_results.json')\nprint(f\"Found {output.element_count} elements\")\n</code></pre> Found 5 elements</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"LayoutOutput\":\n    \"\"\"\n    Load a LayoutOutput instance from a JSON file.\n\n    Reads a JSON file and deserializes its contents into a LayoutOutput object.\n    Uses Pydantic's model_validate_json for proper handling of nested objects.\n\n    Args:\n        file_path: Path to JSON file containing serialized LayoutOutput data.\n                  Can be string or pathlib.Path object.\n\n    Returns:\n        LayoutOutput: Deserialized layout output instance from file.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n        UnicodeDecodeError: If file cannot be decoded as UTF-8.\n        ValueError: If file contents are not valid JSON.\n        ValidationError: If JSON data doesn't match LayoutOutput schema.\n\n    Example:\n        ```python\n        output = LayoutOutput.load_json('layout_results.json')\n        print(f\"Found {output.element_count} elements\")\n        ```\n        Found 5 elements\n    \"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/layout_extraction/models/#omnidocs.tasks.layout_extraction.models.LayoutOutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save LayoutOutput instance to a JSON file.</p> <p>Serializes the LayoutOutput object to JSON and writes it to a file. Automatically creates parent directories if they don't exist. Uses UTF-8 encoding for compatibility and proper handling of special characters.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path where JSON file should be saved. Can be string or       pathlib.Path object. Parent directories will be created       if they don't exist.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> RAISES DESCRIPTION <code>OSError</code> <p>If file cannot be written due to permission or disk errors.</p> <code>TypeError</code> <p>If file_path is not a string or Path object.</p> Example <pre><code>output = LayoutOutput(bboxes=[], image_width=800, image_height=600)\noutput.save_json('results/layout_output.json')\n# File is created at results/layout_output.json\n# Parent 'results' directory is created if it didn't exist\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"\n    Save LayoutOutput instance to a JSON file.\n\n    Serializes the LayoutOutput object to JSON and writes it to a file.\n    Automatically creates parent directories if they don't exist. Uses UTF-8\n    encoding for compatibility and proper handling of special characters.\n\n    Args:\n        file_path: Path where JSON file should be saved. Can be string or\n                  pathlib.Path object. Parent directories will be created\n                  if they don't exist.\n\n    Returns:\n        None\n\n    Raises:\n        OSError: If file cannot be written due to permission or disk errors.\n        TypeError: If file_path is not a string or Path object.\n\n    Example:\n        ```python\n        output = LayoutOutput(bboxes=[], image_width=800, image_height=600)\n        output.save_json('results/layout_output.json')\n        # File is created at results/layout_output.json\n        # Parent 'results' directory is created if it didn't exist\n        ```\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/","title":"Overview","text":"<p>Layout Extraction Module.</p> <p>Provides extractors for detecting document layout elements such as titles, text blocks, figures, tables, formulas, and captions.</p> Available Extractors <ul> <li>DocLayoutYOLO: YOLO-based layout detector (fast, accurate)</li> <li>RTDETRLayoutExtractor: Transformer-based detector (more categories)</li> <li>QwenLayoutDetector: VLM-based detector with custom label support (multi-backend)</li> </ul> Example <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\nextractor = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n# VLM-based detection with custom labels\nfrom omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BaseLayoutExtractor","title":"BaseLayoutExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for layout extractors.</p> <p>All layout extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyLayoutExtractor(BaseLayoutExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Load model weights\n            pass\n\n        def extract(self, image):\n            # Run extraction\n            return LayoutOutput(...)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BaseLayoutExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput containing detected layout boxes with standardized labels</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format is not supported</p> <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/layout_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n\n    Returns:\n        LayoutOutput containing detected layout boxes with standardized labels\n\n    Raises:\n        ValueError: If image format is not supported\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BaseLayoutExtractor.batch_extract","title":"batch_extract","text":"<pre><code>batch_extract(\n    images: List[Union[Image, ndarray, str, Path]],\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[LayoutOutput]\n</code></pre> <p>Run layout extraction on multiple images.</p> <p>Default implementation loops over extract(). Subclasses can override for optimized batching.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of images in any supported format</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[LayoutOutput]</code> <p>List of LayoutOutput in same order as input</p> <p>Examples:</p> <pre><code>images = [doc.get_page(i) for i in range(doc.page_count)]\nresults = extractor.batch_extract(images)\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/base.py</code> <pre><code>def batch_extract(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[LayoutOutput]:\n    \"\"\"\n    Run layout extraction on multiple images.\n\n    Default implementation loops over extract(). Subclasses can override\n    for optimized batching.\n\n    Args:\n        images: List of images in any supported format\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of LayoutOutput in same order as input\n\n    Examples:\n        ```python\n        images = [doc.get_page(i) for i in range(doc.page_count)]\n        results = extractor.batch_extract(images)\n        ```\n    \"\"\"\n    results = []\n    total = len(images)\n\n    for i, image in enumerate(images):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(image)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BaseLayoutExtractor.extract_document","title":"extract_document","text":"<pre><code>extract_document(\n    document: Document,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[LayoutOutput]\n</code></pre> <p>Run layout extraction on all pages of a document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[LayoutOutput]</code> <p>List of LayoutOutput, one per page</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresults = extractor.extract_document(doc)\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/base.py</code> <pre><code>def extract_document(\n    self,\n    document: \"Document\",\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[LayoutOutput]:\n    \"\"\"\n    Run layout extraction on all pages of a document.\n\n    Args:\n        document: Document instance\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of LayoutOutput, one per page\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        results = extractor.extract_document(doc)\n        ```\n    \"\"\"\n    results = []\n    total = document.page_count\n\n    for i, page in enumerate(document.iter_pages()):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(page)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.DocLayoutYOLO","title":"DocLayoutYOLO","text":"<pre><code>DocLayoutYOLO(config: DocLayoutYOLOConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>DocLayout-YOLO layout extractor.</p> <p>A YOLO-based model optimized for document layout detection. Detects: title, text, figure, table, formula, captions, etc.</p> <p>This is a single-backend model (PyTorch only).</p> Example <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\nextractor = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre> <p>Initialize DocLayout-YOLO extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object with device, model_path, etc.</p> <p> TYPE: <code>DocLayoutYOLOConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/doc_layout_yolo.py</code> <pre><code>def __init__(self, config: DocLayoutYOLOConfig):\n    \"\"\"\n    Initialize DocLayout-YOLO extractor.\n\n    Args:\n        config: Configuration object with device, model_path, etc.\n    \"\"\"\n    self.config = config\n    self._model = None\n    self._device = self._resolve_device(config.device)\n    self._model_path = self._resolve_model_path(config.model_path)\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.DocLayoutYOLO.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> Source code in <code>omnidocs/tasks/layout_extraction/doc_layout_yolo.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n    if self._model is None:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    img_width, img_height = pil_image.size\n\n    # Run inference\n    results = self._model.predict(\n        pil_image,\n        imgsz=self.config.img_size,\n        conf=self.config.confidence,\n        device=self._device,\n    )\n\n    result = results[0]\n\n    # Parse detections\n    layout_boxes = []\n\n    if hasattr(result, \"boxes\") and result.boxes is not None:\n        boxes = result.boxes\n\n        for i in range(len(boxes)):\n            # Get coordinates\n            bbox_coords = boxes.xyxy[i].cpu().numpy().tolist()\n\n            # Get class and confidence\n            class_id = int(boxes.cls[i].item())\n            confidence = float(boxes.conf[i].item())\n\n            # Get original label from class names\n            original_label = DOCLAYOUT_YOLO_CLASS_NAMES.get(class_id, f\"class_{class_id}\")\n\n            # Map to standardized label\n            standard_label = DOCLAYOUT_YOLO_MAPPING.to_standard(original_label)\n\n            layout_boxes.append(\n                LayoutBox(\n                    label=standard_label,\n                    bbox=BoundingBox.from_list(bbox_coords),\n                    confidence=confidence,\n                    class_id=class_id,\n                    original_label=original_label,\n                )\n            )\n\n    # Sort by y-coordinate (top to bottom reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=img_width,\n        image_height=img_height,\n        model_name=\"DocLayout-YOLO\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.DocLayoutYOLOConfig","title":"DocLayoutYOLOConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for DocLayout-YOLO layout extractor.</p> <p>This is a single-backend model (PyTorch only).</p> Example <pre><code>config = DocLayoutYOLOConfig(device=\"cuda\", confidence=0.3)\nextractor = DocLayoutYOLO(config=config)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box coordinates in pixel space.</p> <p>Coordinates follow the convention: (x1, y1) is top-left, (x2, y2) is bottom-right.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x, y, width, height) format.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_xywh(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x, y, width, height) format.\"\"\"\n    return (self.x1, self.y1, self.width, self.height)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> <p>Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas. This provides consistent coordinates regardless of original image size.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Example <pre><code>bbox = BoundingBox(x1=100, y1=50, x2=500, y2=300)\nnormalized = bbox.to_normalized(1000, 800)\n# x: 100/1000*1024 = 102.4, y: 50/800*1024 = 64\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas.\n    This provides consistent coordinates regardless of original image size.\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n\n    Example:\n        ```python\n        bbox = BoundingBox(x1=100, y1=50, x2=500, y2=300)\n        normalized = bbox.to_normalized(1000, 800)\n        # x: 100/1000*1024 = 102.4, y: 50/800*1024 = 64\n        ```\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.BoundingBox.to_absolute","title":"to_absolute","text":"<pre><code>to_absolute(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert from normalized (0-1024) to absolute pixel coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Target image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Target image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with absolute pixel coordinates</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_absolute(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert from normalized (0-1024) to absolute pixel coordinates.\n\n    Args:\n        image_width: Target image width in pixels\n        image_height: Target image height in pixels\n\n    Returns:\n        New BoundingBox with absolute pixel coordinates\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / NORMALIZED_SIZE * image_width,\n        y1=self.y1 / NORMALIZED_SIZE * image_height,\n        x2=self.x2 / NORMALIZED_SIZE * image_width,\n        y2=self.y2 / NORMALIZED_SIZE * image_height,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.CustomLabel","title":"CustomLabel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Type-safe custom layout label definition for VLM-based models.</p> <p>VLM models like Qwen3-VL support flexible custom labels beyond the standard LayoutLabel enum. Use this class to define custom labels with validation.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import CustomLabel\n\n# Simple custom label\ncode_block = CustomLabel(name=\"code_block\")\n\n# With metadata\nsidebar = CustomLabel(\n        name=\"sidebar\",\n        description=\"Secondary content panel\",\n        color=\"#9B59B6\",\n    )\n\n# Use with QwenLayoutDetector\nresult = detector.extract(image, custom_labels=[code_block, sidebar])\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LabelMapping","title":"LabelMapping","text":"<pre><code>LabelMapping(mapping: Dict[str, LayoutLabel])\n</code></pre> <p>Base class for model-specific label mappings.</p> <p>Each model maps its native labels to standardized LayoutLabel values.</p> <p>Initialize label mapping.</p> PARAMETER DESCRIPTION <code>mapping</code> <p>Dict mapping model-specific labels to LayoutLabel enum values</p> <p> TYPE: <code>Dict[str, LayoutLabel]</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def __init__(self, mapping: Dict[str, LayoutLabel]):\n    \"\"\"\n    Initialize label mapping.\n\n    Args:\n        mapping: Dict mapping model-specific labels to LayoutLabel enum values\n    \"\"\"\n    self._mapping = {k.lower(): v for k, v in mapping.items()}\n    self._reverse_mapping = {v: k for k, v in mapping.items()}\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LabelMapping.supported_labels","title":"supported_labels  <code>property</code>","text":"<pre><code>supported_labels: List[str]\n</code></pre> <p>Get list of supported model-specific labels.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LabelMapping.standard_labels","title":"standard_labels  <code>property</code>","text":"<pre><code>standard_labels: List[LayoutLabel]\n</code></pre> <p>Get list of standard labels this mapping produces.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LabelMapping.to_standard","title":"to_standard","text":"<pre><code>to_standard(model_label: str) -&gt; LayoutLabel\n</code></pre> <p>Convert model-specific label to standardized LayoutLabel.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_standard(self, model_label: str) -&gt; LayoutLabel:\n    \"\"\"Convert model-specific label to standardized LayoutLabel.\"\"\"\n    return self._mapping.get(model_label.lower(), LayoutLabel.UNKNOWN)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LabelMapping.from_standard","title":"from_standard","text":"<pre><code>from_standard(standard_label: LayoutLabel) -&gt; Optional[str]\n</code></pre> <p>Convert standardized LayoutLabel to model-specific label.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def from_standard(self, standard_label: LayoutLabel) -&gt; Optional[str]:\n    \"\"\"Convert standardized LayoutLabel to model-specific label.\"\"\"\n    return self._reverse_mapping.get(standard_label)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutBox","title":"LayoutBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single detected layout element with label, bounding box, and confidence.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutBox.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"label\": self.label.value,\n        \"bbox\": self.bbox.to_list(),\n        \"confidence\": self.confidence,\n        \"class_id\": self.class_id,\n        \"original_label\": self.original_label,\n    }\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutBox.get_normalized_bbox","title":"get_normalized_bbox","text":"<pre><code>get_normalized_bbox(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Get bounding box in normalized (0-1024) coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox with normalized coordinates</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def get_normalized_bbox(self, image_width: int, image_height: int) -&gt; BoundingBox:\n    \"\"\"\n    Get bounding box in normalized (0-1024) coordinates.\n\n    Args:\n        image_width: Original image width\n        image_height: Original image height\n\n    Returns:\n        BoundingBox with normalized coordinates\n    \"\"\"\n    return self.bbox.to_normalized(image_width, image_height)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutLabel","title":"LayoutLabel","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Standardized layout labels used across all layout extractors.</p> <p>These provide a consistent vocabulary regardless of which model is used.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput","title":"LayoutOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete layout extraction results for a single image.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.element_count","title":"element_count  <code>property</code>","text":"<pre><code>element_count: int\n</code></pre> <p>Number of detected elements.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.labels_found","title":"labels_found  <code>property</code>","text":"<pre><code>labels_found: List[str]\n</code></pre> <p>Unique labels found in detections.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.filter_by_label","title":"filter_by_label","text":"<pre><code>filter_by_label(label: LayoutLabel) -&gt; List[LayoutBox]\n</code></pre> <p>Filter boxes by label.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def filter_by_label(self, label: LayoutLabel) -&gt; List[LayoutBox]:\n    \"\"\"Filter boxes by label.\"\"\"\n    return [box for box in self.bboxes if box.label == label]\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(\n    min_confidence: float,\n) -&gt; List[LayoutBox]\n</code></pre> <p>Filter boxes by minimum confidence.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def filter_by_confidence(self, min_confidence: float) -&gt; List[LayoutBox]:\n    \"\"\"Filter boxes by minimum confidence.\"\"\"\n    return [box for box in self.bboxes if box.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"bboxes\": [box.to_dict() for box in self.bboxes],\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"element_count\": self.element_count,\n        \"labels_found\": self.labels_found,\n    }\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.sort_by_position","title":"sort_by_position","text":"<pre><code>sort_by_position(\n    top_to_bottom: bool = True,\n) -&gt; LayoutOutput\n</code></pre> <p>Return a new LayoutOutput with boxes sorted by position.</p> PARAMETER DESCRIPTION <code>top_to_bottom</code> <p>If True, sort by y-coordinate (reading order)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def sort_by_position(self, top_to_bottom: bool = True) -&gt; \"LayoutOutput\":\n    \"\"\"\n    Return a new LayoutOutput with boxes sorted by position.\n\n    Args:\n        top_to_bottom: If True, sort by y-coordinate (reading order)\n    \"\"\"\n    sorted_boxes = sorted(self.bboxes, key=lambda b: (b.bbox.y1, b.bbox.x1), reverse=not top_to_bottom)\n    return LayoutOutput(\n        bboxes=sorted_boxes,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        model_name=self.model_name,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.get_normalized_bboxes","title":"get_normalized_bboxes","text":"<pre><code>get_normalized_bboxes() -&gt; List[Dict]\n</code></pre> <p>Get all bounding boxes in normalized (0-1024) coordinates.</p> RETURNS DESCRIPTION <code>List[Dict]</code> <p>List of dicts with normalized bbox coordinates and metadata.</p> Example <pre><code>result = extractor.extract(image)\nnormalized = result.get_normalized_bboxes()\nfor box in normalized:\n        print(f\"{box['label']}: {box['bbox']}\")  # coords in 0-1024 range\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def get_normalized_bboxes(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all bounding boxes in normalized (0-1024) coordinates.\n\n    Returns:\n        List of dicts with normalized bbox coordinates and metadata.\n\n    Example:\n        ```python\n        result = extractor.extract(image)\n        normalized = result.get_normalized_bboxes()\n        for box in normalized:\n                print(f\"{box['label']}: {box['bbox']}\")  # coords in 0-1024 range\n        ```\n    \"\"\"\n    normalized = []\n    for box in self.bboxes:\n        norm_bbox = box.bbox.to_normalized(self.image_width, self.image_height)\n        normalized.append(\n            {\n                \"label\": box.label.value,\n                \"bbox\": norm_bbox.to_list(),\n                \"confidence\": box.confidence,\n                \"class_id\": box.class_id,\n                \"original_label\": box.original_label,\n            }\n        )\n    return normalized\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.visualize","title":"visualize","text":"<pre><code>visualize(\n    image: Image,\n    output_path: Optional[Union[str, Path]] = None,\n    show_labels: bool = True,\n    show_confidence: bool = True,\n    line_width: int = 3,\n    font_size: int = 12,\n) -&gt; Image.Image\n</code></pre> <p>Visualize layout detection results on the image.</p> <p>Draws bounding boxes with labels and confidence scores on the image. Each layout category has a distinct color for easy identification.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to draw on (will be copied, not modified)</p> <p> TYPE: <code>Image</code> </p> <code>output_path</code> <p>Optional path to save the visualization</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> <code>show_labels</code> <p>Whether to show label text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_confidence</code> <p>Whether to show confidence scores</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>line_width</code> <p>Width of bounding box lines</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>font_size</code> <p>Size of label text (note: uses default font)</p> <p> TYPE: <code>int</code> DEFAULT: <code>12</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image with visualizations drawn</p> Example <pre><code>result = extractor.extract(image)\nviz = result.visualize(image, output_path=\"layout_viz.png\")\nviz.show()  # Display in notebook/viewer\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def visualize(\n    self,\n    image: \"Image.Image\",\n    output_path: Optional[Union[str, Path]] = None,\n    show_labels: bool = True,\n    show_confidence: bool = True,\n    line_width: int = 3,\n    font_size: int = 12,\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Visualize layout detection results on the image.\n\n    Draws bounding boxes with labels and confidence scores on the image.\n    Each layout category has a distinct color for easy identification.\n\n    Args:\n        image: PIL Image to draw on (will be copied, not modified)\n        output_path: Optional path to save the visualization\n        show_labels: Whether to show label text\n        show_confidence: Whether to show confidence scores\n        line_width: Width of bounding box lines\n        font_size: Size of label text (note: uses default font)\n\n    Returns:\n        PIL Image with visualizations drawn\n\n    Example:\n        ```python\n        result = extractor.extract(image)\n        viz = result.visualize(image, output_path=\"layout_viz.png\")\n        viz.show()  # Display in notebook/viewer\n        ```\n    \"\"\"\n    from PIL import ImageDraw\n\n    # Copy image to avoid modifying original\n    viz_image = image.copy().convert(\"RGB\")\n    draw = ImageDraw.Draw(viz_image)\n\n    for box in self.bboxes:\n        # Get color for this label\n        color = LABEL_COLORS.get(box.label, \"#95A5A6\")\n\n        # Draw bounding box\n        coords = box.bbox.to_xyxy()\n        draw.rectangle(coords, outline=color, width=line_width)\n\n        # Build label text\n        if show_labels or show_confidence:\n            label_parts = []\n            if show_labels:\n                label_parts.append(box.label.value)\n            if show_confidence:\n                label_parts.append(f\"{box.confidence:.2f}\")\n            label_text = \" \".join(label_parts)\n\n            # Draw label background\n            text_bbox = draw.textbbox((coords[0], coords[1] - 20), label_text)\n            draw.rectangle(text_bbox, fill=color)\n\n            # Draw label text\n            draw.text(\n                (coords[0], coords[1] - 20),\n                label_text,\n                fill=\"white\",\n            )\n\n    # Save if path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        viz_image.save(output_path)\n\n    return viz_image\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; LayoutOutput\n</code></pre> <p>Load a LayoutOutput instance from a JSON file.</p> <p>Reads a JSON file and deserializes its contents into a LayoutOutput object. Uses Pydantic's model_validate_json for proper handling of nested objects.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to JSON file containing serialized LayoutOutput data.       Can be string or pathlib.Path object.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>Deserialized layout output instance from file.</p> <p> TYPE: <code>LayoutOutput</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>UnicodeDecodeError</code> <p>If file cannot be decoded as UTF-8.</p> <code>ValueError</code> <p>If file contents are not valid JSON.</p> <code>ValidationError</code> <p>If JSON data doesn't match LayoutOutput schema.</p> Example <p><pre><code>output = LayoutOutput.load_json('layout_results.json')\nprint(f\"Found {output.element_count} elements\")\n</code></pre> Found 5 elements</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"LayoutOutput\":\n    \"\"\"\n    Load a LayoutOutput instance from a JSON file.\n\n    Reads a JSON file and deserializes its contents into a LayoutOutput object.\n    Uses Pydantic's model_validate_json for proper handling of nested objects.\n\n    Args:\n        file_path: Path to JSON file containing serialized LayoutOutput data.\n                  Can be string or pathlib.Path object.\n\n    Returns:\n        LayoutOutput: Deserialized layout output instance from file.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n        UnicodeDecodeError: If file cannot be decoded as UTF-8.\n        ValueError: If file contents are not valid JSON.\n        ValidationError: If JSON data doesn't match LayoutOutput schema.\n\n    Example:\n        ```python\n        output = LayoutOutput.load_json('layout_results.json')\n        print(f\"Found {output.element_count} elements\")\n        ```\n        Found 5 elements\n    \"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.LayoutOutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save LayoutOutput instance to a JSON file.</p> <p>Serializes the LayoutOutput object to JSON and writes it to a file. Automatically creates parent directories if they don't exist. Uses UTF-8 encoding for compatibility and proper handling of special characters.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path where JSON file should be saved. Can be string or       pathlib.Path object. Parent directories will be created       if they don't exist.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> RAISES DESCRIPTION <code>OSError</code> <p>If file cannot be written due to permission or disk errors.</p> <code>TypeError</code> <p>If file_path is not a string or Path object.</p> Example <pre><code>output = LayoutOutput(bboxes=[], image_width=800, image_height=600)\noutput.save_json('results/layout_output.json')\n# File is created at results/layout_output.json\n# Parent 'results' directory is created if it didn't exist\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"\n    Save LayoutOutput instance to a JSON file.\n\n    Serializes the LayoutOutput object to JSON and writes it to a file.\n    Automatically creates parent directories if they don't exist. Uses UTF-8\n    encoding for compatibility and proper handling of special characters.\n\n    Args:\n        file_path: Path where JSON file should be saved. Can be string or\n                  pathlib.Path object. Parent directories will be created\n                  if they don't exist.\n\n    Returns:\n        None\n\n    Raises:\n        OSError: If file cannot be written due to permission or disk errors.\n        TypeError: If file_path is not a string or Path object.\n\n    Example:\n        ```python\n        output = LayoutOutput(bboxes=[], image_width=800, image_height=600)\n        output.save_json('results/layout_output.json')\n        # File is created at results/layout_output.json\n        # Parent 'results' directory is created if it didn't exist\n        ```\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.QwenLayoutDetector","title":"QwenLayoutDetector","text":"<pre><code>QwenLayoutDetector(backend: QwenLayoutBackendConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>Qwen3-VL Vision-Language Model layout detector.</p> <p>A flexible VLM-based layout detector that supports custom labels. Unlike fixed-label models (DocLayoutYOLO, RT-DETR), Qwen can detect any document elements specified at runtime.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\n# Initialize with PyTorch backend\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Basic extraction with default labels\nresult = detector.extract(image)\n\n# With custom labels (strings)\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n\n# With typed custom labels\nlabels = [\n        CustomLabel(name=\"code_block\", color=\"#E74C3C\"),\n        CustomLabel(name=\"sidebar\", description=\"Side panel content\"),\n    ]\nresult = detector.extract(image, custom_labels=labels)\n</code></pre> <p>Initialize Qwen layout detector.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend - QwenLayoutVLLMConfig: VLLM high-throughput backend - QwenLayoutMLXConfig: MLX backend for Apple Silicon - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenLayoutBackendConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def __init__(self, backend: QwenLayoutBackendConfig):\n    \"\"\"\n    Initialize Qwen layout detector.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenLayoutVLLMConfig: VLLM high-throughput backend\n            - QwenLayoutMLXConfig: MLX backend for Apple Silicon\n            - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.QwenLayoutDetector.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    custom_labels: Optional[\n        List[Union[str, CustomLabel]]\n    ] = None,\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout detection on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>custom_labels</code> <p>Optional custom labels to detect. Can be: - None: Use default labels (title, text, table, figure, etc.) - List[str]: Simple label names [\"code_block\", \"sidebar\"] - List[CustomLabel]: Typed labels with metadata</p> <p> TYPE: <code>Optional[List[Union[str, CustomLabel]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    custom_labels: Optional[List[Union[str, CustomLabel]]] = None,\n) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout detection on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        custom_labels: Optional custom labels to detect. Can be:\n            - None: Use default labels (title, text, table, figure, etc.)\n            - List[str]: Simple label names [\"code_block\", \"sidebar\"]\n            - List[CustomLabel]: Typed labels with metadata\n\n    Returns:\n        LayoutOutput with detected layout boxes\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Normalize labels\n    label_names = self._normalize_labels(custom_labels)\n\n    # Build prompt\n    prompt = self._build_detection_prompt(label_names)\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenLayoutPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenLayoutVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenLayoutMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenLayoutAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse detections\n    detections = self._parse_json_output(raw_output)\n\n    # Convert to LayoutOutput\n    layout_boxes = self._build_layout_boxes(detections, width, height)\n\n    # Sort by position (reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.RTDETRConfig","title":"RTDETRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for RT-DETR layout extractor.</p> <p>This is a single-backend model (PyTorch/Transformers only).</p> Example <pre><code>config = RTDETRConfig(device=\"cuda\", confidence=0.4)\nextractor = RTDETRLayoutExtractor(config=config)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.RTDETRLayoutExtractor","title":"RTDETRLayoutExtractor","text":"<pre><code>RTDETRLayoutExtractor(config: RTDETRConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>RT-DETR layout extractor using HuggingFace Transformers.</p> <p>A transformer-based real-time detection model for document layout. Detects: title, text, table, figure, list, formula, captions, headers, footers.</p> <p>This is a single-backend model (PyTorch/Transformers only).</p> Example <pre><code>from omnidocs.tasks.layout_extraction import RTDETRLayoutExtractor, RTDETRConfig\n\nextractor = RTDETRLayoutExtractor(config=RTDETRConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre> <p>Initialize RT-DETR layout extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object with device, model settings, etc.</p> <p> TYPE: <code>RTDETRConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/rtdetr.py</code> <pre><code>def __init__(self, config: RTDETRConfig):\n    \"\"\"\n    Initialize RT-DETR layout extractor.\n\n    Args:\n        config: Configuration object with device, model settings, etc.\n    \"\"\"\n    self.config = config\n    self._model = None\n    self._processor = None\n    self._device = self._resolve_device(config.device)\n    self._model_path = self._resolve_model_path(config.model_path)\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.RTDETRLayoutExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> Source code in <code>omnidocs/tasks/layout_extraction/rtdetr.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n    import torch\n\n    if self._model is None or self._processor is None:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    img_width, img_height = pil_image.size\n\n    # Preprocess\n    inputs = self._processor(\n        images=pil_image,\n        return_tensors=\"pt\",\n        size={\"height\": self.config.image_size, \"width\": self.config.image_size},\n    )\n\n    # Move to device\n    inputs = {k: v.to(self._device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n\n    # Run inference\n    with torch.no_grad():\n        outputs = self._model(**inputs)\n\n    # Post-process results\n    target_sizes = torch.tensor([[img_height, img_width]])\n    results = self._processor.post_process_object_detection(\n        outputs,\n        target_sizes=target_sizes,\n        threshold=self.config.confidence,\n    )[0]\n\n    # Parse detections\n    layout_boxes = []\n\n    for score, label_id, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n        confidence = float(score.item())\n        class_id = int(label_id.item())\n\n        # Get original label from model config\n        # Note: The model outputs 0-indexed class IDs, but id2label has background at index 0,\n        # so we add 1 to map correctly (e.g., model output 8 -&gt; id2label[9] = \"Table\")\n        original_label = self._model.config.id2label.get(class_id + 1, f\"class_{class_id}\")\n\n        # Map to standardized label\n        standard_label = RTDETR_MAPPING.to_standard(original_label)\n\n        # Box coordinates\n        box_coords = box.cpu().tolist()\n\n        layout_boxes.append(\n            LayoutBox(\n                label=standard_label,\n                bbox=BoundingBox.from_list(box_coords),\n                confidence=confidence,\n                class_id=class_id,\n                original_label=original_label,\n            )\n        )\n\n    # Sort by y-coordinate (top to bottom reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=img_width,\n        image_height=img_height,\n        model_name=\"RT-DETR (docling-layout)\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.base","title":"base","text":"<p>Base class for layout extractors.</p> <p>Defines the abstract interface that all layout extractors must implement.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.base.BaseLayoutExtractor","title":"BaseLayoutExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for layout extractors.</p> <p>All layout extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyLayoutExtractor(BaseLayoutExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Load model weights\n            pass\n\n        def extract(self, image):\n            # Run extraction\n            return LayoutOutput(...)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.base.BaseLayoutExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput containing detected layout boxes with standardized labels</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format is not supported</p> <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/layout_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n\n    Returns:\n        LayoutOutput containing detected layout boxes with standardized labels\n\n    Raises:\n        ValueError: If image format is not supported\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.base.BaseLayoutExtractor.batch_extract","title":"batch_extract","text":"<pre><code>batch_extract(\n    images: List[Union[Image, ndarray, str, Path]],\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[LayoutOutput]\n</code></pre> <p>Run layout extraction on multiple images.</p> <p>Default implementation loops over extract(). Subclasses can override for optimized batching.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of images in any supported format</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[LayoutOutput]</code> <p>List of LayoutOutput in same order as input</p> <p>Examples:</p> <pre><code>images = [doc.get_page(i) for i in range(doc.page_count)]\nresults = extractor.batch_extract(images)\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/base.py</code> <pre><code>def batch_extract(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[LayoutOutput]:\n    \"\"\"\n    Run layout extraction on multiple images.\n\n    Default implementation loops over extract(). Subclasses can override\n    for optimized batching.\n\n    Args:\n        images: List of images in any supported format\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of LayoutOutput in same order as input\n\n    Examples:\n        ```python\n        images = [doc.get_page(i) for i in range(doc.page_count)]\n        results = extractor.batch_extract(images)\n        ```\n    \"\"\"\n    results = []\n    total = len(images)\n\n    for i, image in enumerate(images):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(image)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.base.BaseLayoutExtractor.extract_document","title":"extract_document","text":"<pre><code>extract_document(\n    document: Document,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[LayoutOutput]\n</code></pre> <p>Run layout extraction on all pages of a document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[LayoutOutput]</code> <p>List of LayoutOutput, one per page</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresults = extractor.extract_document(doc)\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/base.py</code> <pre><code>def extract_document(\n    self,\n    document: \"Document\",\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[LayoutOutput]:\n    \"\"\"\n    Run layout extraction on all pages of a document.\n\n    Args:\n        document: Document instance\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of LayoutOutput, one per page\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        results = extractor.extract_document(doc)\n        ```\n    \"\"\"\n    results = []\n    total = document.page_count\n\n    for i, page in enumerate(document.iter_pages()):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(page)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.doc_layout_yolo","title":"doc_layout_yolo","text":"<p>DocLayout-YOLO layout extractor.</p> <p>A YOLO-based model for document layout detection, optimized for academic papers and technical documents.</p> <p>Model: juliozhao/DocLayout-YOLO-DocStructBench</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.doc_layout_yolo.DocLayoutYOLOConfig","title":"DocLayoutYOLOConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for DocLayout-YOLO layout extractor.</p> <p>This is a single-backend model (PyTorch only).</p> Example <pre><code>config = DocLayoutYOLOConfig(device=\"cuda\", confidence=0.3)\nextractor = DocLayoutYOLO(config=config)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.doc_layout_yolo.DocLayoutYOLO","title":"DocLayoutYOLO","text":"<pre><code>DocLayoutYOLO(config: DocLayoutYOLOConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>DocLayout-YOLO layout extractor.</p> <p>A YOLO-based model optimized for document layout detection. Detects: title, text, figure, table, formula, captions, etc.</p> <p>This is a single-backend model (PyTorch only).</p> Example <pre><code>from omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\n\nextractor = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre> <p>Initialize DocLayout-YOLO extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object with device, model_path, etc.</p> <p> TYPE: <code>DocLayoutYOLOConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/doc_layout_yolo.py</code> <pre><code>def __init__(self, config: DocLayoutYOLOConfig):\n    \"\"\"\n    Initialize DocLayout-YOLO extractor.\n\n    Args:\n        config: Configuration object with device, model_path, etc.\n    \"\"\"\n    self.config = config\n    self._model = None\n    self._device = self._resolve_device(config.device)\n    self._model_path = self._resolve_model_path(config.model_path)\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.doc_layout_yolo.DocLayoutYOLO.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> Source code in <code>omnidocs/tasks/layout_extraction/doc_layout_yolo.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n    if self._model is None:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    img_width, img_height = pil_image.size\n\n    # Run inference\n    results = self._model.predict(\n        pil_image,\n        imgsz=self.config.img_size,\n        conf=self.config.confidence,\n        device=self._device,\n    )\n\n    result = results[0]\n\n    # Parse detections\n    layout_boxes = []\n\n    if hasattr(result, \"boxes\") and result.boxes is not None:\n        boxes = result.boxes\n\n        for i in range(len(boxes)):\n            # Get coordinates\n            bbox_coords = boxes.xyxy[i].cpu().numpy().tolist()\n\n            # Get class and confidence\n            class_id = int(boxes.cls[i].item())\n            confidence = float(boxes.conf[i].item())\n\n            # Get original label from class names\n            original_label = DOCLAYOUT_YOLO_CLASS_NAMES.get(class_id, f\"class_{class_id}\")\n\n            # Map to standardized label\n            standard_label = DOCLAYOUT_YOLO_MAPPING.to_standard(original_label)\n\n            layout_boxes.append(\n                LayoutBox(\n                    label=standard_label,\n                    bbox=BoundingBox.from_list(bbox_coords),\n                    confidence=confidence,\n                    class_id=class_id,\n                    original_label=original_label,\n                )\n            )\n\n    # Sort by y-coordinate (top to bottom reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=img_width,\n        image_height=img_height,\n        model_name=\"DocLayout-YOLO\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models","title":"models","text":"<p>Pydantic models for layout extraction outputs.</p> <p>Defines standardized output types and label enums for layout detection.</p> Coordinate Systems <ul> <li>Absolute (default): Coordinates in pixels relative to original image size</li> <li>Normalized (0-1024): Coordinates scaled to 0-1024 range (virtual 1024x1024 canvas)</li> </ul> <p>Use <code>bbox.to_normalized(width, height)</code> or <code>output.get_normalized_bboxes()</code> to convert to normalized coordinates.</p> Example <pre><code>result = extractor.extract(image)  # Returns absolute pixel coordinates\nnormalized = result.get_normalized_bboxes()  # Returns 0-1024 normalized coords\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutLabel","title":"LayoutLabel","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Standardized layout labels used across all layout extractors.</p> <p>These provide a consistent vocabulary regardless of which model is used.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.CustomLabel","title":"CustomLabel","text":"<p>               Bases: <code>BaseModel</code></p> <p>Type-safe custom layout label definition for VLM-based models.</p> <p>VLM models like Qwen3-VL support flexible custom labels beyond the standard LayoutLabel enum. Use this class to define custom labels with validation.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import CustomLabel\n\n# Simple custom label\ncode_block = CustomLabel(name=\"code_block\")\n\n# With metadata\nsidebar = CustomLabel(\n        name=\"sidebar\",\n        description=\"Secondary content panel\",\n        color=\"#9B59B6\",\n    )\n\n# Use with QwenLayoutDetector\nresult = detector.extract(image, custom_labels=[code_block, sidebar])\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LabelMapping","title":"LabelMapping","text":"<pre><code>LabelMapping(mapping: Dict[str, LayoutLabel])\n</code></pre> <p>Base class for model-specific label mappings.</p> <p>Each model maps its native labels to standardized LayoutLabel values.</p> <p>Initialize label mapping.</p> PARAMETER DESCRIPTION <code>mapping</code> <p>Dict mapping model-specific labels to LayoutLabel enum values</p> <p> TYPE: <code>Dict[str, LayoutLabel]</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def __init__(self, mapping: Dict[str, LayoutLabel]):\n    \"\"\"\n    Initialize label mapping.\n\n    Args:\n        mapping: Dict mapping model-specific labels to LayoutLabel enum values\n    \"\"\"\n    self._mapping = {k.lower(): v for k, v in mapping.items()}\n    self._reverse_mapping = {v: k for k, v in mapping.items()}\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LabelMapping.supported_labels","title":"supported_labels  <code>property</code>","text":"<pre><code>supported_labels: List[str]\n</code></pre> <p>Get list of supported model-specific labels.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LabelMapping.standard_labels","title":"standard_labels  <code>property</code>","text":"<pre><code>standard_labels: List[LayoutLabel]\n</code></pre> <p>Get list of standard labels this mapping produces.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LabelMapping.to_standard","title":"to_standard","text":"<pre><code>to_standard(model_label: str) -&gt; LayoutLabel\n</code></pre> <p>Convert model-specific label to standardized LayoutLabel.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_standard(self, model_label: str) -&gt; LayoutLabel:\n    \"\"\"Convert model-specific label to standardized LayoutLabel.\"\"\"\n    return self._mapping.get(model_label.lower(), LayoutLabel.UNKNOWN)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LabelMapping.from_standard","title":"from_standard","text":"<pre><code>from_standard(standard_label: LayoutLabel) -&gt; Optional[str]\n</code></pre> <p>Convert standardized LayoutLabel to model-specific label.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def from_standard(self, standard_label: LayoutLabel) -&gt; Optional[str]:\n    \"\"\"Convert standardized LayoutLabel to model-specific label.\"\"\"\n    return self._reverse_mapping.get(standard_label)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box coordinates in pixel space.</p> <p>Coordinates follow the convention: (x1, y1) is top-left, (x2, y2) is bottom-right.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x, y, width, height) format.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_xywh(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x, y, width, height) format.\"\"\"\n    return (self.x1, self.y1, self.width, self.height)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> <p>Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas. This provides consistent coordinates regardless of original image size.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Example <pre><code>bbox = BoundingBox(x1=100, y1=50, x2=500, y2=300)\nnormalized = bbox.to_normalized(1000, 800)\n# x: 100/1000*1024 = 102.4, y: 50/800*1024 = 64\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas.\n    This provides consistent coordinates regardless of original image size.\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n\n    Example:\n        ```python\n        bbox = BoundingBox(x1=100, y1=50, x2=500, y2=300)\n        normalized = bbox.to_normalized(1000, 800)\n        # x: 100/1000*1024 = 102.4, y: 50/800*1024 = 64\n        ```\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.BoundingBox.to_absolute","title":"to_absolute","text":"<pre><code>to_absolute(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert from normalized (0-1024) to absolute pixel coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Target image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Target image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with absolute pixel coordinates</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_absolute(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert from normalized (0-1024) to absolute pixel coordinates.\n\n    Args:\n        image_width: Target image width in pixels\n        image_height: Target image height in pixels\n\n    Returns:\n        New BoundingBox with absolute pixel coordinates\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / NORMALIZED_SIZE * image_width,\n        y1=self.y1 / NORMALIZED_SIZE * image_height,\n        x2=self.x2 / NORMALIZED_SIZE * image_width,\n        y2=self.y2 / NORMALIZED_SIZE * image_height,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutBox","title":"LayoutBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single detected layout element with label, bounding box, and confidence.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutBox.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"label\": self.label.value,\n        \"bbox\": self.bbox.to_list(),\n        \"confidence\": self.confidence,\n        \"class_id\": self.class_id,\n        \"original_label\": self.original_label,\n    }\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutBox.get_normalized_bbox","title":"get_normalized_bbox","text":"<pre><code>get_normalized_bbox(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Get bounding box in normalized (0-1024) coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox with normalized coordinates</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def get_normalized_bbox(self, image_width: int, image_height: int) -&gt; BoundingBox:\n    \"\"\"\n    Get bounding box in normalized (0-1024) coordinates.\n\n    Args:\n        image_width: Original image width\n        image_height: Original image height\n\n    Returns:\n        BoundingBox with normalized coordinates\n    \"\"\"\n    return self.bbox.to_normalized(image_width, image_height)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput","title":"LayoutOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete layout extraction results for a single image.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.element_count","title":"element_count  <code>property</code>","text":"<pre><code>element_count: int\n</code></pre> <p>Number of detected elements.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.labels_found","title":"labels_found  <code>property</code>","text":"<pre><code>labels_found: List[str]\n</code></pre> <p>Unique labels found in detections.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.filter_by_label","title":"filter_by_label","text":"<pre><code>filter_by_label(label: LayoutLabel) -&gt; List[LayoutBox]\n</code></pre> <p>Filter boxes by label.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def filter_by_label(self, label: LayoutLabel) -&gt; List[LayoutBox]:\n    \"\"\"Filter boxes by label.\"\"\"\n    return [box for box in self.bboxes if box.label == label]\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(\n    min_confidence: float,\n) -&gt; List[LayoutBox]\n</code></pre> <p>Filter boxes by minimum confidence.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def filter_by_confidence(self, min_confidence: float) -&gt; List[LayoutBox]:\n    \"\"\"Filter boxes by minimum confidence.\"\"\"\n    return [box for box in self.bboxes if box.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"bboxes\": [box.to_dict() for box in self.bboxes],\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"element_count\": self.element_count,\n        \"labels_found\": self.labels_found,\n    }\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.sort_by_position","title":"sort_by_position","text":"<pre><code>sort_by_position(\n    top_to_bottom: bool = True,\n) -&gt; LayoutOutput\n</code></pre> <p>Return a new LayoutOutput with boxes sorted by position.</p> PARAMETER DESCRIPTION <code>top_to_bottom</code> <p>If True, sort by y-coordinate (reading order)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def sort_by_position(self, top_to_bottom: bool = True) -&gt; \"LayoutOutput\":\n    \"\"\"\n    Return a new LayoutOutput with boxes sorted by position.\n\n    Args:\n        top_to_bottom: If True, sort by y-coordinate (reading order)\n    \"\"\"\n    sorted_boxes = sorted(self.bboxes, key=lambda b: (b.bbox.y1, b.bbox.x1), reverse=not top_to_bottom)\n    return LayoutOutput(\n        bboxes=sorted_boxes,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        model_name=self.model_name,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.get_normalized_bboxes","title":"get_normalized_bboxes","text":"<pre><code>get_normalized_bboxes() -&gt; List[Dict]\n</code></pre> <p>Get all bounding boxes in normalized (0-1024) coordinates.</p> RETURNS DESCRIPTION <code>List[Dict]</code> <p>List of dicts with normalized bbox coordinates and metadata.</p> Example <pre><code>result = extractor.extract(image)\nnormalized = result.get_normalized_bboxes()\nfor box in normalized:\n        print(f\"{box['label']}: {box['bbox']}\")  # coords in 0-1024 range\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def get_normalized_bboxes(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all bounding boxes in normalized (0-1024) coordinates.\n\n    Returns:\n        List of dicts with normalized bbox coordinates and metadata.\n\n    Example:\n        ```python\n        result = extractor.extract(image)\n        normalized = result.get_normalized_bboxes()\n        for box in normalized:\n                print(f\"{box['label']}: {box['bbox']}\")  # coords in 0-1024 range\n        ```\n    \"\"\"\n    normalized = []\n    for box in self.bboxes:\n        norm_bbox = box.bbox.to_normalized(self.image_width, self.image_height)\n        normalized.append(\n            {\n                \"label\": box.label.value,\n                \"bbox\": norm_bbox.to_list(),\n                \"confidence\": box.confidence,\n                \"class_id\": box.class_id,\n                \"original_label\": box.original_label,\n            }\n        )\n    return normalized\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.visualize","title":"visualize","text":"<pre><code>visualize(\n    image: Image,\n    output_path: Optional[Union[str, Path]] = None,\n    show_labels: bool = True,\n    show_confidence: bool = True,\n    line_width: int = 3,\n    font_size: int = 12,\n) -&gt; Image.Image\n</code></pre> <p>Visualize layout detection results on the image.</p> <p>Draws bounding boxes with labels and confidence scores on the image. Each layout category has a distinct color for easy identification.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to draw on (will be copied, not modified)</p> <p> TYPE: <code>Image</code> </p> <code>output_path</code> <p>Optional path to save the visualization</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> <code>show_labels</code> <p>Whether to show label text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_confidence</code> <p>Whether to show confidence scores</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>line_width</code> <p>Width of bounding box lines</p> <p> TYPE: <code>int</code> DEFAULT: <code>3</code> </p> <code>font_size</code> <p>Size of label text (note: uses default font)</p> <p> TYPE: <code>int</code> DEFAULT: <code>12</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image with visualizations drawn</p> Example <pre><code>result = extractor.extract(image)\nviz = result.visualize(image, output_path=\"layout_viz.png\")\nviz.show()  # Display in notebook/viewer\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def visualize(\n    self,\n    image: \"Image.Image\",\n    output_path: Optional[Union[str, Path]] = None,\n    show_labels: bool = True,\n    show_confidence: bool = True,\n    line_width: int = 3,\n    font_size: int = 12,\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Visualize layout detection results on the image.\n\n    Draws bounding boxes with labels and confidence scores on the image.\n    Each layout category has a distinct color for easy identification.\n\n    Args:\n        image: PIL Image to draw on (will be copied, not modified)\n        output_path: Optional path to save the visualization\n        show_labels: Whether to show label text\n        show_confidence: Whether to show confidence scores\n        line_width: Width of bounding box lines\n        font_size: Size of label text (note: uses default font)\n\n    Returns:\n        PIL Image with visualizations drawn\n\n    Example:\n        ```python\n        result = extractor.extract(image)\n        viz = result.visualize(image, output_path=\"layout_viz.png\")\n        viz.show()  # Display in notebook/viewer\n        ```\n    \"\"\"\n    from PIL import ImageDraw\n\n    # Copy image to avoid modifying original\n    viz_image = image.copy().convert(\"RGB\")\n    draw = ImageDraw.Draw(viz_image)\n\n    for box in self.bboxes:\n        # Get color for this label\n        color = LABEL_COLORS.get(box.label, \"#95A5A6\")\n\n        # Draw bounding box\n        coords = box.bbox.to_xyxy()\n        draw.rectangle(coords, outline=color, width=line_width)\n\n        # Build label text\n        if show_labels or show_confidence:\n            label_parts = []\n            if show_labels:\n                label_parts.append(box.label.value)\n            if show_confidence:\n                label_parts.append(f\"{box.confidence:.2f}\")\n            label_text = \" \".join(label_parts)\n\n            # Draw label background\n            text_bbox = draw.textbbox((coords[0], coords[1] - 20), label_text)\n            draw.rectangle(text_bbox, fill=color)\n\n            # Draw label text\n            draw.text(\n                (coords[0], coords[1] - 20),\n                label_text,\n                fill=\"white\",\n            )\n\n    # Save if path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        viz_image.save(output_path)\n\n    return viz_image\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; LayoutOutput\n</code></pre> <p>Load a LayoutOutput instance from a JSON file.</p> <p>Reads a JSON file and deserializes its contents into a LayoutOutput object. Uses Pydantic's model_validate_json for proper handling of nested objects.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to JSON file containing serialized LayoutOutput data.       Can be string or pathlib.Path object.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>Deserialized layout output instance from file.</p> <p> TYPE: <code>LayoutOutput</code> </p> RAISES DESCRIPTION <code>FileNotFoundError</code> <p>If the specified file does not exist.</p> <code>UnicodeDecodeError</code> <p>If file cannot be decoded as UTF-8.</p> <code>ValueError</code> <p>If file contents are not valid JSON.</p> <code>ValidationError</code> <p>If JSON data doesn't match LayoutOutput schema.</p> Example <p><pre><code>output = LayoutOutput.load_json('layout_results.json')\nprint(f\"Found {output.element_count} elements\")\n</code></pre> Found 5 elements</p> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"LayoutOutput\":\n    \"\"\"\n    Load a LayoutOutput instance from a JSON file.\n\n    Reads a JSON file and deserializes its contents into a LayoutOutput object.\n    Uses Pydantic's model_validate_json for proper handling of nested objects.\n\n    Args:\n        file_path: Path to JSON file containing serialized LayoutOutput data.\n                  Can be string or pathlib.Path object.\n\n    Returns:\n        LayoutOutput: Deserialized layout output instance from file.\n\n    Raises:\n        FileNotFoundError: If the specified file does not exist.\n        UnicodeDecodeError: If file cannot be decoded as UTF-8.\n        ValueError: If file contents are not valid JSON.\n        ValidationError: If JSON data doesn't match LayoutOutput schema.\n\n    Example:\n        ```python\n        output = LayoutOutput.load_json('layout_results.json')\n        print(f\"Found {output.element_count} elements\")\n        ```\n        Found 5 elements\n    \"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.models.LayoutOutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save LayoutOutput instance to a JSON file.</p> <p>Serializes the LayoutOutput object to JSON and writes it to a file. Automatically creates parent directories if they don't exist. Uses UTF-8 encoding for compatibility and proper handling of special characters.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path where JSON file should be saved. Can be string or       pathlib.Path object. Parent directories will be created       if they don't exist.</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>None</code> <p>None</p> RAISES DESCRIPTION <code>OSError</code> <p>If file cannot be written due to permission or disk errors.</p> <code>TypeError</code> <p>If file_path is not a string or Path object.</p> Example <pre><code>output = LayoutOutput(bboxes=[], image_width=800, image_height=600)\noutput.save_json('results/layout_output.json')\n# File is created at results/layout_output.json\n# Parent 'results' directory is created if it didn't exist\n</code></pre> Source code in <code>omnidocs/tasks/layout_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"\n    Save LayoutOutput instance to a JSON file.\n\n    Serializes the LayoutOutput object to JSON and writes it to a file.\n    Automatically creates parent directories if they don't exist. Uses UTF-8\n    encoding for compatibility and proper handling of special characters.\n\n    Args:\n        file_path: Path where JSON file should be saved. Can be string or\n                  pathlib.Path object. Parent directories will be created\n                  if they don't exist.\n\n    Returns:\n        None\n\n    Raises:\n        OSError: If file cannot be written due to permission or disk errors.\n        TypeError: If file_path is not a string or Path object.\n\n    Example:\n        ```python\n        output = LayoutOutput(bboxes=[], image_width=800, image_height=600)\n        output.save_json('results/layout_output.json')\n        # File is created at results/layout_output.json\n        # Parent 'results' directory is created if it didn't exist\n        ```\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen","title":"qwen","text":"<p>Qwen3-VL backend configurations and detector for layout detection.</p> Available backends <ul> <li>QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend</li> <li>QwenLayoutVLLMConfig: VLLM high-throughput backend</li> <li>QwenLayoutMLXConfig: MLX backend for Apple Silicon</li> <li>QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</li> </ul> Example <pre><code>from omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\nconfig = QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutAPIConfig","title":"QwenLayoutAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen layout detection.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenLayoutAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutDetector","title":"QwenLayoutDetector","text":"<pre><code>QwenLayoutDetector(backend: QwenLayoutBackendConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>Qwen3-VL Vision-Language Model layout detector.</p> <p>A flexible VLM-based layout detector that supports custom labels. Unlike fixed-label models (DocLayoutYOLO, RT-DETR), Qwen can detect any document elements specified at runtime.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\n# Initialize with PyTorch backend\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Basic extraction with default labels\nresult = detector.extract(image)\n\n# With custom labels (strings)\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n\n# With typed custom labels\nlabels = [\n        CustomLabel(name=\"code_block\", color=\"#E74C3C\"),\n        CustomLabel(name=\"sidebar\", description=\"Side panel content\"),\n    ]\nresult = detector.extract(image, custom_labels=labels)\n</code></pre> <p>Initialize Qwen layout detector.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend - QwenLayoutVLLMConfig: VLLM high-throughput backend - QwenLayoutMLXConfig: MLX backend for Apple Silicon - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenLayoutBackendConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def __init__(self, backend: QwenLayoutBackendConfig):\n    \"\"\"\n    Initialize Qwen layout detector.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenLayoutVLLMConfig: VLLM high-throughput backend\n            - QwenLayoutMLXConfig: MLX backend for Apple Silicon\n            - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutDetector.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    custom_labels: Optional[\n        List[Union[str, CustomLabel]]\n    ] = None,\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout detection on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>custom_labels</code> <p>Optional custom labels to detect. Can be: - None: Use default labels (title, text, table, figure, etc.) - List[str]: Simple label names [\"code_block\", \"sidebar\"] - List[CustomLabel]: Typed labels with metadata</p> <p> TYPE: <code>Optional[List[Union[str, CustomLabel]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    custom_labels: Optional[List[Union[str, CustomLabel]]] = None,\n) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout detection on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        custom_labels: Optional custom labels to detect. Can be:\n            - None: Use default labels (title, text, table, figure, etc.)\n            - List[str]: Simple label names [\"code_block\", \"sidebar\"]\n            - List[CustomLabel]: Typed labels with metadata\n\n    Returns:\n        LayoutOutput with detected layout boxes\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Normalize labels\n    label_names = self._normalize_labels(custom_labels)\n\n    # Build prompt\n    prompt = self._build_detection_prompt(label_names)\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenLayoutPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenLayoutVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenLayoutMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenLayoutAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse detections\n    detections = self._parse_json_output(raw_output)\n\n    # Convert to LayoutOutput\n    layout_boxes = self._build_layout_boxes(detections, width, height)\n\n    # Sort by position (reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutMLXConfig","title":"QwenLayoutMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen layout detection.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenLayoutMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutPyTorchConfig","title":"QwenLayoutPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen layout detection.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutVLLMConfig","title":"QwenLayoutVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen layout detection.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.api","title":"api","text":"<p>API backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.api.QwenLayoutAPIConfig","title":"QwenLayoutAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen layout detection.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenLayoutAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.detector","title":"detector","text":"<p>Qwen3-VL layout detector.</p> <p>A Vision-Language Model for flexible layout detection with custom label support. Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = detector.extract(image)\n\n# With custom labels\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.detector.QwenLayoutDetector","title":"QwenLayoutDetector","text":"<pre><code>QwenLayoutDetector(backend: QwenLayoutBackendConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>Qwen3-VL Vision-Language Model layout detector.</p> <p>A flexible VLM-based layout detector that supports custom labels. Unlike fixed-label models (DocLayoutYOLO, RT-DETR), Qwen can detect any document elements specified at runtime.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\n# Initialize with PyTorch backend\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Basic extraction with default labels\nresult = detector.extract(image)\n\n# With custom labels (strings)\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n\n# With typed custom labels\nlabels = [\n        CustomLabel(name=\"code_block\", color=\"#E74C3C\"),\n        CustomLabel(name=\"sidebar\", description=\"Side panel content\"),\n    ]\nresult = detector.extract(image, custom_labels=labels)\n</code></pre> <p>Initialize Qwen layout detector.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend - QwenLayoutVLLMConfig: VLLM high-throughput backend - QwenLayoutMLXConfig: MLX backend for Apple Silicon - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenLayoutBackendConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def __init__(self, backend: QwenLayoutBackendConfig):\n    \"\"\"\n    Initialize Qwen layout detector.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenLayoutVLLMConfig: VLLM high-throughput backend\n            - QwenLayoutMLXConfig: MLX backend for Apple Silicon\n            - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.detector.QwenLayoutDetector.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    custom_labels: Optional[\n        List[Union[str, CustomLabel]]\n    ] = None,\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout detection on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>custom_labels</code> <p>Optional custom labels to detect. Can be: - None: Use default labels (title, text, table, figure, etc.) - List[str]: Simple label names [\"code_block\", \"sidebar\"] - List[CustomLabel]: Typed labels with metadata</p> <p> TYPE: <code>Optional[List[Union[str, CustomLabel]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    custom_labels: Optional[List[Union[str, CustomLabel]]] = None,\n) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout detection on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        custom_labels: Optional custom labels to detect. Can be:\n            - None: Use default labels (title, text, table, figure, etc.)\n            - List[str]: Simple label names [\"code_block\", \"sidebar\"]\n            - List[CustomLabel]: Typed labels with metadata\n\n    Returns:\n        LayoutOutput with detected layout boxes\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Normalize labels\n    label_names = self._normalize_labels(custom_labels)\n\n    # Build prompt\n    prompt = self._build_detection_prompt(label_names)\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenLayoutPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenLayoutVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenLayoutMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenLayoutAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse detections\n    detections = self._parse_json_output(raw_output)\n\n    # Convert to LayoutOutput\n    layout_boxes = self._build_layout_boxes(detections, width, height)\n\n    # Sort by position (reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.mlx","title":"mlx","text":"<p>MLX backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.mlx.QwenLayoutMLXConfig","title":"QwenLayoutMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen layout detection.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenLayoutMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.pytorch","title":"pytorch","text":"<p>PyTorch/HuggingFace backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.pytorch.QwenLayoutPyTorchConfig","title":"QwenLayoutPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen layout detection.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.vllm","title":"vllm","text":"<p>VLLM backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.qwen.vllm.QwenLayoutVLLMConfig","title":"QwenLayoutVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen layout detection.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.rtdetr","title":"rtdetr","text":"<p>RT-DETR layout extractor.</p> <p>A transformer-based real-time detection model for document layout detection. Uses HuggingFace Transformers implementation.</p> <p>Model: HuggingPanda/docling-layout</p>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.rtdetr.RTDETRConfig","title":"RTDETRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for RT-DETR layout extractor.</p> <p>This is a single-backend model (PyTorch/Transformers only).</p> Example <pre><code>config = RTDETRConfig(device=\"cuda\", confidence=0.4)\nextractor = RTDETRLayoutExtractor(config=config)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.rtdetr.RTDETRLayoutExtractor","title":"RTDETRLayoutExtractor","text":"<pre><code>RTDETRLayoutExtractor(config: RTDETRConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>RT-DETR layout extractor using HuggingFace Transformers.</p> <p>A transformer-based real-time detection model for document layout. Detects: title, text, table, figure, list, formula, captions, headers, footers.</p> <p>This is a single-backend model (PyTorch/Transformers only).</p> Example <pre><code>from omnidocs.tasks.layout_extraction import RTDETRLayoutExtractor, RTDETRConfig\n\nextractor = RTDETRLayoutExtractor(config=RTDETRConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre> <p>Initialize RT-DETR layout extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object with device, model settings, etc.</p> <p> TYPE: <code>RTDETRConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/rtdetr.py</code> <pre><code>def __init__(self, config: RTDETRConfig):\n    \"\"\"\n    Initialize RT-DETR layout extractor.\n\n    Args:\n        config: Configuration object with device, model settings, etc.\n    \"\"\"\n    self.config = config\n    self._model = None\n    self._processor = None\n    self._device = self._resolve_device(config.device)\n    self._model_path = self._resolve_model_path(config.model_path)\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/overview/#omnidocs.tasks.layout_extraction.rtdetr.RTDETRLayoutExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> Source code in <code>omnidocs/tasks/layout_extraction/rtdetr.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n    import torch\n\n    if self._model is None or self._processor is None:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    img_width, img_height = pil_image.size\n\n    # Preprocess\n    inputs = self._processor(\n        images=pil_image,\n        return_tensors=\"pt\",\n        size={\"height\": self.config.image_size, \"width\": self.config.image_size},\n    )\n\n    # Move to device\n    inputs = {k: v.to(self._device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n\n    # Run inference\n    with torch.no_grad():\n        outputs = self._model(**inputs)\n\n    # Post-process results\n    target_sizes = torch.tensor([[img_height, img_width]])\n    results = self._processor.post_process_object_detection(\n        outputs,\n        target_sizes=target_sizes,\n        threshold=self.config.confidence,\n    )[0]\n\n    # Parse detections\n    layout_boxes = []\n\n    for score, label_id, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n        confidence = float(score.item())\n        class_id = int(label_id.item())\n\n        # Get original label from model config\n        # Note: The model outputs 0-indexed class IDs, but id2label has background at index 0,\n        # so we add 1 to map correctly (e.g., model output 8 -&gt; id2label[9] = \"Table\")\n        original_label = self._model.config.id2label.get(class_id + 1, f\"class_{class_id}\")\n\n        # Map to standardized label\n        standard_label = RTDETR_MAPPING.to_standard(original_label)\n\n        # Box coordinates\n        box_coords = box.cpu().tolist()\n\n        layout_boxes.append(\n            LayoutBox(\n                label=standard_label,\n                bbox=BoundingBox.from_list(box_coords),\n                confidence=confidence,\n                class_id=class_id,\n                original_label=original_label,\n            )\n        )\n\n    # Sort by y-coordinate (top to bottom reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=img_width,\n        image_height=img_height,\n        model_name=\"RT-DETR (docling-layout)\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/rtdetr/","title":"Rtdetr","text":"<p>RT-DETR layout extractor.</p> <p>A transformer-based real-time detection model for document layout detection. Uses HuggingFace Transformers implementation.</p> <p>Model: HuggingPanda/docling-layout</p>"},{"location":"reference/tasks/layout_extraction/rtdetr/#omnidocs.tasks.layout_extraction.rtdetr.RTDETRConfig","title":"RTDETRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for RT-DETR layout extractor.</p> <p>This is a single-backend model (PyTorch/Transformers only).</p> Example <pre><code>config = RTDETRConfig(device=\"cuda\", confidence=0.4)\nextractor = RTDETRLayoutExtractor(config=config)\n</code></pre>"},{"location":"reference/tasks/layout_extraction/rtdetr/#omnidocs.tasks.layout_extraction.rtdetr.RTDETRLayoutExtractor","title":"RTDETRLayoutExtractor","text":"<pre><code>RTDETRLayoutExtractor(config: RTDETRConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>RT-DETR layout extractor using HuggingFace Transformers.</p> <p>A transformer-based real-time detection model for document layout. Detects: title, text, table, figure, list, formula, captions, headers, footers.</p> <p>This is a single-backend model (PyTorch/Transformers only).</p> Example <pre><code>from omnidocs.tasks.layout_extraction import RTDETRLayoutExtractor, RTDETRConfig\n\nextractor = RTDETRLayoutExtractor(config=RTDETRConfig(device=\"cuda\"))\nresult = extractor.extract(image)\n\nfor box in result.bboxes:\n        print(f\"{box.label.value}: {box.confidence:.2f}\")\n</code></pre> <p>Initialize RT-DETR layout extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object with device, model settings, etc.</p> <p> TYPE: <code>RTDETRConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/rtdetr.py</code> <pre><code>def __init__(self, config: RTDETRConfig):\n    \"\"\"\n    Initialize RT-DETR layout extractor.\n\n    Args:\n        config: Configuration object with device, model settings, etc.\n    \"\"\"\n    self.config = config\n    self._model = None\n    self._processor = None\n    self._device = self._resolve_device(config.device)\n    self._model_path = self._resolve_model_path(config.model_path)\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/rtdetr/#omnidocs.tasks.layout_extraction.rtdetr.RTDETRLayoutExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> Source code in <code>omnidocs/tasks/layout_extraction/rtdetr.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout extraction on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        LayoutOutput with detected layout boxes\n    \"\"\"\n    import torch\n\n    if self._model is None or self._processor is None:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    img_width, img_height = pil_image.size\n\n    # Preprocess\n    inputs = self._processor(\n        images=pil_image,\n        return_tensors=\"pt\",\n        size={\"height\": self.config.image_size, \"width\": self.config.image_size},\n    )\n\n    # Move to device\n    inputs = {k: v.to(self._device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n\n    # Run inference\n    with torch.no_grad():\n        outputs = self._model(**inputs)\n\n    # Post-process results\n    target_sizes = torch.tensor([[img_height, img_width]])\n    results = self._processor.post_process_object_detection(\n        outputs,\n        target_sizes=target_sizes,\n        threshold=self.config.confidence,\n    )[0]\n\n    # Parse detections\n    layout_boxes = []\n\n    for score, label_id, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n        confidence = float(score.item())\n        class_id = int(label_id.item())\n\n        # Get original label from model config\n        # Note: The model outputs 0-indexed class IDs, but id2label has background at index 0,\n        # so we add 1 to map correctly (e.g., model output 8 -&gt; id2label[9] = \"Table\")\n        original_label = self._model.config.id2label.get(class_id + 1, f\"class_{class_id}\")\n\n        # Map to standardized label\n        standard_label = RTDETR_MAPPING.to_standard(original_label)\n\n        # Box coordinates\n        box_coords = box.cpu().tolist()\n\n        layout_boxes.append(\n            LayoutBox(\n                label=standard_label,\n                bbox=BoundingBox.from_list(box_coords),\n                confidence=confidence,\n                class_id=class_id,\n                original_label=original_label,\n            )\n        )\n\n    # Sort by y-coordinate (top to bottom reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=img_width,\n        image_height=img_height,\n        model_name=\"RT-DETR (docling-layout)\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/api/","title":"API","text":"<p>API backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/qwen/api/#omnidocs.tasks.layout_extraction.qwen.api.QwenLayoutAPIConfig","title":"QwenLayoutAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen layout detection.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenLayoutAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/detector/","title":"Detector","text":"<p>Qwen3-VL layout detector.</p> <p>A Vision-Language Model for flexible layout detection with custom label support. Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = detector.extract(image)\n\n# With custom labels\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/detector/#omnidocs.tasks.layout_extraction.qwen.detector.QwenLayoutDetector","title":"QwenLayoutDetector","text":"<pre><code>QwenLayoutDetector(backend: QwenLayoutBackendConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>Qwen3-VL Vision-Language Model layout detector.</p> <p>A flexible VLM-based layout detector that supports custom labels. Unlike fixed-label models (DocLayoutYOLO, RT-DETR), Qwen can detect any document elements specified at runtime.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\n# Initialize with PyTorch backend\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Basic extraction with default labels\nresult = detector.extract(image)\n\n# With custom labels (strings)\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n\n# With typed custom labels\nlabels = [\n        CustomLabel(name=\"code_block\", color=\"#E74C3C\"),\n        CustomLabel(name=\"sidebar\", description=\"Side panel content\"),\n    ]\nresult = detector.extract(image, custom_labels=labels)\n</code></pre> <p>Initialize Qwen layout detector.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend - QwenLayoutVLLMConfig: VLLM high-throughput backend - QwenLayoutMLXConfig: MLX backend for Apple Silicon - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenLayoutBackendConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def __init__(self, backend: QwenLayoutBackendConfig):\n    \"\"\"\n    Initialize Qwen layout detector.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenLayoutVLLMConfig: VLLM high-throughput backend\n            - QwenLayoutMLXConfig: MLX backend for Apple Silicon\n            - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/detector/#omnidocs.tasks.layout_extraction.qwen.detector.QwenLayoutDetector.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    custom_labels: Optional[\n        List[Union[str, CustomLabel]]\n    ] = None,\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout detection on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>custom_labels</code> <p>Optional custom labels to detect. Can be: - None: Use default labels (title, text, table, figure, etc.) - List[str]: Simple label names [\"code_block\", \"sidebar\"] - List[CustomLabel]: Typed labels with metadata</p> <p> TYPE: <code>Optional[List[Union[str, CustomLabel]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    custom_labels: Optional[List[Union[str, CustomLabel]]] = None,\n) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout detection on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        custom_labels: Optional custom labels to detect. Can be:\n            - None: Use default labels (title, text, table, figure, etc.)\n            - List[str]: Simple label names [\"code_block\", \"sidebar\"]\n            - List[CustomLabel]: Typed labels with metadata\n\n    Returns:\n        LayoutOutput with detected layout boxes\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Normalize labels\n    label_names = self._normalize_labels(custom_labels)\n\n    # Build prompt\n    prompt = self._build_detection_prompt(label_names)\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenLayoutPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenLayoutVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenLayoutMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenLayoutAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse detections\n    detections = self._parse_json_output(raw_output)\n\n    # Convert to LayoutOutput\n    layout_boxes = self._build_layout_boxes(detections, width, height)\n\n    # Sort by position (reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/mlx/","title":"MLX","text":"<p>MLX backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/qwen/mlx/#omnidocs.tasks.layout_extraction.qwen.mlx.QwenLayoutMLXConfig","title":"QwenLayoutMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen layout detection.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenLayoutMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/","title":"Overview","text":"<p>Qwen3-VL backend configurations and detector for layout detection.</p> Available backends <ul> <li>QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend</li> <li>QwenLayoutVLLMConfig: VLLM high-throughput backend</li> <li>QwenLayoutMLXConfig: MLX backend for Apple Silicon</li> <li>QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</li> </ul> Example <pre><code>from omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\nconfig = QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutAPIConfig","title":"QwenLayoutAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen layout detection.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenLayoutAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutDetector","title":"QwenLayoutDetector","text":"<pre><code>QwenLayoutDetector(backend: QwenLayoutBackendConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>Qwen3-VL Vision-Language Model layout detector.</p> <p>A flexible VLM-based layout detector that supports custom labels. Unlike fixed-label models (DocLayoutYOLO, RT-DETR), Qwen can detect any document elements specified at runtime.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\n# Initialize with PyTorch backend\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Basic extraction with default labels\nresult = detector.extract(image)\n\n# With custom labels (strings)\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n\n# With typed custom labels\nlabels = [\n        CustomLabel(name=\"code_block\", color=\"#E74C3C\"),\n        CustomLabel(name=\"sidebar\", description=\"Side panel content\"),\n    ]\nresult = detector.extract(image, custom_labels=labels)\n</code></pre> <p>Initialize Qwen layout detector.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend - QwenLayoutVLLMConfig: VLLM high-throughput backend - QwenLayoutMLXConfig: MLX backend for Apple Silicon - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenLayoutBackendConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def __init__(self, backend: QwenLayoutBackendConfig):\n    \"\"\"\n    Initialize Qwen layout detector.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenLayoutVLLMConfig: VLLM high-throughput backend\n            - QwenLayoutMLXConfig: MLX backend for Apple Silicon\n            - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutDetector.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    custom_labels: Optional[\n        List[Union[str, CustomLabel]]\n    ] = None,\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout detection on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>custom_labels</code> <p>Optional custom labels to detect. Can be: - None: Use default labels (title, text, table, figure, etc.) - List[str]: Simple label names [\"code_block\", \"sidebar\"] - List[CustomLabel]: Typed labels with metadata</p> <p> TYPE: <code>Optional[List[Union[str, CustomLabel]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    custom_labels: Optional[List[Union[str, CustomLabel]]] = None,\n) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout detection on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        custom_labels: Optional custom labels to detect. Can be:\n            - None: Use default labels (title, text, table, figure, etc.)\n            - List[str]: Simple label names [\"code_block\", \"sidebar\"]\n            - List[CustomLabel]: Typed labels with metadata\n\n    Returns:\n        LayoutOutput with detected layout boxes\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Normalize labels\n    label_names = self._normalize_labels(custom_labels)\n\n    # Build prompt\n    prompt = self._build_detection_prompt(label_names)\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenLayoutPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenLayoutVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenLayoutMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenLayoutAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse detections\n    detections = self._parse_json_output(raw_output)\n\n    # Convert to LayoutOutput\n    layout_boxes = self._build_layout_boxes(detections, width, height)\n\n    # Sort by position (reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutMLXConfig","title":"QwenLayoutMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen layout detection.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenLayoutMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutPyTorchConfig","title":"QwenLayoutPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen layout detection.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.QwenLayoutVLLMConfig","title":"QwenLayoutVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen layout detection.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.api","title":"api","text":"<p>API backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.api.QwenLayoutAPIConfig","title":"QwenLayoutAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen layout detection.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenLayoutAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.detector","title":"detector","text":"<p>Qwen3-VL layout detector.</p> <p>A Vision-Language Model for flexible layout detection with custom label support. Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = detector.extract(image)\n\n# With custom labels\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.detector.QwenLayoutDetector","title":"QwenLayoutDetector","text":"<pre><code>QwenLayoutDetector(backend: QwenLayoutBackendConfig)\n</code></pre> <p>               Bases: <code>BaseLayoutExtractor</code></p> <p>Qwen3-VL Vision-Language Model layout detector.</p> <p>A flexible VLM-based layout detector that supports custom labels. Unlike fixed-label models (DocLayoutYOLO, RT-DETR), Qwen can detect any document elements specified at runtime.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.layout_extraction import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_extraction.qwen import QwenLayoutPyTorchConfig\n\n# Initialize with PyTorch backend\ndetector = QwenLayoutDetector(\n        backend=QwenLayoutPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Basic extraction with default labels\nresult = detector.extract(image)\n\n# With custom labels (strings)\nresult = detector.extract(image, custom_labels=[\"code_block\", \"sidebar\"])\n\n# With typed custom labels\nlabels = [\n        CustomLabel(name=\"code_block\", color=\"#E74C3C\"),\n        CustomLabel(name=\"sidebar\", description=\"Side panel content\"),\n    ]\nresult = detector.extract(image, custom_labels=labels)\n</code></pre> <p>Initialize Qwen layout detector.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend - QwenLayoutVLLMConfig: VLLM high-throughput backend - QwenLayoutMLXConfig: MLX backend for Apple Silicon - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenLayoutBackendConfig</code> </p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def __init__(self, backend: QwenLayoutBackendConfig):\n    \"\"\"\n    Initialize Qwen layout detector.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenLayoutPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenLayoutVLLMConfig: VLLM high-throughput backend\n            - QwenLayoutMLXConfig: MLX backend for Apple Silicon\n            - QwenLayoutAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.detector.QwenLayoutDetector.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    custom_labels: Optional[\n        List[Union[str, CustomLabel]]\n    ] = None,\n) -&gt; LayoutOutput\n</code></pre> <p>Run layout detection on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>custom_labels</code> <p>Optional custom labels to detect. Can be: - None: Use default labels (title, text, table, figure, etc.) - List[str]: Simple label names [\"code_block\", \"sidebar\"] - List[CustomLabel]: Typed labels with metadata</p> <p> TYPE: <code>Optional[List[Union[str, CustomLabel]]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>LayoutOutput</code> <p>LayoutOutput with detected layout boxes</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/layout_extraction/qwen/detector.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    custom_labels: Optional[List[Union[str, CustomLabel]]] = None,\n) -&gt; LayoutOutput:\n    \"\"\"\n    Run layout detection on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        custom_labels: Optional custom labels to detect. Can be:\n            - None: Use default labels (title, text, table, figure, etc.)\n            - List[str]: Simple label names [\"code_block\", \"sidebar\"]\n            - List[CustomLabel]: Typed labels with metadata\n\n    Returns:\n        LayoutOutput with detected layout boxes\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Normalize labels\n    label_names = self._normalize_labels(custom_labels)\n\n    # Build prompt\n    prompt = self._build_detection_prompt(label_names)\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenLayoutPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenLayoutVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenLayoutMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenLayoutAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse detections\n    detections = self._parse_json_output(raw_output)\n\n    # Convert to LayoutOutput\n    layout_boxes = self._build_layout_boxes(detections, width, height)\n\n    # Sort by position (reading order)\n    layout_boxes.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return LayoutOutput(\n        bboxes=layout_boxes,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.mlx","title":"mlx","text":"<p>MLX backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.mlx.QwenLayoutMLXConfig","title":"QwenLayoutMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen layout detection.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenLayoutMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.pytorch","title":"pytorch","text":"<p>PyTorch/HuggingFace backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.pytorch.QwenLayoutPyTorchConfig","title":"QwenLayoutPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen layout detection.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.vllm","title":"vllm","text":"<p>VLLM backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/qwen/overview/#omnidocs.tasks.layout_extraction.qwen.vllm.QwenLayoutVLLMConfig","title":"QwenLayoutVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen layout detection.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/pytorch/","title":"PyTorch","text":"<p>PyTorch/HuggingFace backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/qwen/pytorch/#omnidocs.tasks.layout_extraction.qwen.pytorch.QwenLayoutPyTorchConfig","title":"QwenLayoutPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen layout detection.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/layout_extraction/qwen/vllm/","title":"VLLM","text":"<p>VLLM backend configuration for Qwen3-VL layout detection.</p>"},{"location":"reference/tasks/layout_extraction/qwen/vllm/#omnidocs.tasks.layout_extraction.qwen.vllm.QwenLayoutVLLMConfig","title":"QwenLayoutVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen layout detection.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenLayoutVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/base/","title":"Base","text":"<p>Base class for OCR extractors.</p> <p>Defines the abstract interface that all OCR extractors must implement.</p>"},{"location":"reference/tasks/ocr_extraction/base/#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor","title":"BaseOCRExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for OCR extractors.</p> <p>All OCR extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyOCRExtractor(BaseOCRExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Initialize OCR engine\n            pass\n\n        def extract(self, image):\n            # Run OCR extraction\n            return OCROutput(...)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/base/#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput containing detected text blocks with bounding boxes</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format is not supported</p> <code>RuntimeError</code> <p>If OCR engine is not initialized or extraction fails</p> Source code in <code>omnidocs/tasks/ocr_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR extraction on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n\n    Returns:\n        OCROutput containing detected text blocks with bounding boxes\n\n    Raises:\n        ValueError: If image format is not supported\n        RuntimeError: If OCR engine is not initialized or extraction fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/base/#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.batch_extract","title":"batch_extract","text":"<pre><code>batch_extract(\n    images: List[Union[Image, ndarray, str, Path]],\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[OCROutput]\n</code></pre> <p>Run OCR extraction on multiple images.</p> <p>Default implementation loops over extract(). Subclasses can override for optimized batching.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of images in any supported format</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[OCROutput]</code> <p>List of OCROutput in same order as input</p> <p>Examples:</p> <pre><code>images = [doc.get_page(i) for i in range(doc.page_count)]\nresults = extractor.batch_extract(images)\n</code></pre> Source code in <code>omnidocs/tasks/ocr_extraction/base.py</code> <pre><code>def batch_extract(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[OCROutput]:\n    \"\"\"\n    Run OCR extraction on multiple images.\n\n    Default implementation loops over extract(). Subclasses can override\n    for optimized batching.\n\n    Args:\n        images: List of images in any supported format\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of OCROutput in same order as input\n\n    Examples:\n        ```python\n        images = [doc.get_page(i) for i in range(doc.page_count)]\n        results = extractor.batch_extract(images)\n        ```\n    \"\"\"\n    results = []\n    total = len(images)\n\n    for i, image in enumerate(images):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(image)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/base/#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.extract_document","title":"extract_document","text":"<pre><code>extract_document(\n    document: Document,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[OCROutput]\n</code></pre> <p>Run OCR extraction on all pages of a document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[OCROutput]</code> <p>List of OCROutput, one per page</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresults = extractor.extract_document(doc)\n</code></pre> Source code in <code>omnidocs/tasks/ocr_extraction/base.py</code> <pre><code>def extract_document(\n    self,\n    document: \"Document\",\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[OCROutput]:\n    \"\"\"\n    Run OCR extraction on all pages of a document.\n\n    Args:\n        document: Document instance\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of OCROutput, one per page\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        results = extractor.extract_document(doc)\n        ```\n    \"\"\"\n    results = []\n    total = document.page_count\n\n    for i, page in enumerate(document.iter_pages()):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(page)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/easyocr/","title":"EasyOCR","text":"<p>EasyOCR extractor.</p> <p>EasyOCR is a PyTorch-based OCR engine with excellent multi-language support. - GPU accelerated (optional) - Supports 80+ languages - Good for scene text and printed documents</p> Python Package <p>pip install easyocr</p> Model Download Location <p>By default, EasyOCR downloads models to ~/.EasyOCR/ Can be overridden with model_storage_directory parameter</p>"},{"location":"reference/tasks/ocr_extraction/easyocr/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCRConfig","title":"EasyOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for EasyOCR extractor.</p> <p>This is a single-backend model (PyTorch - CPU/GPU).</p> Example <pre><code>config = EasyOCRConfig(languages=[\"en\", \"ch_sim\"], gpu=True)\nocr = EasyOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/easyocr/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCR","title":"EasyOCR","text":"<pre><code>EasyOCR(config: EasyOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>EasyOCR text extractor.</p> <p>Single-backend model (PyTorch - CPU/GPU).</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\nocr = EasyOCR(config=EasyOCRConfig(languages=[\"en\"], gpu=True))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize EasyOCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>EasyOCRConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If easyocr is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def __init__(self, config: EasyOCRConfig):\n    \"\"\"\n    Initialize EasyOCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        ImportError: If easyocr is not installed\n    \"\"\"\n    self.config = config\n    self._reader = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/easyocr/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    detail: int = 1,\n    paragraph: bool = False,\n    min_size: int = 10,\n    text_threshold: float = 0.7,\n    low_text: float = 0.4,\n    link_threshold: float = 0.4,\n    canvas_size: int = 2560,\n    mag_ratio: float = 1.0,\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>detail</code> <p>0 = simple output, 1 = detailed with boxes</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>paragraph</code> <p>Combine results into paragraphs</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>min_size</code> <p>Minimum text box size</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>text_threshold</code> <p>Text confidence threshold</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.7</code> </p> <code>low_text</code> <p>Low text bound</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>link_threshold</code> <p>Link threshold for text joining</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>canvas_size</code> <p>Max image dimension for processing</p> <p> TYPE: <code>int</code> DEFAULT: <code>2560</code> </p> <code>mag_ratio</code> <p>Magnification ratio</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks</p> RAISES DESCRIPTION <code>ValueError</code> <p>If detail is not 0 or 1</p> <code>RuntimeError</code> <p>If EasyOCR is not initialized</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    detail: int = 1,\n    paragraph: bool = False,\n    min_size: int = 10,\n    text_threshold: float = 0.7,\n    low_text: float = 0.4,\n    link_threshold: float = 0.4,\n    canvas_size: int = 2560,\n    mag_ratio: float = 1.0,\n) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n        detail: 0 = simple output, 1 = detailed with boxes\n        paragraph: Combine results into paragraphs\n        min_size: Minimum text box size\n        text_threshold: Text confidence threshold\n        low_text: Low text bound\n        link_threshold: Link threshold for text joining\n        canvas_size: Max image dimension for processing\n        mag_ratio: Magnification ratio\n\n    Returns:\n        OCROutput with detected text blocks\n\n    Raises:\n        ValueError: If detail is not 0 or 1\n        RuntimeError: If EasyOCR is not initialized\n    \"\"\"\n    if self._reader is None:\n        raise RuntimeError(\"EasyOCR not initialized. Call _load_model() first.\")\n\n    # Validate detail parameter\n    if detail not in (0, 1):\n        raise ValueError(f\"detail must be 0 or 1, got {detail}\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Convert to numpy array for EasyOCR\n    image_array = np.array(pil_image)\n\n    # Run EasyOCR\n    results = self._reader.readtext(\n        image_array,\n        detail=detail,\n        paragraph=paragraph,\n        min_size=min_size,\n        text_threshold=text_threshold,\n        low_text=low_text,\n        link_threshold=link_threshold,\n        canvas_size=canvas_size,\n        mag_ratio=mag_ratio,\n    )\n\n    # Parse results\n    text_blocks = []\n    full_text_parts = []\n\n    for result in results:\n        if detail == 0:\n            # Simple output: just text\n            text = result\n            confidence = 1.0\n            bbox = BoundingBox(x1=0, y1=0, x2=0, y2=0)\n            polygon = None\n        else:\n            # Detailed output: [polygon, text, confidence]\n            polygon_points, text, confidence = result\n\n            # EasyOCR returns 4 corner points: [[x1,y1], [x2,y1], [x2,y2], [x1,y2]]\n            # Convert to list of lists for storage\n            polygon = [list(p) for p in polygon_points]\n\n            # Convert to axis-aligned bounding box\n            bbox = BoundingBox.from_polygon(polygon)\n\n        if not text.strip():\n            continue\n\n        text_blocks.append(\n            TextBlock(\n                text=text,\n                bbox=bbox,\n                confidence=float(confidence),\n                granularity=(OCRGranularity.LINE if paragraph else OCRGranularity.WORD),\n                polygon=polygon,\n                language=\"+\".join(self.config.languages),\n            )\n        )\n\n        full_text_parts.append(text)\n\n    # Sort by position\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\" \".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/easyocr/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCR.extract_batch","title":"extract_batch","text":"<pre><code>extract_batch(\n    images: List[Union[Image, ndarray, str, Path]], **kwargs\n) -&gt; List[OCROutput]\n</code></pre> <p>Run OCR on multiple images.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of input images</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>**kwargs</code> <p>Arguments passed to extract()</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>List[OCROutput]</code> <p>List of OCROutput objects</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def extract_batch(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    **kwargs,\n) -&gt; List[OCROutput]:\n    \"\"\"\n    Run OCR on multiple images.\n\n    Args:\n        images: List of input images\n        **kwargs: Arguments passed to extract()\n\n    Returns:\n        List of OCROutput objects\n    \"\"\"\n    results = []\n    for img in images:\n        results.append(self.extract(img, **kwargs))\n    return results\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/","title":"Models","text":"<p>Pydantic models for OCR extraction outputs.</p> <p>Defines standardized output types for OCR detection including text blocks with bounding boxes, confidence scores, and granularity levels.</p> <p>Key difference from Text Extraction: - OCR returns text WITH bounding boxes (word/line/character level) - Text Extraction returns formatted text (MD/HTML) WITHOUT bboxes</p> Coordinate Systems <ul> <li>Absolute (default): Coordinates in pixels relative to original image size</li> <li>Normalized (0-1024): Coordinates scaled to 0-1024 range (virtual 1024x1024 canvas)</li> </ul> <p>Use <code>bbox.to_normalized(width, height)</code> or <code>output.get_normalized_blocks()</code> to convert to normalized coordinates.</p> Example <pre><code>result = ocr.extract(image)  # Returns absolute pixel coordinates\nnormalized = result.get_normalized_blocks()  # Returns 0-1024 normalized coords\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCRGranularity","title":"OCRGranularity","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>OCR detection granularity levels.</p> <p>Different OCR engines return results at different granularity levels. This enum standardizes the options across all extractors.</p>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box coordinates in pixel space.</p> <p>Coordinates follow the convention: (x1, y1) is top-left, (x2, y2) is bottom-right. For rotated text, use the polygon field in TextBlock instead.</p> Example <pre><code>bbox = BoundingBox(x1=100, y1=50, x2=300, y2=80)\nprint(bbox.width, bbox.height)  # 200, 30\nprint(bbox.center)  # (200.0, 65.0)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x, y, width, height) format.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_xywh(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x, y, width, height) format.\"\"\"\n    return (self.x1, self.y1, self.width, self.height)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.from_polygon","title":"from_polygon  <code>classmethod</code>","text":"<pre><code>from_polygon(polygon: List[List[float]]) -&gt; BoundingBox\n</code></pre> <p>Create axis-aligned bounding box from polygon points.</p> PARAMETER DESCRIPTION <code>polygon</code> <p>List of [x, y] points (usually 4 for quadrilateral)</p> <p> TYPE: <code>List[List[float]]</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox that encloses all polygon points</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef from_polygon(cls, polygon: List[List[float]]) -&gt; \"BoundingBox\":\n    \"\"\"\n    Create axis-aligned bounding box from polygon points.\n\n    Args:\n        polygon: List of [x, y] points (usually 4 for quadrilateral)\n\n    Returns:\n        BoundingBox that encloses all polygon points\n    \"\"\"\n    if not polygon:\n        raise ValueError(\"Polygon cannot be empty\")\n\n    xs = [p[0] for p in polygon]\n    ys = [p[1] for p in polygon]\n    return cls(x1=min(xs), y1=min(ys), x2=max(xs), y2=max(ys))\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> <p>Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas. This provides consistent coordinates regardless of original image size.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas.\n    This provides consistent coordinates regardless of original image size.\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_absolute","title":"to_absolute","text":"<pre><code>to_absolute(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert from normalized (0-1024) to absolute pixel coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Target image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Target image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with absolute pixel coordinates</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_absolute(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert from normalized (0-1024) to absolute pixel coordinates.\n\n    Args:\n        image_width: Target image width in pixels\n        image_height: Target image height in pixels\n\n    Returns:\n        New BoundingBox with absolute pixel coordinates\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / NORMALIZED_SIZE * image_width,\n        y1=self.y1 / NORMALIZED_SIZE * image_height,\n        x2=self.x2 / NORMALIZED_SIZE * image_width,\n        y2=self.y2 / NORMALIZED_SIZE * image_height,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.TextBlock","title":"TextBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single detected text element with text, bounding box, and confidence.</p> <p>This is the fundamental unit of OCR output - can represent a character, word, line, or block depending on the OCR model and configuration.</p> Example <pre><code>block = TextBlock(\n        text=\"Hello\",\n        bbox=BoundingBox(x1=100, y1=50, x2=200, y2=80),\n        confidence=0.95,\n        granularity=OCRGranularity.WORD,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.TextBlock.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"text\": self.text,\n        \"bbox\": self.bbox.to_list(),\n        \"confidence\": self.confidence,\n        \"granularity\": self.granularity.value,\n        \"polygon\": self.polygon,\n        \"language\": self.language,\n    }\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.TextBlock.get_normalized_bbox","title":"get_normalized_bbox","text":"<pre><code>get_normalized_bbox(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Get bounding box in normalized (0-1024) coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox with normalized coordinates</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def get_normalized_bbox(self, image_width: int, image_height: int) -&gt; BoundingBox:\n    \"\"\"\n    Get bounding box in normalized (0-1024) coordinates.\n\n    Args:\n        image_width: Original image width\n        image_height: Original image height\n\n    Returns:\n        BoundingBox with normalized coordinates\n    \"\"\"\n    return self.bbox.to_normalized(image_width, image_height)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput","title":"OCROutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete OCR extraction results for a single image.</p> <p>Contains all detected text blocks with their bounding boxes, plus metadata about the extraction.</p> Example <pre><code>result = ocr.extract(image)\nprint(f\"Found {result.block_count} blocks\")\nprint(f\"Full text: {result.full_text}\")\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.block_count","title":"block_count  <code>property</code>","text":"<pre><code>block_count: int\n</code></pre> <p>Number of detected text blocks.</p>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.word_count","title":"word_count  <code>property</code>","text":"<pre><code>word_count: int\n</code></pre> <p>Approximate word count from full text.</p>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.average_confidence","title":"average_confidence  <code>property</code>","text":"<pre><code>average_confidence: float\n</code></pre> <p>Average confidence across all text blocks.</p>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(\n    min_confidence: float,\n) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by minimum confidence.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def filter_by_confidence(self, min_confidence: float) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by minimum confidence.\"\"\"\n    return [b for b in self.text_blocks if b.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.filter_by_granularity","title":"filter_by_granularity","text":"<pre><code>filter_by_granularity(\n    granularity: OCRGranularity,\n) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by granularity level.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def filter_by_granularity(self, granularity: OCRGranularity) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by granularity level.\"\"\"\n    return [b for b in self.text_blocks if b.granularity == granularity]\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"text_blocks\": [b.to_dict() for b in self.text_blocks],\n        \"full_text\": self.full_text,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"languages_detected\": self.languages_detected,\n        \"block_count\": self.block_count,\n        \"word_count\": self.word_count,\n        \"average_confidence\": self.average_confidence,\n    }\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.sort_by_position","title":"sort_by_position","text":"<pre><code>sort_by_position(top_to_bottom: bool = True) -&gt; OCROutput\n</code></pre> <p>Return a new OCROutput with blocks sorted by position.</p> PARAMETER DESCRIPTION <code>top_to_bottom</code> <p>If True, sort by y-coordinate (reading order)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>New OCROutput with sorted text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def sort_by_position(self, top_to_bottom: bool = True) -&gt; \"OCROutput\":\n    \"\"\"\n    Return a new OCROutput with blocks sorted by position.\n\n    Args:\n        top_to_bottom: If True, sort by y-coordinate (reading order)\n\n    Returns:\n        New OCROutput with sorted text blocks\n    \"\"\"\n    sorted_blocks = sorted(\n        self.text_blocks,\n        key=lambda b: (b.bbox.y1, b.bbox.x1),\n        reverse=not top_to_bottom,\n    )\n    # Regenerate full_text in sorted order\n    full_text = \" \".join(b.text for b in sorted_blocks)\n\n    return OCROutput(\n        text_blocks=sorted_blocks,\n        full_text=full_text,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        model_name=self.model_name,\n        languages_detected=self.languages_detected,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.get_normalized_blocks","title":"get_normalized_blocks","text":"<pre><code>get_normalized_blocks() -&gt; List[Dict]\n</code></pre> <p>Get all text blocks with normalized (0-1024) coordinates.</p> RETURNS DESCRIPTION <code>List[Dict]</code> <p>List of dicts with normalized bbox coordinates and metadata.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def get_normalized_blocks(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all text blocks with normalized (0-1024) coordinates.\n\n    Returns:\n        List of dicts with normalized bbox coordinates and metadata.\n    \"\"\"\n    normalized = []\n    for block in self.text_blocks:\n        norm_bbox = block.bbox.to_normalized(self.image_width, self.image_height)\n        normalized.append(\n            {\n                \"text\": block.text,\n                \"bbox\": norm_bbox.to_list(),\n                \"confidence\": block.confidence,\n                \"granularity\": block.granularity.value,\n                \"language\": block.language,\n            }\n        )\n    return normalized\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.visualize","title":"visualize","text":"<pre><code>visualize(\n    image: Image,\n    output_path: Optional[Union[str, Path]] = None,\n    show_text: bool = True,\n    show_confidence: bool = False,\n    line_width: int = 2,\n    box_color: str = \"#2ECC71\",\n    text_color: str = \"#000000\",\n) -&gt; Image.Image\n</code></pre> <p>Visualize OCR results on the image.</p> <p>Draws bounding boxes around detected text with optional labels.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to draw on (will be copied, not modified)</p> <p> TYPE: <code>Image</code> </p> <code>output_path</code> <p>Optional path to save the visualization</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> <code>show_text</code> <p>Whether to show detected text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_confidence</code> <p>Whether to show confidence scores</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>line_width</code> <p>Width of bounding box lines</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>box_color</code> <p>Color for bounding boxes (hex)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'#2ECC71'</code> </p> <code>text_color</code> <p>Color for text labels (hex)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'#000000'</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image with visualizations drawn</p> Example <pre><code>result = ocr.extract(image)\nviz = result.visualize(image, output_path=\"ocr_viz.png\")\n</code></pre> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def visualize(\n    self,\n    image: \"Image.Image\",\n    output_path: Optional[Union[str, Path]] = None,\n    show_text: bool = True,\n    show_confidence: bool = False,\n    line_width: int = 2,\n    box_color: str = \"#2ECC71\",\n    text_color: str = \"#000000\",\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Visualize OCR results on the image.\n\n    Draws bounding boxes around detected text with optional labels.\n\n    Args:\n        image: PIL Image to draw on (will be copied, not modified)\n        output_path: Optional path to save the visualization\n        show_text: Whether to show detected text\n        show_confidence: Whether to show confidence scores\n        line_width: Width of bounding box lines\n        box_color: Color for bounding boxes (hex)\n        text_color: Color for text labels (hex)\n\n    Returns:\n        PIL Image with visualizations drawn\n\n    Example:\n        ```python\n        result = ocr.extract(image)\n        viz = result.visualize(image, output_path=\"ocr_viz.png\")\n        ```\n    \"\"\"\n    from PIL import ImageDraw, ImageFont\n\n    # Copy image to avoid modifying original\n    viz_image = image.copy().convert(\"RGB\")\n    draw = ImageDraw.Draw(viz_image)\n\n    # Try to get a font\n    try:\n        font = ImageFont.truetype(\"/System/Library/Fonts/Helvetica.ttc\", 12)\n    except Exception:\n        try:\n            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 12)\n        except Exception:\n            font = ImageFont.load_default()\n\n    for block in self.text_blocks:\n        coords = block.bbox.to_xyxy()\n\n        # Draw polygon if available, otherwise draw rectangle\n        if block.polygon:\n            flat_polygon = [coord for point in block.polygon for coord in point]\n            draw.polygon(flat_polygon, outline=box_color, width=line_width)\n        else:\n            draw.rectangle(coords, outline=box_color, width=line_width)\n\n        # Build label text\n        if show_text or show_confidence:\n            label_parts = []\n            if show_text:\n                # Truncate long text\n                text = block.text[:25] + \"...\" if len(block.text) &gt; 25 else block.text\n                label_parts.append(text)\n            if show_confidence:\n                label_parts.append(f\"{block.confidence:.2f}\")\n            label_text = \" | \".join(label_parts)\n\n            # Position label below the box\n            label_x = coords[0]\n            label_y = coords[3] + 2  # Below bottom edge\n\n            # Draw label with background\n            text_bbox = draw.textbbox((label_x, label_y), label_text, font=font)\n            padding = 2\n            draw.rectangle(\n                [\n                    text_bbox[0] - padding,\n                    text_bbox[1] - padding,\n                    text_bbox[2] + padding,\n                    text_bbox[3] + padding,\n                ],\n                fill=\"#FFFFFF\",\n                outline=box_color,\n            )\n            draw.text((label_x, label_y), label_text, fill=text_color, font=font)\n\n    # Save if path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        viz_image.save(output_path)\n\n    return viz_image\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; OCROutput\n</code></pre> <p>Load an OCROutput instance from a JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to JSON file</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput instance</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"OCROutput\":\n    \"\"\"\n    Load an OCROutput instance from a JSON file.\n\n    Args:\n        file_path: Path to JSON file\n\n    Returns:\n        OCROutput instance\n    \"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/models/#omnidocs.tasks.ocr_extraction.models.OCROutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save OCROutput instance to a JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path where JSON file should be saved</p> <p> TYPE: <code>Union[str, Path]</code> </p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"\n    Save OCROutput instance to a JSON file.\n\n    Args:\n        file_path: Path where JSON file should be saved\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/","title":"Overview","text":"<p>OCR Extraction Module.</p> <p>Provides extractors for detecting text with bounding boxes from document images. Returns text content along with spatial coordinates (unlike Text Extraction which returns formatted Markdown/HTML without coordinates).</p> Available Extractors <ul> <li>TesseractOCR: Open-source OCR (CPU, requires system Tesseract)</li> <li>EasyOCR: PyTorch-based OCR (CPU/GPU, 80+ languages)</li> <li>PaddleOCR: PaddlePaddle-based OCR (CPU/GPU, excellent CJK support)</li> </ul> Key Difference from Text Extraction <ul> <li>OCR Extraction: Text + Bounding Boxes (spatial location)</li> <li>Text Extraction: Markdown/HTML (formatted document export)</li> </ul> Example <pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\n\nocr = TesseractOCR(config=TesseractOCRConfig(languages=[\"eng\"]))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()} (conf: {block.confidence:.2f})\")\n# With EasyOCR\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\nocr = EasyOCR(config=EasyOCRConfig(languages=[\"en\", \"ch_sim\"], gpu=True))\nresult = ocr.extract(image)\n# With PaddleOCR\nfrom omnidocs.tasks.ocr_extraction import PaddleOCR, PaddleOCRConfig\n\nocr = PaddleOCR(config=PaddleOCRConfig(lang=\"ch\", device=\"cpu\"))\nresult = ocr.extract(image)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BaseOCRExtractor","title":"BaseOCRExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for OCR extractors.</p> <p>All OCR extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyOCRExtractor(BaseOCRExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Initialize OCR engine\n            pass\n\n        def extract(self, image):\n            # Run OCR extraction\n            return OCROutput(...)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BaseOCRExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput containing detected text blocks with bounding boxes</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format is not supported</p> <code>RuntimeError</code> <p>If OCR engine is not initialized or extraction fails</p> Source code in <code>omnidocs/tasks/ocr_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR extraction on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n\n    Returns:\n        OCROutput containing detected text blocks with bounding boxes\n\n    Raises:\n        ValueError: If image format is not supported\n        RuntimeError: If OCR engine is not initialized or extraction fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BaseOCRExtractor.batch_extract","title":"batch_extract","text":"<pre><code>batch_extract(\n    images: List[Union[Image, ndarray, str, Path]],\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[OCROutput]\n</code></pre> <p>Run OCR extraction on multiple images.</p> <p>Default implementation loops over extract(). Subclasses can override for optimized batching.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of images in any supported format</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[OCROutput]</code> <p>List of OCROutput in same order as input</p> <p>Examples:</p> <pre><code>images = [doc.get_page(i) for i in range(doc.page_count)]\nresults = extractor.batch_extract(images)\n</code></pre> Source code in <code>omnidocs/tasks/ocr_extraction/base.py</code> <pre><code>def batch_extract(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[OCROutput]:\n    \"\"\"\n    Run OCR extraction on multiple images.\n\n    Default implementation loops over extract(). Subclasses can override\n    for optimized batching.\n\n    Args:\n        images: List of images in any supported format\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of OCROutput in same order as input\n\n    Examples:\n        ```python\n        images = [doc.get_page(i) for i in range(doc.page_count)]\n        results = extractor.batch_extract(images)\n        ```\n    \"\"\"\n    results = []\n    total = len(images)\n\n    for i, image in enumerate(images):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(image)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BaseOCRExtractor.extract_document","title":"extract_document","text":"<pre><code>extract_document(\n    document: Document,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[OCROutput]\n</code></pre> <p>Run OCR extraction on all pages of a document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[OCROutput]</code> <p>List of OCROutput, one per page</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresults = extractor.extract_document(doc)\n</code></pre> Source code in <code>omnidocs/tasks/ocr_extraction/base.py</code> <pre><code>def extract_document(\n    self,\n    document: \"Document\",\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[OCROutput]:\n    \"\"\"\n    Run OCR extraction on all pages of a document.\n\n    Args:\n        document: Document instance\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of OCROutput, one per page\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        results = extractor.extract_document(doc)\n        ```\n    \"\"\"\n    results = []\n    total = document.page_count\n\n    for i, page in enumerate(document.iter_pages()):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(page)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.EasyOCR","title":"EasyOCR","text":"<pre><code>EasyOCR(config: EasyOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>EasyOCR text extractor.</p> <p>Single-backend model (PyTorch - CPU/GPU).</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\nocr = EasyOCR(config=EasyOCRConfig(languages=[\"en\"], gpu=True))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize EasyOCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>EasyOCRConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If easyocr is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def __init__(self, config: EasyOCRConfig):\n    \"\"\"\n    Initialize EasyOCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        ImportError: If easyocr is not installed\n    \"\"\"\n    self.config = config\n    self._reader = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.EasyOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    detail: int = 1,\n    paragraph: bool = False,\n    min_size: int = 10,\n    text_threshold: float = 0.7,\n    low_text: float = 0.4,\n    link_threshold: float = 0.4,\n    canvas_size: int = 2560,\n    mag_ratio: float = 1.0,\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>detail</code> <p>0 = simple output, 1 = detailed with boxes</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>paragraph</code> <p>Combine results into paragraphs</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>min_size</code> <p>Minimum text box size</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>text_threshold</code> <p>Text confidence threshold</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.7</code> </p> <code>low_text</code> <p>Low text bound</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>link_threshold</code> <p>Link threshold for text joining</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>canvas_size</code> <p>Max image dimension for processing</p> <p> TYPE: <code>int</code> DEFAULT: <code>2560</code> </p> <code>mag_ratio</code> <p>Magnification ratio</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks</p> RAISES DESCRIPTION <code>ValueError</code> <p>If detail is not 0 or 1</p> <code>RuntimeError</code> <p>If EasyOCR is not initialized</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    detail: int = 1,\n    paragraph: bool = False,\n    min_size: int = 10,\n    text_threshold: float = 0.7,\n    low_text: float = 0.4,\n    link_threshold: float = 0.4,\n    canvas_size: int = 2560,\n    mag_ratio: float = 1.0,\n) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n        detail: 0 = simple output, 1 = detailed with boxes\n        paragraph: Combine results into paragraphs\n        min_size: Minimum text box size\n        text_threshold: Text confidence threshold\n        low_text: Low text bound\n        link_threshold: Link threshold for text joining\n        canvas_size: Max image dimension for processing\n        mag_ratio: Magnification ratio\n\n    Returns:\n        OCROutput with detected text blocks\n\n    Raises:\n        ValueError: If detail is not 0 or 1\n        RuntimeError: If EasyOCR is not initialized\n    \"\"\"\n    if self._reader is None:\n        raise RuntimeError(\"EasyOCR not initialized. Call _load_model() first.\")\n\n    # Validate detail parameter\n    if detail not in (0, 1):\n        raise ValueError(f\"detail must be 0 or 1, got {detail}\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Convert to numpy array for EasyOCR\n    image_array = np.array(pil_image)\n\n    # Run EasyOCR\n    results = self._reader.readtext(\n        image_array,\n        detail=detail,\n        paragraph=paragraph,\n        min_size=min_size,\n        text_threshold=text_threshold,\n        low_text=low_text,\n        link_threshold=link_threshold,\n        canvas_size=canvas_size,\n        mag_ratio=mag_ratio,\n    )\n\n    # Parse results\n    text_blocks = []\n    full_text_parts = []\n\n    for result in results:\n        if detail == 0:\n            # Simple output: just text\n            text = result\n            confidence = 1.0\n            bbox = BoundingBox(x1=0, y1=0, x2=0, y2=0)\n            polygon = None\n        else:\n            # Detailed output: [polygon, text, confidence]\n            polygon_points, text, confidence = result\n\n            # EasyOCR returns 4 corner points: [[x1,y1], [x2,y1], [x2,y2], [x1,y2]]\n            # Convert to list of lists for storage\n            polygon = [list(p) for p in polygon_points]\n\n            # Convert to axis-aligned bounding box\n            bbox = BoundingBox.from_polygon(polygon)\n\n        if not text.strip():\n            continue\n\n        text_blocks.append(\n            TextBlock(\n                text=text,\n                bbox=bbox,\n                confidence=float(confidence),\n                granularity=(OCRGranularity.LINE if paragraph else OCRGranularity.WORD),\n                polygon=polygon,\n                language=\"+\".join(self.config.languages),\n            )\n        )\n\n        full_text_parts.append(text)\n\n    # Sort by position\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\" \".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.EasyOCR.extract_batch","title":"extract_batch","text":"<pre><code>extract_batch(\n    images: List[Union[Image, ndarray, str, Path]], **kwargs\n) -&gt; List[OCROutput]\n</code></pre> <p>Run OCR on multiple images.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of input images</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>**kwargs</code> <p>Arguments passed to extract()</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>List[OCROutput]</code> <p>List of OCROutput objects</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def extract_batch(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    **kwargs,\n) -&gt; List[OCROutput]:\n    \"\"\"\n    Run OCR on multiple images.\n\n    Args:\n        images: List of input images\n        **kwargs: Arguments passed to extract()\n\n    Returns:\n        List of OCROutput objects\n    \"\"\"\n    results = []\n    for img in images:\n        results.append(self.extract(img, **kwargs))\n    return results\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.EasyOCRConfig","title":"EasyOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for EasyOCR extractor.</p> <p>This is a single-backend model (PyTorch - CPU/GPU).</p> Example <pre><code>config = EasyOCRConfig(languages=[\"en\", \"ch_sim\"], gpu=True)\nocr = EasyOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box coordinates in pixel space.</p> <p>Coordinates follow the convention: (x1, y1) is top-left, (x2, y2) is bottom-right. For rotated text, use the polygon field in TextBlock instead.</p> Example <pre><code>bbox = BoundingBox(x1=100, y1=50, x2=300, y2=80)\nprint(bbox.width, bbox.height)  # 200, 30\nprint(bbox.center)  # (200.0, 65.0)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x, y, width, height) format.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_xywh(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x, y, width, height) format.\"\"\"\n    return (self.x1, self.y1, self.width, self.height)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.from_polygon","title":"from_polygon  <code>classmethod</code>","text":"<pre><code>from_polygon(polygon: List[List[float]]) -&gt; BoundingBox\n</code></pre> <p>Create axis-aligned bounding box from polygon points.</p> PARAMETER DESCRIPTION <code>polygon</code> <p>List of [x, y] points (usually 4 for quadrilateral)</p> <p> TYPE: <code>List[List[float]]</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox that encloses all polygon points</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef from_polygon(cls, polygon: List[List[float]]) -&gt; \"BoundingBox\":\n    \"\"\"\n    Create axis-aligned bounding box from polygon points.\n\n    Args:\n        polygon: List of [x, y] points (usually 4 for quadrilateral)\n\n    Returns:\n        BoundingBox that encloses all polygon points\n    \"\"\"\n    if not polygon:\n        raise ValueError(\"Polygon cannot be empty\")\n\n    xs = [p[0] for p in polygon]\n    ys = [p[1] for p in polygon]\n    return cls(x1=min(xs), y1=min(ys), x2=max(xs), y2=max(ys))\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> <p>Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas. This provides consistent coordinates regardless of original image size.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas.\n    This provides consistent coordinates regardless of original image size.\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.BoundingBox.to_absolute","title":"to_absolute","text":"<pre><code>to_absolute(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert from normalized (0-1024) to absolute pixel coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Target image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Target image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with absolute pixel coordinates</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_absolute(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert from normalized (0-1024) to absolute pixel coordinates.\n\n    Args:\n        image_width: Target image width in pixels\n        image_height: Target image height in pixels\n\n    Returns:\n        New BoundingBox with absolute pixel coordinates\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / NORMALIZED_SIZE * image_width,\n        y1=self.y1 / NORMALIZED_SIZE * image_height,\n        x2=self.x2 / NORMALIZED_SIZE * image_width,\n        y2=self.y2 / NORMALIZED_SIZE * image_height,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCRGranularity","title":"OCRGranularity","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>OCR detection granularity levels.</p> <p>Different OCR engines return results at different granularity levels. This enum standardizes the options across all extractors.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput","title":"OCROutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete OCR extraction results for a single image.</p> <p>Contains all detected text blocks with their bounding boxes, plus metadata about the extraction.</p> Example <pre><code>result = ocr.extract(image)\nprint(f\"Found {result.block_count} blocks\")\nprint(f\"Full text: {result.full_text}\")\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.block_count","title":"block_count  <code>property</code>","text":"<pre><code>block_count: int\n</code></pre> <p>Number of detected text blocks.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.word_count","title":"word_count  <code>property</code>","text":"<pre><code>word_count: int\n</code></pre> <p>Approximate word count from full text.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.average_confidence","title":"average_confidence  <code>property</code>","text":"<pre><code>average_confidence: float\n</code></pre> <p>Average confidence across all text blocks.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(\n    min_confidence: float,\n) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by minimum confidence.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def filter_by_confidence(self, min_confidence: float) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by minimum confidence.\"\"\"\n    return [b for b in self.text_blocks if b.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.filter_by_granularity","title":"filter_by_granularity","text":"<pre><code>filter_by_granularity(\n    granularity: OCRGranularity,\n) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by granularity level.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def filter_by_granularity(self, granularity: OCRGranularity) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by granularity level.\"\"\"\n    return [b for b in self.text_blocks if b.granularity == granularity]\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"text_blocks\": [b.to_dict() for b in self.text_blocks],\n        \"full_text\": self.full_text,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"languages_detected\": self.languages_detected,\n        \"block_count\": self.block_count,\n        \"word_count\": self.word_count,\n        \"average_confidence\": self.average_confidence,\n    }\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.sort_by_position","title":"sort_by_position","text":"<pre><code>sort_by_position(top_to_bottom: bool = True) -&gt; OCROutput\n</code></pre> <p>Return a new OCROutput with blocks sorted by position.</p> PARAMETER DESCRIPTION <code>top_to_bottom</code> <p>If True, sort by y-coordinate (reading order)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>New OCROutput with sorted text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def sort_by_position(self, top_to_bottom: bool = True) -&gt; \"OCROutput\":\n    \"\"\"\n    Return a new OCROutput with blocks sorted by position.\n\n    Args:\n        top_to_bottom: If True, sort by y-coordinate (reading order)\n\n    Returns:\n        New OCROutput with sorted text blocks\n    \"\"\"\n    sorted_blocks = sorted(\n        self.text_blocks,\n        key=lambda b: (b.bbox.y1, b.bbox.x1),\n        reverse=not top_to_bottom,\n    )\n    # Regenerate full_text in sorted order\n    full_text = \" \".join(b.text for b in sorted_blocks)\n\n    return OCROutput(\n        text_blocks=sorted_blocks,\n        full_text=full_text,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        model_name=self.model_name,\n        languages_detected=self.languages_detected,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.get_normalized_blocks","title":"get_normalized_blocks","text":"<pre><code>get_normalized_blocks() -&gt; List[Dict]\n</code></pre> <p>Get all text blocks with normalized (0-1024) coordinates.</p> RETURNS DESCRIPTION <code>List[Dict]</code> <p>List of dicts with normalized bbox coordinates and metadata.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def get_normalized_blocks(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all text blocks with normalized (0-1024) coordinates.\n\n    Returns:\n        List of dicts with normalized bbox coordinates and metadata.\n    \"\"\"\n    normalized = []\n    for block in self.text_blocks:\n        norm_bbox = block.bbox.to_normalized(self.image_width, self.image_height)\n        normalized.append(\n            {\n                \"text\": block.text,\n                \"bbox\": norm_bbox.to_list(),\n                \"confidence\": block.confidence,\n                \"granularity\": block.granularity.value,\n                \"language\": block.language,\n            }\n        )\n    return normalized\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.visualize","title":"visualize","text":"<pre><code>visualize(\n    image: Image,\n    output_path: Optional[Union[str, Path]] = None,\n    show_text: bool = True,\n    show_confidence: bool = False,\n    line_width: int = 2,\n    box_color: str = \"#2ECC71\",\n    text_color: str = \"#000000\",\n) -&gt; Image.Image\n</code></pre> <p>Visualize OCR results on the image.</p> <p>Draws bounding boxes around detected text with optional labels.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to draw on (will be copied, not modified)</p> <p> TYPE: <code>Image</code> </p> <code>output_path</code> <p>Optional path to save the visualization</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> <code>show_text</code> <p>Whether to show detected text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_confidence</code> <p>Whether to show confidence scores</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>line_width</code> <p>Width of bounding box lines</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>box_color</code> <p>Color for bounding boxes (hex)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'#2ECC71'</code> </p> <code>text_color</code> <p>Color for text labels (hex)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'#000000'</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image with visualizations drawn</p> Example <pre><code>result = ocr.extract(image)\nviz = result.visualize(image, output_path=\"ocr_viz.png\")\n</code></pre> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def visualize(\n    self,\n    image: \"Image.Image\",\n    output_path: Optional[Union[str, Path]] = None,\n    show_text: bool = True,\n    show_confidence: bool = False,\n    line_width: int = 2,\n    box_color: str = \"#2ECC71\",\n    text_color: str = \"#000000\",\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Visualize OCR results on the image.\n\n    Draws bounding boxes around detected text with optional labels.\n\n    Args:\n        image: PIL Image to draw on (will be copied, not modified)\n        output_path: Optional path to save the visualization\n        show_text: Whether to show detected text\n        show_confidence: Whether to show confidence scores\n        line_width: Width of bounding box lines\n        box_color: Color for bounding boxes (hex)\n        text_color: Color for text labels (hex)\n\n    Returns:\n        PIL Image with visualizations drawn\n\n    Example:\n        ```python\n        result = ocr.extract(image)\n        viz = result.visualize(image, output_path=\"ocr_viz.png\")\n        ```\n    \"\"\"\n    from PIL import ImageDraw, ImageFont\n\n    # Copy image to avoid modifying original\n    viz_image = image.copy().convert(\"RGB\")\n    draw = ImageDraw.Draw(viz_image)\n\n    # Try to get a font\n    try:\n        font = ImageFont.truetype(\"/System/Library/Fonts/Helvetica.ttc\", 12)\n    except Exception:\n        try:\n            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 12)\n        except Exception:\n            font = ImageFont.load_default()\n\n    for block in self.text_blocks:\n        coords = block.bbox.to_xyxy()\n\n        # Draw polygon if available, otherwise draw rectangle\n        if block.polygon:\n            flat_polygon = [coord for point in block.polygon for coord in point]\n            draw.polygon(flat_polygon, outline=box_color, width=line_width)\n        else:\n            draw.rectangle(coords, outline=box_color, width=line_width)\n\n        # Build label text\n        if show_text or show_confidence:\n            label_parts = []\n            if show_text:\n                # Truncate long text\n                text = block.text[:25] + \"...\" if len(block.text) &gt; 25 else block.text\n                label_parts.append(text)\n            if show_confidence:\n                label_parts.append(f\"{block.confidence:.2f}\")\n            label_text = \" | \".join(label_parts)\n\n            # Position label below the box\n            label_x = coords[0]\n            label_y = coords[3] + 2  # Below bottom edge\n\n            # Draw label with background\n            text_bbox = draw.textbbox((label_x, label_y), label_text, font=font)\n            padding = 2\n            draw.rectangle(\n                [\n                    text_bbox[0] - padding,\n                    text_bbox[1] - padding,\n                    text_bbox[2] + padding,\n                    text_bbox[3] + padding,\n                ],\n                fill=\"#FFFFFF\",\n                outline=box_color,\n            )\n            draw.text((label_x, label_y), label_text, fill=text_color, font=font)\n\n    # Save if path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        viz_image.save(output_path)\n\n    return viz_image\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; OCROutput\n</code></pre> <p>Load an OCROutput instance from a JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to JSON file</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput instance</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"OCROutput\":\n    \"\"\"\n    Load an OCROutput instance from a JSON file.\n\n    Args:\n        file_path: Path to JSON file\n\n    Returns:\n        OCROutput instance\n    \"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.OCROutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save OCROutput instance to a JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path where JSON file should be saved</p> <p> TYPE: <code>Union[str, Path]</code> </p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"\n    Save OCROutput instance to a JSON file.\n\n    Args:\n        file_path: Path where JSON file should be saved\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.TextBlock","title":"TextBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single detected text element with text, bounding box, and confidence.</p> <p>This is the fundamental unit of OCR output - can represent a character, word, line, or block depending on the OCR model and configuration.</p> Example <pre><code>block = TextBlock(\n        text=\"Hello\",\n        bbox=BoundingBox(x1=100, y1=50, x2=200, y2=80),\n        confidence=0.95,\n        granularity=OCRGranularity.WORD,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.TextBlock.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"text\": self.text,\n        \"bbox\": self.bbox.to_list(),\n        \"confidence\": self.confidence,\n        \"granularity\": self.granularity.value,\n        \"polygon\": self.polygon,\n        \"language\": self.language,\n    }\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.TextBlock.get_normalized_bbox","title":"get_normalized_bbox","text":"<pre><code>get_normalized_bbox(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Get bounding box in normalized (0-1024) coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox with normalized coordinates</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def get_normalized_bbox(self, image_width: int, image_height: int) -&gt; BoundingBox:\n    \"\"\"\n    Get bounding box in normalized (0-1024) coordinates.\n\n    Args:\n        image_width: Original image width\n        image_height: Original image height\n\n    Returns:\n        BoundingBox with normalized coordinates\n    \"\"\"\n    return self.bbox.to_normalized(image_width, image_height)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.PaddleOCR","title":"PaddleOCR","text":"<pre><code>PaddleOCR(config: PaddleOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>PaddleOCR text extractor.</p> <p>Single-backend model (PaddlePaddle - CPU/GPU).</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import PaddleOCR, PaddleOCRConfig\n\nocr = PaddleOCR(config=PaddleOCRConfig(lang=\"en\", device=\"cpu\"))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize PaddleOCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>PaddleOCRConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If paddleocr or paddlepaddle is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/paddleocr.py</code> <pre><code>def __init__(self, config: PaddleOCRConfig):\n    \"\"\"\n    Initialize PaddleOCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        ImportError: If paddleocr or paddlepaddle is not installed\n    \"\"\"\n    self.config = config\n    self._ocr = None\n\n    # Normalize language code\n    self._lang = LANG_CODES.get(config.lang.lower(), config.lang)\n\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.PaddleOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/paddleocr.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with detected text blocks\n    \"\"\"\n    if self._ocr is None:\n        raise RuntimeError(\"PaddleOCR not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Convert to numpy array\n    image_array = np.array(pil_image)\n\n    # Run PaddleOCR v3.x - use predict() method\n    results = self._ocr.predict(image_array)\n\n    # Parse results\n    text_blocks = []\n\n    # PaddleOCR may return None or empty results\n    if results is None or len(results) == 0:\n        return OCROutput(\n            text_blocks=[],\n            full_text=\"\",\n            image_width=image_width,\n            image_height=image_height,\n            model_name=self.MODEL_NAME,\n            languages_detected=[self._lang],\n        )\n\n    # PaddleOCR v3.x returns list of dicts with 'rec_texts', 'rec_scores', 'dt_polys'\n    for result in results:\n        if result is None:\n            continue\n\n        rec_texts = result.get(\"rec_texts\", [])\n        rec_scores = result.get(\"rec_scores\", [])\n        dt_polys = result.get(\"dt_polys\", [])\n\n        for i, text in enumerate(rec_texts):\n            if not text.strip():\n                continue\n\n            confidence = rec_scores[i] if i &lt; len(rec_scores) else 1.0\n\n            # Get polygon and convert to list\n            polygon: Optional[List[List[float]]] = None\n            if i &lt; len(dt_polys) and dt_polys[i] is not None:\n                poly_array = dt_polys[i]\n                # Handle numpy array\n                if hasattr(poly_array, \"tolist\"):\n                    polygon = poly_array.tolist()\n                else:\n                    polygon = list(poly_array)\n\n            # Convert polygon to bbox\n            if polygon:\n                bbox = BoundingBox.from_polygon(polygon)\n            else:\n                bbox = BoundingBox(x1=0, y1=0, x2=0, y2=0)\n\n            text_blocks.append(\n                TextBlock(\n                    text=text,\n                    bbox=bbox,\n                    confidence=float(confidence),\n                    granularity=OCRGranularity.LINE,\n                    polygon=polygon,\n                    language=self._lang,\n                )\n            )\n\n    # Sort by position (top to bottom, left to right)\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    # Build full_text from sorted blocks to ensure reading order\n    full_text = \" \".join(block.text for block in text_blocks)\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=full_text,\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=[self._lang],\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.PaddleOCRConfig","title":"PaddleOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for PaddleOCR extractor.</p> <p>This is a single-backend model (PaddlePaddle - CPU/GPU).</p> Example <pre><code>config = PaddleOCRConfig(lang=\"ch\", device=\"gpu\")\nocr = PaddleOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.TesseractOCR","title":"TesseractOCR","text":"<pre><code>TesseractOCR(config: TesseractOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>Tesseract OCR extractor.</p> <p>Single-backend model (CPU only). Requires system Tesseract installation.</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\n\nocr = TesseractOCR(config=TesseractOCRConfig(languages=[\"eng\"]))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize Tesseract OCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>TesseractOCRConfig</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If Tesseract is not installed</p> <code>ImportError</code> <p>If pytesseract is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def __init__(self, config: TesseractOCRConfig):\n    \"\"\"\n    Initialize Tesseract OCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        RuntimeError: If Tesseract is not installed\n        ImportError: If pytesseract is not installed\n    \"\"\"\n    self.config = config\n    self._pytesseract = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.TesseractOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks at word level</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with detected text blocks at word level\n    \"\"\"\n    if self._pytesseract is None:\n        raise RuntimeError(\"Tesseract not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Build config string\n    config = f\"--oem {self.config.oem} --psm {self.config.psm}\"\n    if self.config.config_params:\n        for key, value in self.config.config_params.items():\n            config += f\" -c {key}={value}\"\n\n    # Language string\n    lang_str = \"+\".join(self.config.languages)\n\n    # Get detailed data (word-level boxes)\n    data = self._pytesseract.image_to_data(\n        pil_image,\n        lang=lang_str,\n        config=config,\n        output_type=self._pytesseract.Output.DICT,\n    )\n\n    # Parse results into TextBlocks\n    text_blocks = []\n    full_text_parts = []\n\n    n_boxes = len(data[\"text\"])\n    for i in range(n_boxes):\n        text = data[\"text\"][i].strip()\n        # Safely convert conf to float (handles string values from some Tesseract versions)\n        try:\n            conf = float(data[\"conf\"][i])\n        except (ValueError, TypeError):\n            conf = -1\n\n        # Skip empty text or low confidence (-1 means no confidence)\n        if not text or conf == -1:\n            continue\n\n        # Tesseract returns confidence as 0-100, normalize to 0-1\n        confidence = conf / 100.0\n\n        # Get bounding box\n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n\n        bbox = BoundingBox(\n            x1=float(x),\n            y1=float(y),\n            x2=float(x + w),\n            y2=float(y + h),\n        )\n\n        text_blocks.append(\n            TextBlock(\n                text=text,\n                bbox=bbox,\n                confidence=confidence,\n                granularity=OCRGranularity.WORD,\n                language=lang_str,\n            )\n        )\n\n        full_text_parts.append(text)\n\n    # Sort by position (top to bottom, left to right)\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\" \".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.TesseractOCR.extract_lines","title":"extract_lines","text":"<pre><code>extract_lines(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR and return line-level blocks.</p> <p>Groups words into lines based on Tesseract's line detection.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with line-level text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def extract_lines(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR and return line-level blocks.\n\n    Groups words into lines based on Tesseract's line detection.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with line-level text blocks\n    \"\"\"\n    if self._pytesseract is None:\n        raise RuntimeError(\"Tesseract not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Build config string (including config_params like extract method)\n    config = f\"--oem {self.config.oem} --psm {self.config.psm}\"\n    if self.config.config_params:\n        for key, value in self.config.config_params.items():\n            config += f\" -c {key}={value}\"\n\n    # Language string\n    lang_str = \"+\".join(self.config.languages)\n\n    # Get detailed data\n    data = self._pytesseract.image_to_data(\n        pil_image,\n        lang=lang_str,\n        config=config,\n        output_type=self._pytesseract.Output.DICT,\n    )\n\n    # Group words into lines\n    lines: Dict[tuple, Dict] = {}\n    n_boxes = len(data[\"text\"])\n\n    for i in range(n_boxes):\n        text = data[\"text\"][i].strip()\n        # Safely convert conf to float (handles string values from some Tesseract versions)\n        try:\n            conf = float(data[\"conf\"][i])\n        except (ValueError, TypeError):\n            conf = -1\n\n        if not text or conf == -1:\n            continue\n\n        # Tesseract provides block_num, par_num, line_num\n        line_key = (data[\"block_num\"][i], data[\"par_num\"][i], data[\"line_num\"][i])\n\n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n\n        if line_key not in lines:\n            lines[line_key] = {\n                \"words\": [],\n                \"confidences\": [],\n                \"x1\": x,\n                \"y1\": y,\n                \"x2\": x + w,\n                \"y2\": y + h,\n            }\n\n        lines[line_key][\"words\"].append(text)\n        lines[line_key][\"confidences\"].append(conf / 100.0)\n        lines[line_key][\"x1\"] = min(lines[line_key][\"x1\"], x)\n        lines[line_key][\"y1\"] = min(lines[line_key][\"y1\"], y)\n        lines[line_key][\"x2\"] = max(lines[line_key][\"x2\"], x + w)\n        lines[line_key][\"y2\"] = max(lines[line_key][\"y2\"], y + h)\n\n    # Convert to TextBlocks\n    text_blocks = []\n    full_text_parts = []\n\n    for line_key in sorted(lines.keys()):\n        line = lines[line_key]\n        line_text = \" \".join(line[\"words\"])\n        avg_conf = sum(line[\"confidences\"]) / len(line[\"confidences\"])\n\n        bbox = BoundingBox(\n            x1=float(line[\"x1\"]),\n            y1=float(line[\"y1\"]),\n            x2=float(line[\"x2\"]),\n            y2=float(line[\"y2\"]),\n        )\n\n        text_blocks.append(\n            TextBlock(\n                text=line_text,\n                bbox=bbox,\n                confidence=avg_conf,\n                granularity=OCRGranularity.LINE,\n                language=lang_str,\n            )\n        )\n\n        full_text_parts.append(line_text)\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\"\\n\".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.TesseractOCRConfig","title":"TesseractOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Tesseract OCR extractor.</p> <p>This is a single-backend model (CPU only, requires system Tesseract).</p> Example <pre><code>config = TesseractOCRConfig(languages=[\"eng\", \"fra\"], psm=3)\nocr = TesseractOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.base","title":"base","text":"<p>Base class for OCR extractors.</p> <p>Defines the abstract interface that all OCR extractors must implement.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor","title":"BaseOCRExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for OCR extractors.</p> <p>All OCR extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyOCRExtractor(BaseOCRExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Initialize OCR engine\n            pass\n\n        def extract(self, image):\n            # Run OCR extraction\n            return OCROutput(...)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR extraction on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput containing detected text blocks with bounding boxes</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format is not supported</p> <code>RuntimeError</code> <p>If OCR engine is not initialized or extraction fails</p> Source code in <code>omnidocs/tasks/ocr_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR extraction on an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n\n    Returns:\n        OCROutput containing detected text blocks with bounding boxes\n\n    Raises:\n        ValueError: If image format is not supported\n        RuntimeError: If OCR engine is not initialized or extraction fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.batch_extract","title":"batch_extract","text":"<pre><code>batch_extract(\n    images: List[Union[Image, ndarray, str, Path]],\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[OCROutput]\n</code></pre> <p>Run OCR extraction on multiple images.</p> <p>Default implementation loops over extract(). Subclasses can override for optimized batching.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of images in any supported format</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[OCROutput]</code> <p>List of OCROutput in same order as input</p> <p>Examples:</p> <pre><code>images = [doc.get_page(i) for i in range(doc.page_count)]\nresults = extractor.batch_extract(images)\n</code></pre> Source code in <code>omnidocs/tasks/ocr_extraction/base.py</code> <pre><code>def batch_extract(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[OCROutput]:\n    \"\"\"\n    Run OCR extraction on multiple images.\n\n    Default implementation loops over extract(). Subclasses can override\n    for optimized batching.\n\n    Args:\n        images: List of images in any supported format\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of OCROutput in same order as input\n\n    Examples:\n        ```python\n        images = [doc.get_page(i) for i in range(doc.page_count)]\n        results = extractor.batch_extract(images)\n        ```\n    \"\"\"\n    results = []\n    total = len(images)\n\n    for i, image in enumerate(images):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(image)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.base.BaseOCRExtractor.extract_document","title":"extract_document","text":"<pre><code>extract_document(\n    document: Document,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[OCROutput]\n</code></pre> <p>Run OCR extraction on all pages of a document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[OCROutput]</code> <p>List of OCROutput, one per page</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresults = extractor.extract_document(doc)\n</code></pre> Source code in <code>omnidocs/tasks/ocr_extraction/base.py</code> <pre><code>def extract_document(\n    self,\n    document: \"Document\",\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[OCROutput]:\n    \"\"\"\n    Run OCR extraction on all pages of a document.\n\n    Args:\n        document: Document instance\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of OCROutput, one per page\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        results = extractor.extract_document(doc)\n        ```\n    \"\"\"\n    results = []\n    total = document.page_count\n\n    for i, page in enumerate(document.iter_pages()):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(page)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.easyocr","title":"easyocr","text":"<p>EasyOCR extractor.</p> <p>EasyOCR is a PyTorch-based OCR engine with excellent multi-language support. - GPU accelerated (optional) - Supports 80+ languages - Good for scene text and printed documents</p> Python Package <p>pip install easyocr</p> Model Download Location <p>By default, EasyOCR downloads models to ~/.EasyOCR/ Can be overridden with model_storage_directory parameter</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCRConfig","title":"EasyOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for EasyOCR extractor.</p> <p>This is a single-backend model (PyTorch - CPU/GPU).</p> Example <pre><code>config = EasyOCRConfig(languages=[\"en\", \"ch_sim\"], gpu=True)\nocr = EasyOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCR","title":"EasyOCR","text":"<pre><code>EasyOCR(config: EasyOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>EasyOCR text extractor.</p> <p>Single-backend model (PyTorch - CPU/GPU).</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\nocr = EasyOCR(config=EasyOCRConfig(languages=[\"en\"], gpu=True))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize EasyOCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>EasyOCRConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If easyocr is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def __init__(self, config: EasyOCRConfig):\n    \"\"\"\n    Initialize EasyOCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        ImportError: If easyocr is not installed\n    \"\"\"\n    self.config = config\n    self._reader = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    detail: int = 1,\n    paragraph: bool = False,\n    min_size: int = 10,\n    text_threshold: float = 0.7,\n    low_text: float = 0.4,\n    link_threshold: float = 0.4,\n    canvas_size: int = 2560,\n    mag_ratio: float = 1.0,\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>detail</code> <p>0 = simple output, 1 = detailed with boxes</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <code>paragraph</code> <p>Combine results into paragraphs</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>min_size</code> <p>Minimum text box size</p> <p> TYPE: <code>int</code> DEFAULT: <code>10</code> </p> <code>text_threshold</code> <p>Text confidence threshold</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.7</code> </p> <code>low_text</code> <p>Low text bound</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>link_threshold</code> <p>Link threshold for text joining</p> <p> TYPE: <code>float</code> DEFAULT: <code>0.4</code> </p> <code>canvas_size</code> <p>Max image dimension for processing</p> <p> TYPE: <code>int</code> DEFAULT: <code>2560</code> </p> <code>mag_ratio</code> <p>Magnification ratio</p> <p> TYPE: <code>float</code> DEFAULT: <code>1.0</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks</p> RAISES DESCRIPTION <code>ValueError</code> <p>If detail is not 0 or 1</p> <code>RuntimeError</code> <p>If EasyOCR is not initialized</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    detail: int = 1,\n    paragraph: bool = False,\n    min_size: int = 10,\n    text_threshold: float = 0.7,\n    low_text: float = 0.4,\n    link_threshold: float = 0.4,\n    canvas_size: int = 2560,\n    mag_ratio: float = 1.0,\n) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n        detail: 0 = simple output, 1 = detailed with boxes\n        paragraph: Combine results into paragraphs\n        min_size: Minimum text box size\n        text_threshold: Text confidence threshold\n        low_text: Low text bound\n        link_threshold: Link threshold for text joining\n        canvas_size: Max image dimension for processing\n        mag_ratio: Magnification ratio\n\n    Returns:\n        OCROutput with detected text blocks\n\n    Raises:\n        ValueError: If detail is not 0 or 1\n        RuntimeError: If EasyOCR is not initialized\n    \"\"\"\n    if self._reader is None:\n        raise RuntimeError(\"EasyOCR not initialized. Call _load_model() first.\")\n\n    # Validate detail parameter\n    if detail not in (0, 1):\n        raise ValueError(f\"detail must be 0 or 1, got {detail}\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Convert to numpy array for EasyOCR\n    image_array = np.array(pil_image)\n\n    # Run EasyOCR\n    results = self._reader.readtext(\n        image_array,\n        detail=detail,\n        paragraph=paragraph,\n        min_size=min_size,\n        text_threshold=text_threshold,\n        low_text=low_text,\n        link_threshold=link_threshold,\n        canvas_size=canvas_size,\n        mag_ratio=mag_ratio,\n    )\n\n    # Parse results\n    text_blocks = []\n    full_text_parts = []\n\n    for result in results:\n        if detail == 0:\n            # Simple output: just text\n            text = result\n            confidence = 1.0\n            bbox = BoundingBox(x1=0, y1=0, x2=0, y2=0)\n            polygon = None\n        else:\n            # Detailed output: [polygon, text, confidence]\n            polygon_points, text, confidence = result\n\n            # EasyOCR returns 4 corner points: [[x1,y1], [x2,y1], [x2,y2], [x1,y2]]\n            # Convert to list of lists for storage\n            polygon = [list(p) for p in polygon_points]\n\n            # Convert to axis-aligned bounding box\n            bbox = BoundingBox.from_polygon(polygon)\n\n        if not text.strip():\n            continue\n\n        text_blocks.append(\n            TextBlock(\n                text=text,\n                bbox=bbox,\n                confidence=float(confidence),\n                granularity=(OCRGranularity.LINE if paragraph else OCRGranularity.WORD),\n                polygon=polygon,\n                language=\"+\".join(self.config.languages),\n            )\n        )\n\n        full_text_parts.append(text)\n\n    # Sort by position\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\" \".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.easyocr.EasyOCR.extract_batch","title":"extract_batch","text":"<pre><code>extract_batch(\n    images: List[Union[Image, ndarray, str, Path]], **kwargs\n) -&gt; List[OCROutput]\n</code></pre> <p>Run OCR on multiple images.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of input images</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>**kwargs</code> <p>Arguments passed to extract()</p> <p> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>List[OCROutput]</code> <p>List of OCROutput objects</p> Source code in <code>omnidocs/tasks/ocr_extraction/easyocr.py</code> <pre><code>def extract_batch(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    **kwargs,\n) -&gt; List[OCROutput]:\n    \"\"\"\n    Run OCR on multiple images.\n\n    Args:\n        images: List of input images\n        **kwargs: Arguments passed to extract()\n\n    Returns:\n        List of OCROutput objects\n    \"\"\"\n    results = []\n    for img in images:\n        results.append(self.extract(img, **kwargs))\n    return results\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models","title":"models","text":"<p>Pydantic models for OCR extraction outputs.</p> <p>Defines standardized output types for OCR detection including text blocks with bounding boxes, confidence scores, and granularity levels.</p> <p>Key difference from Text Extraction: - OCR returns text WITH bounding boxes (word/line/character level) - Text Extraction returns formatted text (MD/HTML) WITHOUT bboxes</p> Coordinate Systems <ul> <li>Absolute (default): Coordinates in pixels relative to original image size</li> <li>Normalized (0-1024): Coordinates scaled to 0-1024 range (virtual 1024x1024 canvas)</li> </ul> <p>Use <code>bbox.to_normalized(width, height)</code> or <code>output.get_normalized_blocks()</code> to convert to normalized coordinates.</p> Example <pre><code>result = ocr.extract(image)  # Returns absolute pixel coordinates\nnormalized = result.get_normalized_blocks()  # Returns 0-1024 normalized coords\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCRGranularity","title":"OCRGranularity","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>OCR detection granularity levels.</p> <p>Different OCR engines return results at different granularity levels. This enum standardizes the options across all extractors.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box coordinates in pixel space.</p> <p>Coordinates follow the convention: (x1, y1) is top-left, (x2, y2) is bottom-right. For rotated text, use the polygon field in TextBlock instead.</p> Example <pre><code>bbox = BoundingBox(x1=100, y1=50, x2=300, y2=80)\nprint(bbox.width, bbox.height)  # 200, 30\nprint(bbox.center)  # (200.0, 65.0)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_xywh","title":"to_xywh","text":"<pre><code>to_xywh() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x, y, width, height) format.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_xywh(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x, y, width, height) format.\"\"\"\n    return (self.x1, self.y1, self.width, self.height)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.from_polygon","title":"from_polygon  <code>classmethod</code>","text":"<pre><code>from_polygon(polygon: List[List[float]]) -&gt; BoundingBox\n</code></pre> <p>Create axis-aligned bounding box from polygon points.</p> PARAMETER DESCRIPTION <code>polygon</code> <p>List of [x, y] points (usually 4 for quadrilateral)</p> <p> TYPE: <code>List[List[float]]</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox that encloses all polygon points</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef from_polygon(cls, polygon: List[List[float]]) -&gt; \"BoundingBox\":\n    \"\"\"\n    Create axis-aligned bounding box from polygon points.\n\n    Args:\n        polygon: List of [x, y] points (usually 4 for quadrilateral)\n\n    Returns:\n        BoundingBox that encloses all polygon points\n    \"\"\"\n    if not polygon:\n        raise ValueError(\"Polygon cannot be empty\")\n\n    xs = [p[0] for p in polygon]\n    ys = [p[1] for p in polygon]\n    return cls(x1=min(xs), y1=min(ys), x2=max(xs), y2=max(ys))\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> <p>Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas. This provides consistent coordinates regardless of original image size.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Scales coordinates from absolute pixel values to a virtual 1024x1024 canvas.\n    This provides consistent coordinates regardless of original image size.\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.BoundingBox.to_absolute","title":"to_absolute","text":"<pre><code>to_absolute(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert from normalized (0-1024) to absolute pixel coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Target image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Target image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with absolute pixel coordinates</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_absolute(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert from normalized (0-1024) to absolute pixel coordinates.\n\n    Args:\n        image_width: Target image width in pixels\n        image_height: Target image height in pixels\n\n    Returns:\n        New BoundingBox with absolute pixel coordinates\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / NORMALIZED_SIZE * image_width,\n        y1=self.y1 / NORMALIZED_SIZE * image_height,\n        x2=self.x2 / NORMALIZED_SIZE * image_width,\n        y2=self.y2 / NORMALIZED_SIZE * image_height,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.TextBlock","title":"TextBlock","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single detected text element with text, bounding box, and confidence.</p> <p>This is the fundamental unit of OCR output - can represent a character, word, line, or block depending on the OCR model and configuration.</p> Example <pre><code>block = TextBlock(\n        text=\"Hello\",\n        bbox=BoundingBox(x1=100, y1=50, x2=200, y2=80),\n        confidence=0.95,\n        granularity=OCRGranularity.WORD,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.TextBlock.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"text\": self.text,\n        \"bbox\": self.bbox.to_list(),\n        \"confidence\": self.confidence,\n        \"granularity\": self.granularity.value,\n        \"polygon\": self.polygon,\n        \"language\": self.language,\n    }\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.TextBlock.get_normalized_bbox","title":"get_normalized_bbox","text":"<pre><code>get_normalized_bbox(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Get bounding box in normalized (0-1024) coordinates.</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>BoundingBox with normalized coordinates</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def get_normalized_bbox(self, image_width: int, image_height: int) -&gt; BoundingBox:\n    \"\"\"\n    Get bounding box in normalized (0-1024) coordinates.\n\n    Args:\n        image_width: Original image width\n        image_height: Original image height\n\n    Returns:\n        BoundingBox with normalized coordinates\n    \"\"\"\n    return self.bbox.to_normalized(image_width, image_height)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput","title":"OCROutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete OCR extraction results for a single image.</p> <p>Contains all detected text blocks with their bounding boxes, plus metadata about the extraction.</p> Example <pre><code>result = ocr.extract(image)\nprint(f\"Found {result.block_count} blocks\")\nprint(f\"Full text: {result.full_text}\")\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.block_count","title":"block_count  <code>property</code>","text":"<pre><code>block_count: int\n</code></pre> <p>Number of detected text blocks.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.word_count","title":"word_count  <code>property</code>","text":"<pre><code>word_count: int\n</code></pre> <p>Approximate word count from full text.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.average_confidence","title":"average_confidence  <code>property</code>","text":"<pre><code>average_confidence: float\n</code></pre> <p>Average confidence across all text blocks.</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.filter_by_confidence","title":"filter_by_confidence","text":"<pre><code>filter_by_confidence(\n    min_confidence: float,\n) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by minimum confidence.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def filter_by_confidence(self, min_confidence: float) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by minimum confidence.\"\"\"\n    return [b for b in self.text_blocks if b.confidence &gt;= min_confidence]\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.filter_by_granularity","title":"filter_by_granularity","text":"<pre><code>filter_by_granularity(\n    granularity: OCRGranularity,\n) -&gt; List[TextBlock]\n</code></pre> <p>Filter text blocks by granularity level.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def filter_by_granularity(self, granularity: OCRGranularity) -&gt; List[TextBlock]:\n    \"\"\"Filter text blocks by granularity level.\"\"\"\n    return [b for b in self.text_blocks if b.granularity == granularity]\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"text_blocks\": [b.to_dict() for b in self.text_blocks],\n        \"full_text\": self.full_text,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"languages_detected\": self.languages_detected,\n        \"block_count\": self.block_count,\n        \"word_count\": self.word_count,\n        \"average_confidence\": self.average_confidence,\n    }\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.sort_by_position","title":"sort_by_position","text":"<pre><code>sort_by_position(top_to_bottom: bool = True) -&gt; OCROutput\n</code></pre> <p>Return a new OCROutput with blocks sorted by position.</p> PARAMETER DESCRIPTION <code>top_to_bottom</code> <p>If True, sort by y-coordinate (reading order)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>New OCROutput with sorted text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def sort_by_position(self, top_to_bottom: bool = True) -&gt; \"OCROutput\":\n    \"\"\"\n    Return a new OCROutput with blocks sorted by position.\n\n    Args:\n        top_to_bottom: If True, sort by y-coordinate (reading order)\n\n    Returns:\n        New OCROutput with sorted text blocks\n    \"\"\"\n    sorted_blocks = sorted(\n        self.text_blocks,\n        key=lambda b: (b.bbox.y1, b.bbox.x1),\n        reverse=not top_to_bottom,\n    )\n    # Regenerate full_text in sorted order\n    full_text = \" \".join(b.text for b in sorted_blocks)\n\n    return OCROutput(\n        text_blocks=sorted_blocks,\n        full_text=full_text,\n        image_width=self.image_width,\n        image_height=self.image_height,\n        model_name=self.model_name,\n        languages_detected=self.languages_detected,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.get_normalized_blocks","title":"get_normalized_blocks","text":"<pre><code>get_normalized_blocks() -&gt; List[Dict]\n</code></pre> <p>Get all text blocks with normalized (0-1024) coordinates.</p> RETURNS DESCRIPTION <code>List[Dict]</code> <p>List of dicts with normalized bbox coordinates and metadata.</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def get_normalized_blocks(self) -&gt; List[Dict]:\n    \"\"\"\n    Get all text blocks with normalized (0-1024) coordinates.\n\n    Returns:\n        List of dicts with normalized bbox coordinates and metadata.\n    \"\"\"\n    normalized = []\n    for block in self.text_blocks:\n        norm_bbox = block.bbox.to_normalized(self.image_width, self.image_height)\n        normalized.append(\n            {\n                \"text\": block.text,\n                \"bbox\": norm_bbox.to_list(),\n                \"confidence\": block.confidence,\n                \"granularity\": block.granularity.value,\n                \"language\": block.language,\n            }\n        )\n    return normalized\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.visualize","title":"visualize","text":"<pre><code>visualize(\n    image: Image,\n    output_path: Optional[Union[str, Path]] = None,\n    show_text: bool = True,\n    show_confidence: bool = False,\n    line_width: int = 2,\n    box_color: str = \"#2ECC71\",\n    text_color: str = \"#000000\",\n) -&gt; Image.Image\n</code></pre> <p>Visualize OCR results on the image.</p> <p>Draws bounding boxes around detected text with optional labels.</p> PARAMETER DESCRIPTION <code>image</code> <p>PIL Image to draw on (will be copied, not modified)</p> <p> TYPE: <code>Image</code> </p> <code>output_path</code> <p>Optional path to save the visualization</p> <p> TYPE: <code>Optional[Union[str, Path]]</code> DEFAULT: <code>None</code> </p> <code>show_text</code> <p>Whether to show detected text</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>show_confidence</code> <p>Whether to show confidence scores</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>line_width</code> <p>Width of bounding box lines</p> <p> TYPE: <code>int</code> DEFAULT: <code>2</code> </p> <code>box_color</code> <p>Color for bounding boxes (hex)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'#2ECC71'</code> </p> <code>text_color</code> <p>Color for text labels (hex)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'#000000'</code> </p> RETURNS DESCRIPTION <code>Image</code> <p>PIL Image with visualizations drawn</p> Example <pre><code>result = ocr.extract(image)\nviz = result.visualize(image, output_path=\"ocr_viz.png\")\n</code></pre> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def visualize(\n    self,\n    image: \"Image.Image\",\n    output_path: Optional[Union[str, Path]] = None,\n    show_text: bool = True,\n    show_confidence: bool = False,\n    line_width: int = 2,\n    box_color: str = \"#2ECC71\",\n    text_color: str = \"#000000\",\n) -&gt; \"Image.Image\":\n    \"\"\"\n    Visualize OCR results on the image.\n\n    Draws bounding boxes around detected text with optional labels.\n\n    Args:\n        image: PIL Image to draw on (will be copied, not modified)\n        output_path: Optional path to save the visualization\n        show_text: Whether to show detected text\n        show_confidence: Whether to show confidence scores\n        line_width: Width of bounding box lines\n        box_color: Color for bounding boxes (hex)\n        text_color: Color for text labels (hex)\n\n    Returns:\n        PIL Image with visualizations drawn\n\n    Example:\n        ```python\n        result = ocr.extract(image)\n        viz = result.visualize(image, output_path=\"ocr_viz.png\")\n        ```\n    \"\"\"\n    from PIL import ImageDraw, ImageFont\n\n    # Copy image to avoid modifying original\n    viz_image = image.copy().convert(\"RGB\")\n    draw = ImageDraw.Draw(viz_image)\n\n    # Try to get a font\n    try:\n        font = ImageFont.truetype(\"/System/Library/Fonts/Helvetica.ttc\", 12)\n    except Exception:\n        try:\n            font = ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", 12)\n        except Exception:\n            font = ImageFont.load_default()\n\n    for block in self.text_blocks:\n        coords = block.bbox.to_xyxy()\n\n        # Draw polygon if available, otherwise draw rectangle\n        if block.polygon:\n            flat_polygon = [coord for point in block.polygon for coord in point]\n            draw.polygon(flat_polygon, outline=box_color, width=line_width)\n        else:\n            draw.rectangle(coords, outline=box_color, width=line_width)\n\n        # Build label text\n        if show_text or show_confidence:\n            label_parts = []\n            if show_text:\n                # Truncate long text\n                text = block.text[:25] + \"...\" if len(block.text) &gt; 25 else block.text\n                label_parts.append(text)\n            if show_confidence:\n                label_parts.append(f\"{block.confidence:.2f}\")\n            label_text = \" | \".join(label_parts)\n\n            # Position label below the box\n            label_x = coords[0]\n            label_y = coords[3] + 2  # Below bottom edge\n\n            # Draw label with background\n            text_bbox = draw.textbbox((label_x, label_y), label_text, font=font)\n            padding = 2\n            draw.rectangle(\n                [\n                    text_bbox[0] - padding,\n                    text_bbox[1] - padding,\n                    text_bbox[2] + padding,\n                    text_bbox[3] + padding,\n                ],\n                fill=\"#FFFFFF\",\n                outline=box_color,\n            )\n            draw.text((label_x, label_y), label_text, fill=text_color, font=font)\n\n    # Save if path provided\n    if output_path:\n        output_path = Path(output_path)\n        output_path.parent.mkdir(parents=True, exist_ok=True)\n        viz_image.save(output_path)\n\n    return viz_image\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; OCROutput\n</code></pre> <p>Load an OCROutput instance from a JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path to JSON file</p> <p> TYPE: <code>Union[str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput instance</p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"OCROutput\":\n    \"\"\"\n    Load an OCROutput instance from a JSON file.\n\n    Args:\n        file_path: Path to JSON file\n\n    Returns:\n        OCROutput instance\n    \"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.models.OCROutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save OCROutput instance to a JSON file.</p> PARAMETER DESCRIPTION <code>file_path</code> <p>Path where JSON file should be saved</p> <p> TYPE: <code>Union[str, Path]</code> </p> Source code in <code>omnidocs/tasks/ocr_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"\n    Save OCROutput instance to a JSON file.\n\n    Args:\n        file_path: Path where JSON file should be saved\n    \"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.paddleocr","title":"paddleocr","text":"<p>PaddleOCR extractor.</p> <p>PaddleOCR is an OCR toolkit developed by Baidu/PaddlePaddle. - Excellent for CJK languages (Chinese, Japanese, Korean) - GPU accelerated - Supports layout analysis + OCR</p> Python Package <p>pip install paddleocr paddlepaddle  # CPU version pip install paddleocr paddlepaddle-gpu  # GPU version</p> Model Download Location <p>By default, PaddleOCR downloads models to ~/.paddleocr/</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.paddleocr.PaddleOCRConfig","title":"PaddleOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for PaddleOCR extractor.</p> <p>This is a single-backend model (PaddlePaddle - CPU/GPU).</p> Example <pre><code>config = PaddleOCRConfig(lang=\"ch\", device=\"gpu\")\nocr = PaddleOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.paddleocr.PaddleOCR","title":"PaddleOCR","text":"<pre><code>PaddleOCR(config: PaddleOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>PaddleOCR text extractor.</p> <p>Single-backend model (PaddlePaddle - CPU/GPU).</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import PaddleOCR, PaddleOCRConfig\n\nocr = PaddleOCR(config=PaddleOCRConfig(lang=\"en\", device=\"cpu\"))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize PaddleOCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>PaddleOCRConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If paddleocr or paddlepaddle is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/paddleocr.py</code> <pre><code>def __init__(self, config: PaddleOCRConfig):\n    \"\"\"\n    Initialize PaddleOCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        ImportError: If paddleocr or paddlepaddle is not installed\n    \"\"\"\n    self.config = config\n    self._ocr = None\n\n    # Normalize language code\n    self._lang = LANG_CODES.get(config.lang.lower(), config.lang)\n\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.paddleocr.PaddleOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/paddleocr.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with detected text blocks\n    \"\"\"\n    if self._ocr is None:\n        raise RuntimeError(\"PaddleOCR not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Convert to numpy array\n    image_array = np.array(pil_image)\n\n    # Run PaddleOCR v3.x - use predict() method\n    results = self._ocr.predict(image_array)\n\n    # Parse results\n    text_blocks = []\n\n    # PaddleOCR may return None or empty results\n    if results is None or len(results) == 0:\n        return OCROutput(\n            text_blocks=[],\n            full_text=\"\",\n            image_width=image_width,\n            image_height=image_height,\n            model_name=self.MODEL_NAME,\n            languages_detected=[self._lang],\n        )\n\n    # PaddleOCR v3.x returns list of dicts with 'rec_texts', 'rec_scores', 'dt_polys'\n    for result in results:\n        if result is None:\n            continue\n\n        rec_texts = result.get(\"rec_texts\", [])\n        rec_scores = result.get(\"rec_scores\", [])\n        dt_polys = result.get(\"dt_polys\", [])\n\n        for i, text in enumerate(rec_texts):\n            if not text.strip():\n                continue\n\n            confidence = rec_scores[i] if i &lt; len(rec_scores) else 1.0\n\n            # Get polygon and convert to list\n            polygon: Optional[List[List[float]]] = None\n            if i &lt; len(dt_polys) and dt_polys[i] is not None:\n                poly_array = dt_polys[i]\n                # Handle numpy array\n                if hasattr(poly_array, \"tolist\"):\n                    polygon = poly_array.tolist()\n                else:\n                    polygon = list(poly_array)\n\n            # Convert polygon to bbox\n            if polygon:\n                bbox = BoundingBox.from_polygon(polygon)\n            else:\n                bbox = BoundingBox(x1=0, y1=0, x2=0, y2=0)\n\n            text_blocks.append(\n                TextBlock(\n                    text=text,\n                    bbox=bbox,\n                    confidence=float(confidence),\n                    granularity=OCRGranularity.LINE,\n                    polygon=polygon,\n                    language=self._lang,\n                )\n            )\n\n    # Sort by position (top to bottom, left to right)\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    # Build full_text from sorted blocks to ensure reading order\n    full_text = \" \".join(block.text for block in text_blocks)\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=full_text,\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=[self._lang],\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.tesseract","title":"tesseract","text":"<p>Tesseract OCR extractor.</p> <p>Tesseract is an open-source OCR engine maintained by Google. - CPU-based (no GPU required) - Requires system installation of Tesseract - Good for printed text, supports 100+ languages</p> System Requirements <p>macOS: brew install tesseract Ubuntu: sudo apt-get install tesseract-ocr Windows: Download from https://github.com/UB-Mannheim/tesseract/wiki</p> Python Package <p>pip install pytesseract</p>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCRConfig","title":"TesseractOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Tesseract OCR extractor.</p> <p>This is a single-backend model (CPU only, requires system Tesseract).</p> Example <pre><code>config = TesseractOCRConfig(languages=[\"eng\", \"fra\"], psm=3)\nocr = TesseractOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCR","title":"TesseractOCR","text":"<pre><code>TesseractOCR(config: TesseractOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>Tesseract OCR extractor.</p> <p>Single-backend model (CPU only). Requires system Tesseract installation.</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\n\nocr = TesseractOCR(config=TesseractOCRConfig(languages=[\"eng\"]))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize Tesseract OCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>TesseractOCRConfig</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If Tesseract is not installed</p> <code>ImportError</code> <p>If pytesseract is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def __init__(self, config: TesseractOCRConfig):\n    \"\"\"\n    Initialize Tesseract OCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        RuntimeError: If Tesseract is not installed\n        ImportError: If pytesseract is not installed\n    \"\"\"\n    self.config = config\n    self._pytesseract = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks at word level</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with detected text blocks at word level\n    \"\"\"\n    if self._pytesseract is None:\n        raise RuntimeError(\"Tesseract not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Build config string\n    config = f\"--oem {self.config.oem} --psm {self.config.psm}\"\n    if self.config.config_params:\n        for key, value in self.config.config_params.items():\n            config += f\" -c {key}={value}\"\n\n    # Language string\n    lang_str = \"+\".join(self.config.languages)\n\n    # Get detailed data (word-level boxes)\n    data = self._pytesseract.image_to_data(\n        pil_image,\n        lang=lang_str,\n        config=config,\n        output_type=self._pytesseract.Output.DICT,\n    )\n\n    # Parse results into TextBlocks\n    text_blocks = []\n    full_text_parts = []\n\n    n_boxes = len(data[\"text\"])\n    for i in range(n_boxes):\n        text = data[\"text\"][i].strip()\n        # Safely convert conf to float (handles string values from some Tesseract versions)\n        try:\n            conf = float(data[\"conf\"][i])\n        except (ValueError, TypeError):\n            conf = -1\n\n        # Skip empty text or low confidence (-1 means no confidence)\n        if not text or conf == -1:\n            continue\n\n        # Tesseract returns confidence as 0-100, normalize to 0-1\n        confidence = conf / 100.0\n\n        # Get bounding box\n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n\n        bbox = BoundingBox(\n            x1=float(x),\n            y1=float(y),\n            x2=float(x + w),\n            y2=float(y + h),\n        )\n\n        text_blocks.append(\n            TextBlock(\n                text=text,\n                bbox=bbox,\n                confidence=confidence,\n                granularity=OCRGranularity.WORD,\n                language=lang_str,\n            )\n        )\n\n        full_text_parts.append(text)\n\n    # Sort by position (top to bottom, left to right)\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\" \".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/overview/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCR.extract_lines","title":"extract_lines","text":"<pre><code>extract_lines(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR and return line-level blocks.</p> <p>Groups words into lines based on Tesseract's line detection.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with line-level text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def extract_lines(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR and return line-level blocks.\n\n    Groups words into lines based on Tesseract's line detection.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with line-level text blocks\n    \"\"\"\n    if self._pytesseract is None:\n        raise RuntimeError(\"Tesseract not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Build config string (including config_params like extract method)\n    config = f\"--oem {self.config.oem} --psm {self.config.psm}\"\n    if self.config.config_params:\n        for key, value in self.config.config_params.items():\n            config += f\" -c {key}={value}\"\n\n    # Language string\n    lang_str = \"+\".join(self.config.languages)\n\n    # Get detailed data\n    data = self._pytesseract.image_to_data(\n        pil_image,\n        lang=lang_str,\n        config=config,\n        output_type=self._pytesseract.Output.DICT,\n    )\n\n    # Group words into lines\n    lines: Dict[tuple, Dict] = {}\n    n_boxes = len(data[\"text\"])\n\n    for i in range(n_boxes):\n        text = data[\"text\"][i].strip()\n        # Safely convert conf to float (handles string values from some Tesseract versions)\n        try:\n            conf = float(data[\"conf\"][i])\n        except (ValueError, TypeError):\n            conf = -1\n\n        if not text or conf == -1:\n            continue\n\n        # Tesseract provides block_num, par_num, line_num\n        line_key = (data[\"block_num\"][i], data[\"par_num\"][i], data[\"line_num\"][i])\n\n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n\n        if line_key not in lines:\n            lines[line_key] = {\n                \"words\": [],\n                \"confidences\": [],\n                \"x1\": x,\n                \"y1\": y,\n                \"x2\": x + w,\n                \"y2\": y + h,\n            }\n\n        lines[line_key][\"words\"].append(text)\n        lines[line_key][\"confidences\"].append(conf / 100.0)\n        lines[line_key][\"x1\"] = min(lines[line_key][\"x1\"], x)\n        lines[line_key][\"y1\"] = min(lines[line_key][\"y1\"], y)\n        lines[line_key][\"x2\"] = max(lines[line_key][\"x2\"], x + w)\n        lines[line_key][\"y2\"] = max(lines[line_key][\"y2\"], y + h)\n\n    # Convert to TextBlocks\n    text_blocks = []\n    full_text_parts = []\n\n    for line_key in sorted(lines.keys()):\n        line = lines[line_key]\n        line_text = \" \".join(line[\"words\"])\n        avg_conf = sum(line[\"confidences\"]) / len(line[\"confidences\"])\n\n        bbox = BoundingBox(\n            x1=float(line[\"x1\"]),\n            y1=float(line[\"y1\"]),\n            x2=float(line[\"x2\"]),\n            y2=float(line[\"y2\"]),\n        )\n\n        text_blocks.append(\n            TextBlock(\n                text=line_text,\n                bbox=bbox,\n                confidence=avg_conf,\n                granularity=OCRGranularity.LINE,\n                language=lang_str,\n            )\n        )\n\n        full_text_parts.append(line_text)\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\"\\n\".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/paddleocr/","title":"PaddleOCR","text":"<p>PaddleOCR extractor.</p> <p>PaddleOCR is an OCR toolkit developed by Baidu/PaddlePaddle. - Excellent for CJK languages (Chinese, Japanese, Korean) - GPU accelerated - Supports layout analysis + OCR</p> Python Package <p>pip install paddleocr paddlepaddle  # CPU version pip install paddleocr paddlepaddle-gpu  # GPU version</p> Model Download Location <p>By default, PaddleOCR downloads models to ~/.paddleocr/</p>"},{"location":"reference/tasks/ocr_extraction/paddleocr/#omnidocs.tasks.ocr_extraction.paddleocr.PaddleOCRConfig","title":"PaddleOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for PaddleOCR extractor.</p> <p>This is a single-backend model (PaddlePaddle - CPU/GPU).</p> Example <pre><code>config = PaddleOCRConfig(lang=\"ch\", device=\"gpu\")\nocr = PaddleOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/paddleocr/#omnidocs.tasks.ocr_extraction.paddleocr.PaddleOCR","title":"PaddleOCR","text":"<pre><code>PaddleOCR(config: PaddleOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>PaddleOCR text extractor.</p> <p>Single-backend model (PaddlePaddle - CPU/GPU).</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import PaddleOCR, PaddleOCRConfig\n\nocr = PaddleOCR(config=PaddleOCRConfig(lang=\"en\", device=\"cpu\"))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize PaddleOCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>PaddleOCRConfig</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>If paddleocr or paddlepaddle is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/paddleocr.py</code> <pre><code>def __init__(self, config: PaddleOCRConfig):\n    \"\"\"\n    Initialize PaddleOCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        ImportError: If paddleocr or paddlepaddle is not installed\n    \"\"\"\n    self.config = config\n    self._ocr = None\n\n    # Normalize language code\n    self._lang = LANG_CODES.get(config.lang.lower(), config.lang)\n\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/paddleocr/#omnidocs.tasks.ocr_extraction.paddleocr.PaddleOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/paddleocr.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with detected text blocks\n    \"\"\"\n    if self._ocr is None:\n        raise RuntimeError(\"PaddleOCR not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Convert to numpy array\n    image_array = np.array(pil_image)\n\n    # Run PaddleOCR v3.x - use predict() method\n    results = self._ocr.predict(image_array)\n\n    # Parse results\n    text_blocks = []\n\n    # PaddleOCR may return None or empty results\n    if results is None or len(results) == 0:\n        return OCROutput(\n            text_blocks=[],\n            full_text=\"\",\n            image_width=image_width,\n            image_height=image_height,\n            model_name=self.MODEL_NAME,\n            languages_detected=[self._lang],\n        )\n\n    # PaddleOCR v3.x returns list of dicts with 'rec_texts', 'rec_scores', 'dt_polys'\n    for result in results:\n        if result is None:\n            continue\n\n        rec_texts = result.get(\"rec_texts\", [])\n        rec_scores = result.get(\"rec_scores\", [])\n        dt_polys = result.get(\"dt_polys\", [])\n\n        for i, text in enumerate(rec_texts):\n            if not text.strip():\n                continue\n\n            confidence = rec_scores[i] if i &lt; len(rec_scores) else 1.0\n\n            # Get polygon and convert to list\n            polygon: Optional[List[List[float]]] = None\n            if i &lt; len(dt_polys) and dt_polys[i] is not None:\n                poly_array = dt_polys[i]\n                # Handle numpy array\n                if hasattr(poly_array, \"tolist\"):\n                    polygon = poly_array.tolist()\n                else:\n                    polygon = list(poly_array)\n\n            # Convert polygon to bbox\n            if polygon:\n                bbox = BoundingBox.from_polygon(polygon)\n            else:\n                bbox = BoundingBox(x1=0, y1=0, x2=0, y2=0)\n\n            text_blocks.append(\n                TextBlock(\n                    text=text,\n                    bbox=bbox,\n                    confidence=float(confidence),\n                    granularity=OCRGranularity.LINE,\n                    polygon=polygon,\n                    language=self._lang,\n                )\n            )\n\n    # Sort by position (top to bottom, left to right)\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    # Build full_text from sorted blocks to ensure reading order\n    full_text = \" \".join(block.text for block in text_blocks)\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=full_text,\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=[self._lang],\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/tesseract/","title":"Tesseract","text":"<p>Tesseract OCR extractor.</p> <p>Tesseract is an open-source OCR engine maintained by Google. - CPU-based (no GPU required) - Requires system installation of Tesseract - Good for printed text, supports 100+ languages</p> System Requirements <p>macOS: brew install tesseract Ubuntu: sudo apt-get install tesseract-ocr Windows: Download from https://github.com/UB-Mannheim/tesseract/wiki</p> Python Package <p>pip install pytesseract</p>"},{"location":"reference/tasks/ocr_extraction/tesseract/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCRConfig","title":"TesseractOCRConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Tesseract OCR extractor.</p> <p>This is a single-backend model (CPU only, requires system Tesseract).</p> Example <pre><code>config = TesseractOCRConfig(languages=[\"eng\", \"fra\"], psm=3)\nocr = TesseractOCR(config=config)\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/tesseract/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCR","title":"TesseractOCR","text":"<pre><code>TesseractOCR(config: TesseractOCRConfig)\n</code></pre> <p>               Bases: <code>BaseOCRExtractor</code></p> <p>Tesseract OCR extractor.</p> <p>Single-backend model (CPU only). Requires system Tesseract installation.</p> Example <pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractOCRConfig\n\nocr = TesseractOCR(config=TesseractOCRConfig(languages=[\"eng\"]))\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n        print(f\"'{block.text}' @ {block.bbox.to_list()}\")\n</code></pre> <p>Initialize Tesseract OCR extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>Configuration object</p> <p> TYPE: <code>TesseractOCRConfig</code> </p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If Tesseract is not installed</p> <code>ImportError</code> <p>If pytesseract is not installed</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def __init__(self, config: TesseractOCRConfig):\n    \"\"\"\n    Initialize Tesseract OCR extractor.\n\n    Args:\n        config: Configuration object\n\n    Raises:\n        RuntimeError: If Tesseract is not installed\n        ImportError: If pytesseract is not installed\n    \"\"\"\n    self.config = config\n    self._pytesseract = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/tesseract/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCR.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR on an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with detected text blocks at word level</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def extract(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR on an image.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with detected text blocks at word level\n    \"\"\"\n    if self._pytesseract is None:\n        raise RuntimeError(\"Tesseract not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Build config string\n    config = f\"--oem {self.config.oem} --psm {self.config.psm}\"\n    if self.config.config_params:\n        for key, value in self.config.config_params.items():\n            config += f\" -c {key}={value}\"\n\n    # Language string\n    lang_str = \"+\".join(self.config.languages)\n\n    # Get detailed data (word-level boxes)\n    data = self._pytesseract.image_to_data(\n        pil_image,\n        lang=lang_str,\n        config=config,\n        output_type=self._pytesseract.Output.DICT,\n    )\n\n    # Parse results into TextBlocks\n    text_blocks = []\n    full_text_parts = []\n\n    n_boxes = len(data[\"text\"])\n    for i in range(n_boxes):\n        text = data[\"text\"][i].strip()\n        # Safely convert conf to float (handles string values from some Tesseract versions)\n        try:\n            conf = float(data[\"conf\"][i])\n        except (ValueError, TypeError):\n            conf = -1\n\n        # Skip empty text or low confidence (-1 means no confidence)\n        if not text or conf == -1:\n            continue\n\n        # Tesseract returns confidence as 0-100, normalize to 0-1\n        confidence = conf / 100.0\n\n        # Get bounding box\n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n\n        bbox = BoundingBox(\n            x1=float(x),\n            y1=float(y),\n            x2=float(x + w),\n            y2=float(y + h),\n        )\n\n        text_blocks.append(\n            TextBlock(\n                text=text,\n                bbox=bbox,\n                confidence=confidence,\n                granularity=OCRGranularity.WORD,\n                language=lang_str,\n            )\n        )\n\n        full_text_parts.append(text)\n\n    # Sort by position (top to bottom, left to right)\n    text_blocks.sort(key=lambda b: (b.bbox.y1, b.bbox.x1))\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\" \".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/ocr_extraction/tesseract/#omnidocs.tasks.ocr_extraction.tesseract.TesseractOCR.extract_lines","title":"extract_lines","text":"<pre><code>extract_lines(\n    image: Union[Image, ndarray, str, Path],\n) -&gt; OCROutput\n</code></pre> <p>Run OCR and return line-level blocks.</p> <p>Groups words into lines based on Tesseract's line detection.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> RETURNS DESCRIPTION <code>OCROutput</code> <p>OCROutput with line-level text blocks</p> Source code in <code>omnidocs/tasks/ocr_extraction/tesseract.py</code> <pre><code>def extract_lines(self, image: Union[Image.Image, np.ndarray, str, Path]) -&gt; OCROutput:\n    \"\"\"\n    Run OCR and return line-level blocks.\n\n    Groups words into lines based on Tesseract's line detection.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or path)\n\n    Returns:\n        OCROutput with line-level text blocks\n    \"\"\"\n    if self._pytesseract is None:\n        raise RuntimeError(\"Tesseract not initialized. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    image_width, image_height = pil_image.size\n\n    # Build config string (including config_params like extract method)\n    config = f\"--oem {self.config.oem} --psm {self.config.psm}\"\n    if self.config.config_params:\n        for key, value in self.config.config_params.items():\n            config += f\" -c {key}={value}\"\n\n    # Language string\n    lang_str = \"+\".join(self.config.languages)\n\n    # Get detailed data\n    data = self._pytesseract.image_to_data(\n        pil_image,\n        lang=lang_str,\n        config=config,\n        output_type=self._pytesseract.Output.DICT,\n    )\n\n    # Group words into lines\n    lines: Dict[tuple, Dict] = {}\n    n_boxes = len(data[\"text\"])\n\n    for i in range(n_boxes):\n        text = data[\"text\"][i].strip()\n        # Safely convert conf to float (handles string values from some Tesseract versions)\n        try:\n            conf = float(data[\"conf\"][i])\n        except (ValueError, TypeError):\n            conf = -1\n\n        if not text or conf == -1:\n            continue\n\n        # Tesseract provides block_num, par_num, line_num\n        line_key = (data[\"block_num\"][i], data[\"par_num\"][i], data[\"line_num\"][i])\n\n        x = data[\"left\"][i]\n        y = data[\"top\"][i]\n        w = data[\"width\"][i]\n        h = data[\"height\"][i]\n\n        if line_key not in lines:\n            lines[line_key] = {\n                \"words\": [],\n                \"confidences\": [],\n                \"x1\": x,\n                \"y1\": y,\n                \"x2\": x + w,\n                \"y2\": y + h,\n            }\n\n        lines[line_key][\"words\"].append(text)\n        lines[line_key][\"confidences\"].append(conf / 100.0)\n        lines[line_key][\"x1\"] = min(lines[line_key][\"x1\"], x)\n        lines[line_key][\"y1\"] = min(lines[line_key][\"y1\"], y)\n        lines[line_key][\"x2\"] = max(lines[line_key][\"x2\"], x + w)\n        lines[line_key][\"y2\"] = max(lines[line_key][\"y2\"], y + h)\n\n    # Convert to TextBlocks\n    text_blocks = []\n    full_text_parts = []\n\n    for line_key in sorted(lines.keys()):\n        line = lines[line_key]\n        line_text = \" \".join(line[\"words\"])\n        avg_conf = sum(line[\"confidences\"]) / len(line[\"confidences\"])\n\n        bbox = BoundingBox(\n            x1=float(line[\"x1\"]),\n            y1=float(line[\"y1\"]),\n            x2=float(line[\"x2\"]),\n            y2=float(line[\"y2\"]),\n        )\n\n        text_blocks.append(\n            TextBlock(\n                text=line_text,\n                bbox=bbox,\n                confidence=avg_conf,\n                granularity=OCRGranularity.LINE,\n                language=lang_str,\n            )\n        )\n\n        full_text_parts.append(line_text)\n\n    return OCROutput(\n        text_blocks=text_blocks,\n        full_text=\"\\n\".join(full_text_parts),\n        image_width=image_width,\n        image_height=image_height,\n        model_name=self.MODEL_NAME,\n        languages_detected=self.config.languages,\n    )\n</code></pre>"},{"location":"reference/tasks/reading_order/base/","title":"Base","text":"<p>Base class for reading order predictors.</p> <p>Defines the abstract interface that all reading order predictors must implement.</p>"},{"location":"reference/tasks/reading_order/base/#omnidocs.tasks.reading_order.base.BaseReadingOrderPredictor","title":"BaseReadingOrderPredictor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for reading order predictors.</p> <p>Reading order predictors take layout detection and OCR results and produce a properly ordered sequence of document elements.</p> Example <pre><code>predictor = RuleBasedReadingOrderPredictor()\n\n# Get layout and OCR\nlayout = layout_extractor.extract(image)\nocr = ocr_extractor.extract(image)\n\n# Predict reading order\nresult = predictor.predict(layout, ocr)\n\n# Or with multiple pages\nresults = predictor.predict_multi_page(layouts, ocrs)\n</code></pre>"},{"location":"reference/tasks/reading_order/base/#omnidocs.tasks.reading_order.base.BaseReadingOrderPredictor.predict","title":"predict  <code>abstractmethod</code>","text":"<pre><code>predict(\n    layout: LayoutOutput,\n    ocr: Optional[OCROutput] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput\n</code></pre> <p>Predict reading order for a single page.</p> PARAMETER DESCRIPTION <code>layout</code> <p>Layout detection results with bounding boxes</p> <p> TYPE: <code>LayoutOutput</code> </p> <code>ocr</code> <p>Optional OCR results. If provided, text will be  matched to layout elements by bbox overlap.</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> <code>page_no</code> <p>Page number (for multi-page documents)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ReadingOrderOutput</code> <p>ReadingOrderOutput with ordered elements and associations</p> Example <pre><code>layout = layout_extractor.extract(page_image)\nocr = ocr_extractor.extract(page_image)\norder = predictor.predict(layout, ocr, page_no=0)\n</code></pre> Source code in <code>omnidocs/tasks/reading_order/base.py</code> <pre><code>@abstractmethod\ndef predict(\n    self,\n    layout: \"LayoutOutput\",\n    ocr: Optional[\"OCROutput\"] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput:\n    \"\"\"\n    Predict reading order for a single page.\n\n    Args:\n        layout: Layout detection results with bounding boxes\n        ocr: Optional OCR results. If provided, text will be\n             matched to layout elements by bbox overlap.\n        page_no: Page number (for multi-page documents)\n\n    Returns:\n        ReadingOrderOutput with ordered elements and associations\n\n    Example:\n        ```python\n        layout = layout_extractor.extract(page_image)\n        ocr = ocr_extractor.extract(page_image)\n        order = predictor.predict(layout, ocr, page_no=0)\n        ```\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/reading_order/base/#omnidocs.tasks.reading_order.base.BaseReadingOrderPredictor.predict_multi_page","title":"predict_multi_page","text":"<pre><code>predict_multi_page(\n    layouts: List[LayoutOutput],\n    ocrs: Optional[List[OCROutput]] = None,\n) -&gt; List[ReadingOrderOutput]\n</code></pre> <p>Predict reading order for multiple pages.</p> PARAMETER DESCRIPTION <code>layouts</code> <p>List of layout results, one per page</p> <p> TYPE: <code>List[LayoutOutput]</code> </p> <code>ocrs</code> <p>Optional list of OCR results, one per page</p> <p> TYPE: <code>Optional[List[OCROutput]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[ReadingOrderOutput]</code> <p>List of ReadingOrderOutput, one per page</p> Source code in <code>omnidocs/tasks/reading_order/base.py</code> <pre><code>def predict_multi_page(\n    self,\n    layouts: List[\"LayoutOutput\"],\n    ocrs: Optional[List[\"OCROutput\"]] = None,\n) -&gt; List[ReadingOrderOutput]:\n    \"\"\"\n    Predict reading order for multiple pages.\n\n    Args:\n        layouts: List of layout results, one per page\n        ocrs: Optional list of OCR results, one per page\n\n    Returns:\n        List of ReadingOrderOutput, one per page\n    \"\"\"\n    results = []\n\n    for i, layout in enumerate(layouts):\n        ocr = ocrs[i] if ocrs else None\n        result = self.predict(layout, ocr, page_no=i)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/reading_order/models/","title":"Models","text":"<p>Pydantic models for reading order prediction.</p> <p>Takes layout detection and OCR results, produces ordered element sequence with caption and footnote associations.</p> Example <pre><code># Get layout and OCR\nlayout = layout_extractor.extract(image)\nocr = ocr_extractor.extract(image)\n\n# Predict reading order\nreading_order = predictor.predict(layout, ocr)\n\n# Iterate in reading order\nfor element in reading_order.ordered_elements:\n    print(f\"{element.index}: [{element.element_type}] {element.text[:50]}...\")\n\n# Get caption associations\nfor fig_id, caption_ids in reading_order.caption_map.items():\n    print(f\"Figure {fig_id} has captions: {caption_ids}\")\n</code></pre>"},{"location":"reference/tasks/reading_order/models/#omnidocs.tasks.reading_order.models.ElementType","title":"ElementType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of document element for reading order.</p>"},{"location":"reference/tasks/reading_order/models/#omnidocs.tasks.reading_order.models.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box in pixel coordinates.</p>"},{"location":"reference/tasks/reading_order/models/#omnidocs.tasks.reading_order.models.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/reading_order/models/#omnidocs.tasks.reading_order.models.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/reading_order/models/#omnidocs.tasks.reading_order.models.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/reading_order/models/#omnidocs.tasks.reading_order.models.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/reading_order/models/#omnidocs.tasks.reading_order.models.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/reading_order/models/#omnidocs.tasks.reading_order.models.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/reading_order/models/#omnidocs.tasks.reading_order.models.OrderedElement","title":"OrderedElement","text":"<p>               Bases: <code>BaseModel</code></p> <p>A document element with its reading order position.</p> <p>Combines layout detection results with OCR text and assigns a reading order index.</p>"},{"location":"reference/tasks/reading_order/models/#omnidocs.tasks.reading_order.models.OrderedElement.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"index\": self.index,\n        \"element_type\": self.element_type.value,\n        \"bbox\": self.bbox.to_list(),\n        \"text\": self.text,\n        \"confidence\": self.confidence,\n        \"page_no\": self.page_no,\n        \"original_id\": self.original_id,\n    }\n</code></pre>"},{"location":"reference/tasks/reading_order/models/#omnidocs.tasks.reading_order.models.ReadingOrderOutput","title":"ReadingOrderOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete reading order prediction result.</p> <p>Provides: - Ordered list of document elements - Caption-to-element associations - Footnote-to-element associations - Merge suggestions for split elements</p> Example <pre><code>result = predictor.predict(layout, ocr)\n\n# Get full text in reading order\nfull_text = result.get_full_text()\n\n# Get elements by type\ntables = result.get_elements_by_type(ElementType.TABLE)\n\n# Find caption for a figure\ncaptions = result.get_captions_for(figure_element.original_id)\n</code></pre>"},{"location":"reference/tasks/reading_order/models/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.element_count","title":"element_count  <code>property</code>","text":"<pre><code>element_count: int\n</code></pre> <p>Total number of ordered elements.</p>"},{"location":"reference/tasks/reading_order/models/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.get_full_text","title":"get_full_text","text":"<pre><code>get_full_text(separator: str = '\\n\\n') -&gt; str\n</code></pre> <p>Get concatenated text in reading order.</p> <p>Excludes page headers, footers, captions, and footnotes from main text flow.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def get_full_text(self, separator: str = \"\\n\\n\") -&gt; str:\n    \"\"\"\n    Get concatenated text in reading order.\n\n    Excludes page headers, footers, captions, and footnotes\n    from main text flow.\n    \"\"\"\n    main_elements = [\n        e\n        for e in self.ordered_elements\n        if e.element_type\n        not in (\n            ElementType.PAGE_HEADER,\n            ElementType.PAGE_FOOTER,\n            ElementType.CAPTION,\n            ElementType.FOOTNOTE,\n        )\n    ]\n    return separator.join(e.text for e in main_elements if e.text)\n</code></pre>"},{"location":"reference/tasks/reading_order/models/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.get_elements_by_type","title":"get_elements_by_type","text":"<pre><code>get_elements_by_type(\n    element_type: ElementType,\n) -&gt; List[OrderedElement]\n</code></pre> <p>Filter elements by type.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def get_elements_by_type(self, element_type: ElementType) -&gt; List[OrderedElement]:\n    \"\"\"Filter elements by type.\"\"\"\n    return [e for e in self.ordered_elements if e.element_type == element_type]\n</code></pre>"},{"location":"reference/tasks/reading_order/models/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.get_captions_for","title":"get_captions_for","text":"<pre><code>get_captions_for(element_id: int) -&gt; List[OrderedElement]\n</code></pre> <p>Get caption elements for a given element ID.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def get_captions_for(self, element_id: int) -&gt; List[OrderedElement]:\n    \"\"\"Get caption elements for a given element ID.\"\"\"\n    caption_ids = self.caption_map.get(element_id, [])\n    return [e for e in self.ordered_elements if e.original_id in caption_ids]\n</code></pre>"},{"location":"reference/tasks/reading_order/models/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.get_footnotes_for","title":"get_footnotes_for","text":"<pre><code>get_footnotes_for(element_id: int) -&gt; List[OrderedElement]\n</code></pre> <p>Get footnote elements for a given element ID.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def get_footnotes_for(self, element_id: int) -&gt; List[OrderedElement]:\n    \"\"\"Get footnote elements for a given element ID.\"\"\"\n    footnote_ids = self.footnote_map.get(element_id, [])\n    return [e for e in self.ordered_elements if e.original_id in footnote_ids]\n</code></pre>"},{"location":"reference/tasks/reading_order/models/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"ordered_elements\": [e.to_dict() for e in self.ordered_elements],\n        \"caption_map\": self.caption_map,\n        \"footnote_map\": self.footnote_map,\n        \"merge_map\": self.merge_map,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"element_count\": self.element_count,\n    }\n</code></pre>"},{"location":"reference/tasks/reading_order/models/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save to JSON file.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"Save to JSON file.\"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/reading_order/models/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(\n    file_path: Union[str, Path],\n) -&gt; ReadingOrderOutput\n</code></pre> <p>Load from JSON file.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"ReadingOrderOutput\":\n    \"\"\"Load from JSON file.\"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/","title":"Overview","text":"<p>Reading Order Module.</p> <p>Provides predictors for determining the logical reading sequence of document elements based on layout detection and spatial analysis.</p> Available Predictors <ul> <li>RuleBasedReadingOrderPredictor: Rule-based predictor using R-tree indexing</li> </ul> Example <pre><code>from omnidocs.tasks.reading_order import RuleBasedReadingOrderPredictor\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\n# Initialize components\nlayout_extractor = DocLayoutYOLO(config=DocLayoutYOLOConfig())\nocr = EasyOCR(config=EasyOCRConfig())\npredictor = RuleBasedReadingOrderPredictor()\n\n# Process document\nlayout = layout_extractor.extract(image)\nocr_result = ocr.extract(image)\nreading_order = predictor.predict(layout, ocr_result)\n\n# Get text in reading order\ntext = reading_order.get_full_text()\n\n# Get elements by type\ntables = reading_order.get_elements_by_type(ElementType.TABLE)\n\n# Get caption associations\nfor elem in reading_order.ordered_elements:\n    if elem.element_type == ElementType.FIGURE:\n        captions = reading_order.get_captions_for(elem.original_id)\n        print(f\"Figure {elem.original_id} captions: {[c.text for c in captions]}\")\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.BaseReadingOrderPredictor","title":"BaseReadingOrderPredictor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for reading order predictors.</p> <p>Reading order predictors take layout detection and OCR results and produce a properly ordered sequence of document elements.</p> Example <pre><code>predictor = RuleBasedReadingOrderPredictor()\n\n# Get layout and OCR\nlayout = layout_extractor.extract(image)\nocr = ocr_extractor.extract(image)\n\n# Predict reading order\nresult = predictor.predict(layout, ocr)\n\n# Or with multiple pages\nresults = predictor.predict_multi_page(layouts, ocrs)\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.BaseReadingOrderPredictor.predict","title":"predict  <code>abstractmethod</code>","text":"<pre><code>predict(\n    layout: LayoutOutput,\n    ocr: Optional[OCROutput] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput\n</code></pre> <p>Predict reading order for a single page.</p> PARAMETER DESCRIPTION <code>layout</code> <p>Layout detection results with bounding boxes</p> <p> TYPE: <code>LayoutOutput</code> </p> <code>ocr</code> <p>Optional OCR results. If provided, text will be  matched to layout elements by bbox overlap.</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> <code>page_no</code> <p>Page number (for multi-page documents)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ReadingOrderOutput</code> <p>ReadingOrderOutput with ordered elements and associations</p> Example <pre><code>layout = layout_extractor.extract(page_image)\nocr = ocr_extractor.extract(page_image)\norder = predictor.predict(layout, ocr, page_no=0)\n</code></pre> Source code in <code>omnidocs/tasks/reading_order/base.py</code> <pre><code>@abstractmethod\ndef predict(\n    self,\n    layout: \"LayoutOutput\",\n    ocr: Optional[\"OCROutput\"] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput:\n    \"\"\"\n    Predict reading order for a single page.\n\n    Args:\n        layout: Layout detection results with bounding boxes\n        ocr: Optional OCR results. If provided, text will be\n             matched to layout elements by bbox overlap.\n        page_no: Page number (for multi-page documents)\n\n    Returns:\n        ReadingOrderOutput with ordered elements and associations\n\n    Example:\n        ```python\n        layout = layout_extractor.extract(page_image)\n        ocr = ocr_extractor.extract(page_image)\n        order = predictor.predict(layout, ocr, page_no=0)\n        ```\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.BaseReadingOrderPredictor.predict_multi_page","title":"predict_multi_page","text":"<pre><code>predict_multi_page(\n    layouts: List[LayoutOutput],\n    ocrs: Optional[List[OCROutput]] = None,\n) -&gt; List[ReadingOrderOutput]\n</code></pre> <p>Predict reading order for multiple pages.</p> PARAMETER DESCRIPTION <code>layouts</code> <p>List of layout results, one per page</p> <p> TYPE: <code>List[LayoutOutput]</code> </p> <code>ocrs</code> <p>Optional list of OCR results, one per page</p> <p> TYPE: <code>Optional[List[OCROutput]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[ReadingOrderOutput]</code> <p>List of ReadingOrderOutput, one per page</p> Source code in <code>omnidocs/tasks/reading_order/base.py</code> <pre><code>def predict_multi_page(\n    self,\n    layouts: List[\"LayoutOutput\"],\n    ocrs: Optional[List[\"OCROutput\"]] = None,\n) -&gt; List[ReadingOrderOutput]:\n    \"\"\"\n    Predict reading order for multiple pages.\n\n    Args:\n        layouts: List of layout results, one per page\n        ocrs: Optional list of OCR results, one per page\n\n    Returns:\n        List of ReadingOrderOutput, one per page\n    \"\"\"\n    results = []\n\n    for i, layout in enumerate(layouts):\n        ocr = ocrs[i] if ocrs else None\n        result = self.predict(layout, ocr, page_no=i)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box in pixel coordinates.</p>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.ElementType","title":"ElementType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of document element for reading order.</p>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.OrderedElement","title":"OrderedElement","text":"<p>               Bases: <code>BaseModel</code></p> <p>A document element with its reading order position.</p> <p>Combines layout detection results with OCR text and assigns a reading order index.</p>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.OrderedElement.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"index\": self.index,\n        \"element_type\": self.element_type.value,\n        \"bbox\": self.bbox.to_list(),\n        \"text\": self.text,\n        \"confidence\": self.confidence,\n        \"page_no\": self.page_no,\n        \"original_id\": self.original_id,\n    }\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.ReadingOrderOutput","title":"ReadingOrderOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete reading order prediction result.</p> <p>Provides: - Ordered list of document elements - Caption-to-element associations - Footnote-to-element associations - Merge suggestions for split elements</p> Example <pre><code>result = predictor.predict(layout, ocr)\n\n# Get full text in reading order\nfull_text = result.get_full_text()\n\n# Get elements by type\ntables = result.get_elements_by_type(ElementType.TABLE)\n\n# Find caption for a figure\ncaptions = result.get_captions_for(figure_element.original_id)\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.ReadingOrderOutput.element_count","title":"element_count  <code>property</code>","text":"<pre><code>element_count: int\n</code></pre> <p>Total number of ordered elements.</p>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.ReadingOrderOutput.get_full_text","title":"get_full_text","text":"<pre><code>get_full_text(separator: str = '\\n\\n') -&gt; str\n</code></pre> <p>Get concatenated text in reading order.</p> <p>Excludes page headers, footers, captions, and footnotes from main text flow.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def get_full_text(self, separator: str = \"\\n\\n\") -&gt; str:\n    \"\"\"\n    Get concatenated text in reading order.\n\n    Excludes page headers, footers, captions, and footnotes\n    from main text flow.\n    \"\"\"\n    main_elements = [\n        e\n        for e in self.ordered_elements\n        if e.element_type\n        not in (\n            ElementType.PAGE_HEADER,\n            ElementType.PAGE_FOOTER,\n            ElementType.CAPTION,\n            ElementType.FOOTNOTE,\n        )\n    ]\n    return separator.join(e.text for e in main_elements if e.text)\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.ReadingOrderOutput.get_elements_by_type","title":"get_elements_by_type","text":"<pre><code>get_elements_by_type(\n    element_type: ElementType,\n) -&gt; List[OrderedElement]\n</code></pre> <p>Filter elements by type.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def get_elements_by_type(self, element_type: ElementType) -&gt; List[OrderedElement]:\n    \"\"\"Filter elements by type.\"\"\"\n    return [e for e in self.ordered_elements if e.element_type == element_type]\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.ReadingOrderOutput.get_captions_for","title":"get_captions_for","text":"<pre><code>get_captions_for(element_id: int) -&gt; List[OrderedElement]\n</code></pre> <p>Get caption elements for a given element ID.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def get_captions_for(self, element_id: int) -&gt; List[OrderedElement]:\n    \"\"\"Get caption elements for a given element ID.\"\"\"\n    caption_ids = self.caption_map.get(element_id, [])\n    return [e for e in self.ordered_elements if e.original_id in caption_ids]\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.ReadingOrderOutput.get_footnotes_for","title":"get_footnotes_for","text":"<pre><code>get_footnotes_for(element_id: int) -&gt; List[OrderedElement]\n</code></pre> <p>Get footnote elements for a given element ID.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def get_footnotes_for(self, element_id: int) -&gt; List[OrderedElement]:\n    \"\"\"Get footnote elements for a given element ID.\"\"\"\n    footnote_ids = self.footnote_map.get(element_id, [])\n    return [e for e in self.ordered_elements if e.original_id in footnote_ids]\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.ReadingOrderOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"ordered_elements\": [e.to_dict() for e in self.ordered_elements],\n        \"caption_map\": self.caption_map,\n        \"footnote_map\": self.footnote_map,\n        \"merge_map\": self.merge_map,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"element_count\": self.element_count,\n    }\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.ReadingOrderOutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save to JSON file.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"Save to JSON file.\"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.ReadingOrderOutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(\n    file_path: Union[str, Path],\n) -&gt; ReadingOrderOutput\n</code></pre> <p>Load from JSON file.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"ReadingOrderOutput\":\n    \"\"\"Load from JSON file.\"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.RuleBasedReadingOrderPredictor","title":"RuleBasedReadingOrderPredictor","text":"<pre><code>RuleBasedReadingOrderPredictor()\n</code></pre> <p>               Bases: <code>BaseReadingOrderPredictor</code></p> <p>Rule-based reading order predictor using spatial analysis.</p> <p>Uses R-tree spatial indexing and rule-based algorithms to determine the logical reading sequence of document elements. This is a CPU-only implementation that doesn't require GPU resources.</p> <p>Features: - Multi-column layout detection - Header/footer separation - Caption-to-figure/table association - Footnote linking - Element merge suggestions</p> Example <pre><code>from omnidocs.tasks.reading_order import RuleBasedReadingOrderPredictor\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\n# Initialize components\nlayout_extractor = DocLayoutYOLO(config=DocLayoutYOLOConfig())\nocr = EasyOCR(config=EasyOCRConfig())\npredictor = RuleBasedReadingOrderPredictor()\n\n# Process document\nlayout = layout_extractor.extract(image)\nocr_result = ocr.extract(image)\nreading_order = predictor.predict(layout, ocr_result)\n\n# Get text in reading order\ntext = reading_order.get_full_text()\n</code></pre> <p>Initialize the reading order predictor.</p> Source code in <code>omnidocs/tasks/reading_order/rule_based/predictor.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the reading order predictor.\"\"\"\n    self.dilated_page_element = True\n    # Apply horizontal dilation only if less than this page-width normalized threshold\n    self._horizontal_dilation_threshold_norm = 0.15\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.RuleBasedReadingOrderPredictor.predict","title":"predict","text":"<pre><code>predict(\n    layout: LayoutOutput,\n    ocr: Optional[OCROutput] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput\n</code></pre> <p>Predict reading order for a single page.</p> PARAMETER DESCRIPTION <code>layout</code> <p>Layout detection results with bounding boxes</p> <p> TYPE: <code>LayoutOutput</code> </p> <code>ocr</code> <p>Optional OCR results for text content</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> <code>page_no</code> <p>Page number (for multi-page documents)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ReadingOrderOutput</code> <p>ReadingOrderOutput with ordered elements and associations</p> Source code in <code>omnidocs/tasks/reading_order/rule_based/predictor.py</code> <pre><code>def predict(\n    self,\n    layout: \"LayoutOutput\",\n    ocr: Optional[\"OCROutput\"] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput:\n    \"\"\"\n    Predict reading order for a single page.\n\n    Args:\n        layout: Layout detection results with bounding boxes\n        ocr: Optional OCR results for text content\n        page_no: Page number (for multi-page documents)\n\n    Returns:\n        ReadingOrderOutput with ordered elements and associations\n    \"\"\"\n    page_width = layout.image_width\n    page_height = layout.image_height\n\n    # Build text map from OCR if available\n    text_map: Dict[int, str] = {}\n    if ocr:\n        text_map = self._build_text_map(layout, ocr)\n\n    # Convert layout boxes to internal PageElements\n    page_elements: List[_PageElement] = []\n    for i, box in enumerate(layout.bboxes):\n        label_str = box.label.value.lower()\n        element_type = LABEL_TO_ELEMENT_TYPE.get(label_str, ElementType.OTHER)\n\n        # Convert from top-left origin to bottom-left origin\n        elem = _PageElement(\n            cid=i,\n            text=text_map.get(i, \"\"),\n            page_no=page_no,\n            page_width=page_width,\n            page_height=page_height,\n            label=element_type,\n            left=box.bbox.x1,\n            bottom=page_height - box.bbox.y2,  # Convert y2 to bottom\n            right=box.bbox.x2,\n            top=page_height - box.bbox.y1,  # Convert y1 to top\n        )\n        page_elements.append(elem)\n\n    # Run reading order prediction\n    sorted_elements = self._predict_reading_order(page_elements)\n\n    # Get caption associations\n    caption_map = self._find_to_captions(sorted_elements)\n\n    # Get footnote associations\n    footnote_map = self._find_to_footnotes(sorted_elements)\n\n    # Get merge suggestions\n    merge_map = self._predict_merges(sorted_elements)\n\n    # Convert to OrderedElements\n    ordered_elements: List[OrderedElement] = []\n    for idx, elem in enumerate(sorted_elements):\n        # Convert back from bottom-left to top-left origin\n        bbox = BoundingBox(\n            x1=elem.left,\n            y1=page_height - elem.top,\n            x2=elem.right,\n            y2=page_height - elem.bottom,\n        )\n\n        confidence = 1.0\n        if elem.cid &lt; len(layout.bboxes):\n            confidence = layout.bboxes[elem.cid].confidence\n\n        ordered_elem = OrderedElement(\n            index=idx,\n            element_type=elem.label,\n            bbox=bbox,\n            text=elem.text,\n            confidence=confidence,\n            page_no=page_no,\n            original_id=elem.cid,\n        )\n        ordered_elements.append(ordered_elem)\n\n    return ReadingOrderOutput(\n        ordered_elements=ordered_elements,\n        caption_map=caption_map,\n        footnote_map=footnote_map,\n        merge_map=merge_map,\n        image_width=page_width,\n        image_height=page_height,\n        model_name=\"RuleBasedReadingOrderPredictor\",\n    )\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.base","title":"base","text":"<p>Base class for reading order predictors.</p> <p>Defines the abstract interface that all reading order predictors must implement.</p>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.base.BaseReadingOrderPredictor","title":"BaseReadingOrderPredictor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for reading order predictors.</p> <p>Reading order predictors take layout detection and OCR results and produce a properly ordered sequence of document elements.</p> Example <pre><code>predictor = RuleBasedReadingOrderPredictor()\n\n# Get layout and OCR\nlayout = layout_extractor.extract(image)\nocr = ocr_extractor.extract(image)\n\n# Predict reading order\nresult = predictor.predict(layout, ocr)\n\n# Or with multiple pages\nresults = predictor.predict_multi_page(layouts, ocrs)\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.base.BaseReadingOrderPredictor.predict","title":"predict  <code>abstractmethod</code>","text":"<pre><code>predict(\n    layout: LayoutOutput,\n    ocr: Optional[OCROutput] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput\n</code></pre> <p>Predict reading order for a single page.</p> PARAMETER DESCRIPTION <code>layout</code> <p>Layout detection results with bounding boxes</p> <p> TYPE: <code>LayoutOutput</code> </p> <code>ocr</code> <p>Optional OCR results. If provided, text will be  matched to layout elements by bbox overlap.</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> <code>page_no</code> <p>Page number (for multi-page documents)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ReadingOrderOutput</code> <p>ReadingOrderOutput with ordered elements and associations</p> Example <pre><code>layout = layout_extractor.extract(page_image)\nocr = ocr_extractor.extract(page_image)\norder = predictor.predict(layout, ocr, page_no=0)\n</code></pre> Source code in <code>omnidocs/tasks/reading_order/base.py</code> <pre><code>@abstractmethod\ndef predict(\n    self,\n    layout: \"LayoutOutput\",\n    ocr: Optional[\"OCROutput\"] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput:\n    \"\"\"\n    Predict reading order for a single page.\n\n    Args:\n        layout: Layout detection results with bounding boxes\n        ocr: Optional OCR results. If provided, text will be\n             matched to layout elements by bbox overlap.\n        page_no: Page number (for multi-page documents)\n\n    Returns:\n        ReadingOrderOutput with ordered elements and associations\n\n    Example:\n        ```python\n        layout = layout_extractor.extract(page_image)\n        ocr = ocr_extractor.extract(page_image)\n        order = predictor.predict(layout, ocr, page_no=0)\n        ```\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.base.BaseReadingOrderPredictor.predict_multi_page","title":"predict_multi_page","text":"<pre><code>predict_multi_page(\n    layouts: List[LayoutOutput],\n    ocrs: Optional[List[OCROutput]] = None,\n) -&gt; List[ReadingOrderOutput]\n</code></pre> <p>Predict reading order for multiple pages.</p> PARAMETER DESCRIPTION <code>layouts</code> <p>List of layout results, one per page</p> <p> TYPE: <code>List[LayoutOutput]</code> </p> <code>ocrs</code> <p>Optional list of OCR results, one per page</p> <p> TYPE: <code>Optional[List[OCROutput]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[ReadingOrderOutput]</code> <p>List of ReadingOrderOutput, one per page</p> Source code in <code>omnidocs/tasks/reading_order/base.py</code> <pre><code>def predict_multi_page(\n    self,\n    layouts: List[\"LayoutOutput\"],\n    ocrs: Optional[List[\"OCROutput\"]] = None,\n) -&gt; List[ReadingOrderOutput]:\n    \"\"\"\n    Predict reading order for multiple pages.\n\n    Args:\n        layouts: List of layout results, one per page\n        ocrs: Optional list of OCR results, one per page\n\n    Returns:\n        List of ReadingOrderOutput, one per page\n    \"\"\"\n    results = []\n\n    for i, layout in enumerate(layouts):\n        ocr = ocrs[i] if ocrs else None\n        result = self.predict(layout, ocr, page_no=i)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.models","title":"models","text":"<p>Pydantic models for reading order prediction.</p> <p>Takes layout detection and OCR results, produces ordered element sequence with caption and footnote associations.</p> Example <pre><code># Get layout and OCR\nlayout = layout_extractor.extract(image)\nocr = ocr_extractor.extract(image)\n\n# Predict reading order\nreading_order = predictor.predict(layout, ocr)\n\n# Iterate in reading order\nfor element in reading_order.ordered_elements:\n    print(f\"{element.index}: [{element.element_type}] {element.text[:50]}...\")\n\n# Get caption associations\nfor fig_id, caption_ids in reading_order.caption_map.items():\n    print(f\"Figure {fig_id} has captions: {caption_ids}\")\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.models.ElementType","title":"ElementType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of document element for reading order.</p>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.models.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box in pixel coordinates.</p>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.models.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.models.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.models.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.models.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.models.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.models.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.models.OrderedElement","title":"OrderedElement","text":"<p>               Bases: <code>BaseModel</code></p> <p>A document element with its reading order position.</p> <p>Combines layout detection results with OCR text and assigns a reading order index.</p>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.models.OrderedElement.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"index\": self.index,\n        \"element_type\": self.element_type.value,\n        \"bbox\": self.bbox.to_list(),\n        \"text\": self.text,\n        \"confidence\": self.confidence,\n        \"page_no\": self.page_no,\n        \"original_id\": self.original_id,\n    }\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.models.ReadingOrderOutput","title":"ReadingOrderOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete reading order prediction result.</p> <p>Provides: - Ordered list of document elements - Caption-to-element associations - Footnote-to-element associations - Merge suggestions for split elements</p> Example <pre><code>result = predictor.predict(layout, ocr)\n\n# Get full text in reading order\nfull_text = result.get_full_text()\n\n# Get elements by type\ntables = result.get_elements_by_type(ElementType.TABLE)\n\n# Find caption for a figure\ncaptions = result.get_captions_for(figure_element.original_id)\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.element_count","title":"element_count  <code>property</code>","text":"<pre><code>element_count: int\n</code></pre> <p>Total number of ordered elements.</p>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.get_full_text","title":"get_full_text","text":"<pre><code>get_full_text(separator: str = '\\n\\n') -&gt; str\n</code></pre> <p>Get concatenated text in reading order.</p> <p>Excludes page headers, footers, captions, and footnotes from main text flow.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def get_full_text(self, separator: str = \"\\n\\n\") -&gt; str:\n    \"\"\"\n    Get concatenated text in reading order.\n\n    Excludes page headers, footers, captions, and footnotes\n    from main text flow.\n    \"\"\"\n    main_elements = [\n        e\n        for e in self.ordered_elements\n        if e.element_type\n        not in (\n            ElementType.PAGE_HEADER,\n            ElementType.PAGE_FOOTER,\n            ElementType.CAPTION,\n            ElementType.FOOTNOTE,\n        )\n    ]\n    return separator.join(e.text for e in main_elements if e.text)\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.get_elements_by_type","title":"get_elements_by_type","text":"<pre><code>get_elements_by_type(\n    element_type: ElementType,\n) -&gt; List[OrderedElement]\n</code></pre> <p>Filter elements by type.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def get_elements_by_type(self, element_type: ElementType) -&gt; List[OrderedElement]:\n    \"\"\"Filter elements by type.\"\"\"\n    return [e for e in self.ordered_elements if e.element_type == element_type]\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.get_captions_for","title":"get_captions_for","text":"<pre><code>get_captions_for(element_id: int) -&gt; List[OrderedElement]\n</code></pre> <p>Get caption elements for a given element ID.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def get_captions_for(self, element_id: int) -&gt; List[OrderedElement]:\n    \"\"\"Get caption elements for a given element ID.\"\"\"\n    caption_ids = self.caption_map.get(element_id, [])\n    return [e for e in self.ordered_elements if e.original_id in caption_ids]\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.get_footnotes_for","title":"get_footnotes_for","text":"<pre><code>get_footnotes_for(element_id: int) -&gt; List[OrderedElement]\n</code></pre> <p>Get footnote elements for a given element ID.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def get_footnotes_for(self, element_id: int) -&gt; List[OrderedElement]:\n    \"\"\"Get footnote elements for a given element ID.\"\"\"\n    footnote_ids = self.footnote_map.get(element_id, [])\n    return [e for e in self.ordered_elements if e.original_id in footnote_ids]\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"ordered_elements\": [e.to_dict() for e in self.ordered_elements],\n        \"caption_map\": self.caption_map,\n        \"footnote_map\": self.footnote_map,\n        \"merge_map\": self.merge_map,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"element_count\": self.element_count,\n    }\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save to JSON file.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"Save to JSON file.\"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.models.ReadingOrderOutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(\n    file_path: Union[str, Path],\n) -&gt; ReadingOrderOutput\n</code></pre> <p>Load from JSON file.</p> Source code in <code>omnidocs/tasks/reading_order/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"ReadingOrderOutput\":\n    \"\"\"Load from JSON file.\"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.rule_based","title":"rule_based","text":"<p>Rule-based reading order predictor module.</p> <p>Provides rule-based reading order prediction using spatial analysis.</p>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.rule_based.RuleBasedReadingOrderPredictor","title":"RuleBasedReadingOrderPredictor","text":"<pre><code>RuleBasedReadingOrderPredictor()\n</code></pre> <p>               Bases: <code>BaseReadingOrderPredictor</code></p> <p>Rule-based reading order predictor using spatial analysis.</p> <p>Uses R-tree spatial indexing and rule-based algorithms to determine the logical reading sequence of document elements. This is a CPU-only implementation that doesn't require GPU resources.</p> <p>Features: - Multi-column layout detection - Header/footer separation - Caption-to-figure/table association - Footnote linking - Element merge suggestions</p> Example <pre><code>from omnidocs.tasks.reading_order import RuleBasedReadingOrderPredictor\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\n# Initialize components\nlayout_extractor = DocLayoutYOLO(config=DocLayoutYOLOConfig())\nocr = EasyOCR(config=EasyOCRConfig())\npredictor = RuleBasedReadingOrderPredictor()\n\n# Process document\nlayout = layout_extractor.extract(image)\nocr_result = ocr.extract(image)\nreading_order = predictor.predict(layout, ocr_result)\n\n# Get text in reading order\ntext = reading_order.get_full_text()\n</code></pre> <p>Initialize the reading order predictor.</p> Source code in <code>omnidocs/tasks/reading_order/rule_based/predictor.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the reading order predictor.\"\"\"\n    self.dilated_page_element = True\n    # Apply horizontal dilation only if less than this page-width normalized threshold\n    self._horizontal_dilation_threshold_norm = 0.15\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.rule_based.RuleBasedReadingOrderPredictor.predict","title":"predict","text":"<pre><code>predict(\n    layout: LayoutOutput,\n    ocr: Optional[OCROutput] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput\n</code></pre> <p>Predict reading order for a single page.</p> PARAMETER DESCRIPTION <code>layout</code> <p>Layout detection results with bounding boxes</p> <p> TYPE: <code>LayoutOutput</code> </p> <code>ocr</code> <p>Optional OCR results for text content</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> <code>page_no</code> <p>Page number (for multi-page documents)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ReadingOrderOutput</code> <p>ReadingOrderOutput with ordered elements and associations</p> Source code in <code>omnidocs/tasks/reading_order/rule_based/predictor.py</code> <pre><code>def predict(\n    self,\n    layout: \"LayoutOutput\",\n    ocr: Optional[\"OCROutput\"] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput:\n    \"\"\"\n    Predict reading order for a single page.\n\n    Args:\n        layout: Layout detection results with bounding boxes\n        ocr: Optional OCR results for text content\n        page_no: Page number (for multi-page documents)\n\n    Returns:\n        ReadingOrderOutput with ordered elements and associations\n    \"\"\"\n    page_width = layout.image_width\n    page_height = layout.image_height\n\n    # Build text map from OCR if available\n    text_map: Dict[int, str] = {}\n    if ocr:\n        text_map = self._build_text_map(layout, ocr)\n\n    # Convert layout boxes to internal PageElements\n    page_elements: List[_PageElement] = []\n    for i, box in enumerate(layout.bboxes):\n        label_str = box.label.value.lower()\n        element_type = LABEL_TO_ELEMENT_TYPE.get(label_str, ElementType.OTHER)\n\n        # Convert from top-left origin to bottom-left origin\n        elem = _PageElement(\n            cid=i,\n            text=text_map.get(i, \"\"),\n            page_no=page_no,\n            page_width=page_width,\n            page_height=page_height,\n            label=element_type,\n            left=box.bbox.x1,\n            bottom=page_height - box.bbox.y2,  # Convert y2 to bottom\n            right=box.bbox.x2,\n            top=page_height - box.bbox.y1,  # Convert y1 to top\n        )\n        page_elements.append(elem)\n\n    # Run reading order prediction\n    sorted_elements = self._predict_reading_order(page_elements)\n\n    # Get caption associations\n    caption_map = self._find_to_captions(sorted_elements)\n\n    # Get footnote associations\n    footnote_map = self._find_to_footnotes(sorted_elements)\n\n    # Get merge suggestions\n    merge_map = self._predict_merges(sorted_elements)\n\n    # Convert to OrderedElements\n    ordered_elements: List[OrderedElement] = []\n    for idx, elem in enumerate(sorted_elements):\n        # Convert back from bottom-left to top-left origin\n        bbox = BoundingBox(\n            x1=elem.left,\n            y1=page_height - elem.top,\n            x2=elem.right,\n            y2=page_height - elem.bottom,\n        )\n\n        confidence = 1.0\n        if elem.cid &lt; len(layout.bboxes):\n            confidence = layout.bboxes[elem.cid].confidence\n\n        ordered_elem = OrderedElement(\n            index=idx,\n            element_type=elem.label,\n            bbox=bbox,\n            text=elem.text,\n            confidence=confidence,\n            page_no=page_no,\n            original_id=elem.cid,\n        )\n        ordered_elements.append(ordered_elem)\n\n    return ReadingOrderOutput(\n        ordered_elements=ordered_elements,\n        caption_map=caption_map,\n        footnote_map=footnote_map,\n        merge_map=merge_map,\n        image_width=page_width,\n        image_height=page_height,\n        model_name=\"RuleBasedReadingOrderPredictor\",\n    )\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.rule_based.predictor","title":"predictor","text":"<p>Rule-based reading order predictor.</p> <p>Uses spatial analysis and R-tree indexing to determine the logical reading sequence of document elements. Self-contained implementation without external dependencies on docling-ibm-models.</p> <p>Based on the algorithm from docling-ibm-models, adapted for omnidocs.</p>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.rule_based.predictor.RuleBasedReadingOrderPredictor","title":"RuleBasedReadingOrderPredictor","text":"<pre><code>RuleBasedReadingOrderPredictor()\n</code></pre> <p>               Bases: <code>BaseReadingOrderPredictor</code></p> <p>Rule-based reading order predictor using spatial analysis.</p> <p>Uses R-tree spatial indexing and rule-based algorithms to determine the logical reading sequence of document elements. This is a CPU-only implementation that doesn't require GPU resources.</p> <p>Features: - Multi-column layout detection - Header/footer separation - Caption-to-figure/table association - Footnote linking - Element merge suggestions</p> Example <pre><code>from omnidocs.tasks.reading_order import RuleBasedReadingOrderPredictor\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\n# Initialize components\nlayout_extractor = DocLayoutYOLO(config=DocLayoutYOLOConfig())\nocr = EasyOCR(config=EasyOCRConfig())\npredictor = RuleBasedReadingOrderPredictor()\n\n# Process document\nlayout = layout_extractor.extract(image)\nocr_result = ocr.extract(image)\nreading_order = predictor.predict(layout, ocr_result)\n\n# Get text in reading order\ntext = reading_order.get_full_text()\n</code></pre> <p>Initialize the reading order predictor.</p> Source code in <code>omnidocs/tasks/reading_order/rule_based/predictor.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the reading order predictor.\"\"\"\n    self.dilated_page_element = True\n    # Apply horizontal dilation only if less than this page-width normalized threshold\n    self._horizontal_dilation_threshold_norm = 0.15\n</code></pre>"},{"location":"reference/tasks/reading_order/overview/#omnidocs.tasks.reading_order.rule_based.predictor.RuleBasedReadingOrderPredictor.predict","title":"predict","text":"<pre><code>predict(\n    layout: LayoutOutput,\n    ocr: Optional[OCROutput] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput\n</code></pre> <p>Predict reading order for a single page.</p> PARAMETER DESCRIPTION <code>layout</code> <p>Layout detection results with bounding boxes</p> <p> TYPE: <code>LayoutOutput</code> </p> <code>ocr</code> <p>Optional OCR results for text content</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> <code>page_no</code> <p>Page number (for multi-page documents)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ReadingOrderOutput</code> <p>ReadingOrderOutput with ordered elements and associations</p> Source code in <code>omnidocs/tasks/reading_order/rule_based/predictor.py</code> <pre><code>def predict(\n    self,\n    layout: \"LayoutOutput\",\n    ocr: Optional[\"OCROutput\"] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput:\n    \"\"\"\n    Predict reading order for a single page.\n\n    Args:\n        layout: Layout detection results with bounding boxes\n        ocr: Optional OCR results for text content\n        page_no: Page number (for multi-page documents)\n\n    Returns:\n        ReadingOrderOutput with ordered elements and associations\n    \"\"\"\n    page_width = layout.image_width\n    page_height = layout.image_height\n\n    # Build text map from OCR if available\n    text_map: Dict[int, str] = {}\n    if ocr:\n        text_map = self._build_text_map(layout, ocr)\n\n    # Convert layout boxes to internal PageElements\n    page_elements: List[_PageElement] = []\n    for i, box in enumerate(layout.bboxes):\n        label_str = box.label.value.lower()\n        element_type = LABEL_TO_ELEMENT_TYPE.get(label_str, ElementType.OTHER)\n\n        # Convert from top-left origin to bottom-left origin\n        elem = _PageElement(\n            cid=i,\n            text=text_map.get(i, \"\"),\n            page_no=page_no,\n            page_width=page_width,\n            page_height=page_height,\n            label=element_type,\n            left=box.bbox.x1,\n            bottom=page_height - box.bbox.y2,  # Convert y2 to bottom\n            right=box.bbox.x2,\n            top=page_height - box.bbox.y1,  # Convert y1 to top\n        )\n        page_elements.append(elem)\n\n    # Run reading order prediction\n    sorted_elements = self._predict_reading_order(page_elements)\n\n    # Get caption associations\n    caption_map = self._find_to_captions(sorted_elements)\n\n    # Get footnote associations\n    footnote_map = self._find_to_footnotes(sorted_elements)\n\n    # Get merge suggestions\n    merge_map = self._predict_merges(sorted_elements)\n\n    # Convert to OrderedElements\n    ordered_elements: List[OrderedElement] = []\n    for idx, elem in enumerate(sorted_elements):\n        # Convert back from bottom-left to top-left origin\n        bbox = BoundingBox(\n            x1=elem.left,\n            y1=page_height - elem.top,\n            x2=elem.right,\n            y2=page_height - elem.bottom,\n        )\n\n        confidence = 1.0\n        if elem.cid &lt; len(layout.bboxes):\n            confidence = layout.bboxes[elem.cid].confidence\n\n        ordered_elem = OrderedElement(\n            index=idx,\n            element_type=elem.label,\n            bbox=bbox,\n            text=elem.text,\n            confidence=confidence,\n            page_no=page_no,\n            original_id=elem.cid,\n        )\n        ordered_elements.append(ordered_elem)\n\n    return ReadingOrderOutput(\n        ordered_elements=ordered_elements,\n        caption_map=caption_map,\n        footnote_map=footnote_map,\n        merge_map=merge_map,\n        image_width=page_width,\n        image_height=page_height,\n        model_name=\"RuleBasedReadingOrderPredictor\",\n    )\n</code></pre>"},{"location":"reference/tasks/reading_order/rule_based/overview/","title":"Overview","text":"<p>Rule-based reading order predictor module.</p> <p>Provides rule-based reading order prediction using spatial analysis.</p>"},{"location":"reference/tasks/reading_order/rule_based/overview/#omnidocs.tasks.reading_order.rule_based.RuleBasedReadingOrderPredictor","title":"RuleBasedReadingOrderPredictor","text":"<pre><code>RuleBasedReadingOrderPredictor()\n</code></pre> <p>               Bases: <code>BaseReadingOrderPredictor</code></p> <p>Rule-based reading order predictor using spatial analysis.</p> <p>Uses R-tree spatial indexing and rule-based algorithms to determine the logical reading sequence of document elements. This is a CPU-only implementation that doesn't require GPU resources.</p> <p>Features: - Multi-column layout detection - Header/footer separation - Caption-to-figure/table association - Footnote linking - Element merge suggestions</p> Example <pre><code>from omnidocs.tasks.reading_order import RuleBasedReadingOrderPredictor\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\n# Initialize components\nlayout_extractor = DocLayoutYOLO(config=DocLayoutYOLOConfig())\nocr = EasyOCR(config=EasyOCRConfig())\npredictor = RuleBasedReadingOrderPredictor()\n\n# Process document\nlayout = layout_extractor.extract(image)\nocr_result = ocr.extract(image)\nreading_order = predictor.predict(layout, ocr_result)\n\n# Get text in reading order\ntext = reading_order.get_full_text()\n</code></pre> <p>Initialize the reading order predictor.</p> Source code in <code>omnidocs/tasks/reading_order/rule_based/predictor.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the reading order predictor.\"\"\"\n    self.dilated_page_element = True\n    # Apply horizontal dilation only if less than this page-width normalized threshold\n    self._horizontal_dilation_threshold_norm = 0.15\n</code></pre>"},{"location":"reference/tasks/reading_order/rule_based/overview/#omnidocs.tasks.reading_order.rule_based.RuleBasedReadingOrderPredictor.predict","title":"predict","text":"<pre><code>predict(\n    layout: LayoutOutput,\n    ocr: Optional[OCROutput] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput\n</code></pre> <p>Predict reading order for a single page.</p> PARAMETER DESCRIPTION <code>layout</code> <p>Layout detection results with bounding boxes</p> <p> TYPE: <code>LayoutOutput</code> </p> <code>ocr</code> <p>Optional OCR results for text content</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> <code>page_no</code> <p>Page number (for multi-page documents)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ReadingOrderOutput</code> <p>ReadingOrderOutput with ordered elements and associations</p> Source code in <code>omnidocs/tasks/reading_order/rule_based/predictor.py</code> <pre><code>def predict(\n    self,\n    layout: \"LayoutOutput\",\n    ocr: Optional[\"OCROutput\"] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput:\n    \"\"\"\n    Predict reading order for a single page.\n\n    Args:\n        layout: Layout detection results with bounding boxes\n        ocr: Optional OCR results for text content\n        page_no: Page number (for multi-page documents)\n\n    Returns:\n        ReadingOrderOutput with ordered elements and associations\n    \"\"\"\n    page_width = layout.image_width\n    page_height = layout.image_height\n\n    # Build text map from OCR if available\n    text_map: Dict[int, str] = {}\n    if ocr:\n        text_map = self._build_text_map(layout, ocr)\n\n    # Convert layout boxes to internal PageElements\n    page_elements: List[_PageElement] = []\n    for i, box in enumerate(layout.bboxes):\n        label_str = box.label.value.lower()\n        element_type = LABEL_TO_ELEMENT_TYPE.get(label_str, ElementType.OTHER)\n\n        # Convert from top-left origin to bottom-left origin\n        elem = _PageElement(\n            cid=i,\n            text=text_map.get(i, \"\"),\n            page_no=page_no,\n            page_width=page_width,\n            page_height=page_height,\n            label=element_type,\n            left=box.bbox.x1,\n            bottom=page_height - box.bbox.y2,  # Convert y2 to bottom\n            right=box.bbox.x2,\n            top=page_height - box.bbox.y1,  # Convert y1 to top\n        )\n        page_elements.append(elem)\n\n    # Run reading order prediction\n    sorted_elements = self._predict_reading_order(page_elements)\n\n    # Get caption associations\n    caption_map = self._find_to_captions(sorted_elements)\n\n    # Get footnote associations\n    footnote_map = self._find_to_footnotes(sorted_elements)\n\n    # Get merge suggestions\n    merge_map = self._predict_merges(sorted_elements)\n\n    # Convert to OrderedElements\n    ordered_elements: List[OrderedElement] = []\n    for idx, elem in enumerate(sorted_elements):\n        # Convert back from bottom-left to top-left origin\n        bbox = BoundingBox(\n            x1=elem.left,\n            y1=page_height - elem.top,\n            x2=elem.right,\n            y2=page_height - elem.bottom,\n        )\n\n        confidence = 1.0\n        if elem.cid &lt; len(layout.bboxes):\n            confidence = layout.bboxes[elem.cid].confidence\n\n        ordered_elem = OrderedElement(\n            index=idx,\n            element_type=elem.label,\n            bbox=bbox,\n            text=elem.text,\n            confidence=confidence,\n            page_no=page_no,\n            original_id=elem.cid,\n        )\n        ordered_elements.append(ordered_elem)\n\n    return ReadingOrderOutput(\n        ordered_elements=ordered_elements,\n        caption_map=caption_map,\n        footnote_map=footnote_map,\n        merge_map=merge_map,\n        image_width=page_width,\n        image_height=page_height,\n        model_name=\"RuleBasedReadingOrderPredictor\",\n    )\n</code></pre>"},{"location":"reference/tasks/reading_order/rule_based/overview/#omnidocs.tasks.reading_order.rule_based.predictor","title":"predictor","text":"<p>Rule-based reading order predictor.</p> <p>Uses spatial analysis and R-tree indexing to determine the logical reading sequence of document elements. Self-contained implementation without external dependencies on docling-ibm-models.</p> <p>Based on the algorithm from docling-ibm-models, adapted for omnidocs.</p>"},{"location":"reference/tasks/reading_order/rule_based/overview/#omnidocs.tasks.reading_order.rule_based.predictor.RuleBasedReadingOrderPredictor","title":"RuleBasedReadingOrderPredictor","text":"<pre><code>RuleBasedReadingOrderPredictor()\n</code></pre> <p>               Bases: <code>BaseReadingOrderPredictor</code></p> <p>Rule-based reading order predictor using spatial analysis.</p> <p>Uses R-tree spatial indexing and rule-based algorithms to determine the logical reading sequence of document elements. This is a CPU-only implementation that doesn't require GPU resources.</p> <p>Features: - Multi-column layout detection - Header/footer separation - Caption-to-figure/table association - Footnote linking - Element merge suggestions</p> Example <pre><code>from omnidocs.tasks.reading_order import RuleBasedReadingOrderPredictor\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\n# Initialize components\nlayout_extractor = DocLayoutYOLO(config=DocLayoutYOLOConfig())\nocr = EasyOCR(config=EasyOCRConfig())\npredictor = RuleBasedReadingOrderPredictor()\n\n# Process document\nlayout = layout_extractor.extract(image)\nocr_result = ocr.extract(image)\nreading_order = predictor.predict(layout, ocr_result)\n\n# Get text in reading order\ntext = reading_order.get_full_text()\n</code></pre> <p>Initialize the reading order predictor.</p> Source code in <code>omnidocs/tasks/reading_order/rule_based/predictor.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the reading order predictor.\"\"\"\n    self.dilated_page_element = True\n    # Apply horizontal dilation only if less than this page-width normalized threshold\n    self._horizontal_dilation_threshold_norm = 0.15\n</code></pre>"},{"location":"reference/tasks/reading_order/rule_based/overview/#omnidocs.tasks.reading_order.rule_based.predictor.RuleBasedReadingOrderPredictor.predict","title":"predict","text":"<pre><code>predict(\n    layout: LayoutOutput,\n    ocr: Optional[OCROutput] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput\n</code></pre> <p>Predict reading order for a single page.</p> PARAMETER DESCRIPTION <code>layout</code> <p>Layout detection results with bounding boxes</p> <p> TYPE: <code>LayoutOutput</code> </p> <code>ocr</code> <p>Optional OCR results for text content</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> <code>page_no</code> <p>Page number (for multi-page documents)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ReadingOrderOutput</code> <p>ReadingOrderOutput with ordered elements and associations</p> Source code in <code>omnidocs/tasks/reading_order/rule_based/predictor.py</code> <pre><code>def predict(\n    self,\n    layout: \"LayoutOutput\",\n    ocr: Optional[\"OCROutput\"] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput:\n    \"\"\"\n    Predict reading order for a single page.\n\n    Args:\n        layout: Layout detection results with bounding boxes\n        ocr: Optional OCR results for text content\n        page_no: Page number (for multi-page documents)\n\n    Returns:\n        ReadingOrderOutput with ordered elements and associations\n    \"\"\"\n    page_width = layout.image_width\n    page_height = layout.image_height\n\n    # Build text map from OCR if available\n    text_map: Dict[int, str] = {}\n    if ocr:\n        text_map = self._build_text_map(layout, ocr)\n\n    # Convert layout boxes to internal PageElements\n    page_elements: List[_PageElement] = []\n    for i, box in enumerate(layout.bboxes):\n        label_str = box.label.value.lower()\n        element_type = LABEL_TO_ELEMENT_TYPE.get(label_str, ElementType.OTHER)\n\n        # Convert from top-left origin to bottom-left origin\n        elem = _PageElement(\n            cid=i,\n            text=text_map.get(i, \"\"),\n            page_no=page_no,\n            page_width=page_width,\n            page_height=page_height,\n            label=element_type,\n            left=box.bbox.x1,\n            bottom=page_height - box.bbox.y2,  # Convert y2 to bottom\n            right=box.bbox.x2,\n            top=page_height - box.bbox.y1,  # Convert y1 to top\n        )\n        page_elements.append(elem)\n\n    # Run reading order prediction\n    sorted_elements = self._predict_reading_order(page_elements)\n\n    # Get caption associations\n    caption_map = self._find_to_captions(sorted_elements)\n\n    # Get footnote associations\n    footnote_map = self._find_to_footnotes(sorted_elements)\n\n    # Get merge suggestions\n    merge_map = self._predict_merges(sorted_elements)\n\n    # Convert to OrderedElements\n    ordered_elements: List[OrderedElement] = []\n    for idx, elem in enumerate(sorted_elements):\n        # Convert back from bottom-left to top-left origin\n        bbox = BoundingBox(\n            x1=elem.left,\n            y1=page_height - elem.top,\n            x2=elem.right,\n            y2=page_height - elem.bottom,\n        )\n\n        confidence = 1.0\n        if elem.cid &lt; len(layout.bboxes):\n            confidence = layout.bboxes[elem.cid].confidence\n\n        ordered_elem = OrderedElement(\n            index=idx,\n            element_type=elem.label,\n            bbox=bbox,\n            text=elem.text,\n            confidence=confidence,\n            page_no=page_no,\n            original_id=elem.cid,\n        )\n        ordered_elements.append(ordered_elem)\n\n    return ReadingOrderOutput(\n        ordered_elements=ordered_elements,\n        caption_map=caption_map,\n        footnote_map=footnote_map,\n        merge_map=merge_map,\n        image_width=page_width,\n        image_height=page_height,\n        model_name=\"RuleBasedReadingOrderPredictor\",\n    )\n</code></pre>"},{"location":"reference/tasks/reading_order/rule_based/predictor/","title":"Predictor","text":"<p>Rule-based reading order predictor.</p> <p>Uses spatial analysis and R-tree indexing to determine the logical reading sequence of document elements. Self-contained implementation without external dependencies on docling-ibm-models.</p> <p>Based on the algorithm from docling-ibm-models, adapted for omnidocs.</p>"},{"location":"reference/tasks/reading_order/rule_based/predictor/#omnidocs.tasks.reading_order.rule_based.predictor.RuleBasedReadingOrderPredictor","title":"RuleBasedReadingOrderPredictor","text":"<pre><code>RuleBasedReadingOrderPredictor()\n</code></pre> <p>               Bases: <code>BaseReadingOrderPredictor</code></p> <p>Rule-based reading order predictor using spatial analysis.</p> <p>Uses R-tree spatial indexing and rule-based algorithms to determine the logical reading sequence of document elements. This is a CPU-only implementation that doesn't require GPU resources.</p> <p>Features: - Multi-column layout detection - Header/footer separation - Caption-to-figure/table association - Footnote linking - Element merge suggestions</p> Example <pre><code>from omnidocs.tasks.reading_order import RuleBasedReadingOrderPredictor\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\n# Initialize components\nlayout_extractor = DocLayoutYOLO(config=DocLayoutYOLOConfig())\nocr = EasyOCR(config=EasyOCRConfig())\npredictor = RuleBasedReadingOrderPredictor()\n\n# Process document\nlayout = layout_extractor.extract(image)\nocr_result = ocr.extract(image)\nreading_order = predictor.predict(layout, ocr_result)\n\n# Get text in reading order\ntext = reading_order.get_full_text()\n</code></pre> <p>Initialize the reading order predictor.</p> Source code in <code>omnidocs/tasks/reading_order/rule_based/predictor.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize the reading order predictor.\"\"\"\n    self.dilated_page_element = True\n    # Apply horizontal dilation only if less than this page-width normalized threshold\n    self._horizontal_dilation_threshold_norm = 0.15\n</code></pre>"},{"location":"reference/tasks/reading_order/rule_based/predictor/#omnidocs.tasks.reading_order.rule_based.predictor.RuleBasedReadingOrderPredictor.predict","title":"predict","text":"<pre><code>predict(\n    layout: LayoutOutput,\n    ocr: Optional[OCROutput] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput\n</code></pre> <p>Predict reading order for a single page.</p> PARAMETER DESCRIPTION <code>layout</code> <p>Layout detection results with bounding boxes</p> <p> TYPE: <code>LayoutOutput</code> </p> <code>ocr</code> <p>Optional OCR results for text content</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> <code>page_no</code> <p>Page number (for multi-page documents)</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> RETURNS DESCRIPTION <code>ReadingOrderOutput</code> <p>ReadingOrderOutput with ordered elements and associations</p> Source code in <code>omnidocs/tasks/reading_order/rule_based/predictor.py</code> <pre><code>def predict(\n    self,\n    layout: \"LayoutOutput\",\n    ocr: Optional[\"OCROutput\"] = None,\n    page_no: int = 0,\n) -&gt; ReadingOrderOutput:\n    \"\"\"\n    Predict reading order for a single page.\n\n    Args:\n        layout: Layout detection results with bounding boxes\n        ocr: Optional OCR results for text content\n        page_no: Page number (for multi-page documents)\n\n    Returns:\n        ReadingOrderOutput with ordered elements and associations\n    \"\"\"\n    page_width = layout.image_width\n    page_height = layout.image_height\n\n    # Build text map from OCR if available\n    text_map: Dict[int, str] = {}\n    if ocr:\n        text_map = self._build_text_map(layout, ocr)\n\n    # Convert layout boxes to internal PageElements\n    page_elements: List[_PageElement] = []\n    for i, box in enumerate(layout.bboxes):\n        label_str = box.label.value.lower()\n        element_type = LABEL_TO_ELEMENT_TYPE.get(label_str, ElementType.OTHER)\n\n        # Convert from top-left origin to bottom-left origin\n        elem = _PageElement(\n            cid=i,\n            text=text_map.get(i, \"\"),\n            page_no=page_no,\n            page_width=page_width,\n            page_height=page_height,\n            label=element_type,\n            left=box.bbox.x1,\n            bottom=page_height - box.bbox.y2,  # Convert y2 to bottom\n            right=box.bbox.x2,\n            top=page_height - box.bbox.y1,  # Convert y1 to top\n        )\n        page_elements.append(elem)\n\n    # Run reading order prediction\n    sorted_elements = self._predict_reading_order(page_elements)\n\n    # Get caption associations\n    caption_map = self._find_to_captions(sorted_elements)\n\n    # Get footnote associations\n    footnote_map = self._find_to_footnotes(sorted_elements)\n\n    # Get merge suggestions\n    merge_map = self._predict_merges(sorted_elements)\n\n    # Convert to OrderedElements\n    ordered_elements: List[OrderedElement] = []\n    for idx, elem in enumerate(sorted_elements):\n        # Convert back from bottom-left to top-left origin\n        bbox = BoundingBox(\n            x1=elem.left,\n            y1=page_height - elem.top,\n            x2=elem.right,\n            y2=page_height - elem.bottom,\n        )\n\n        confidence = 1.0\n        if elem.cid &lt; len(layout.bboxes):\n            confidence = layout.bboxes[elem.cid].confidence\n\n        ordered_elem = OrderedElement(\n            index=idx,\n            element_type=elem.label,\n            bbox=bbox,\n            text=elem.text,\n            confidence=confidence,\n            page_no=page_no,\n            original_id=elem.cid,\n        )\n        ordered_elements.append(ordered_elem)\n\n    return ReadingOrderOutput(\n        ordered_elements=ordered_elements,\n        caption_map=caption_map,\n        footnote_map=footnote_map,\n        merge_map=merge_map,\n        image_width=page_width,\n        image_height=page_height,\n        model_name=\"RuleBasedReadingOrderPredictor\",\n    )\n</code></pre>"},{"location":"reference/tasks/table_extraction/base/","title":"Base","text":"<p>Base class for table extractors.</p> <p>Defines the abstract interface that all table extractors must implement.</p>"},{"location":"reference/tasks/table_extraction/base/#omnidocs.tasks.table_extraction.base.BaseTableExtractor","title":"BaseTableExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for table structure extractors.</p> <p>Table extractors analyze table images to detect cell structure, identify headers, and extract text content.</p> Example <pre><code>class MyTableExtractor(BaseTableExtractor):\n    def __init__(self, config: MyConfig):\n        self.config = config\n        self._load_model()\n\n    def _load_model(self):\n        # Load model weights\n        pass\n\n    def extract(self, image):\n        # Run extraction\n        return TableOutput(...)\n</code></pre>"},{"location":"reference/tasks/table_extraction/base/#omnidocs.tasks.table_extraction.base.BaseTableExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    ocr_output: Optional[OCROutput] = None,\n) -&gt; TableOutput\n</code></pre> <p>Extract table structure from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Table image (should be cropped to table region)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>ocr_output</code> <p>Optional OCR results for cell text matching.        If not provided, model will attempt to extract text.</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TableOutput</code> <p>TableOutput with cells, structure, and export methods</p> Example <pre><code># Without OCR (model extracts text)\nresult = extractor.extract(table_image)\n\n# With OCR (better text quality)\nocr = some_ocr.extract(table_image)\nresult = extractor.extract(table_image, ocr_output=ocr)\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    ocr_output: Optional[\"OCROutput\"] = None,\n) -&gt; TableOutput:\n    \"\"\"\n    Extract table structure from an image.\n\n    Args:\n        image: Table image (should be cropped to table region)\n        ocr_output: Optional OCR results for cell text matching.\n                   If not provided, model will attempt to extract text.\n\n    Returns:\n        TableOutput with cells, structure, and export methods\n\n    Example:\n        ```python\n        # Without OCR (model extracts text)\n        result = extractor.extract(table_image)\n\n        # With OCR (better text quality)\n        ocr = some_ocr.extract(table_image)\n        result = extractor.extract(table_image, ocr_output=ocr)\n        ```\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/table_extraction/base/#omnidocs.tasks.table_extraction.base.BaseTableExtractor.batch_extract","title":"batch_extract","text":"<pre><code>batch_extract(\n    images: List[Union[Image, ndarray, str, Path]],\n    ocr_outputs: Optional[List[OCROutput]] = None,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[TableOutput]\n</code></pre> <p>Extract tables from multiple images.</p> <p>Default implementation loops over extract(). Subclasses can override for optimized batching.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of table images</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>ocr_outputs</code> <p>Optional list of OCR results (same length as images)</p> <p> TYPE: <code>Optional[List[OCROutput]]</code> DEFAULT: <code>None</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TableOutput]</code> <p>List of TableOutput in same order as input</p> <p>Examples:</p> <pre><code>results = extractor.batch_extract(table_images)\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def batch_extract(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    ocr_outputs: Optional[List[\"OCROutput\"]] = None,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[TableOutput]:\n    \"\"\"\n    Extract tables from multiple images.\n\n    Default implementation loops over extract(). Subclasses can override\n    for optimized batching.\n\n    Args:\n        images: List of table images\n        ocr_outputs: Optional list of OCR results (same length as images)\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of TableOutput in same order as input\n\n    Examples:\n        ```python\n        results = extractor.batch_extract(table_images)\n        ```\n    \"\"\"\n    results = []\n    total = len(images)\n\n    for i, image in enumerate(images):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        ocr = ocr_outputs[i] if ocr_outputs else None\n        result = self.extract(image, ocr_output=ocr)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/table_extraction/base/#omnidocs.tasks.table_extraction.base.BaseTableExtractor.extract_document","title":"extract_document","text":"<pre><code>extract_document(\n    document: Document,\n    table_bboxes: Optional[List[List[float]]] = None,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[TableOutput]\n</code></pre> <p>Extract tables from all pages of a document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>table_bboxes</code> <p>Optional list of table bounding boxes per page.          Each element should be a list of [x1, y1, x2, y2] coords.</p> <p> TYPE: <code>Optional[List[List[float]]]</code> DEFAULT: <code>None</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TableOutput]</code> <p>List of TableOutput, one per detected table</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresults = extractor.extract_document(doc)\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def extract_document(\n    self,\n    document: \"Document\",\n    table_bboxes: Optional[List[List[float]]] = None,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[TableOutput]:\n    \"\"\"\n    Extract tables from all pages of a document.\n\n    Args:\n        document: Document instance\n        table_bboxes: Optional list of table bounding boxes per page.\n                     Each element should be a list of [x1, y1, x2, y2] coords.\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of TableOutput, one per detected table\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        results = extractor.extract_document(doc)\n        ```\n    \"\"\"\n    results = []\n    total = document.page_count\n\n    for i, page in enumerate(document.iter_pages()):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        # If no bboxes provided, process entire page\n        if table_bboxes is None:\n            result = self.extract(page)\n            results.append(result)\n        else:\n            # Crop and process each table region\n            for bbox in table_bboxes:\n                x1, y1, x2, y2 = bbox\n                table_region = page.crop((x1, y1, x2, y2))\n                result = self.extract(table_region)\n                results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/table_extraction/models/","title":"Models","text":"<p>Pydantic models for table extraction outputs.</p> <p>Provides structured table data with cells, spans, and multiple export formats including HTML, Markdown, and Pandas DataFrame conversion.</p> Example <pre><code>result = extractor.extract(table_image)\n\n# Get HTML\nhtml = result.to_html()\n\n# Get Pandas DataFrame\ndf = result.to_dataframe()\n\n# Access cells\nfor cell in result.cells:\n    print(f\"[{cell.row},{cell.col}] {cell.text}\")\n</code></pre>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.CellType","title":"CellType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of table cell.</p>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box in pixel coordinates.</p>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.BoundingBox.from_ltrb","title":"from_ltrb  <code>classmethod</code>","text":"<pre><code>from_ltrb(\n    left: float, top: float, right: float, bottom: float\n) -&gt; BoundingBox\n</code></pre> <p>Create from left, top, right, bottom coordinates.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>@classmethod\ndef from_ltrb(cls, left: float, top: float, right: float, bottom: float) -&gt; \"BoundingBox\":\n    \"\"\"Create from left, top, right, bottom coordinates.\"\"\"\n    return cls(x1=left, y1=top, x2=right, y2=bottom)\n</code></pre>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.TableCell","title":"TableCell","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single table cell with position, span, and content.</p> <p>The cell position uses 0-indexed row/column indices. Spans indicate how many rows/columns the cell occupies.</p>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.TableCell.end_row","title":"end_row  <code>property</code>","text":"<pre><code>end_row: int\n</code></pre> <p>Ending row index (exclusive).</p>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.TableCell.end_col","title":"end_col  <code>property</code>","text":"<pre><code>end_col: int\n</code></pre> <p>Ending column index (exclusive).</p>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.TableCell.is_header","title":"is_header  <code>property</code>","text":"<pre><code>is_header: bool\n</code></pre> <p>Check if cell is any type of header.</p>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.TableCell.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"row\": self.row,\n        \"col\": self.col,\n        \"row_span\": self.row_span,\n        \"col_span\": self.col_span,\n        \"text\": self.text,\n        \"cell_type\": self.cell_type.value,\n        \"bbox\": self.bbox.to_list() if self.bbox else None,\n        \"confidence\": self.confidence,\n    }\n</code></pre>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.TableOutput","title":"TableOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete table extraction result.</p> <p>Provides multiple export formats and utility methods for working with extracted table data.</p> Example <pre><code>result = extractor.extract(table_image)\n\n# Basic info\nprint(f\"Table: {result.num_rows}x{result.num_cols}\")\n\n# Export to HTML\nhtml = result.to_html()\n\n# Export to Pandas\ndf = result.to_dataframe()\n\n# Export to Markdown\nmd = result.to_markdown()\n\n# Access specific cell\ncell = result.get_cell(row=0, col=0)\n</code></pre>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.TableOutput.cell_count","title":"cell_count  <code>property</code>","text":"<pre><code>cell_count: int\n</code></pre> <p>Number of cells in the table.</p>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.TableOutput.has_headers","title":"has_headers  <code>property</code>","text":"<pre><code>has_headers: bool\n</code></pre> <p>Check if table has header cells.</p>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.TableOutput.get_cell","title":"get_cell","text":"<pre><code>get_cell(row: int, col: int) -&gt; Optional[TableCell]\n</code></pre> <p>Get cell at specific position.</p> <p>Handles merged cells by returning the cell that covers the position.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def get_cell(self, row: int, col: int) -&gt; Optional[TableCell]:\n    \"\"\"\n    Get cell at specific position.\n\n    Handles merged cells by returning the cell that covers the position.\n    \"\"\"\n    for cell in self.cells:\n        if cell.row &lt;= row &lt; cell.end_row and cell.col &lt;= col &lt; cell.end_col:\n            return cell\n    return None\n</code></pre>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.TableOutput.get_row","title":"get_row","text":"<pre><code>get_row(row: int) -&gt; List[TableCell]\n</code></pre> <p>Get all cells in a specific row.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def get_row(self, row: int) -&gt; List[TableCell]:\n    \"\"\"Get all cells in a specific row.\"\"\"\n    return [c for c in self.cells if c.row == row]\n</code></pre>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.TableOutput.get_column","title":"get_column","text":"<pre><code>get_column(col: int) -&gt; List[TableCell]\n</code></pre> <p>Get all cells in a specific column.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def get_column(self, col: int) -&gt; List[TableCell]:\n    \"\"\"Get all cells in a specific column.\"\"\"\n    return [c for c in self.cells if c.col == col]\n</code></pre>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.TableOutput.to_html","title":"to_html","text":"<pre><code>to_html(include_styles: bool = True) -&gt; str\n</code></pre> <p>Convert table to HTML string.</p> PARAMETER DESCRIPTION <code>include_styles</code> <p>Whether to include basic CSS styling</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>str</code> <p>HTML table string</p> Example <pre><code>html = result.to_html()\nwith open(\"table.html\", \"w\") as f:\n    f.write(html)\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_html(self, include_styles: bool = True) -&gt; str:\n    \"\"\"\n    Convert table to HTML string.\n\n    Args:\n        include_styles: Whether to include basic CSS styling\n\n    Returns:\n        HTML table string\n\n    Example:\n        ```python\n        html = result.to_html()\n        with open(\"table.html\", \"w\") as f:\n            f.write(html)\n        ```\n    \"\"\"\n    # Build 2D grid accounting for spans\n    grid: List[List[Optional[TableCell]]] = [[None for _ in range(self.num_cols)] for _ in range(self.num_rows)]\n\n    for cell in self.cells:\n        for r in range(cell.row, cell.end_row):\n            for c in range(cell.col, cell.end_col):\n                if r &lt; self.num_rows and c &lt; self.num_cols:\n                    grid[r][c] = cell\n\n    # Generate HTML\n    lines = []\n\n    if include_styles:\n        lines.append('&lt;table style=\"border-collapse: collapse; width: 100%;\"&gt;')\n    else:\n        lines.append(\"&lt;table&gt;\")\n\n    processed: set[Tuple[int, int]] = set()  # Track cells we've already output\n\n    for row_idx in range(self.num_rows):\n        lines.append(\"  &lt;tr&gt;\")\n\n        for col_idx in range(self.num_cols):\n            cell = grid[row_idx][col_idx]\n\n            if cell is None:\n                lines.append(\"    &lt;td&gt;&lt;/td&gt;\")\n                continue\n\n            # Skip if this cell was already output (merged cell)\n            cell_id = (cell.row, cell.col)\n            if cell_id in processed:\n                continue\n            processed.add(cell_id)\n\n            # Determine tag based on cell type\n            tag = \"th\" if cell.is_header else \"td\"\n\n            # Build attributes\n            attrs = []\n            if cell.row_span &gt; 1:\n                attrs.append(f'rowspan=\"{cell.row_span}\"')\n            if cell.col_span &gt; 1:\n                attrs.append(f'colspan=\"{cell.col_span}\"')\n            if include_styles:\n                attrs.append('style=\"border: 1px solid #ddd; padding: 8px;\"')\n\n            attr_str = \" \" + \" \".join(attrs) if attrs else \"\"\n\n            # Escape HTML in text\n            text = (cell.text or \"\").replace(\"&amp;\", \"&amp;amp;\").replace(\"&lt;\", \"&amp;lt;\").replace(\"&gt;\", \"&amp;gt;\")\n\n            lines.append(f\"    &lt;{tag}{attr_str}&gt;{text}&lt;/{tag}&gt;\")\n\n        lines.append(\"  &lt;/tr&gt;\")\n\n    lines.append(\"&lt;/table&gt;\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.TableOutput.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe()\n</code></pre> <p>Convert table to Pandas DataFrame.</p> RETURNS DESCRIPTION <p>pandas.DataFrame with table data</p> RAISES DESCRIPTION <code>ImportError</code> <p>If pandas is not installed</p> Example <pre><code>df = result.to_dataframe()\nprint(df.head())\ndf.to_csv(\"table.csv\")\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_dataframe(self):\n    \"\"\"\n    Convert table to Pandas DataFrame.\n\n    Returns:\n        pandas.DataFrame with table data\n\n    Raises:\n        ImportError: If pandas is not installed\n\n    Example:\n        ```python\n        df = result.to_dataframe()\n        print(df.head())\n        df.to_csv(\"table.csv\")\n        ```\n    \"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\"pandas is required for to_dataframe(). Install with: pip install pandas\")\n\n    # Build 2D array\n    data: List[List[Optional[str]]] = [[None for _ in range(self.num_cols)] for _ in range(self.num_rows)]\n\n    for cell in self.cells:\n        # For merged cells, put value in top-left position\n        if cell.row &lt; self.num_rows and cell.col &lt; self.num_cols:\n            data[cell.row][cell.col] = cell.text\n\n    # Determine if first row is header\n    first_row_cells = self.get_row(0)\n    use_header = all(c.cell_type == CellType.COLUMN_HEADER for c in first_row_cells) if first_row_cells else False\n\n    if use_header and self.num_rows &gt; 1:\n        headers = data[0]\n        data = data[1:]\n        return pd.DataFrame(data, columns=headers)\n    else:\n        return pd.DataFrame(data)\n</code></pre>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.TableOutput.to_markdown","title":"to_markdown","text":"<pre><code>to_markdown() -&gt; str\n</code></pre> <p>Convert table to Markdown format.</p> <p>Note: Markdown tables don't support merged cells, so spans are ignored and only the top-left cell value is used.</p> RETURNS DESCRIPTION <code>str</code> <p>Markdown table string</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_markdown(self) -&gt; str:\n    \"\"\"\n    Convert table to Markdown format.\n\n    Note: Markdown tables don't support merged cells, so spans\n    are ignored and only the top-left cell value is used.\n\n    Returns:\n        Markdown table string\n    \"\"\"\n    if self.num_rows == 0 or self.num_cols == 0:\n        return \"\"\n\n    # Build 2D grid\n    grid: List[List[str]] = [[\"\" for _ in range(self.num_cols)] for _ in range(self.num_rows)]\n\n    for cell in self.cells:\n        if cell.row &lt; self.num_rows and cell.col &lt; self.num_cols:\n            grid[cell.row][cell.col] = cell.text or \"\"\n\n    lines = []\n\n    # Header row\n    lines.append(\"| \" + \" | \".join(grid[0]) + \" |\")\n\n    # Separator\n    lines.append(\"| \" + \" | \".join([\"---\"] * self.num_cols) + \" |\")\n\n    # Data rows\n    for row in grid[1:]:\n        lines.append(\"| \" + \" | \".join(row) + \" |\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.TableOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"cells\": [c.to_dict() for c in self.cells],\n        \"num_rows\": self.num_rows,\n        \"num_cols\": self.num_cols,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"html\": self.to_html(include_styles=False),\n    }\n</code></pre>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.TableOutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save to JSON file.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"Save to JSON file.\"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/table_extraction/models/#omnidocs.tasks.table_extraction.models.TableOutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; TableOutput\n</code></pre> <p>Load from JSON file.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"TableOutput\":\n    \"\"\"Load from JSON file.\"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/","title":"Overview","text":"<p>Table Extraction Module.</p> <p>Provides extractors for detecting and extracting table structure from document images. Outputs structured table data with cells, spans, and multiple export formats (HTML, Markdown, Pandas DataFrame).</p> Available Extractors <ul> <li>TableFormerExtractor: Transformer-based table structure extractor</li> </ul> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Initialize extractor\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(mode=\"fast\", device=\"cuda\")\n)\n\n# Extract table structure\nresult = extractor.extract(table_image)\n\n# Get HTML output\nhtml = result.to_html()\n\n# Get DataFrame\ndf = result.to_dataframe()\n\n# Get Markdown\nmd = result.to_markdown()\n\n# Access cells\nfor cell in result.cells:\n    print(f\"[{cell.row},{cell.col}] {cell.text}\")\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.BaseTableExtractor","title":"BaseTableExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for table structure extractors.</p> <p>Table extractors analyze table images to detect cell structure, identify headers, and extract text content.</p> Example <pre><code>class MyTableExtractor(BaseTableExtractor):\n    def __init__(self, config: MyConfig):\n        self.config = config\n        self._load_model()\n\n    def _load_model(self):\n        # Load model weights\n        pass\n\n    def extract(self, image):\n        # Run extraction\n        return TableOutput(...)\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.BaseTableExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    ocr_output: Optional[OCROutput] = None,\n) -&gt; TableOutput\n</code></pre> <p>Extract table structure from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Table image (should be cropped to table region)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>ocr_output</code> <p>Optional OCR results for cell text matching.        If not provided, model will attempt to extract text.</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TableOutput</code> <p>TableOutput with cells, structure, and export methods</p> Example <pre><code># Without OCR (model extracts text)\nresult = extractor.extract(table_image)\n\n# With OCR (better text quality)\nocr = some_ocr.extract(table_image)\nresult = extractor.extract(table_image, ocr_output=ocr)\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    ocr_output: Optional[\"OCROutput\"] = None,\n) -&gt; TableOutput:\n    \"\"\"\n    Extract table structure from an image.\n\n    Args:\n        image: Table image (should be cropped to table region)\n        ocr_output: Optional OCR results for cell text matching.\n                   If not provided, model will attempt to extract text.\n\n    Returns:\n        TableOutput with cells, structure, and export methods\n\n    Example:\n        ```python\n        # Without OCR (model extracts text)\n        result = extractor.extract(table_image)\n\n        # With OCR (better text quality)\n        ocr = some_ocr.extract(table_image)\n        result = extractor.extract(table_image, ocr_output=ocr)\n        ```\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.BaseTableExtractor.batch_extract","title":"batch_extract","text":"<pre><code>batch_extract(\n    images: List[Union[Image, ndarray, str, Path]],\n    ocr_outputs: Optional[List[OCROutput]] = None,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[TableOutput]\n</code></pre> <p>Extract tables from multiple images.</p> <p>Default implementation loops over extract(). Subclasses can override for optimized batching.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of table images</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>ocr_outputs</code> <p>Optional list of OCR results (same length as images)</p> <p> TYPE: <code>Optional[List[OCROutput]]</code> DEFAULT: <code>None</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TableOutput]</code> <p>List of TableOutput in same order as input</p> <p>Examples:</p> <pre><code>results = extractor.batch_extract(table_images)\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def batch_extract(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    ocr_outputs: Optional[List[\"OCROutput\"]] = None,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[TableOutput]:\n    \"\"\"\n    Extract tables from multiple images.\n\n    Default implementation loops over extract(). Subclasses can override\n    for optimized batching.\n\n    Args:\n        images: List of table images\n        ocr_outputs: Optional list of OCR results (same length as images)\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of TableOutput in same order as input\n\n    Examples:\n        ```python\n        results = extractor.batch_extract(table_images)\n        ```\n    \"\"\"\n    results = []\n    total = len(images)\n\n    for i, image in enumerate(images):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        ocr = ocr_outputs[i] if ocr_outputs else None\n        result = self.extract(image, ocr_output=ocr)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.BaseTableExtractor.extract_document","title":"extract_document","text":"<pre><code>extract_document(\n    document: Document,\n    table_bboxes: Optional[List[List[float]]] = None,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[TableOutput]\n</code></pre> <p>Extract tables from all pages of a document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>table_bboxes</code> <p>Optional list of table bounding boxes per page.          Each element should be a list of [x1, y1, x2, y2] coords.</p> <p> TYPE: <code>Optional[List[List[float]]]</code> DEFAULT: <code>None</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TableOutput]</code> <p>List of TableOutput, one per detected table</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresults = extractor.extract_document(doc)\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def extract_document(\n    self,\n    document: \"Document\",\n    table_bboxes: Optional[List[List[float]]] = None,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[TableOutput]:\n    \"\"\"\n    Extract tables from all pages of a document.\n\n    Args:\n        document: Document instance\n        table_bboxes: Optional list of table bounding boxes per page.\n                     Each element should be a list of [x1, y1, x2, y2] coords.\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of TableOutput, one per detected table\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        results = extractor.extract_document(doc)\n        ```\n    \"\"\"\n    results = []\n    total = document.page_count\n\n    for i, page in enumerate(document.iter_pages()):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        # If no bboxes provided, process entire page\n        if table_bboxes is None:\n            result = self.extract(page)\n            results.append(result)\n        else:\n            # Crop and process each table region\n            for bbox in table_bboxes:\n                x1, y1, x2, y2 = bbox\n                table_region = page.crop((x1, y1, x2, y2))\n                result = self.extract(table_region)\n                results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box in pixel coordinates.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.BoundingBox.from_ltrb","title":"from_ltrb  <code>classmethod</code>","text":"<pre><code>from_ltrb(\n    left: float, top: float, right: float, bottom: float\n) -&gt; BoundingBox\n</code></pre> <p>Create from left, top, right, bottom coordinates.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>@classmethod\ndef from_ltrb(cls, left: float, top: float, right: float, bottom: float) -&gt; \"BoundingBox\":\n    \"\"\"Create from left, top, right, bottom coordinates.\"\"\"\n    return cls(x1=left, y1=top, x2=right, y2=bottom)\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.CellType","title":"CellType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of table cell.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableCell","title":"TableCell","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single table cell with position, span, and content.</p> <p>The cell position uses 0-indexed row/column indices. Spans indicate how many rows/columns the cell occupies.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableCell.end_row","title":"end_row  <code>property</code>","text":"<pre><code>end_row: int\n</code></pre> <p>Ending row index (exclusive).</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableCell.end_col","title":"end_col  <code>property</code>","text":"<pre><code>end_col: int\n</code></pre> <p>Ending column index (exclusive).</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableCell.is_header","title":"is_header  <code>property</code>","text":"<pre><code>is_header: bool\n</code></pre> <p>Check if cell is any type of header.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableCell.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"row\": self.row,\n        \"col\": self.col,\n        \"row_span\": self.row_span,\n        \"col_span\": self.col_span,\n        \"text\": self.text,\n        \"cell_type\": self.cell_type.value,\n        \"bbox\": self.bbox.to_list() if self.bbox else None,\n        \"confidence\": self.confidence,\n    }\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableOutput","title":"TableOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete table extraction result.</p> <p>Provides multiple export formats and utility methods for working with extracted table data.</p> Example <pre><code>result = extractor.extract(table_image)\n\n# Basic info\nprint(f\"Table: {result.num_rows}x{result.num_cols}\")\n\n# Export to HTML\nhtml = result.to_html()\n\n# Export to Pandas\ndf = result.to_dataframe()\n\n# Export to Markdown\nmd = result.to_markdown()\n\n# Access specific cell\ncell = result.get_cell(row=0, col=0)\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableOutput.cell_count","title":"cell_count  <code>property</code>","text":"<pre><code>cell_count: int\n</code></pre> <p>Number of cells in the table.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableOutput.has_headers","title":"has_headers  <code>property</code>","text":"<pre><code>has_headers: bool\n</code></pre> <p>Check if table has header cells.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableOutput.get_cell","title":"get_cell","text":"<pre><code>get_cell(row: int, col: int) -&gt; Optional[TableCell]\n</code></pre> <p>Get cell at specific position.</p> <p>Handles merged cells by returning the cell that covers the position.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def get_cell(self, row: int, col: int) -&gt; Optional[TableCell]:\n    \"\"\"\n    Get cell at specific position.\n\n    Handles merged cells by returning the cell that covers the position.\n    \"\"\"\n    for cell in self.cells:\n        if cell.row &lt;= row &lt; cell.end_row and cell.col &lt;= col &lt; cell.end_col:\n            return cell\n    return None\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableOutput.get_row","title":"get_row","text":"<pre><code>get_row(row: int) -&gt; List[TableCell]\n</code></pre> <p>Get all cells in a specific row.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def get_row(self, row: int) -&gt; List[TableCell]:\n    \"\"\"Get all cells in a specific row.\"\"\"\n    return [c for c in self.cells if c.row == row]\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableOutput.get_column","title":"get_column","text":"<pre><code>get_column(col: int) -&gt; List[TableCell]\n</code></pre> <p>Get all cells in a specific column.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def get_column(self, col: int) -&gt; List[TableCell]:\n    \"\"\"Get all cells in a specific column.\"\"\"\n    return [c for c in self.cells if c.col == col]\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableOutput.to_html","title":"to_html","text":"<pre><code>to_html(include_styles: bool = True) -&gt; str\n</code></pre> <p>Convert table to HTML string.</p> PARAMETER DESCRIPTION <code>include_styles</code> <p>Whether to include basic CSS styling</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>str</code> <p>HTML table string</p> Example <pre><code>html = result.to_html()\nwith open(\"table.html\", \"w\") as f:\n    f.write(html)\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_html(self, include_styles: bool = True) -&gt; str:\n    \"\"\"\n    Convert table to HTML string.\n\n    Args:\n        include_styles: Whether to include basic CSS styling\n\n    Returns:\n        HTML table string\n\n    Example:\n        ```python\n        html = result.to_html()\n        with open(\"table.html\", \"w\") as f:\n            f.write(html)\n        ```\n    \"\"\"\n    # Build 2D grid accounting for spans\n    grid: List[List[Optional[TableCell]]] = [[None for _ in range(self.num_cols)] for _ in range(self.num_rows)]\n\n    for cell in self.cells:\n        for r in range(cell.row, cell.end_row):\n            for c in range(cell.col, cell.end_col):\n                if r &lt; self.num_rows and c &lt; self.num_cols:\n                    grid[r][c] = cell\n\n    # Generate HTML\n    lines = []\n\n    if include_styles:\n        lines.append('&lt;table style=\"border-collapse: collapse; width: 100%;\"&gt;')\n    else:\n        lines.append(\"&lt;table&gt;\")\n\n    processed: set[Tuple[int, int]] = set()  # Track cells we've already output\n\n    for row_idx in range(self.num_rows):\n        lines.append(\"  &lt;tr&gt;\")\n\n        for col_idx in range(self.num_cols):\n            cell = grid[row_idx][col_idx]\n\n            if cell is None:\n                lines.append(\"    &lt;td&gt;&lt;/td&gt;\")\n                continue\n\n            # Skip if this cell was already output (merged cell)\n            cell_id = (cell.row, cell.col)\n            if cell_id in processed:\n                continue\n            processed.add(cell_id)\n\n            # Determine tag based on cell type\n            tag = \"th\" if cell.is_header else \"td\"\n\n            # Build attributes\n            attrs = []\n            if cell.row_span &gt; 1:\n                attrs.append(f'rowspan=\"{cell.row_span}\"')\n            if cell.col_span &gt; 1:\n                attrs.append(f'colspan=\"{cell.col_span}\"')\n            if include_styles:\n                attrs.append('style=\"border: 1px solid #ddd; padding: 8px;\"')\n\n            attr_str = \" \" + \" \".join(attrs) if attrs else \"\"\n\n            # Escape HTML in text\n            text = (cell.text or \"\").replace(\"&amp;\", \"&amp;amp;\").replace(\"&lt;\", \"&amp;lt;\").replace(\"&gt;\", \"&amp;gt;\")\n\n            lines.append(f\"    &lt;{tag}{attr_str}&gt;{text}&lt;/{tag}&gt;\")\n\n        lines.append(\"  &lt;/tr&gt;\")\n\n    lines.append(\"&lt;/table&gt;\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableOutput.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe()\n</code></pre> <p>Convert table to Pandas DataFrame.</p> RETURNS DESCRIPTION <p>pandas.DataFrame with table data</p> RAISES DESCRIPTION <code>ImportError</code> <p>If pandas is not installed</p> Example <pre><code>df = result.to_dataframe()\nprint(df.head())\ndf.to_csv(\"table.csv\")\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_dataframe(self):\n    \"\"\"\n    Convert table to Pandas DataFrame.\n\n    Returns:\n        pandas.DataFrame with table data\n\n    Raises:\n        ImportError: If pandas is not installed\n\n    Example:\n        ```python\n        df = result.to_dataframe()\n        print(df.head())\n        df.to_csv(\"table.csv\")\n        ```\n    \"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\"pandas is required for to_dataframe(). Install with: pip install pandas\")\n\n    # Build 2D array\n    data: List[List[Optional[str]]] = [[None for _ in range(self.num_cols)] for _ in range(self.num_rows)]\n\n    for cell in self.cells:\n        # For merged cells, put value in top-left position\n        if cell.row &lt; self.num_rows and cell.col &lt; self.num_cols:\n            data[cell.row][cell.col] = cell.text\n\n    # Determine if first row is header\n    first_row_cells = self.get_row(0)\n    use_header = all(c.cell_type == CellType.COLUMN_HEADER for c in first_row_cells) if first_row_cells else False\n\n    if use_header and self.num_rows &gt; 1:\n        headers = data[0]\n        data = data[1:]\n        return pd.DataFrame(data, columns=headers)\n    else:\n        return pd.DataFrame(data)\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableOutput.to_markdown","title":"to_markdown","text":"<pre><code>to_markdown() -&gt; str\n</code></pre> <p>Convert table to Markdown format.</p> <p>Note: Markdown tables don't support merged cells, so spans are ignored and only the top-left cell value is used.</p> RETURNS DESCRIPTION <code>str</code> <p>Markdown table string</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_markdown(self) -&gt; str:\n    \"\"\"\n    Convert table to Markdown format.\n\n    Note: Markdown tables don't support merged cells, so spans\n    are ignored and only the top-left cell value is used.\n\n    Returns:\n        Markdown table string\n    \"\"\"\n    if self.num_rows == 0 or self.num_cols == 0:\n        return \"\"\n\n    # Build 2D grid\n    grid: List[List[str]] = [[\"\" for _ in range(self.num_cols)] for _ in range(self.num_rows)]\n\n    for cell in self.cells:\n        if cell.row &lt; self.num_rows and cell.col &lt; self.num_cols:\n            grid[cell.row][cell.col] = cell.text or \"\"\n\n    lines = []\n\n    # Header row\n    lines.append(\"| \" + \" | \".join(grid[0]) + \" |\")\n\n    # Separator\n    lines.append(\"| \" + \" | \".join([\"---\"] * self.num_cols) + \" |\")\n\n    # Data rows\n    for row in grid[1:]:\n        lines.append(\"| \" + \" | \".join(row) + \" |\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"cells\": [c.to_dict() for c in self.cells],\n        \"num_rows\": self.num_rows,\n        \"num_cols\": self.num_cols,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"html\": self.to_html(include_styles=False),\n    }\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableOutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save to JSON file.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"Save to JSON file.\"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableOutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; TableOutput\n</code></pre> <p>Load from JSON file.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"TableOutput\":\n    \"\"\"Load from JSON file.\"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableFormerConfig","title":"TableFormerConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for TableFormer table structure extractor.</p> <p>TableFormer is a transformer-based model that predicts table structure using OTSL (Optimal Table Structure Language) tags and cell bounding boxes.</p> ATTRIBUTE DESCRIPTION <code>mode</code> <p>Inference mode - \"fast\" or \"accurate\"</p> <p> TYPE: <code>TableFormerMode</code> </p> <code>device</code> <p>Device for inference - \"cpu\", \"cuda\", \"mps\", or \"auto\"</p> <p> TYPE: <code>Literal['cpu', 'cuda', 'mps', 'auto']</code> </p> <code>num_threads</code> <p>Number of CPU threads for inference</p> <p> TYPE: <code>int</code> </p> <code>do_cell_matching</code> <p>Whether to match predicted cells with OCR text cells</p> <p> TYPE: <code>bool</code> </p> <code>artifacts_path</code> <p>Path to pre-downloaded model artifacts</p> <p> TYPE: <code>Optional[str]</code> </p> <code>repo_id</code> <p>HuggingFace model repository</p> <p> TYPE: <code>str</code> </p> <code>revision</code> <p>Model revision/tag</p> <p> TYPE: <code>str</code> </p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Fast mode\nextractor = TableFormerExtractor(config=TableFormerConfig(mode=\"fast\"))\n\n# Accurate mode with GPU\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(\n        mode=\"accurate\",\n        device=\"cuda\",\n        do_cell_matching=True,\n    )\n)\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableFormerExtractor","title":"TableFormerExtractor","text":"<pre><code>TableFormerExtractor(config: TableFormerConfig)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>Table structure extractor using TableFormer model.</p> <p>TableFormer is a transformer-based model that predicts table structure using OTSL (Optimal Table Structure Language) tags. It can detect: - Cell boundaries (bounding boxes) - Row and column spans - Header cells (column and row headers) - Section rows</p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Initialize extractor\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(mode=\"fast\", device=\"cuda\")\n)\n\n# Extract table structure\nresult = extractor.extract(table_image)\n\n# Get HTML output\nhtml = result.to_html()\n\n# Get DataFrame\ndf = result.to_dataframe()\n</code></pre> <p>Initialize TableFormer extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>TableFormerConfig with model settings</p> <p> TYPE: <code>TableFormerConfig</code> </p> Source code in <code>omnidocs/tasks/table_extraction/tableformer/pytorch.py</code> <pre><code>def __init__(self, config: TableFormerConfig):\n    \"\"\"\n    Initialize TableFormer extractor.\n\n    Args:\n        config: TableFormerConfig with model settings\n    \"\"\"\n    self.config = config\n    self._device = _resolve_device(config.device)\n    self._predictor = None\n    self._model_config: Optional[Dict] = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableFormerExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    ocr_output: Optional[OCROutput] = None,\n) -&gt; TableOutput\n</code></pre> <p>Extract table structure from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Table image (should be cropped to table region)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>ocr_output</code> <p>Optional OCR results for cell text matching</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TableOutput</code> <p>TableOutput with cells, structure, and export methods</p> Example <pre><code>result = extractor.extract(table_image)\nprint(f\"Table: {result.num_rows}x{result.num_cols}\")\nhtml = result.to_html()\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/tableformer/pytorch.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    ocr_output: Optional[\"OCROutput\"] = None,\n) -&gt; TableOutput:\n    \"\"\"\n    Extract table structure from an image.\n\n    Args:\n        image: Table image (should be cropped to table region)\n        ocr_output: Optional OCR results for cell text matching\n\n    Returns:\n        TableOutput with cells, structure, and export methods\n\n    Example:\n        ```python\n        result = extractor.extract(table_image)\n        print(f\"Table: {result.num_rows}x{result.num_cols}\")\n        html = result.to_html()\n        ```\n    \"\"\"\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Convert to OpenCV format (required by TFPredictor)\n    try:\n        import cv2\n    except ImportError:\n        raise ImportError(\n            \"opencv-python is required for TableFormerExtractor. Install with: pip install opencv-python-headless\"\n        )\n\n    cv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n\n    # Build iOCR page data\n    tokens = self._build_tokens_from_ocr(ocr_output) if ocr_output else []\n    iocr_page = {\n        \"width\": width,\n        \"height\": height,\n        \"image\": cv_image,\n        \"tokens\": tokens,\n    }\n\n    # Table bbox is the entire image\n    table_bbox = [0, 0, width, height]\n\n    # Run prediction\n    results = self._predictor.multi_table_predict(\n        iocr_page=iocr_page,\n        table_bboxes=[table_bbox],\n        do_matching=self.config.do_cell_matching,\n        correct_overlapping_cells=self.config.correct_overlapping_cells,\n        sort_row_col_indexes=self.config.sort_row_col_indexes,\n    )\n\n    # Convert results to TableOutput\n    return self._convert_results(results, width, height)\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.TableFormerMode","title":"TableFormerMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>TableFormer inference mode.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.base","title":"base","text":"<p>Base class for table extractors.</p> <p>Defines the abstract interface that all table extractors must implement.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.base.BaseTableExtractor","title":"BaseTableExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for table structure extractors.</p> <p>Table extractors analyze table images to detect cell structure, identify headers, and extract text content.</p> Example <pre><code>class MyTableExtractor(BaseTableExtractor):\n    def __init__(self, config: MyConfig):\n        self.config = config\n        self._load_model()\n\n    def _load_model(self):\n        # Load model weights\n        pass\n\n    def extract(self, image):\n        # Run extraction\n        return TableOutput(...)\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.base.BaseTableExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    ocr_output: Optional[OCROutput] = None,\n) -&gt; TableOutput\n</code></pre> <p>Extract table structure from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Table image (should be cropped to table region)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>ocr_output</code> <p>Optional OCR results for cell text matching.        If not provided, model will attempt to extract text.</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TableOutput</code> <p>TableOutput with cells, structure, and export methods</p> Example <pre><code># Without OCR (model extracts text)\nresult = extractor.extract(table_image)\n\n# With OCR (better text quality)\nocr = some_ocr.extract(table_image)\nresult = extractor.extract(table_image, ocr_output=ocr)\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    ocr_output: Optional[\"OCROutput\"] = None,\n) -&gt; TableOutput:\n    \"\"\"\n    Extract table structure from an image.\n\n    Args:\n        image: Table image (should be cropped to table region)\n        ocr_output: Optional OCR results for cell text matching.\n                   If not provided, model will attempt to extract text.\n\n    Returns:\n        TableOutput with cells, structure, and export methods\n\n    Example:\n        ```python\n        # Without OCR (model extracts text)\n        result = extractor.extract(table_image)\n\n        # With OCR (better text quality)\n        ocr = some_ocr.extract(table_image)\n        result = extractor.extract(table_image, ocr_output=ocr)\n        ```\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.base.BaseTableExtractor.batch_extract","title":"batch_extract","text":"<pre><code>batch_extract(\n    images: List[Union[Image, ndarray, str, Path]],\n    ocr_outputs: Optional[List[OCROutput]] = None,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[TableOutput]\n</code></pre> <p>Extract tables from multiple images.</p> <p>Default implementation loops over extract(). Subclasses can override for optimized batching.</p> PARAMETER DESCRIPTION <code>images</code> <p>List of table images</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>ocr_outputs</code> <p>Optional list of OCR results (same length as images)</p> <p> TYPE: <code>Optional[List[OCROutput]]</code> DEFAULT: <code>None</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TableOutput]</code> <p>List of TableOutput in same order as input</p> <p>Examples:</p> <pre><code>results = extractor.batch_extract(table_images)\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def batch_extract(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    ocr_outputs: Optional[List[\"OCROutput\"]] = None,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[TableOutput]:\n    \"\"\"\n    Extract tables from multiple images.\n\n    Default implementation loops over extract(). Subclasses can override\n    for optimized batching.\n\n    Args:\n        images: List of table images\n        ocr_outputs: Optional list of OCR results (same length as images)\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of TableOutput in same order as input\n\n    Examples:\n        ```python\n        results = extractor.batch_extract(table_images)\n        ```\n    \"\"\"\n    results = []\n    total = len(images)\n\n    for i, image in enumerate(images):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        ocr = ocr_outputs[i] if ocr_outputs else None\n        result = self.extract(image, ocr_output=ocr)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.base.BaseTableExtractor.extract_document","title":"extract_document","text":"<pre><code>extract_document(\n    document: Document,\n    table_bboxes: Optional[List[List[float]]] = None,\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[TableOutput]\n</code></pre> <p>Extract tables from all pages of a document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>table_bboxes</code> <p>Optional list of table bounding boxes per page.          Each element should be a list of [x1, y1, x2, y2] coords.</p> <p> TYPE: <code>Optional[List[List[float]]]</code> DEFAULT: <code>None</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TableOutput]</code> <p>List of TableOutput, one per detected table</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresults = extractor.extract_document(doc)\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/base.py</code> <pre><code>def extract_document(\n    self,\n    document: \"Document\",\n    table_bboxes: Optional[List[List[float]]] = None,\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[TableOutput]:\n    \"\"\"\n    Extract tables from all pages of a document.\n\n    Args:\n        document: Document instance\n        table_bboxes: Optional list of table bounding boxes per page.\n                     Each element should be a list of [x1, y1, x2, y2] coords.\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of TableOutput, one per detected table\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        results = extractor.extract_document(doc)\n        ```\n    \"\"\"\n    results = []\n    total = document.page_count\n\n    for i, page in enumerate(document.iter_pages()):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        # If no bboxes provided, process entire page\n        if table_bboxes is None:\n            result = self.extract(page)\n            results.append(result)\n        else:\n            # Crop and process each table region\n            for bbox in table_bboxes:\n                x1, y1, x2, y2 = bbox\n                table_region = page.crop((x1, y1, x2, y2))\n                result = self.extract(table_region)\n                results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models","title":"models","text":"<p>Pydantic models for table extraction outputs.</p> <p>Provides structured table data with cells, spans, and multiple export formats including HTML, Markdown, and Pandas DataFrame conversion.</p> Example <pre><code>result = extractor.extract(table_image)\n\n# Get HTML\nhtml = result.to_html()\n\n# Get Pandas DataFrame\ndf = result.to_dataframe()\n\n# Access cells\nfor cell in result.cells:\n    print(f\"[{cell.row},{cell.col}] {cell.text}\")\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.CellType","title":"CellType","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Type of table cell.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.BoundingBox","title":"BoundingBox","text":"<p>               Bases: <code>BaseModel</code></p> <p>Bounding box in pixel coordinates.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.BoundingBox.width","title":"width  <code>property</code>","text":"<pre><code>width: float\n</code></pre> <p>Width of the bounding box.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.BoundingBox.height","title":"height  <code>property</code>","text":"<pre><code>height: float\n</code></pre> <p>Height of the bounding box.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.BoundingBox.area","title":"area  <code>property</code>","text":"<pre><code>area: float\n</code></pre> <p>Area of the bounding box.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.BoundingBox.center","title":"center  <code>property</code>","text":"<pre><code>center: Tuple[float, float]\n</code></pre> <p>Center point of the bounding box.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.BoundingBox.to_list","title":"to_list","text":"<pre><code>to_list() -&gt; List[float]\n</code></pre> <p>Convert to [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_list(self) -&gt; List[float]:\n    \"\"\"Convert to [x1, y1, x2, y2] list.\"\"\"\n    return [self.x1, self.y1, self.x2, self.y2]\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.BoundingBox.to_xyxy","title":"to_xyxy","text":"<pre><code>to_xyxy() -&gt; Tuple[float, float, float, float]\n</code></pre> <p>Convert to (x1, y1, x2, y2) tuple.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_xyxy(self) -&gt; Tuple[float, float, float, float]:\n    \"\"\"Convert to (x1, y1, x2, y2) tuple.\"\"\"\n    return (self.x1, self.y1, self.x2, self.y2)\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.BoundingBox.from_list","title":"from_list  <code>classmethod</code>","text":"<pre><code>from_list(coords: List[float]) -&gt; BoundingBox\n</code></pre> <p>Create from [x1, y1, x2, y2] list.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>@classmethod\ndef from_list(cls, coords: List[float]) -&gt; \"BoundingBox\":\n    \"\"\"Create from [x1, y1, x2, y2] list.\"\"\"\n    if len(coords) != 4:\n        raise ValueError(f\"Expected 4 coordinates, got {len(coords)}\")\n    return cls(x1=coords[0], y1=coords[1], x2=coords[2], y2=coords[3])\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.BoundingBox.from_ltrb","title":"from_ltrb  <code>classmethod</code>","text":"<pre><code>from_ltrb(\n    left: float, top: float, right: float, bottom: float\n) -&gt; BoundingBox\n</code></pre> <p>Create from left, top, right, bottom coordinates.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>@classmethod\ndef from_ltrb(cls, left: float, top: float, right: float, bottom: float) -&gt; \"BoundingBox\":\n    \"\"\"Create from left, top, right, bottom coordinates.\"\"\"\n    return cls(x1=left, y1=top, x2=right, y2=bottom)\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.BoundingBox.to_normalized","title":"to_normalized","text":"<pre><code>to_normalized(\n    image_width: int, image_height: int\n) -&gt; BoundingBox\n</code></pre> <p>Convert to normalized coordinates (0-1024 range).</p> PARAMETER DESCRIPTION <code>image_width</code> <p>Original image width in pixels</p> <p> TYPE: <code>int</code> </p> <code>image_height</code> <p>Original image height in pixels</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>BoundingBox</code> <p>New BoundingBox with coordinates in 0-1024 range</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_normalized(self, image_width: int, image_height: int) -&gt; \"BoundingBox\":\n    \"\"\"\n    Convert to normalized coordinates (0-1024 range).\n\n    Args:\n        image_width: Original image width in pixels\n        image_height: Original image height in pixels\n\n    Returns:\n        New BoundingBox with coordinates in 0-1024 range\n    \"\"\"\n    return BoundingBox(\n        x1=self.x1 / image_width * NORMALIZED_SIZE,\n        y1=self.y1 / image_height * NORMALIZED_SIZE,\n        x2=self.x2 / image_width * NORMALIZED_SIZE,\n        y2=self.y2 / image_height * NORMALIZED_SIZE,\n    )\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.TableCell","title":"TableCell","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single table cell with position, span, and content.</p> <p>The cell position uses 0-indexed row/column indices. Spans indicate how many rows/columns the cell occupies.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.TableCell.end_row","title":"end_row  <code>property</code>","text":"<pre><code>end_row: int\n</code></pre> <p>Ending row index (exclusive).</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.TableCell.end_col","title":"end_col  <code>property</code>","text":"<pre><code>end_col: int\n</code></pre> <p>Ending column index (exclusive).</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.TableCell.is_header","title":"is_header  <code>property</code>","text":"<pre><code>is_header: bool\n</code></pre> <p>Check if cell is any type of header.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.TableCell.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"row\": self.row,\n        \"col\": self.col,\n        \"row_span\": self.row_span,\n        \"col_span\": self.col_span,\n        \"text\": self.text,\n        \"cell_type\": self.cell_type.value,\n        \"bbox\": self.bbox.to_list() if self.bbox else None,\n        \"confidence\": self.confidence,\n    }\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.TableOutput","title":"TableOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Complete table extraction result.</p> <p>Provides multiple export formats and utility methods for working with extracted table data.</p> Example <pre><code>result = extractor.extract(table_image)\n\n# Basic info\nprint(f\"Table: {result.num_rows}x{result.num_cols}\")\n\n# Export to HTML\nhtml = result.to_html()\n\n# Export to Pandas\ndf = result.to_dataframe()\n\n# Export to Markdown\nmd = result.to_markdown()\n\n# Access specific cell\ncell = result.get_cell(row=0, col=0)\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.TableOutput.cell_count","title":"cell_count  <code>property</code>","text":"<pre><code>cell_count: int\n</code></pre> <p>Number of cells in the table.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.TableOutput.has_headers","title":"has_headers  <code>property</code>","text":"<pre><code>has_headers: bool\n</code></pre> <p>Check if table has header cells.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.TableOutput.get_cell","title":"get_cell","text":"<pre><code>get_cell(row: int, col: int) -&gt; Optional[TableCell]\n</code></pre> <p>Get cell at specific position.</p> <p>Handles merged cells by returning the cell that covers the position.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def get_cell(self, row: int, col: int) -&gt; Optional[TableCell]:\n    \"\"\"\n    Get cell at specific position.\n\n    Handles merged cells by returning the cell that covers the position.\n    \"\"\"\n    for cell in self.cells:\n        if cell.row &lt;= row &lt; cell.end_row and cell.col &lt;= col &lt; cell.end_col:\n            return cell\n    return None\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.TableOutput.get_row","title":"get_row","text":"<pre><code>get_row(row: int) -&gt; List[TableCell]\n</code></pre> <p>Get all cells in a specific row.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def get_row(self, row: int) -&gt; List[TableCell]:\n    \"\"\"Get all cells in a specific row.\"\"\"\n    return [c for c in self.cells if c.row == row]\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.TableOutput.get_column","title":"get_column","text":"<pre><code>get_column(col: int) -&gt; List[TableCell]\n</code></pre> <p>Get all cells in a specific column.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def get_column(self, col: int) -&gt; List[TableCell]:\n    \"\"\"Get all cells in a specific column.\"\"\"\n    return [c for c in self.cells if c.col == col]\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.TableOutput.to_html","title":"to_html","text":"<pre><code>to_html(include_styles: bool = True) -&gt; str\n</code></pre> <p>Convert table to HTML string.</p> PARAMETER DESCRIPTION <code>include_styles</code> <p>Whether to include basic CSS styling</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>str</code> <p>HTML table string</p> Example <pre><code>html = result.to_html()\nwith open(\"table.html\", \"w\") as f:\n    f.write(html)\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_html(self, include_styles: bool = True) -&gt; str:\n    \"\"\"\n    Convert table to HTML string.\n\n    Args:\n        include_styles: Whether to include basic CSS styling\n\n    Returns:\n        HTML table string\n\n    Example:\n        ```python\n        html = result.to_html()\n        with open(\"table.html\", \"w\") as f:\n            f.write(html)\n        ```\n    \"\"\"\n    # Build 2D grid accounting for spans\n    grid: List[List[Optional[TableCell]]] = [[None for _ in range(self.num_cols)] for _ in range(self.num_rows)]\n\n    for cell in self.cells:\n        for r in range(cell.row, cell.end_row):\n            for c in range(cell.col, cell.end_col):\n                if r &lt; self.num_rows and c &lt; self.num_cols:\n                    grid[r][c] = cell\n\n    # Generate HTML\n    lines = []\n\n    if include_styles:\n        lines.append('&lt;table style=\"border-collapse: collapse; width: 100%;\"&gt;')\n    else:\n        lines.append(\"&lt;table&gt;\")\n\n    processed: set[Tuple[int, int]] = set()  # Track cells we've already output\n\n    for row_idx in range(self.num_rows):\n        lines.append(\"  &lt;tr&gt;\")\n\n        for col_idx in range(self.num_cols):\n            cell = grid[row_idx][col_idx]\n\n            if cell is None:\n                lines.append(\"    &lt;td&gt;&lt;/td&gt;\")\n                continue\n\n            # Skip if this cell was already output (merged cell)\n            cell_id = (cell.row, cell.col)\n            if cell_id in processed:\n                continue\n            processed.add(cell_id)\n\n            # Determine tag based on cell type\n            tag = \"th\" if cell.is_header else \"td\"\n\n            # Build attributes\n            attrs = []\n            if cell.row_span &gt; 1:\n                attrs.append(f'rowspan=\"{cell.row_span}\"')\n            if cell.col_span &gt; 1:\n                attrs.append(f'colspan=\"{cell.col_span}\"')\n            if include_styles:\n                attrs.append('style=\"border: 1px solid #ddd; padding: 8px;\"')\n\n            attr_str = \" \" + \" \".join(attrs) if attrs else \"\"\n\n            # Escape HTML in text\n            text = (cell.text or \"\").replace(\"&amp;\", \"&amp;amp;\").replace(\"&lt;\", \"&amp;lt;\").replace(\"&gt;\", \"&amp;gt;\")\n\n            lines.append(f\"    &lt;{tag}{attr_str}&gt;{text}&lt;/{tag}&gt;\")\n\n        lines.append(\"  &lt;/tr&gt;\")\n\n    lines.append(\"&lt;/table&gt;\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.TableOutput.to_dataframe","title":"to_dataframe","text":"<pre><code>to_dataframe()\n</code></pre> <p>Convert table to Pandas DataFrame.</p> RETURNS DESCRIPTION <p>pandas.DataFrame with table data</p> RAISES DESCRIPTION <code>ImportError</code> <p>If pandas is not installed</p> Example <pre><code>df = result.to_dataframe()\nprint(df.head())\ndf.to_csv(\"table.csv\")\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_dataframe(self):\n    \"\"\"\n    Convert table to Pandas DataFrame.\n\n    Returns:\n        pandas.DataFrame with table data\n\n    Raises:\n        ImportError: If pandas is not installed\n\n    Example:\n        ```python\n        df = result.to_dataframe()\n        print(df.head())\n        df.to_csv(\"table.csv\")\n        ```\n    \"\"\"\n    try:\n        import pandas as pd\n    except ImportError:\n        raise ImportError(\"pandas is required for to_dataframe(). Install with: pip install pandas\")\n\n    # Build 2D array\n    data: List[List[Optional[str]]] = [[None for _ in range(self.num_cols)] for _ in range(self.num_rows)]\n\n    for cell in self.cells:\n        # For merged cells, put value in top-left position\n        if cell.row &lt; self.num_rows and cell.col &lt; self.num_cols:\n            data[cell.row][cell.col] = cell.text\n\n    # Determine if first row is header\n    first_row_cells = self.get_row(0)\n    use_header = all(c.cell_type == CellType.COLUMN_HEADER for c in first_row_cells) if first_row_cells else False\n\n    if use_header and self.num_rows &gt; 1:\n        headers = data[0]\n        data = data[1:]\n        return pd.DataFrame(data, columns=headers)\n    else:\n        return pd.DataFrame(data)\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.TableOutput.to_markdown","title":"to_markdown","text":"<pre><code>to_markdown() -&gt; str\n</code></pre> <p>Convert table to Markdown format.</p> <p>Note: Markdown tables don't support merged cells, so spans are ignored and only the top-left cell value is used.</p> RETURNS DESCRIPTION <code>str</code> <p>Markdown table string</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_markdown(self) -&gt; str:\n    \"\"\"\n    Convert table to Markdown format.\n\n    Note: Markdown tables don't support merged cells, so spans\n    are ignored and only the top-left cell value is used.\n\n    Returns:\n        Markdown table string\n    \"\"\"\n    if self.num_rows == 0 or self.num_cols == 0:\n        return \"\"\n\n    # Build 2D grid\n    grid: List[List[str]] = [[\"\" for _ in range(self.num_cols)] for _ in range(self.num_rows)]\n\n    for cell in self.cells:\n        if cell.row &lt; self.num_rows and cell.col &lt; self.num_cols:\n            grid[cell.row][cell.col] = cell.text or \"\"\n\n    lines = []\n\n    # Header row\n    lines.append(\"| \" + \" | \".join(grid[0]) + \" |\")\n\n    # Separator\n    lines.append(\"| \" + \" | \".join([\"---\"] * self.num_cols) + \" |\")\n\n    # Data rows\n    for row in grid[1:]:\n        lines.append(\"| \" + \" | \".join(row) + \" |\")\n\n    return \"\\n\".join(lines)\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.TableOutput.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; Dict\n</code></pre> <p>Convert to dictionary representation.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def to_dict(self) -&gt; Dict:\n    \"\"\"Convert to dictionary representation.\"\"\"\n    return {\n        \"cells\": [c.to_dict() for c in self.cells],\n        \"num_rows\": self.num_rows,\n        \"num_cols\": self.num_cols,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"model_name\": self.model_name,\n        \"html\": self.to_html(include_styles=False),\n    }\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.TableOutput.save_json","title":"save_json","text":"<pre><code>save_json(file_path: Union[str, Path]) -&gt; None\n</code></pre> <p>Save to JSON file.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>def save_json(self, file_path: Union[str, Path]) -&gt; None:\n    \"\"\"Save to JSON file.\"\"\"\n    path = Path(file_path)\n    path.parent.mkdir(parents=True, exist_ok=True)\n    path.write_text(self.model_dump_json(indent=2), encoding=\"utf-8\")\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.models.TableOutput.load_json","title":"load_json  <code>classmethod</code>","text":"<pre><code>load_json(file_path: Union[str, Path]) -&gt; TableOutput\n</code></pre> <p>Load from JSON file.</p> Source code in <code>omnidocs/tasks/table_extraction/models.py</code> <pre><code>@classmethod\ndef load_json(cls, file_path: Union[str, Path]) -&gt; \"TableOutput\":\n    \"\"\"Load from JSON file.\"\"\"\n    path = Path(file_path)\n    return cls.model_validate_json(path.read_text(encoding=\"utf-8\"))\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.tableformer","title":"tableformer","text":"<p>TableFormer module for table structure extraction.</p> <p>Provides the TableFormer-based table structure extractor.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.tableformer.TableFormerConfig","title":"TableFormerConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for TableFormer table structure extractor.</p> <p>TableFormer is a transformer-based model that predicts table structure using OTSL (Optimal Table Structure Language) tags and cell bounding boxes.</p> ATTRIBUTE DESCRIPTION <code>mode</code> <p>Inference mode - \"fast\" or \"accurate\"</p> <p> TYPE: <code>TableFormerMode</code> </p> <code>device</code> <p>Device for inference - \"cpu\", \"cuda\", \"mps\", or \"auto\"</p> <p> TYPE: <code>Literal['cpu', 'cuda', 'mps', 'auto']</code> </p> <code>num_threads</code> <p>Number of CPU threads for inference</p> <p> TYPE: <code>int</code> </p> <code>do_cell_matching</code> <p>Whether to match predicted cells with OCR text cells</p> <p> TYPE: <code>bool</code> </p> <code>artifacts_path</code> <p>Path to pre-downloaded model artifacts</p> <p> TYPE: <code>Optional[str]</code> </p> <code>repo_id</code> <p>HuggingFace model repository</p> <p> TYPE: <code>str</code> </p> <code>revision</code> <p>Model revision/tag</p> <p> TYPE: <code>str</code> </p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Fast mode\nextractor = TableFormerExtractor(config=TableFormerConfig(mode=\"fast\"))\n\n# Accurate mode with GPU\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(\n        mode=\"accurate\",\n        device=\"cuda\",\n        do_cell_matching=True,\n    )\n)\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.tableformer.TableFormerMode","title":"TableFormerMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>TableFormer inference mode.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.tableformer.TableFormerExtractor","title":"TableFormerExtractor","text":"<pre><code>TableFormerExtractor(config: TableFormerConfig)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>Table structure extractor using TableFormer model.</p> <p>TableFormer is a transformer-based model that predicts table structure using OTSL (Optimal Table Structure Language) tags. It can detect: - Cell boundaries (bounding boxes) - Row and column spans - Header cells (column and row headers) - Section rows</p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Initialize extractor\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(mode=\"fast\", device=\"cuda\")\n)\n\n# Extract table structure\nresult = extractor.extract(table_image)\n\n# Get HTML output\nhtml = result.to_html()\n\n# Get DataFrame\ndf = result.to_dataframe()\n</code></pre> <p>Initialize TableFormer extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>TableFormerConfig with model settings</p> <p> TYPE: <code>TableFormerConfig</code> </p> Source code in <code>omnidocs/tasks/table_extraction/tableformer/pytorch.py</code> <pre><code>def __init__(self, config: TableFormerConfig):\n    \"\"\"\n    Initialize TableFormer extractor.\n\n    Args:\n        config: TableFormerConfig with model settings\n    \"\"\"\n    self.config = config\n    self._device = _resolve_device(config.device)\n    self._predictor = None\n    self._model_config: Optional[Dict] = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.tableformer.TableFormerExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    ocr_output: Optional[OCROutput] = None,\n) -&gt; TableOutput\n</code></pre> <p>Extract table structure from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Table image (should be cropped to table region)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>ocr_output</code> <p>Optional OCR results for cell text matching</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TableOutput</code> <p>TableOutput with cells, structure, and export methods</p> Example <pre><code>result = extractor.extract(table_image)\nprint(f\"Table: {result.num_rows}x{result.num_cols}\")\nhtml = result.to_html()\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/tableformer/pytorch.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    ocr_output: Optional[\"OCROutput\"] = None,\n) -&gt; TableOutput:\n    \"\"\"\n    Extract table structure from an image.\n\n    Args:\n        image: Table image (should be cropped to table region)\n        ocr_output: Optional OCR results for cell text matching\n\n    Returns:\n        TableOutput with cells, structure, and export methods\n\n    Example:\n        ```python\n        result = extractor.extract(table_image)\n        print(f\"Table: {result.num_rows}x{result.num_cols}\")\n        html = result.to_html()\n        ```\n    \"\"\"\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Convert to OpenCV format (required by TFPredictor)\n    try:\n        import cv2\n    except ImportError:\n        raise ImportError(\n            \"opencv-python is required for TableFormerExtractor. Install with: pip install opencv-python-headless\"\n        )\n\n    cv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n\n    # Build iOCR page data\n    tokens = self._build_tokens_from_ocr(ocr_output) if ocr_output else []\n    iocr_page = {\n        \"width\": width,\n        \"height\": height,\n        \"image\": cv_image,\n        \"tokens\": tokens,\n    }\n\n    # Table bbox is the entire image\n    table_bbox = [0, 0, width, height]\n\n    # Run prediction\n    results = self._predictor.multi_table_predict(\n        iocr_page=iocr_page,\n        table_bboxes=[table_bbox],\n        do_matching=self.config.do_cell_matching,\n        correct_overlapping_cells=self.config.correct_overlapping_cells,\n        sort_row_col_indexes=self.config.sort_row_col_indexes,\n    )\n\n    # Convert results to TableOutput\n    return self._convert_results(results, width, height)\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.tableformer.config","title":"config","text":"<p>Configuration for TableFormer table structure extractor.</p> <p>TableFormer uses a dual-decoder transformer architecture with OTSL+ support for recognizing table structure from images.</p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Fast mode (default)\nextractor = TableFormerExtractor(config=TableFormerConfig())\n\n# Accurate mode with GPU\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(\n        mode=\"accurate\",\n        device=\"cuda\",\n        do_cell_matching=True,\n    )\n)\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.tableformer.config.TableFormerMode","title":"TableFormerMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>TableFormer inference mode.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.tableformer.config.TableFormerConfig","title":"TableFormerConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for TableFormer table structure extractor.</p> <p>TableFormer is a transformer-based model that predicts table structure using OTSL (Optimal Table Structure Language) tags and cell bounding boxes.</p> ATTRIBUTE DESCRIPTION <code>mode</code> <p>Inference mode - \"fast\" or \"accurate\"</p> <p> TYPE: <code>TableFormerMode</code> </p> <code>device</code> <p>Device for inference - \"cpu\", \"cuda\", \"mps\", or \"auto\"</p> <p> TYPE: <code>Literal['cpu', 'cuda', 'mps', 'auto']</code> </p> <code>num_threads</code> <p>Number of CPU threads for inference</p> <p> TYPE: <code>int</code> </p> <code>do_cell_matching</code> <p>Whether to match predicted cells with OCR text cells</p> <p> TYPE: <code>bool</code> </p> <code>artifacts_path</code> <p>Path to pre-downloaded model artifacts</p> <p> TYPE: <code>Optional[str]</code> </p> <code>repo_id</code> <p>HuggingFace model repository</p> <p> TYPE: <code>str</code> </p> <code>revision</code> <p>Model revision/tag</p> <p> TYPE: <code>str</code> </p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Fast mode\nextractor = TableFormerExtractor(config=TableFormerConfig(mode=\"fast\"))\n\n# Accurate mode with GPU\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(\n        mode=\"accurate\",\n        device=\"cuda\",\n        do_cell_matching=True,\n    )\n)\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.tableformer.pytorch","title":"pytorch","text":"<p>TableFormer extractor implementation using PyTorch backend.</p> <p>Uses the TFPredictor from docling-ibm-models for table structure recognition.</p>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.tableformer.pytorch.TableFormerExtractor","title":"TableFormerExtractor","text":"<pre><code>TableFormerExtractor(config: TableFormerConfig)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>Table structure extractor using TableFormer model.</p> <p>TableFormer is a transformer-based model that predicts table structure using OTSL (Optimal Table Structure Language) tags. It can detect: - Cell boundaries (bounding boxes) - Row and column spans - Header cells (column and row headers) - Section rows</p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Initialize extractor\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(mode=\"fast\", device=\"cuda\")\n)\n\n# Extract table structure\nresult = extractor.extract(table_image)\n\n# Get HTML output\nhtml = result.to_html()\n\n# Get DataFrame\ndf = result.to_dataframe()\n</code></pre> <p>Initialize TableFormer extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>TableFormerConfig with model settings</p> <p> TYPE: <code>TableFormerConfig</code> </p> Source code in <code>omnidocs/tasks/table_extraction/tableformer/pytorch.py</code> <pre><code>def __init__(self, config: TableFormerConfig):\n    \"\"\"\n    Initialize TableFormer extractor.\n\n    Args:\n        config: TableFormerConfig with model settings\n    \"\"\"\n    self.config = config\n    self._device = _resolve_device(config.device)\n    self._predictor = None\n    self._model_config: Optional[Dict] = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/table_extraction/overview/#omnidocs.tasks.table_extraction.tableformer.pytorch.TableFormerExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    ocr_output: Optional[OCROutput] = None,\n) -&gt; TableOutput\n</code></pre> <p>Extract table structure from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Table image (should be cropped to table region)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>ocr_output</code> <p>Optional OCR results for cell text matching</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TableOutput</code> <p>TableOutput with cells, structure, and export methods</p> Example <pre><code>result = extractor.extract(table_image)\nprint(f\"Table: {result.num_rows}x{result.num_cols}\")\nhtml = result.to_html()\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/tableformer/pytorch.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    ocr_output: Optional[\"OCROutput\"] = None,\n) -&gt; TableOutput:\n    \"\"\"\n    Extract table structure from an image.\n\n    Args:\n        image: Table image (should be cropped to table region)\n        ocr_output: Optional OCR results for cell text matching\n\n    Returns:\n        TableOutput with cells, structure, and export methods\n\n    Example:\n        ```python\n        result = extractor.extract(table_image)\n        print(f\"Table: {result.num_rows}x{result.num_cols}\")\n        html = result.to_html()\n        ```\n    \"\"\"\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Convert to OpenCV format (required by TFPredictor)\n    try:\n        import cv2\n    except ImportError:\n        raise ImportError(\n            \"opencv-python is required for TableFormerExtractor. Install with: pip install opencv-python-headless\"\n        )\n\n    cv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n\n    # Build iOCR page data\n    tokens = self._build_tokens_from_ocr(ocr_output) if ocr_output else []\n    iocr_page = {\n        \"width\": width,\n        \"height\": height,\n        \"image\": cv_image,\n        \"tokens\": tokens,\n    }\n\n    # Table bbox is the entire image\n    table_bbox = [0, 0, width, height]\n\n    # Run prediction\n    results = self._predictor.multi_table_predict(\n        iocr_page=iocr_page,\n        table_bboxes=[table_bbox],\n        do_matching=self.config.do_cell_matching,\n        correct_overlapping_cells=self.config.correct_overlapping_cells,\n        sort_row_col_indexes=self.config.sort_row_col_indexes,\n    )\n\n    # Convert results to TableOutput\n    return self._convert_results(results, width, height)\n</code></pre>"},{"location":"reference/tasks/table_extraction/tableformer/config/","title":"Config","text":"<p>Configuration for TableFormer table structure extractor.</p> <p>TableFormer uses a dual-decoder transformer architecture with OTSL+ support for recognizing table structure from images.</p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Fast mode (default)\nextractor = TableFormerExtractor(config=TableFormerConfig())\n\n# Accurate mode with GPU\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(\n        mode=\"accurate\",\n        device=\"cuda\",\n        do_cell_matching=True,\n    )\n)\n</code></pre>"},{"location":"reference/tasks/table_extraction/tableformer/config/#omnidocs.tasks.table_extraction.tableformer.config.TableFormerMode","title":"TableFormerMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>TableFormer inference mode.</p>"},{"location":"reference/tasks/table_extraction/tableformer/config/#omnidocs.tasks.table_extraction.tableformer.config.TableFormerConfig","title":"TableFormerConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for TableFormer table structure extractor.</p> <p>TableFormer is a transformer-based model that predicts table structure using OTSL (Optimal Table Structure Language) tags and cell bounding boxes.</p> ATTRIBUTE DESCRIPTION <code>mode</code> <p>Inference mode - \"fast\" or \"accurate\"</p> <p> TYPE: <code>TableFormerMode</code> </p> <code>device</code> <p>Device for inference - \"cpu\", \"cuda\", \"mps\", or \"auto\"</p> <p> TYPE: <code>Literal['cpu', 'cuda', 'mps', 'auto']</code> </p> <code>num_threads</code> <p>Number of CPU threads for inference</p> <p> TYPE: <code>int</code> </p> <code>do_cell_matching</code> <p>Whether to match predicted cells with OCR text cells</p> <p> TYPE: <code>bool</code> </p> <code>artifacts_path</code> <p>Path to pre-downloaded model artifacts</p> <p> TYPE: <code>Optional[str]</code> </p> <code>repo_id</code> <p>HuggingFace model repository</p> <p> TYPE: <code>str</code> </p> <code>revision</code> <p>Model revision/tag</p> <p> TYPE: <code>str</code> </p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Fast mode\nextractor = TableFormerExtractor(config=TableFormerConfig(mode=\"fast\"))\n\n# Accurate mode with GPU\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(\n        mode=\"accurate\",\n        device=\"cuda\",\n        do_cell_matching=True,\n    )\n)\n</code></pre>"},{"location":"reference/tasks/table_extraction/tableformer/overview/","title":"Overview","text":"<p>TableFormer module for table structure extraction.</p> <p>Provides the TableFormer-based table structure extractor.</p>"},{"location":"reference/tasks/table_extraction/tableformer/overview/#omnidocs.tasks.table_extraction.tableformer.TableFormerConfig","title":"TableFormerConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for TableFormer table structure extractor.</p> <p>TableFormer is a transformer-based model that predicts table structure using OTSL (Optimal Table Structure Language) tags and cell bounding boxes.</p> ATTRIBUTE DESCRIPTION <code>mode</code> <p>Inference mode - \"fast\" or \"accurate\"</p> <p> TYPE: <code>TableFormerMode</code> </p> <code>device</code> <p>Device for inference - \"cpu\", \"cuda\", \"mps\", or \"auto\"</p> <p> TYPE: <code>Literal['cpu', 'cuda', 'mps', 'auto']</code> </p> <code>num_threads</code> <p>Number of CPU threads for inference</p> <p> TYPE: <code>int</code> </p> <code>do_cell_matching</code> <p>Whether to match predicted cells with OCR text cells</p> <p> TYPE: <code>bool</code> </p> <code>artifacts_path</code> <p>Path to pre-downloaded model artifacts</p> <p> TYPE: <code>Optional[str]</code> </p> <code>repo_id</code> <p>HuggingFace model repository</p> <p> TYPE: <code>str</code> </p> <code>revision</code> <p>Model revision/tag</p> <p> TYPE: <code>str</code> </p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Fast mode\nextractor = TableFormerExtractor(config=TableFormerConfig(mode=\"fast\"))\n\n# Accurate mode with GPU\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(\n        mode=\"accurate\",\n        device=\"cuda\",\n        do_cell_matching=True,\n    )\n)\n</code></pre>"},{"location":"reference/tasks/table_extraction/tableformer/overview/#omnidocs.tasks.table_extraction.tableformer.TableFormerMode","title":"TableFormerMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>TableFormer inference mode.</p>"},{"location":"reference/tasks/table_extraction/tableformer/overview/#omnidocs.tasks.table_extraction.tableformer.TableFormerExtractor","title":"TableFormerExtractor","text":"<pre><code>TableFormerExtractor(config: TableFormerConfig)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>Table structure extractor using TableFormer model.</p> <p>TableFormer is a transformer-based model that predicts table structure using OTSL (Optimal Table Structure Language) tags. It can detect: - Cell boundaries (bounding boxes) - Row and column spans - Header cells (column and row headers) - Section rows</p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Initialize extractor\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(mode=\"fast\", device=\"cuda\")\n)\n\n# Extract table structure\nresult = extractor.extract(table_image)\n\n# Get HTML output\nhtml = result.to_html()\n\n# Get DataFrame\ndf = result.to_dataframe()\n</code></pre> <p>Initialize TableFormer extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>TableFormerConfig with model settings</p> <p> TYPE: <code>TableFormerConfig</code> </p> Source code in <code>omnidocs/tasks/table_extraction/tableformer/pytorch.py</code> <pre><code>def __init__(self, config: TableFormerConfig):\n    \"\"\"\n    Initialize TableFormer extractor.\n\n    Args:\n        config: TableFormerConfig with model settings\n    \"\"\"\n    self.config = config\n    self._device = _resolve_device(config.device)\n    self._predictor = None\n    self._model_config: Optional[Dict] = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/table_extraction/tableformer/overview/#omnidocs.tasks.table_extraction.tableformer.TableFormerExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    ocr_output: Optional[OCROutput] = None,\n) -&gt; TableOutput\n</code></pre> <p>Extract table structure from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Table image (should be cropped to table region)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>ocr_output</code> <p>Optional OCR results for cell text matching</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TableOutput</code> <p>TableOutput with cells, structure, and export methods</p> Example <pre><code>result = extractor.extract(table_image)\nprint(f\"Table: {result.num_rows}x{result.num_cols}\")\nhtml = result.to_html()\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/tableformer/pytorch.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    ocr_output: Optional[\"OCROutput\"] = None,\n) -&gt; TableOutput:\n    \"\"\"\n    Extract table structure from an image.\n\n    Args:\n        image: Table image (should be cropped to table region)\n        ocr_output: Optional OCR results for cell text matching\n\n    Returns:\n        TableOutput with cells, structure, and export methods\n\n    Example:\n        ```python\n        result = extractor.extract(table_image)\n        print(f\"Table: {result.num_rows}x{result.num_cols}\")\n        html = result.to_html()\n        ```\n    \"\"\"\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Convert to OpenCV format (required by TFPredictor)\n    try:\n        import cv2\n    except ImportError:\n        raise ImportError(\n            \"opencv-python is required for TableFormerExtractor. Install with: pip install opencv-python-headless\"\n        )\n\n    cv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n\n    # Build iOCR page data\n    tokens = self._build_tokens_from_ocr(ocr_output) if ocr_output else []\n    iocr_page = {\n        \"width\": width,\n        \"height\": height,\n        \"image\": cv_image,\n        \"tokens\": tokens,\n    }\n\n    # Table bbox is the entire image\n    table_bbox = [0, 0, width, height]\n\n    # Run prediction\n    results = self._predictor.multi_table_predict(\n        iocr_page=iocr_page,\n        table_bboxes=[table_bbox],\n        do_matching=self.config.do_cell_matching,\n        correct_overlapping_cells=self.config.correct_overlapping_cells,\n        sort_row_col_indexes=self.config.sort_row_col_indexes,\n    )\n\n    # Convert results to TableOutput\n    return self._convert_results(results, width, height)\n</code></pre>"},{"location":"reference/tasks/table_extraction/tableformer/overview/#omnidocs.tasks.table_extraction.tableformer.config","title":"config","text":"<p>Configuration for TableFormer table structure extractor.</p> <p>TableFormer uses a dual-decoder transformer architecture with OTSL+ support for recognizing table structure from images.</p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Fast mode (default)\nextractor = TableFormerExtractor(config=TableFormerConfig())\n\n# Accurate mode with GPU\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(\n        mode=\"accurate\",\n        device=\"cuda\",\n        do_cell_matching=True,\n    )\n)\n</code></pre>"},{"location":"reference/tasks/table_extraction/tableformer/overview/#omnidocs.tasks.table_extraction.tableformer.config.TableFormerMode","title":"TableFormerMode","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>TableFormer inference mode.</p>"},{"location":"reference/tasks/table_extraction/tableformer/overview/#omnidocs.tasks.table_extraction.tableformer.config.TableFormerConfig","title":"TableFormerConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for TableFormer table structure extractor.</p> <p>TableFormer is a transformer-based model that predicts table structure using OTSL (Optimal Table Structure Language) tags and cell bounding boxes.</p> ATTRIBUTE DESCRIPTION <code>mode</code> <p>Inference mode - \"fast\" or \"accurate\"</p> <p> TYPE: <code>TableFormerMode</code> </p> <code>device</code> <p>Device for inference - \"cpu\", \"cuda\", \"mps\", or \"auto\"</p> <p> TYPE: <code>Literal['cpu', 'cuda', 'mps', 'auto']</code> </p> <code>num_threads</code> <p>Number of CPU threads for inference</p> <p> TYPE: <code>int</code> </p> <code>do_cell_matching</code> <p>Whether to match predicted cells with OCR text cells</p> <p> TYPE: <code>bool</code> </p> <code>artifacts_path</code> <p>Path to pre-downloaded model artifacts</p> <p> TYPE: <code>Optional[str]</code> </p> <code>repo_id</code> <p>HuggingFace model repository</p> <p> TYPE: <code>str</code> </p> <code>revision</code> <p>Model revision/tag</p> <p> TYPE: <code>str</code> </p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Fast mode\nextractor = TableFormerExtractor(config=TableFormerConfig(mode=\"fast\"))\n\n# Accurate mode with GPU\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(\n        mode=\"accurate\",\n        device=\"cuda\",\n        do_cell_matching=True,\n    )\n)\n</code></pre>"},{"location":"reference/tasks/table_extraction/tableformer/overview/#omnidocs.tasks.table_extraction.tableformer.pytorch","title":"pytorch","text":"<p>TableFormer extractor implementation using PyTorch backend.</p> <p>Uses the TFPredictor from docling-ibm-models for table structure recognition.</p>"},{"location":"reference/tasks/table_extraction/tableformer/overview/#omnidocs.tasks.table_extraction.tableformer.pytorch.TableFormerExtractor","title":"TableFormerExtractor","text":"<pre><code>TableFormerExtractor(config: TableFormerConfig)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>Table structure extractor using TableFormer model.</p> <p>TableFormer is a transformer-based model that predicts table structure using OTSL (Optimal Table Structure Language) tags. It can detect: - Cell boundaries (bounding boxes) - Row and column spans - Header cells (column and row headers) - Section rows</p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Initialize extractor\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(mode=\"fast\", device=\"cuda\")\n)\n\n# Extract table structure\nresult = extractor.extract(table_image)\n\n# Get HTML output\nhtml = result.to_html()\n\n# Get DataFrame\ndf = result.to_dataframe()\n</code></pre> <p>Initialize TableFormer extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>TableFormerConfig with model settings</p> <p> TYPE: <code>TableFormerConfig</code> </p> Source code in <code>omnidocs/tasks/table_extraction/tableformer/pytorch.py</code> <pre><code>def __init__(self, config: TableFormerConfig):\n    \"\"\"\n    Initialize TableFormer extractor.\n\n    Args:\n        config: TableFormerConfig with model settings\n    \"\"\"\n    self.config = config\n    self._device = _resolve_device(config.device)\n    self._predictor = None\n    self._model_config: Optional[Dict] = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/table_extraction/tableformer/overview/#omnidocs.tasks.table_extraction.tableformer.pytorch.TableFormerExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    ocr_output: Optional[OCROutput] = None,\n) -&gt; TableOutput\n</code></pre> <p>Extract table structure from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Table image (should be cropped to table region)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>ocr_output</code> <p>Optional OCR results for cell text matching</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TableOutput</code> <p>TableOutput with cells, structure, and export methods</p> Example <pre><code>result = extractor.extract(table_image)\nprint(f\"Table: {result.num_rows}x{result.num_cols}\")\nhtml = result.to_html()\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/tableformer/pytorch.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    ocr_output: Optional[\"OCROutput\"] = None,\n) -&gt; TableOutput:\n    \"\"\"\n    Extract table structure from an image.\n\n    Args:\n        image: Table image (should be cropped to table region)\n        ocr_output: Optional OCR results for cell text matching\n\n    Returns:\n        TableOutput with cells, structure, and export methods\n\n    Example:\n        ```python\n        result = extractor.extract(table_image)\n        print(f\"Table: {result.num_rows}x{result.num_cols}\")\n        html = result.to_html()\n        ```\n    \"\"\"\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Convert to OpenCV format (required by TFPredictor)\n    try:\n        import cv2\n    except ImportError:\n        raise ImportError(\n            \"opencv-python is required for TableFormerExtractor. Install with: pip install opencv-python-headless\"\n        )\n\n    cv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n\n    # Build iOCR page data\n    tokens = self._build_tokens_from_ocr(ocr_output) if ocr_output else []\n    iocr_page = {\n        \"width\": width,\n        \"height\": height,\n        \"image\": cv_image,\n        \"tokens\": tokens,\n    }\n\n    # Table bbox is the entire image\n    table_bbox = [0, 0, width, height]\n\n    # Run prediction\n    results = self._predictor.multi_table_predict(\n        iocr_page=iocr_page,\n        table_bboxes=[table_bbox],\n        do_matching=self.config.do_cell_matching,\n        correct_overlapping_cells=self.config.correct_overlapping_cells,\n        sort_row_col_indexes=self.config.sort_row_col_indexes,\n    )\n\n    # Convert results to TableOutput\n    return self._convert_results(results, width, height)\n</code></pre>"},{"location":"reference/tasks/table_extraction/tableformer/pytorch/","title":"PyTorch","text":"<p>TableFormer extractor implementation using PyTorch backend.</p> <p>Uses the TFPredictor from docling-ibm-models for table structure recognition.</p>"},{"location":"reference/tasks/table_extraction/tableformer/pytorch/#omnidocs.tasks.table_extraction.tableformer.pytorch.TableFormerExtractor","title":"TableFormerExtractor","text":"<pre><code>TableFormerExtractor(config: TableFormerConfig)\n</code></pre> <p>               Bases: <code>BaseTableExtractor</code></p> <p>Table structure extractor using TableFormer model.</p> <p>TableFormer is a transformer-based model that predicts table structure using OTSL (Optimal Table Structure Language) tags. It can detect: - Cell boundaries (bounding boxes) - Row and column spans - Header cells (column and row headers) - Section rows</p> Example <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Initialize extractor\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(mode=\"fast\", device=\"cuda\")\n)\n\n# Extract table structure\nresult = extractor.extract(table_image)\n\n# Get HTML output\nhtml = result.to_html()\n\n# Get DataFrame\ndf = result.to_dataframe()\n</code></pre> <p>Initialize TableFormer extractor.</p> PARAMETER DESCRIPTION <code>config</code> <p>TableFormerConfig with model settings</p> <p> TYPE: <code>TableFormerConfig</code> </p> Source code in <code>omnidocs/tasks/table_extraction/tableformer/pytorch.py</code> <pre><code>def __init__(self, config: TableFormerConfig):\n    \"\"\"\n    Initialize TableFormer extractor.\n\n    Args:\n        config: TableFormerConfig with model settings\n    \"\"\"\n    self.config = config\n    self._device = _resolve_device(config.device)\n    self._predictor = None\n    self._model_config: Optional[Dict] = None\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/table_extraction/tableformer/pytorch/#omnidocs.tasks.table_extraction.tableformer.pytorch.TableFormerExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    ocr_output: Optional[OCROutput] = None,\n) -&gt; TableOutput\n</code></pre> <p>Extract table structure from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Table image (should be cropped to table region)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>ocr_output</code> <p>Optional OCR results for cell text matching</p> <p> TYPE: <code>Optional[OCROutput]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>TableOutput</code> <p>TableOutput with cells, structure, and export methods</p> Example <pre><code>result = extractor.extract(table_image)\nprint(f\"Table: {result.num_rows}x{result.num_cols}\")\nhtml = result.to_html()\n</code></pre> Source code in <code>omnidocs/tasks/table_extraction/tableformer/pytorch.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    ocr_output: Optional[\"OCROutput\"] = None,\n) -&gt; TableOutput:\n    \"\"\"\n    Extract table structure from an image.\n\n    Args:\n        image: Table image (should be cropped to table region)\n        ocr_output: Optional OCR results for cell text matching\n\n    Returns:\n        TableOutput with cells, structure, and export methods\n\n    Example:\n        ```python\n        result = extractor.extract(table_image)\n        print(f\"Table: {result.num_rows}x{result.num_cols}\")\n        html = result.to_html()\n        ```\n    \"\"\"\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Convert to OpenCV format (required by TFPredictor)\n    try:\n        import cv2\n    except ImportError:\n        raise ImportError(\n            \"opencv-python is required for TableFormerExtractor. Install with: pip install opencv-python-headless\"\n        )\n\n    cv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)\n\n    # Build iOCR page data\n    tokens = self._build_tokens_from_ocr(ocr_output) if ocr_output else []\n    iocr_page = {\n        \"width\": width,\n        \"height\": height,\n        \"image\": cv_image,\n        \"tokens\": tokens,\n    }\n\n    # Table bbox is the entire image\n    table_bbox = [0, 0, width, height]\n\n    # Run prediction\n    results = self._predictor.multi_table_predict(\n        iocr_page=iocr_page,\n        table_bboxes=[table_bbox],\n        do_matching=self.config.do_cell_matching,\n        correct_overlapping_cells=self.config.correct_overlapping_cells,\n        sort_row_col_indexes=self.config.sort_row_col_indexes,\n    )\n\n    # Convert results to TableOutput\n    return self._convert_results(results, width, height)\n</code></pre>"},{"location":"reference/tasks/text_extraction/base/","title":"Base","text":"<p>Base class for text extractors.</p> <p>Defines the abstract interface that all text extractors must implement.</p>"},{"location":"reference/tasks/text_extraction/base/#omnidocs.tasks.text_extraction.base.BaseTextExtractor","title":"BaseTextExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for text extractors.</p> <p>All text extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyTextExtractor(BaseTextExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Load model weights\n            pass\n\n        def extract(self, image, output_format=\"markdown\"):\n            # Run extraction\n            return TextOutput(...)\n</code></pre>"},{"location":"reference/tasks/text_extraction/base/#omnidocs.tasks.text_extraction.base.BaseTextExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format or output_format is not supported</p> <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        ValueError: If image format or output_format is not supported\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/text_extraction/base/#omnidocs.tasks.text_extraction.base.BaseTextExtractor.batch_extract","title":"batch_extract","text":"<pre><code>batch_extract(\n    images: List[Union[Image, ndarray, str, Path]],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[TextOutput]\n</code></pre> <p>Extract text from multiple images.</p> <p>Default implementation loops over extract(). Subclasses can override for optimized batching (e.g., VLLM).</p> PARAMETER DESCRIPTION <code>images</code> <p>List of images in any supported format</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>output_format</code> <p>Desired output format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TextOutput]</code> <p>List of TextOutput in same order as input</p> <p>Examples:</p> <pre><code>images = [doc.get_page(i) for i in range(doc.page_count)]\nresults = extractor.batch_extract(images, output_format=\"markdown\")\n</code></pre> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def batch_extract(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[TextOutput]:\n    \"\"\"\n    Extract text from multiple images.\n\n    Default implementation loops over extract(). Subclasses can override\n    for optimized batching (e.g., VLLM).\n\n    Args:\n        images: List of images in any supported format\n        output_format: Desired output format\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of TextOutput in same order as input\n\n    Examples:\n        ```python\n        images = [doc.get_page(i) for i in range(doc.page_count)]\n        results = extractor.batch_extract(images, output_format=\"markdown\")\n        ```\n    \"\"\"\n    results = []\n    total = len(images)\n\n    for i, image in enumerate(images):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(image, output_format=output_format)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/text_extraction/base/#omnidocs.tasks.text_extraction.base.BaseTextExtractor.extract_document","title":"extract_document","text":"<pre><code>extract_document(\n    document: Document,\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[TextOutput]\n</code></pre> <p>Extract text from all pages of a document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>output_format</code> <p>Desired output format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TextOutput]</code> <p>List of TextOutput, one per page</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresults = extractor.extract_document(doc, output_format=\"markdown\")\n</code></pre> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def extract_document(\n    self,\n    document: \"Document\",\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[TextOutput]:\n    \"\"\"\n    Extract text from all pages of a document.\n\n    Args:\n        document: Document instance\n        output_format: Desired output format\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of TextOutput, one per page\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        results = extractor.extract_document(doc, output_format=\"markdown\")\n        ```\n    \"\"\"\n    results = []\n    total = document.page_count\n\n    for i, page in enumerate(document.iter_pages()):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(page, output_format=output_format)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/text_extraction/models/","title":"Models","text":"<p>Pydantic models for text extraction outputs.</p> <p>Defines output types and format enums for text extraction.</p>"},{"location":"reference/tasks/text_extraction/models/#omnidocs.tasks.text_extraction.models.OutputFormat","title":"OutputFormat","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported text extraction output formats.</p> Each format has different characteristics <ul> <li>HTML: Structured with div elements, preserves layout semantics</li> <li>MARKDOWN: Portable, human-readable, good for documentation</li> <li>JSON: Structured data with layout information (Dots OCR)</li> </ul>"},{"location":"reference/tasks/text_extraction/models/#omnidocs.tasks.text_extraction.models.TextOutput","title":"TextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text extraction output from a document image.</p> <p>Contains the extracted text content in the requested format, along with optional raw output and plain text versions.</p> Example <pre><code>result = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)  # Clean markdown\nprint(result.plain_text)  # Plain text without formatting\n</code></pre>"},{"location":"reference/tasks/text_extraction/models/#omnidocs.tasks.text_extraction.models.TextOutput.content_length","title":"content_length  <code>property</code>","text":"<pre><code>content_length: int\n</code></pre> <p>Length of the extracted content in characters.</p>"},{"location":"reference/tasks/text_extraction/models/#omnidocs.tasks.text_extraction.models.TextOutput.word_count","title":"word_count  <code>property</code>","text":"<pre><code>word_count: int\n</code></pre> <p>Approximate word count of the plain text.</p>"},{"location":"reference/tasks/text_extraction/models/#omnidocs.tasks.text_extraction.models.LayoutElement","title":"LayoutElement","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single layout element from document layout detection.</p> <p>Represents a detected region in the document with its bounding box, category label, and extracted text content.</p> ATTRIBUTE DESCRIPTION <code>bbox</code> <p>Bounding box coordinates [x1, y1, x2, y2] (normalized to 0-1024)</p> <p> TYPE: <code>List[int]</code> </p> <code>category</code> <p>Layout category (e.g., \"Text\", \"Title\", \"Table\", \"Formula\")</p> <p> TYPE: <code>str</code> </p> <code>text</code> <p>Extracted text content (None for pictures)</p> <p> TYPE: <code>Optional[str]</code> </p> <code>confidence</code> <p>Detection confidence score (optional)</p> <p> TYPE: <code>Optional[float]</code> </p>"},{"location":"reference/tasks/text_extraction/models/#omnidocs.tasks.text_extraction.models.DotsOCRTextOutput","title":"DotsOCRTextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text extraction output from Dots OCR with layout information.</p> <p>Dots OCR provides structured output with: - Layout detection (11 categories) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown/LaTeX/HTML) - Reading order preservation</p> Layout Categories <p>Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title</p> Text Formatting <ul> <li>Text/Title/Section-header: Markdown</li> <li>Formula: LaTeX</li> <li>Table: HTML</li> <li>Picture: (text omitted)</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nresult = extractor.extract(image, include_layout=True)\nprint(result.content)  # Full text with formatting\nfor elem in result.layout:\n        print(f\"{elem.category}: {elem.bbox}\")\n</code></pre>"},{"location":"reference/tasks/text_extraction/models/#omnidocs.tasks.text_extraction.models.DotsOCRTextOutput.num_layout_elements","title":"num_layout_elements  <code>property</code>","text":"<pre><code>num_layout_elements: int\n</code></pre> <p>Number of detected layout elements.</p>"},{"location":"reference/tasks/text_extraction/models/#omnidocs.tasks.text_extraction.models.DotsOCRTextOutput.content_length","title":"content_length  <code>property</code>","text":"<pre><code>content_length: int\n</code></pre> <p>Length of extracted content in characters.</p>"},{"location":"reference/tasks/text_extraction/overview/","title":"Overview","text":"<p>Text Extraction Module.</p> <p>Provides extractors for converting document images to structured text formats (HTML, Markdown, JSON). Uses Vision-Language Models for accurate text extraction with formatting preservation and optional layout detection.</p> Available Extractors <ul> <li>QwenTextExtractor: Qwen3-VL based extractor (multi-backend)</li> <li>DotsOCRTextExtractor: Dots OCR with layout-aware extraction (PyTorch/VLLM/API)</li> <li>NanonetsTextExtractor: Nanonets OCR2-3B for text extraction (PyTorch/VLLM)</li> <li>GraniteDoclingTextExtractor: IBM Granite Docling for document conversion (multi-backend)</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.BaseTextExtractor","title":"BaseTextExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for text extractors.</p> <p>All text extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyTextExtractor(BaseTextExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Load model weights\n            pass\n\n        def extract(self, image, output_format=\"markdown\"):\n            # Run extraction\n            return TextOutput(...)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.BaseTextExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format or output_format is not supported</p> <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        ValueError: If image format or output_format is not supported\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.BaseTextExtractor.batch_extract","title":"batch_extract","text":"<pre><code>batch_extract(\n    images: List[Union[Image, ndarray, str, Path]],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[TextOutput]\n</code></pre> <p>Extract text from multiple images.</p> <p>Default implementation loops over extract(). Subclasses can override for optimized batching (e.g., VLLM).</p> PARAMETER DESCRIPTION <code>images</code> <p>List of images in any supported format</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>output_format</code> <p>Desired output format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TextOutput]</code> <p>List of TextOutput in same order as input</p> <p>Examples:</p> <pre><code>images = [doc.get_page(i) for i in range(doc.page_count)]\nresults = extractor.batch_extract(images, output_format=\"markdown\")\n</code></pre> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def batch_extract(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[TextOutput]:\n    \"\"\"\n    Extract text from multiple images.\n\n    Default implementation loops over extract(). Subclasses can override\n    for optimized batching (e.g., VLLM).\n\n    Args:\n        images: List of images in any supported format\n        output_format: Desired output format\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of TextOutput in same order as input\n\n    Examples:\n        ```python\n        images = [doc.get_page(i) for i in range(doc.page_count)]\n        results = extractor.batch_extract(images, output_format=\"markdown\")\n        ```\n    \"\"\"\n    results = []\n    total = len(images)\n\n    for i, image in enumerate(images):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(image, output_format=output_format)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.BaseTextExtractor.extract_document","title":"extract_document","text":"<pre><code>extract_document(\n    document: Document,\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[TextOutput]\n</code></pre> <p>Extract text from all pages of a document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>output_format</code> <p>Desired output format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TextOutput]</code> <p>List of TextOutput, one per page</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresults = extractor.extract_document(doc, output_format=\"markdown\")\n</code></pre> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def extract_document(\n    self,\n    document: \"Document\",\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[TextOutput]:\n    \"\"\"\n    Extract text from all pages of a document.\n\n    Args:\n        document: Document instance\n        output_format: Desired output format\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of TextOutput, one per page\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        results = extractor.extract_document(doc, output_format=\"markdown\")\n        ```\n    \"\"\"\n    results = []\n    total = document.page_count\n\n    for i, page in enumerate(document.iter_pages()):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(page, output_format=output_format)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.DotsOCRTextExtractor","title":"DotsOCRTextExtractor","text":"<pre><code>DotsOCRTextExtractor(backend: DotsOCRBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Dots OCR Vision-Language Model text extractor with layout detection.</p> <p>Extracts text from document images with layout information including: - 11 layout categories (Caption, Footnote, Formula, List-item, etc.) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown, LaTeX, HTML) - Reading order preservation</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\n\n# Extract with layout\nresult = extractor.extract(image, include_layout=True)\nprint(f\"Found {result.num_layout_elements} elements\")\nprint(result.content)\n</code></pre> <p>Initialize Dots OCR text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend - DotsOCRVLLMConfig: VLLM high-throughput backend - DotsOCRAPIConfig: API backend (online VLLM server)</p> <p> TYPE: <code>DotsOCRBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def __init__(self, backend: DotsOCRBackendConfig):\n    \"\"\"\n    Initialize Dots OCR text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend\n            - DotsOCRVLLMConfig: VLLM high-throughput backend\n            - DotsOCRAPIConfig: API backend (online VLLM server)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._model: Any = None\n    self._loaded = False\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.DotsOCRTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\n        \"markdown\", \"html\", \"json\"\n    ] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput\n</code></pre> <p>Extract text from image using Dots OCR.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\", \"html\", or \"json\")</p> <p> TYPE: <code>Literal['markdown', 'html', 'json']</code> DEFAULT: <code>'markdown'</code> </p> <code>include_layout</code> <p>Include layout bounding boxes in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>custom_prompt</code> <p>Override default extraction prompt</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_tokens</code> <p>Maximum tokens for generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>8192</code> </p> RETURNS DESCRIPTION <code>DotsOCRTextOutput</code> <p>DotsOCRTextOutput with extracted content and optional layout</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"markdown\", \"html\", \"json\"] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput:\n    \"\"\"\n    Extract text from image using Dots OCR.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\", \"html\", or \"json\")\n        include_layout: Include layout bounding boxes in output\n        custom_prompt: Override default extraction prompt\n        max_tokens: Maximum tokens for generation\n\n    Returns:\n        DotsOCRTextOutput with extracted content and optional layout\n\n    Raises:\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    img = self._prepare_image(image)\n\n    # Get prompt\n    prompt = custom_prompt or DOTS_OCR_PROMPT\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"DotsOCRPyTorchConfig\":\n        raw_output = self._infer_pytorch(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRVLLMConfig\":\n        raw_output = self._infer_vllm(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRAPIConfig\":\n        raw_output = self._infer_api(img, prompt, max_tokens)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse output\n    return self._parse_output(\n        raw_output,\n        img.size,\n        output_format,\n        include_layout,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.GraniteDoclingTextExtractor","title":"GraniteDoclingTextExtractor","text":"<pre><code>GraniteDoclingTextExtractor(\n    backend: GraniteDoclingTextBackendConfig,\n)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Granite Docling text extractor supporting PyTorch, VLLM, MLX, and API backends.</p> <p>Granite Docling is IBM's compact vision-language model optimized for document conversion. It outputs DocTags format which is converted to Markdown using the docling_core library.</p> Example <p>from omnidocs.tasks.text_extraction.granitedocling import ( ...     GraniteDoclingTextExtractor, ...     GraniteDoclingTextPyTorchConfig, ... ) config = GraniteDoclingTextPyTorchConfig(device=\"cuda\") extractor = GraniteDoclingTextExtractor(backend=config) result = extractor.extract(image, output_format=\"markdown\") print(result.content)</p> <p>Initialize Granite Docling extractor with backend configuration.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration (PyTorch, VLLM, MLX, or API config)</p> <p> TYPE: <code>GraniteDoclingTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/granitedocling/extractor.py</code> <pre><code>def __init__(self, backend: GraniteDoclingTextBackendConfig):\n    \"\"\"\n    Initialize Granite Docling extractor with backend configuration.\n\n    Args:\n        backend: Backend configuration (PyTorch, VLLM, MLX, or API config)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded: bool = False\n\n    # Backend-specific helpers\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n    self._sampling_params_class: Any = None\n    self._device: str = \"cpu\"\n\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.GraniteDoclingTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image using Granite Docling.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\" or \"html\")</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput with extracted content</p> Source code in <code>omnidocs/tasks/text_extraction/granitedocling/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image using Granite Docling.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\" or \"html\")\n\n    Returns:\n        TextOutput with extracted content\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}\")\n\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Dispatch to backend-specific inference\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"GraniteDoclingTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image)\n    elif config_type == \"GraniteDoclingTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image)\n    elif config_type == \"GraniteDoclingTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image)\n    elif config_type == \"GraniteDoclingTextAPIConfig\":\n        raw_output = self._infer_api(pil_image)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Convert DocTags to Markdown\n    markdown_output = self._convert_doctags_to_markdown(raw_output, pil_image)\n\n    # For HTML output, wrap in basic HTML structure\n    if output_format == \"html\":\n        content = f\"&lt;html&gt;&lt;body&gt;\\n{markdown_output}\\n&lt;/body&gt;&lt;/html&gt;\"\n    else:\n        content = markdown_output\n\n    return TextOutput(\n        content=content,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=self._extract_plain_text(markdown_output),\n        image_width=width,\n        image_height=height,\n        model_name=f\"Granite-Docling-258M ({config_type.replace('Config', '')})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.DotsOCRTextOutput","title":"DotsOCRTextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text extraction output from Dots OCR with layout information.</p> <p>Dots OCR provides structured output with: - Layout detection (11 categories) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown/LaTeX/HTML) - Reading order preservation</p> Layout Categories <p>Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title</p> Text Formatting <ul> <li>Text/Title/Section-header: Markdown</li> <li>Formula: LaTeX</li> <li>Table: HTML</li> <li>Picture: (text omitted)</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nresult = extractor.extract(image, include_layout=True)\nprint(result.content)  # Full text with formatting\nfor elem in result.layout:\n        print(f\"{elem.category}: {elem.bbox}\")\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.DotsOCRTextOutput.num_layout_elements","title":"num_layout_elements  <code>property</code>","text":"<pre><code>num_layout_elements: int\n</code></pre> <p>Number of detected layout elements.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.DotsOCRTextOutput.content_length","title":"content_length  <code>property</code>","text":"<pre><code>content_length: int\n</code></pre> <p>Length of extracted content in characters.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.LayoutElement","title":"LayoutElement","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single layout element from document layout detection.</p> <p>Represents a detected region in the document with its bounding box, category label, and extracted text content.</p> ATTRIBUTE DESCRIPTION <code>bbox</code> <p>Bounding box coordinates [x1, y1, x2, y2] (normalized to 0-1024)</p> <p> TYPE: <code>List[int]</code> </p> <code>category</code> <p>Layout category (e.g., \"Text\", \"Title\", \"Table\", \"Formula\")</p> <p> TYPE: <code>str</code> </p> <code>text</code> <p>Extracted text content (None for pictures)</p> <p> TYPE: <code>Optional[str]</code> </p> <code>confidence</code> <p>Detection confidence score (optional)</p> <p> TYPE: <code>Optional[float]</code> </p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.OutputFormat","title":"OutputFormat","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported text extraction output formats.</p> Each format has different characteristics <ul> <li>HTML: Structured with div elements, preserves layout semantics</li> <li>MARKDOWN: Portable, human-readable, good for documentation</li> <li>JSON: Structured data with layout information (Dots OCR)</li> </ul>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.TextOutput","title":"TextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text extraction output from a document image.</p> <p>Contains the extracted text content in the requested format, along with optional raw output and plain text versions.</p> Example <pre><code>result = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)  # Clean markdown\nprint(result.plain_text)  # Plain text without formatting\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.TextOutput.content_length","title":"content_length  <code>property</code>","text":"<pre><code>content_length: int\n</code></pre> <p>Length of the extracted content in characters.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.TextOutput.word_count","title":"word_count  <code>property</code>","text":"<pre><code>word_count: int\n</code></pre> <p>Approximate word count of the plain text.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.NanonetsTextExtractor","title":"NanonetsTextExtractor","text":"<pre><code>NanonetsTextExtractor(backend: NanonetsTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Nanonets OCR2-3B Vision-Language Model text extractor.</p> <p>Extracts text from document images with support for: - Tables (output as HTML) - Equations (output as LaTeX) - Image captions (wrapped in  tags) - Watermarks (wrapped in  tags) - Page numbers (wrapped in  tags) - Checkboxes (using \u2610 and \u2611 symbols)</p> <p>Supports PyTorch, VLLM, and MLX backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import NanonetsTextExtractor\nfrom omnidocs.tasks.text_extraction.nanonets import NanonetsTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = NanonetsTextExtractor(\n        backend=NanonetsTextPyTorchConfig()\n    )\n\n# Extract text\nresult = extractor.extract(image)\nprint(result.content)\n</code></pre> <p>Initialize Nanonets text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend - NanonetsTextVLLMConfig: VLLM high-throughput backend - NanonetsTextMLXConfig: MLX backend for Apple Silicon</p> <p> TYPE: <code>NanonetsTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/nanonets/extractor.py</code> <pre><code>def __init__(self, backend: NanonetsTextBackendConfig):\n    \"\"\"\n    Initialize Nanonets text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend\n            - NanonetsTextVLLMConfig: VLLM high-throughput backend\n            - NanonetsTextMLXConfig: MLX backend for Apple Silicon\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._device: str = \"cpu\"\n\n    # MLX-specific helpers\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.NanonetsTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> <p>Note: Nanonets OCR2 produces a unified output format that includes tables as HTML and equations as LaTeX inline. The output_format parameter is accepted for API compatibility but does not change the output structure.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Accepted for API compatibility (default: \"markdown\")</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/nanonets/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Note: Nanonets OCR2 produces a unified output format that includes\n    tables as HTML and equations as LaTeX inline. The output_format\n    parameter is accepted for API compatibility but does not change\n    the output structure.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Accepted for API compatibility (default: \"markdown\")\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"NanonetsTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image)\n    elif config_type == \"NanonetsTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image)\n    elif config_type == \"NanonetsTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    cleaned_output = raw_output.replace(\"&lt;|im_end|&gt;\", \"\").strip()\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=cleaned_output,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Nanonets-OCR2-3B ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.QwenTextExtractor","title":"QwenTextExtractor","text":"<pre><code>QwenTextExtractor(backend: QwenTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Qwen3-VL Vision-Language Model text extractor.</p> <p>Extracts text from document images and outputs as structured HTML or Markdown. Uses Qwen3-VL's built-in document parsing prompts.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Extract as Markdown\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n\n# Extract as HTML\nresult = extractor.extract(image, output_format=\"html\")\nprint(result.content)\n</code></pre> <p>Initialize Qwen text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenTextPyTorchConfig: PyTorch/HuggingFace backend - QwenTextVLLMConfig: VLLM high-throughput backend - QwenTextMLXConfig: MLX backend for Apple Silicon - QwenTextAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def __init__(self, backend: QwenTextBackendConfig):\n    \"\"\"\n    Initialize Qwen text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenTextPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenTextVLLMConfig: VLLM high-throughput backend\n            - QwenTextMLXConfig: MLX backend for Apple Silicon\n            - QwenTextAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.QwenTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML with div elements - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format or output_format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML with div elements\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format or output_format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}. Expected 'html' or 'markdown'.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Get prompt for output format\n    prompt = QWEN_PROMPTS[output_format]\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenTextAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    if output_format == \"html\":\n        cleaned_output = _clean_html_output(raw_output)\n    else:\n        cleaned_output = _clean_markdown_output(raw_output)\n\n    # Extract plain text\n    plain_text = _extract_plain_text(raw_output, output_format)\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=plain_text,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.base","title":"base","text":"<p>Base class for text extractors.</p> <p>Defines the abstract interface that all text extractors must implement.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.base.BaseTextExtractor","title":"BaseTextExtractor","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for text extractors.</p> <p>All text extraction models must inherit from this class and implement the required methods.</p> Example <pre><code>class MyTextExtractor(BaseTextExtractor):\n        def __init__(self, config: MyConfig):\n            self.config = config\n            self._load_model()\n\n        def _load_model(self):\n            # Load model weights\n            pass\n\n        def extract(self, image, output_format=\"markdown\"):\n            # Run extraction\n            return TextOutput(...)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.base.BaseTextExtractor.extract","title":"extract  <code>abstractmethod</code>","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>ValueError</code> <p>If image format or output_format is not supported</p> <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>@abstractmethod\ndef extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        ValueError: If image format or output_format is not supported\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.base.BaseTextExtractor.batch_extract","title":"batch_extract","text":"<pre><code>batch_extract(\n    images: List[Union[Image, ndarray, str, Path]],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[TextOutput]\n</code></pre> <p>Extract text from multiple images.</p> <p>Default implementation loops over extract(). Subclasses can override for optimized batching (e.g., VLLM).</p> PARAMETER DESCRIPTION <code>images</code> <p>List of images in any supported format</p> <p> TYPE: <code>List[Union[Image, ndarray, str, Path]]</code> </p> <code>output_format</code> <p>Desired output format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TextOutput]</code> <p>List of TextOutput in same order as input</p> <p>Examples:</p> <pre><code>images = [doc.get_page(i) for i in range(doc.page_count)]\nresults = extractor.batch_extract(images, output_format=\"markdown\")\n</code></pre> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def batch_extract(\n    self,\n    images: List[Union[Image.Image, np.ndarray, str, Path]],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[TextOutput]:\n    \"\"\"\n    Extract text from multiple images.\n\n    Default implementation loops over extract(). Subclasses can override\n    for optimized batching (e.g., VLLM).\n\n    Args:\n        images: List of images in any supported format\n        output_format: Desired output format\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of TextOutput in same order as input\n\n    Examples:\n        ```python\n        images = [doc.get_page(i) for i in range(doc.page_count)]\n        results = extractor.batch_extract(images, output_format=\"markdown\")\n        ```\n    \"\"\"\n    results = []\n    total = len(images)\n\n    for i, image in enumerate(images):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(image, output_format=output_format)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.base.BaseTextExtractor.extract_document","title":"extract_document","text":"<pre><code>extract_document(\n    document: Document,\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n    progress_callback: Optional[\n        Callable[[int, int], None]\n    ] = None,\n) -&gt; List[TextOutput]\n</code></pre> <p>Extract text from all pages of a document.</p> PARAMETER DESCRIPTION <code>document</code> <p>Document instance</p> <p> TYPE: <code>Document</code> </p> <code>output_format</code> <p>Desired output format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> <code>progress_callback</code> <p>Optional function(current, total) for progress</p> <p> TYPE: <code>Optional[Callable[[int, int], None]]</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>List[TextOutput]</code> <p>List of TextOutput, one per page</p> <p>Examples:</p> <pre><code>doc = Document.from_pdf(\"paper.pdf\")\nresults = extractor.extract_document(doc, output_format=\"markdown\")\n</code></pre> Source code in <code>omnidocs/tasks/text_extraction/base.py</code> <pre><code>def extract_document(\n    self,\n    document: \"Document\",\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n    progress_callback: Optional[Callable[[int, int], None]] = None,\n) -&gt; List[TextOutput]:\n    \"\"\"\n    Extract text from all pages of a document.\n\n    Args:\n        document: Document instance\n        output_format: Desired output format\n        progress_callback: Optional function(current, total) for progress\n\n    Returns:\n        List of TextOutput, one per page\n\n    Examples:\n        ```python\n        doc = Document.from_pdf(\"paper.pdf\")\n        results = extractor.extract_document(doc, output_format=\"markdown\")\n        ```\n    \"\"\"\n    results = []\n    total = document.page_count\n\n    for i, page in enumerate(document.iter_pages()):\n        if progress_callback:\n            progress_callback(i + 1, total)\n\n        result = self.extract(page, output_format=output_format)\n        results.append(result)\n\n    return results\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr","title":"dotsocr","text":"<p>Dots OCR text extractor and backend configurations.</p> <p>Available backends: - PyTorch: DotsOCRPyTorchConfig (local GPU inference) - VLLM: DotsOCRVLLMConfig (offline batch inference) - API: DotsOCRAPIConfig (online VLLM server via OpenAI-compatible API)</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRAPIConfig","title":"DotsOCRAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Dots OCR.</p> <p>This config is for accessing a deployed VLLM server via OpenAI-compatible API. Typically used with modal_dotsocr_vllm_online.py deployment.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRAPIConfig\n\nconfig = DotsOCRAPIConfig(\n        model=\"dotsocr\",\n        api_base=\"https://your-modal-app.modal.run/v1\",\n        api_key=\"optional-key\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRTextExtractor","title":"DotsOCRTextExtractor","text":"<pre><code>DotsOCRTextExtractor(backend: DotsOCRBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Dots OCR Vision-Language Model text extractor with layout detection.</p> <p>Extracts text from document images with layout information including: - 11 layout categories (Caption, Footnote, Formula, List-item, etc.) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown, LaTeX, HTML) - Reading order preservation</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\n\n# Extract with layout\nresult = extractor.extract(image, include_layout=True)\nprint(f\"Found {result.num_layout_elements} elements\")\nprint(result.content)\n</code></pre> <p>Initialize Dots OCR text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend - DotsOCRVLLMConfig: VLLM high-throughput backend - DotsOCRAPIConfig: API backend (online VLLM server)</p> <p> TYPE: <code>DotsOCRBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def __init__(self, backend: DotsOCRBackendConfig):\n    \"\"\"\n    Initialize Dots OCR text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend\n            - DotsOCRVLLMConfig: VLLM high-throughput backend\n            - DotsOCRAPIConfig: API backend (online VLLM server)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._model: Any = None\n    self._loaded = False\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\n        \"markdown\", \"html\", \"json\"\n    ] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput\n</code></pre> <p>Extract text from image using Dots OCR.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\", \"html\", or \"json\")</p> <p> TYPE: <code>Literal['markdown', 'html', 'json']</code> DEFAULT: <code>'markdown'</code> </p> <code>include_layout</code> <p>Include layout bounding boxes in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>custom_prompt</code> <p>Override default extraction prompt</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_tokens</code> <p>Maximum tokens for generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>8192</code> </p> RETURNS DESCRIPTION <code>DotsOCRTextOutput</code> <p>DotsOCRTextOutput with extracted content and optional layout</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"markdown\", \"html\", \"json\"] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput:\n    \"\"\"\n    Extract text from image using Dots OCR.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\", \"html\", or \"json\")\n        include_layout: Include layout bounding boxes in output\n        custom_prompt: Override default extraction prompt\n        max_tokens: Maximum tokens for generation\n\n    Returns:\n        DotsOCRTextOutput with extracted content and optional layout\n\n    Raises:\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    img = self._prepare_image(image)\n\n    # Get prompt\n    prompt = custom_prompt or DOTS_OCR_PROMPT\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"DotsOCRPyTorchConfig\":\n        raw_output = self._infer_pytorch(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRVLLMConfig\":\n        raw_output = self._infer_vllm(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRAPIConfig\":\n        raw_output = self._infer_api(img, prompt, max_tokens)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse output\n    return self._parse_output(\n        raw_output,\n        img.size,\n        output_format,\n        include_layout,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRPyTorchConfig","title":"DotsOCRPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Dots OCR.</p> <p>Dots OCR provides layout-aware text extraction with 11 predefined layout categories (Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title).</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nconfig = DotsOCRPyTorchConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRVLLMConfig","title":"DotsOCRVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Dots OCR.</p> <p>VLLM provides high-throughput inference with optimizations like: - PagedAttention for efficient KV cache management - Continuous batching for higher throughput - Optimized CUDA kernels</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nconfig = DotsOCRVLLMConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.api","title":"api","text":"<p>API backend configuration for Dots OCR (VLLM online server).</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.api.DotsOCRAPIConfig","title":"DotsOCRAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Dots OCR.</p> <p>This config is for accessing a deployed VLLM server via OpenAI-compatible API. Typically used with modal_dotsocr_vllm_online.py deployment.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRAPIConfig\n\nconfig = DotsOCRAPIConfig(\n        model=\"dotsocr\",\n        api_base=\"https://your-modal-app.modal.run/v1\",\n        api_key=\"optional-key\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.extractor","title":"extractor","text":"<p>Dots OCR text extractor with layout-aware extraction.</p> <p>A Vision-Language Model optimized for document OCR with structured output containing layout information, bounding boxes, and multi-format text.</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\nresult = extractor.extract(image, include_layout=True)\nprint(result.content)\nfor elem in result.layout:\n        print(f\"{elem.category}: {elem.bbox}\")\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.extractor.DotsOCRTextExtractor","title":"DotsOCRTextExtractor","text":"<pre><code>DotsOCRTextExtractor(backend: DotsOCRBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Dots OCR Vision-Language Model text extractor with layout detection.</p> <p>Extracts text from document images with layout information including: - 11 layout categories (Caption, Footnote, Formula, List-item, etc.) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown, LaTeX, HTML) - Reading order preservation</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\n\n# Extract with layout\nresult = extractor.extract(image, include_layout=True)\nprint(f\"Found {result.num_layout_elements} elements\")\nprint(result.content)\n</code></pre> <p>Initialize Dots OCR text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend - DotsOCRVLLMConfig: VLLM high-throughput backend - DotsOCRAPIConfig: API backend (online VLLM server)</p> <p> TYPE: <code>DotsOCRBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def __init__(self, backend: DotsOCRBackendConfig):\n    \"\"\"\n    Initialize Dots OCR text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend\n            - DotsOCRVLLMConfig: VLLM high-throughput backend\n            - DotsOCRAPIConfig: API backend (online VLLM server)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._model: Any = None\n    self._loaded = False\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.extractor.DotsOCRTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\n        \"markdown\", \"html\", \"json\"\n    ] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput\n</code></pre> <p>Extract text from image using Dots OCR.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\", \"html\", or \"json\")</p> <p> TYPE: <code>Literal['markdown', 'html', 'json']</code> DEFAULT: <code>'markdown'</code> </p> <code>include_layout</code> <p>Include layout bounding boxes in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>custom_prompt</code> <p>Override default extraction prompt</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_tokens</code> <p>Maximum tokens for generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>8192</code> </p> RETURNS DESCRIPTION <code>DotsOCRTextOutput</code> <p>DotsOCRTextOutput with extracted content and optional layout</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"markdown\", \"html\", \"json\"] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput:\n    \"\"\"\n    Extract text from image using Dots OCR.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\", \"html\", or \"json\")\n        include_layout: Include layout bounding boxes in output\n        custom_prompt: Override default extraction prompt\n        max_tokens: Maximum tokens for generation\n\n    Returns:\n        DotsOCRTextOutput with extracted content and optional layout\n\n    Raises:\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    img = self._prepare_image(image)\n\n    # Get prompt\n    prompt = custom_prompt or DOTS_OCR_PROMPT\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"DotsOCRPyTorchConfig\":\n        raw_output = self._infer_pytorch(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRVLLMConfig\":\n        raw_output = self._infer_vllm(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRAPIConfig\":\n        raw_output = self._infer_api(img, prompt, max_tokens)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse output\n    return self._parse_output(\n        raw_output,\n        img.size,\n        output_format,\n        include_layout,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.pytorch","title":"pytorch","text":"<p>PyTorch backend configuration for Dots OCR.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.pytorch.DotsOCRPyTorchConfig","title":"DotsOCRPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Dots OCR.</p> <p>Dots OCR provides layout-aware text extraction with 11 predefined layout categories (Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title).</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nconfig = DotsOCRPyTorchConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.vllm","title":"vllm","text":"<p>VLLM backend configuration for Dots OCR.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.dotsocr.vllm.DotsOCRVLLMConfig","title":"DotsOCRVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Dots OCR.</p> <p>VLLM provides high-throughput inference with optimizations like: - PagedAttention for efficient KV cache management - Continuous batching for higher throughput - Optimized CUDA kernels</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nconfig = DotsOCRVLLMConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.granitedocling","title":"granitedocling","text":"<p>Granite Docling text extraction with multi-backend support.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.granitedocling.GraniteDoclingTextAPIConfig","title":"GraniteDoclingTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction via API.</p> <p>Uses OpenAI-compatible API endpoints (LiteLLM, OpenRouter, etc.).</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.granitedocling.GraniteDoclingTextExtractor","title":"GraniteDoclingTextExtractor","text":"<pre><code>GraniteDoclingTextExtractor(\n    backend: GraniteDoclingTextBackendConfig,\n)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Granite Docling text extractor supporting PyTorch, VLLM, MLX, and API backends.</p> <p>Granite Docling is IBM's compact vision-language model optimized for document conversion. It outputs DocTags format which is converted to Markdown using the docling_core library.</p> Example <p>from omnidocs.tasks.text_extraction.granitedocling import ( ...     GraniteDoclingTextExtractor, ...     GraniteDoclingTextPyTorchConfig, ... ) config = GraniteDoclingTextPyTorchConfig(device=\"cuda\") extractor = GraniteDoclingTextExtractor(backend=config) result = extractor.extract(image, output_format=\"markdown\") print(result.content)</p> <p>Initialize Granite Docling extractor with backend configuration.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration (PyTorch, VLLM, MLX, or API config)</p> <p> TYPE: <code>GraniteDoclingTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/granitedocling/extractor.py</code> <pre><code>def __init__(self, backend: GraniteDoclingTextBackendConfig):\n    \"\"\"\n    Initialize Granite Docling extractor with backend configuration.\n\n    Args:\n        backend: Backend configuration (PyTorch, VLLM, MLX, or API config)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded: bool = False\n\n    # Backend-specific helpers\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n    self._sampling_params_class: Any = None\n    self._device: str = \"cpu\"\n\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.granitedocling.GraniteDoclingTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image using Granite Docling.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\" or \"html\")</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput with extracted content</p> Source code in <code>omnidocs/tasks/text_extraction/granitedocling/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image using Granite Docling.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\" or \"html\")\n\n    Returns:\n        TextOutput with extracted content\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}\")\n\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Dispatch to backend-specific inference\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"GraniteDoclingTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image)\n    elif config_type == \"GraniteDoclingTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image)\n    elif config_type == \"GraniteDoclingTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image)\n    elif config_type == \"GraniteDoclingTextAPIConfig\":\n        raw_output = self._infer_api(pil_image)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Convert DocTags to Markdown\n    markdown_output = self._convert_doctags_to_markdown(raw_output, pil_image)\n\n    # For HTML output, wrap in basic HTML structure\n    if output_format == \"html\":\n        content = f\"&lt;html&gt;&lt;body&gt;\\n{markdown_output}\\n&lt;/body&gt;&lt;/html&gt;\"\n    else:\n        content = markdown_output\n\n    return TextOutput(\n        content=content,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=self._extract_plain_text(markdown_output),\n        image_width=width,\n        image_height=height,\n        model_name=f\"Granite-Docling-258M ({config_type.replace('Config', '')})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.granitedocling.GraniteDoclingTextMLXConfig","title":"GraniteDoclingTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with MLX backend.</p> <p>This backend is optimized for Apple Silicon Macs (M1/M2/M3/M4). Uses the MLX-optimized model variant.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.granitedocling.GraniteDoclingTextPyTorchConfig","title":"GraniteDoclingTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with PyTorch backend.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.granitedocling.GraniteDoclingTextVLLMConfig","title":"GraniteDoclingTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with VLLM backend.</p> <p>IMPORTANT: This config uses revision=\"untied\" by default, which is required for VLLM compatibility with Granite Docling's tied weights.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.granitedocling.api","title":"api","text":"<p>API backend configuration for Granite Docling text extraction.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.granitedocling.api.GraniteDoclingTextAPIConfig","title":"GraniteDoclingTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction via API.</p> <p>Uses OpenAI-compatible API endpoints (LiteLLM, OpenRouter, etc.).</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.granitedocling.extractor","title":"extractor","text":"<p>Granite Docling text extractor with multi-backend support.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.granitedocling.extractor.GraniteDoclingTextExtractor","title":"GraniteDoclingTextExtractor","text":"<pre><code>GraniteDoclingTextExtractor(\n    backend: GraniteDoclingTextBackendConfig,\n)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Granite Docling text extractor supporting PyTorch, VLLM, MLX, and API backends.</p> <p>Granite Docling is IBM's compact vision-language model optimized for document conversion. It outputs DocTags format which is converted to Markdown using the docling_core library.</p> Example <p>from omnidocs.tasks.text_extraction.granitedocling import ( ...     GraniteDoclingTextExtractor, ...     GraniteDoclingTextPyTorchConfig, ... ) config = GraniteDoclingTextPyTorchConfig(device=\"cuda\") extractor = GraniteDoclingTextExtractor(backend=config) result = extractor.extract(image, output_format=\"markdown\") print(result.content)</p> <p>Initialize Granite Docling extractor with backend configuration.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration (PyTorch, VLLM, MLX, or API config)</p> <p> TYPE: <code>GraniteDoclingTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/granitedocling/extractor.py</code> <pre><code>def __init__(self, backend: GraniteDoclingTextBackendConfig):\n    \"\"\"\n    Initialize Granite Docling extractor with backend configuration.\n\n    Args:\n        backend: Backend configuration (PyTorch, VLLM, MLX, or API config)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded: bool = False\n\n    # Backend-specific helpers\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n    self._sampling_params_class: Any = None\n    self._device: str = \"cpu\"\n\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.granitedocling.extractor.GraniteDoclingTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image using Granite Docling.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\" or \"html\")</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput with extracted content</p> Source code in <code>omnidocs/tasks/text_extraction/granitedocling/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image using Granite Docling.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\" or \"html\")\n\n    Returns:\n        TextOutput with extracted content\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}\")\n\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Dispatch to backend-specific inference\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"GraniteDoclingTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image)\n    elif config_type == \"GraniteDoclingTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image)\n    elif config_type == \"GraniteDoclingTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image)\n    elif config_type == \"GraniteDoclingTextAPIConfig\":\n        raw_output = self._infer_api(pil_image)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Convert DocTags to Markdown\n    markdown_output = self._convert_doctags_to_markdown(raw_output, pil_image)\n\n    # For HTML output, wrap in basic HTML structure\n    if output_format == \"html\":\n        content = f\"&lt;html&gt;&lt;body&gt;\\n{markdown_output}\\n&lt;/body&gt;&lt;/html&gt;\"\n    else:\n        content = markdown_output\n\n    return TextOutput(\n        content=content,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=self._extract_plain_text(markdown_output),\n        image_width=width,\n        image_height=height,\n        model_name=f\"Granite-Docling-258M ({config_type.replace('Config', '')})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.granitedocling.mlx","title":"mlx","text":"<p>MLX backend configuration for Granite Docling text extraction (Apple Silicon).</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.granitedocling.mlx.GraniteDoclingTextMLXConfig","title":"GraniteDoclingTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with MLX backend.</p> <p>This backend is optimized for Apple Silicon Macs (M1/M2/M3/M4). Uses the MLX-optimized model variant.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.granitedocling.pytorch","title":"pytorch","text":"<p>PyTorch backend configuration for Granite Docling text extraction.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.granitedocling.pytorch.GraniteDoclingTextPyTorchConfig","title":"GraniteDoclingTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with PyTorch backend.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.granitedocling.vllm","title":"vllm","text":"<p>VLLM backend configuration for Granite Docling text extraction.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.granitedocling.vllm.GraniteDoclingTextVLLMConfig","title":"GraniteDoclingTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with VLLM backend.</p> <p>IMPORTANT: This config uses revision=\"untied\" by default, which is required for VLLM compatibility with Granite Docling's tied weights.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.models","title":"models","text":"<p>Pydantic models for text extraction outputs.</p> <p>Defines output types and format enums for text extraction.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.models.OutputFormat","title":"OutputFormat","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported text extraction output formats.</p> Each format has different characteristics <ul> <li>HTML: Structured with div elements, preserves layout semantics</li> <li>MARKDOWN: Portable, human-readable, good for documentation</li> <li>JSON: Structured data with layout information (Dots OCR)</li> </ul>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.models.TextOutput","title":"TextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text extraction output from a document image.</p> <p>Contains the extracted text content in the requested format, along with optional raw output and plain text versions.</p> Example <pre><code>result = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)  # Clean markdown\nprint(result.plain_text)  # Plain text without formatting\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.models.TextOutput.content_length","title":"content_length  <code>property</code>","text":"<pre><code>content_length: int\n</code></pre> <p>Length of the extracted content in characters.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.models.TextOutput.word_count","title":"word_count  <code>property</code>","text":"<pre><code>word_count: int\n</code></pre> <p>Approximate word count of the plain text.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.models.LayoutElement","title":"LayoutElement","text":"<p>               Bases: <code>BaseModel</code></p> <p>Single layout element from document layout detection.</p> <p>Represents a detected region in the document with its bounding box, category label, and extracted text content.</p> ATTRIBUTE DESCRIPTION <code>bbox</code> <p>Bounding box coordinates [x1, y1, x2, y2] (normalized to 0-1024)</p> <p> TYPE: <code>List[int]</code> </p> <code>category</code> <p>Layout category (e.g., \"Text\", \"Title\", \"Table\", \"Formula\")</p> <p> TYPE: <code>str</code> </p> <code>text</code> <p>Extracted text content (None for pictures)</p> <p> TYPE: <code>Optional[str]</code> </p> <code>confidence</code> <p>Detection confidence score (optional)</p> <p> TYPE: <code>Optional[float]</code> </p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.models.DotsOCRTextOutput","title":"DotsOCRTextOutput","text":"<p>               Bases: <code>BaseModel</code></p> <p>Text extraction output from Dots OCR with layout information.</p> <p>Dots OCR provides structured output with: - Layout detection (11 categories) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown/LaTeX/HTML) - Reading order preservation</p> Layout Categories <p>Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title</p> Text Formatting <ul> <li>Text/Title/Section-header: Markdown</li> <li>Formula: LaTeX</li> <li>Table: HTML</li> <li>Picture: (text omitted)</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nresult = extractor.extract(image, include_layout=True)\nprint(result.content)  # Full text with formatting\nfor elem in result.layout:\n        print(f\"{elem.category}: {elem.bbox}\")\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.models.DotsOCRTextOutput.num_layout_elements","title":"num_layout_elements  <code>property</code>","text":"<pre><code>num_layout_elements: int\n</code></pre> <p>Number of detected layout elements.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.models.DotsOCRTextOutput.content_length","title":"content_length  <code>property</code>","text":"<pre><code>content_length: int\n</code></pre> <p>Length of extracted content in characters.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.nanonets","title":"nanonets","text":"<p>Nanonets OCR2-3B backend configurations and extractor for text extraction.</p> Available backends <ul> <li>NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend</li> <li>NanonetsTextVLLMConfig: VLLM high-throughput backend</li> <li>NanonetsTextMLXConfig: MLX backend for Apple Silicon</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction.nanonets import NanonetsTextPyTorchConfig\nconfig = NanonetsTextPyTorchConfig()\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.nanonets.NanonetsTextExtractor","title":"NanonetsTextExtractor","text":"<pre><code>NanonetsTextExtractor(backend: NanonetsTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Nanonets OCR2-3B Vision-Language Model text extractor.</p> <p>Extracts text from document images with support for: - Tables (output as HTML) - Equations (output as LaTeX) - Image captions (wrapped in  tags) - Watermarks (wrapped in  tags) - Page numbers (wrapped in  tags) - Checkboxes (using \u2610 and \u2611 symbols)</p> <p>Supports PyTorch, VLLM, and MLX backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import NanonetsTextExtractor\nfrom omnidocs.tasks.text_extraction.nanonets import NanonetsTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = NanonetsTextExtractor(\n        backend=NanonetsTextPyTorchConfig()\n    )\n\n# Extract text\nresult = extractor.extract(image)\nprint(result.content)\n</code></pre> <p>Initialize Nanonets text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend - NanonetsTextVLLMConfig: VLLM high-throughput backend - NanonetsTextMLXConfig: MLX backend for Apple Silicon</p> <p> TYPE: <code>NanonetsTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/nanonets/extractor.py</code> <pre><code>def __init__(self, backend: NanonetsTextBackendConfig):\n    \"\"\"\n    Initialize Nanonets text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend\n            - NanonetsTextVLLMConfig: VLLM high-throughput backend\n            - NanonetsTextMLXConfig: MLX backend for Apple Silicon\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._device: str = \"cpu\"\n\n    # MLX-specific helpers\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.nanonets.NanonetsTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> <p>Note: Nanonets OCR2 produces a unified output format that includes tables as HTML and equations as LaTeX inline. The output_format parameter is accepted for API compatibility but does not change the output structure.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Accepted for API compatibility (default: \"markdown\")</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/nanonets/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Note: Nanonets OCR2 produces a unified output format that includes\n    tables as HTML and equations as LaTeX inline. The output_format\n    parameter is accepted for API compatibility but does not change\n    the output structure.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Accepted for API compatibility (default: \"markdown\")\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"NanonetsTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image)\n    elif config_type == \"NanonetsTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image)\n    elif config_type == \"NanonetsTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    cleaned_output = raw_output.replace(\"&lt;|im_end|&gt;\", \"\").strip()\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=cleaned_output,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Nanonets-OCR2-3B ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.nanonets.NanonetsTextMLXConfig","title":"NanonetsTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3/M4+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = NanonetsTextMLXConfig(\n        model=\"mlx-community/Nanonets-OCR2-3B-bf16\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.nanonets.NanonetsTextPyTorchConfig","title":"NanonetsTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate</p> Example <pre><code>config = NanonetsTextPyTorchConfig(\n        device=\"cuda\",\n        torch_dtype=\"float16\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.nanonets.NanonetsTextVLLMConfig","title":"NanonetsTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = NanonetsTextVLLMConfig(\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.85,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.nanonets.extractor","title":"extractor","text":"<p>Nanonets OCR2-3B text extractor.</p> <p>A Vision-Language Model for extracting text from document images with support for tables (HTML), equations (LaTeX), and image captions.</p> <p>Supports PyTorch and VLLM backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import NanonetsTextExtractor\nfrom omnidocs.tasks.text_extraction.nanonets import NanonetsTextPyTorchConfig\n\nextractor = NanonetsTextExtractor(\n        backend=NanonetsTextPyTorchConfig()\n    )\nresult = extractor.extract(image)\nprint(result.content)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.nanonets.extractor.NanonetsTextExtractor","title":"NanonetsTextExtractor","text":"<pre><code>NanonetsTextExtractor(backend: NanonetsTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Nanonets OCR2-3B Vision-Language Model text extractor.</p> <p>Extracts text from document images with support for: - Tables (output as HTML) - Equations (output as LaTeX) - Image captions (wrapped in  tags) - Watermarks (wrapped in  tags) - Page numbers (wrapped in  tags) - Checkboxes (using \u2610 and \u2611 symbols)</p> <p>Supports PyTorch, VLLM, and MLX backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import NanonetsTextExtractor\nfrom omnidocs.tasks.text_extraction.nanonets import NanonetsTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = NanonetsTextExtractor(\n        backend=NanonetsTextPyTorchConfig()\n    )\n\n# Extract text\nresult = extractor.extract(image)\nprint(result.content)\n</code></pre> <p>Initialize Nanonets text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend - NanonetsTextVLLMConfig: VLLM high-throughput backend - NanonetsTextMLXConfig: MLX backend for Apple Silicon</p> <p> TYPE: <code>NanonetsTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/nanonets/extractor.py</code> <pre><code>def __init__(self, backend: NanonetsTextBackendConfig):\n    \"\"\"\n    Initialize Nanonets text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend\n            - NanonetsTextVLLMConfig: VLLM high-throughput backend\n            - NanonetsTextMLXConfig: MLX backend for Apple Silicon\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._device: str = \"cpu\"\n\n    # MLX-specific helpers\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.nanonets.extractor.NanonetsTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> <p>Note: Nanonets OCR2 produces a unified output format that includes tables as HTML and equations as LaTeX inline. The output_format parameter is accepted for API compatibility but does not change the output structure.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Accepted for API compatibility (default: \"markdown\")</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/nanonets/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Note: Nanonets OCR2 produces a unified output format that includes\n    tables as HTML and equations as LaTeX inline. The output_format\n    parameter is accepted for API compatibility but does not change\n    the output structure.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Accepted for API compatibility (default: \"markdown\")\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"NanonetsTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image)\n    elif config_type == \"NanonetsTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image)\n    elif config_type == \"NanonetsTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    cleaned_output = raw_output.replace(\"&lt;|im_end|&gt;\", \"\").strip()\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=cleaned_output,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Nanonets-OCR2-3B ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.nanonets.mlx","title":"mlx","text":"<p>MLX backend configuration for Nanonets OCR2-3B text extraction.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.nanonets.mlx.NanonetsTextMLXConfig","title":"NanonetsTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3/M4+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = NanonetsTextMLXConfig(\n        model=\"mlx-community/Nanonets-OCR2-3B-bf16\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.nanonets.pytorch","title":"pytorch","text":"<p>PyTorch/HuggingFace backend configuration for Nanonets OCR2-3B text extraction.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.nanonets.pytorch.NanonetsTextPyTorchConfig","title":"NanonetsTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate</p> Example <pre><code>config = NanonetsTextPyTorchConfig(\n        device=\"cuda\",\n        torch_dtype=\"float16\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.nanonets.vllm","title":"vllm","text":"<p>VLLM backend configuration for Nanonets OCR2-3B text extraction.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.nanonets.vllm.NanonetsTextVLLMConfig","title":"NanonetsTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = NanonetsTextVLLMConfig(\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.85,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen","title":"qwen","text":"<p>Qwen3-VL backend configurations and extractor for text extraction.</p> Available backends <ul> <li>QwenTextPyTorchConfig: PyTorch/HuggingFace backend</li> <li>QwenTextVLLMConfig: VLLM high-throughput backend</li> <li>QwenTextMLXConfig: MLX backend for Apple Silicon</li> <li>QwenTextAPIConfig: API backend (OpenRouter, etc.)</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nconfig = QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextAPIConfig","title":"QwenTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen text extraction.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenTextAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextExtractor","title":"QwenTextExtractor","text":"<pre><code>QwenTextExtractor(backend: QwenTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Qwen3-VL Vision-Language Model text extractor.</p> <p>Extracts text from document images and outputs as structured HTML or Markdown. Uses Qwen3-VL's built-in document parsing prompts.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Extract as Markdown\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n\n# Extract as HTML\nresult = extractor.extract(image, output_format=\"html\")\nprint(result.content)\n</code></pre> <p>Initialize Qwen text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenTextPyTorchConfig: PyTorch/HuggingFace backend - QwenTextVLLMConfig: VLLM high-throughput backend - QwenTextMLXConfig: MLX backend for Apple Silicon - QwenTextAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def __init__(self, backend: QwenTextBackendConfig):\n    \"\"\"\n    Initialize Qwen text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenTextPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenTextVLLMConfig: VLLM high-throughput backend\n            - QwenTextMLXConfig: MLX backend for Apple Silicon\n            - QwenTextAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML with div elements - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format or output_format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML with div elements\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format or output_format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}. Expected 'html' or 'markdown'.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Get prompt for output format\n    prompt = QWEN_PROMPTS[output_format]\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenTextAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    if output_format == \"html\":\n        cleaned_output = _clean_html_output(raw_output)\n    else:\n        cleaned_output = _clean_markdown_output(raw_output)\n\n    # Extract plain text\n    plain_text = _extract_plain_text(raw_output, output_format)\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=plain_text,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextMLXConfig","title":"QwenTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenTextMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextPyTorchConfig","title":"QwenTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextVLLMConfig","title":"QwenTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenTextVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.api","title":"api","text":"<p>API backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.api.QwenTextAPIConfig","title":"QwenTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen text extraction.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenTextAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.extractor","title":"extractor","text":"<p>Qwen3-VL text extractor.</p> <p>A Vision-Language Model for extracting text from document images as structured HTML or Markdown.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.extractor.QwenTextExtractor","title":"QwenTextExtractor","text":"<pre><code>QwenTextExtractor(backend: QwenTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Qwen3-VL Vision-Language Model text extractor.</p> <p>Extracts text from document images and outputs as structured HTML or Markdown. Uses Qwen3-VL's built-in document parsing prompts.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Extract as Markdown\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n\n# Extract as HTML\nresult = extractor.extract(image, output_format=\"html\")\nprint(result.content)\n</code></pre> <p>Initialize Qwen text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenTextPyTorchConfig: PyTorch/HuggingFace backend - QwenTextVLLMConfig: VLLM high-throughput backend - QwenTextMLXConfig: MLX backend for Apple Silicon - QwenTextAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def __init__(self, backend: QwenTextBackendConfig):\n    \"\"\"\n    Initialize Qwen text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenTextPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenTextVLLMConfig: VLLM high-throughput backend\n            - QwenTextMLXConfig: MLX backend for Apple Silicon\n            - QwenTextAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.extractor.QwenTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML with div elements - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format or output_format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML with div elements\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format or output_format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}. Expected 'html' or 'markdown'.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Get prompt for output format\n    prompt = QWEN_PROMPTS[output_format]\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenTextAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    if output_format == \"html\":\n        cleaned_output = _clean_html_output(raw_output)\n    else:\n        cleaned_output = _clean_markdown_output(raw_output)\n\n    # Extract plain text\n    plain_text = _extract_plain_text(raw_output, output_format)\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=plain_text,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.mlx","title":"mlx","text":"<p>MLX backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.mlx.QwenTextMLXConfig","title":"QwenTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenTextMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.pytorch","title":"pytorch","text":"<p>PyTorch/HuggingFace backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.pytorch.QwenTextPyTorchConfig","title":"QwenTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.vllm","title":"vllm","text":"<p>VLLM backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/overview/#omnidocs.tasks.text_extraction.qwen.vllm.QwenTextVLLMConfig","title":"QwenTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenTextVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/api/","title":"API","text":"<p>API backend configuration for Dots OCR (VLLM online server).</p>"},{"location":"reference/tasks/text_extraction/dots_ocr/api/#omnidocs.tasks.text_extraction.dotsocr.api.DotsOCRAPIConfig","title":"DotsOCRAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Dots OCR.</p> <p>This config is for accessing a deployed VLLM server via OpenAI-compatible API. Typically used with modal_dotsocr_vllm_online.py deployment.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRAPIConfig\n\nconfig = DotsOCRAPIConfig(\n        model=\"dotsocr\",\n        api_base=\"https://your-modal-app.modal.run/v1\",\n        api_key=\"optional-key\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/extractor/","title":"Extractor","text":"<p>Dots OCR text extractor with layout-aware extraction.</p> <p>A Vision-Language Model optimized for document OCR with structured output containing layout information, bounding boxes, and multi-format text.</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\nresult = extractor.extract(image, include_layout=True)\nprint(result.content)\nfor elem in result.layout:\n        print(f\"{elem.category}: {elem.bbox}\")\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/extractor/#omnidocs.tasks.text_extraction.dotsocr.extractor.DotsOCRTextExtractor","title":"DotsOCRTextExtractor","text":"<pre><code>DotsOCRTextExtractor(backend: DotsOCRBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Dots OCR Vision-Language Model text extractor with layout detection.</p> <p>Extracts text from document images with layout information including: - 11 layout categories (Caption, Footnote, Formula, List-item, etc.) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown, LaTeX, HTML) - Reading order preservation</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\n\n# Extract with layout\nresult = extractor.extract(image, include_layout=True)\nprint(f\"Found {result.num_layout_elements} elements\")\nprint(result.content)\n</code></pre> <p>Initialize Dots OCR text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend - DotsOCRVLLMConfig: VLLM high-throughput backend - DotsOCRAPIConfig: API backend (online VLLM server)</p> <p> TYPE: <code>DotsOCRBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def __init__(self, backend: DotsOCRBackendConfig):\n    \"\"\"\n    Initialize Dots OCR text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend\n            - DotsOCRVLLMConfig: VLLM high-throughput backend\n            - DotsOCRAPIConfig: API backend (online VLLM server)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._model: Any = None\n    self._loaded = False\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/extractor/#omnidocs.tasks.text_extraction.dotsocr.extractor.DotsOCRTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\n        \"markdown\", \"html\", \"json\"\n    ] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput\n</code></pre> <p>Extract text from image using Dots OCR.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\", \"html\", or \"json\")</p> <p> TYPE: <code>Literal['markdown', 'html', 'json']</code> DEFAULT: <code>'markdown'</code> </p> <code>include_layout</code> <p>Include layout bounding boxes in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>custom_prompt</code> <p>Override default extraction prompt</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_tokens</code> <p>Maximum tokens for generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>8192</code> </p> RETURNS DESCRIPTION <code>DotsOCRTextOutput</code> <p>DotsOCRTextOutput with extracted content and optional layout</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"markdown\", \"html\", \"json\"] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput:\n    \"\"\"\n    Extract text from image using Dots OCR.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\", \"html\", or \"json\")\n        include_layout: Include layout bounding boxes in output\n        custom_prompt: Override default extraction prompt\n        max_tokens: Maximum tokens for generation\n\n    Returns:\n        DotsOCRTextOutput with extracted content and optional layout\n\n    Raises:\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    img = self._prepare_image(image)\n\n    # Get prompt\n    prompt = custom_prompt or DOTS_OCR_PROMPT\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"DotsOCRPyTorchConfig\":\n        raw_output = self._infer_pytorch(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRVLLMConfig\":\n        raw_output = self._infer_vllm(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRAPIConfig\":\n        raw_output = self._infer_api(img, prompt, max_tokens)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse output\n    return self._parse_output(\n        raw_output,\n        img.size,\n        output_format,\n        include_layout,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/","title":"Overview","text":"<p>Dots OCR text extractor and backend configurations.</p> <p>Available backends: - PyTorch: DotsOCRPyTorchConfig (local GPU inference) - VLLM: DotsOCRVLLMConfig (offline batch inference) - API: DotsOCRAPIConfig (online VLLM server via OpenAI-compatible API)</p>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRAPIConfig","title":"DotsOCRAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Dots OCR.</p> <p>This config is for accessing a deployed VLLM server via OpenAI-compatible API. Typically used with modal_dotsocr_vllm_online.py deployment.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRAPIConfig\n\nconfig = DotsOCRAPIConfig(\n        model=\"dotsocr\",\n        api_base=\"https://your-modal-app.modal.run/v1\",\n        api_key=\"optional-key\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRTextExtractor","title":"DotsOCRTextExtractor","text":"<pre><code>DotsOCRTextExtractor(backend: DotsOCRBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Dots OCR Vision-Language Model text extractor with layout detection.</p> <p>Extracts text from document images with layout information including: - 11 layout categories (Caption, Footnote, Formula, List-item, etc.) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown, LaTeX, HTML) - Reading order preservation</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\n\n# Extract with layout\nresult = extractor.extract(image, include_layout=True)\nprint(f\"Found {result.num_layout_elements} elements\")\nprint(result.content)\n</code></pre> <p>Initialize Dots OCR text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend - DotsOCRVLLMConfig: VLLM high-throughput backend - DotsOCRAPIConfig: API backend (online VLLM server)</p> <p> TYPE: <code>DotsOCRBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def __init__(self, backend: DotsOCRBackendConfig):\n    \"\"\"\n    Initialize Dots OCR text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend\n            - DotsOCRVLLMConfig: VLLM high-throughput backend\n            - DotsOCRAPIConfig: API backend (online VLLM server)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._model: Any = None\n    self._loaded = False\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\n        \"markdown\", \"html\", \"json\"\n    ] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput\n</code></pre> <p>Extract text from image using Dots OCR.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\", \"html\", or \"json\")</p> <p> TYPE: <code>Literal['markdown', 'html', 'json']</code> DEFAULT: <code>'markdown'</code> </p> <code>include_layout</code> <p>Include layout bounding boxes in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>custom_prompt</code> <p>Override default extraction prompt</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_tokens</code> <p>Maximum tokens for generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>8192</code> </p> RETURNS DESCRIPTION <code>DotsOCRTextOutput</code> <p>DotsOCRTextOutput with extracted content and optional layout</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"markdown\", \"html\", \"json\"] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput:\n    \"\"\"\n    Extract text from image using Dots OCR.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\", \"html\", or \"json\")\n        include_layout: Include layout bounding boxes in output\n        custom_prompt: Override default extraction prompt\n        max_tokens: Maximum tokens for generation\n\n    Returns:\n        DotsOCRTextOutput with extracted content and optional layout\n\n    Raises:\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    img = self._prepare_image(image)\n\n    # Get prompt\n    prompt = custom_prompt or DOTS_OCR_PROMPT\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"DotsOCRPyTorchConfig\":\n        raw_output = self._infer_pytorch(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRVLLMConfig\":\n        raw_output = self._infer_vllm(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRAPIConfig\":\n        raw_output = self._infer_api(img, prompt, max_tokens)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse output\n    return self._parse_output(\n        raw_output,\n        img.size,\n        output_format,\n        include_layout,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRPyTorchConfig","title":"DotsOCRPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Dots OCR.</p> <p>Dots OCR provides layout-aware text extraction with 11 predefined layout categories (Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title).</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nconfig = DotsOCRPyTorchConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.DotsOCRVLLMConfig","title":"DotsOCRVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Dots OCR.</p> <p>VLLM provides high-throughput inference with optimizations like: - PagedAttention for efficient KV cache management - Continuous batching for higher throughput - Optimized CUDA kernels</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nconfig = DotsOCRVLLMConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.api","title":"api","text":"<p>API backend configuration for Dots OCR (VLLM online server).</p>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.api.DotsOCRAPIConfig","title":"DotsOCRAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Dots OCR.</p> <p>This config is for accessing a deployed VLLM server via OpenAI-compatible API. Typically used with modal_dotsocr_vllm_online.py deployment.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRAPIConfig\n\nconfig = DotsOCRAPIConfig(\n        model=\"dotsocr\",\n        api_base=\"https://your-modal-app.modal.run/v1\",\n        api_key=\"optional-key\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.extractor","title":"extractor","text":"<p>Dots OCR text extractor with layout-aware extraction.</p> <p>A Vision-Language Model optimized for document OCR with structured output containing layout information, bounding boxes, and multi-format text.</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\nresult = extractor.extract(image, include_layout=True)\nprint(result.content)\nfor elem in result.layout:\n        print(f\"{elem.category}: {elem.bbox}\")\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.extractor.DotsOCRTextExtractor","title":"DotsOCRTextExtractor","text":"<pre><code>DotsOCRTextExtractor(backend: DotsOCRBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Dots OCR Vision-Language Model text extractor with layout detection.</p> <p>Extracts text from document images with layout information including: - 11 layout categories (Caption, Footnote, Formula, List-item, etc.) - Bounding boxes (normalized to 0-1024) - Multi-format text (Markdown, LaTeX, HTML) - Reading order preservation</p> <p>Supports PyTorch, VLLM, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = DotsOCRTextExtractor(\n        backend=DotsOCRPyTorchConfig(model=\"rednote-hilab/dots.ocr\")\n    )\n\n# Extract with layout\nresult = extractor.extract(image, include_layout=True)\nprint(f\"Found {result.num_layout_elements} elements\")\nprint(result.content)\n</code></pre> <p>Initialize Dots OCR text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend - DotsOCRVLLMConfig: VLLM high-throughput backend - DotsOCRAPIConfig: API backend (online VLLM server)</p> <p> TYPE: <code>DotsOCRBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def __init__(self, backend: DotsOCRBackendConfig):\n    \"\"\"\n    Initialize Dots OCR text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - DotsOCRPyTorchConfig: PyTorch/HuggingFace backend\n            - DotsOCRVLLMConfig: VLLM high-throughput backend\n            - DotsOCRAPIConfig: API backend (online VLLM server)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._model: Any = None\n    self._loaded = False\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.extractor.DotsOCRTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\n        \"markdown\", \"html\", \"json\"\n    ] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput\n</code></pre> <p>Extract text from image using Dots OCR.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\", \"html\", or \"json\")</p> <p> TYPE: <code>Literal['markdown', 'html', 'json']</code> DEFAULT: <code>'markdown'</code> </p> <code>include_layout</code> <p>Include layout bounding boxes in output</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>custom_prompt</code> <p>Override default extraction prompt</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>max_tokens</code> <p>Maximum tokens for generation</p> <p> TYPE: <code>int</code> DEFAULT: <code>8192</code> </p> RETURNS DESCRIPTION <code>DotsOCRTextOutput</code> <p>DotsOCRTextOutput with extracted content and optional layout</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded or inference fails</p> Source code in <code>omnidocs/tasks/text_extraction/dotsocr/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"markdown\", \"html\", \"json\"] = \"markdown\",\n    include_layout: bool = False,\n    custom_prompt: Optional[str] = None,\n    max_tokens: int = 8192,\n) -&gt; DotsOCRTextOutput:\n    \"\"\"\n    Extract text from image using Dots OCR.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\", \"html\", or \"json\")\n        include_layout: Include layout bounding boxes in output\n        custom_prompt: Override default extraction prompt\n        max_tokens: Maximum tokens for generation\n\n    Returns:\n        DotsOCRTextOutput with extracted content and optional layout\n\n    Raises:\n        RuntimeError: If model is not loaded or inference fails\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    img = self._prepare_image(image)\n\n    # Get prompt\n    prompt = custom_prompt or DOTS_OCR_PROMPT\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"DotsOCRPyTorchConfig\":\n        raw_output = self._infer_pytorch(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRVLLMConfig\":\n        raw_output = self._infer_vllm(img, prompt, max_tokens)\n    elif config_type == \"DotsOCRAPIConfig\":\n        raw_output = self._infer_api(img, prompt, max_tokens)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Parse output\n    return self._parse_output(\n        raw_output,\n        img.size,\n        output_format,\n        include_layout,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.pytorch","title":"pytorch","text":"<p>PyTorch backend configuration for Dots OCR.</p>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.pytorch.DotsOCRPyTorchConfig","title":"DotsOCRPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Dots OCR.</p> <p>Dots OCR provides layout-aware text extraction with 11 predefined layout categories (Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title).</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nconfig = DotsOCRPyTorchConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.vllm","title":"vllm","text":"<p>VLLM backend configuration for Dots OCR.</p>"},{"location":"reference/tasks/text_extraction/dots_ocr/overview/#omnidocs.tasks.text_extraction.dotsocr.vllm.DotsOCRVLLMConfig","title":"DotsOCRVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Dots OCR.</p> <p>VLLM provides high-throughput inference with optimizations like: - PagedAttention for efficient KV cache management - Continuous batching for higher throughput - Optimized CUDA kernels</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nconfig = DotsOCRVLLMConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/pytorch/","title":"PyTorch","text":"<p>PyTorch backend configuration for Dots OCR.</p>"},{"location":"reference/tasks/text_extraction/dots_ocr/pytorch/#omnidocs.tasks.text_extraction.dotsocr.pytorch.DotsOCRPyTorchConfig","title":"DotsOCRPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Dots OCR.</p> <p>Dots OCR provides layout-aware text extraction with 11 predefined layout categories (Caption, Footnote, Formula, List-item, Page-footer, Page-header, Picture, Section-header, Table, Text, Title).</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nconfig = DotsOCRPyTorchConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/dots_ocr/vllm/","title":"VLLM","text":"<p>VLLM backend configuration for Dots OCR.</p>"},{"location":"reference/tasks/text_extraction/dots_ocr/vllm/#omnidocs.tasks.text_extraction.dotsocr.vllm.DotsOCRVLLMConfig","title":"DotsOCRVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Dots OCR.</p> <p>VLLM provides high-throughput inference with optimizations like: - PagedAttention for efficient KV cache management - Continuous batching for higher throughput - Optimized CUDA kernels</p> Example <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nconfig = DotsOCRVLLMConfig(\n        model=\"rednote-hilab/dots.ocr\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\nextractor = DotsOCRTextExtractor(backend=config)\n</code></pre>"},{"location":"reference/tasks/text_extraction/granitedocling/api/","title":"API","text":"<p>API backend configuration for Granite Docling text extraction.</p>"},{"location":"reference/tasks/text_extraction/granitedocling/api/#omnidocs.tasks.text_extraction.granitedocling.api.GraniteDoclingTextAPIConfig","title":"GraniteDoclingTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction via API.</p> <p>Uses OpenAI-compatible API endpoints (LiteLLM, OpenRouter, etc.).</p>"},{"location":"reference/tasks/text_extraction/granitedocling/extractor/","title":"Extractor","text":"<p>Granite Docling text extractor with multi-backend support.</p>"},{"location":"reference/tasks/text_extraction/granitedocling/extractor/#omnidocs.tasks.text_extraction.granitedocling.extractor.GraniteDoclingTextExtractor","title":"GraniteDoclingTextExtractor","text":"<pre><code>GraniteDoclingTextExtractor(\n    backend: GraniteDoclingTextBackendConfig,\n)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Granite Docling text extractor supporting PyTorch, VLLM, MLX, and API backends.</p> <p>Granite Docling is IBM's compact vision-language model optimized for document conversion. It outputs DocTags format which is converted to Markdown using the docling_core library.</p> Example <p>from omnidocs.tasks.text_extraction.granitedocling import ( ...     GraniteDoclingTextExtractor, ...     GraniteDoclingTextPyTorchConfig, ... ) config = GraniteDoclingTextPyTorchConfig(device=\"cuda\") extractor = GraniteDoclingTextExtractor(backend=config) result = extractor.extract(image, output_format=\"markdown\") print(result.content)</p> <p>Initialize Granite Docling extractor with backend configuration.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration (PyTorch, VLLM, MLX, or API config)</p> <p> TYPE: <code>GraniteDoclingTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/granitedocling/extractor.py</code> <pre><code>def __init__(self, backend: GraniteDoclingTextBackendConfig):\n    \"\"\"\n    Initialize Granite Docling extractor with backend configuration.\n\n    Args:\n        backend: Backend configuration (PyTorch, VLLM, MLX, or API config)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded: bool = False\n\n    # Backend-specific helpers\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n    self._sampling_params_class: Any = None\n    self._device: str = \"cpu\"\n\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/granitedocling/extractor/#omnidocs.tasks.text_extraction.granitedocling.extractor.GraniteDoclingTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image using Granite Docling.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\" or \"html\")</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput with extracted content</p> Source code in <code>omnidocs/tasks/text_extraction/granitedocling/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image using Granite Docling.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\" or \"html\")\n\n    Returns:\n        TextOutput with extracted content\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}\")\n\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Dispatch to backend-specific inference\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"GraniteDoclingTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image)\n    elif config_type == \"GraniteDoclingTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image)\n    elif config_type == \"GraniteDoclingTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image)\n    elif config_type == \"GraniteDoclingTextAPIConfig\":\n        raw_output = self._infer_api(pil_image)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Convert DocTags to Markdown\n    markdown_output = self._convert_doctags_to_markdown(raw_output, pil_image)\n\n    # For HTML output, wrap in basic HTML structure\n    if output_format == \"html\":\n        content = f\"&lt;html&gt;&lt;body&gt;\\n{markdown_output}\\n&lt;/body&gt;&lt;/html&gt;\"\n    else:\n        content = markdown_output\n\n    return TextOutput(\n        content=content,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=self._extract_plain_text(markdown_output),\n        image_width=width,\n        image_height=height,\n        model_name=f\"Granite-Docling-258M ({config_type.replace('Config', '')})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/granitedocling/mlx/","title":"MLX","text":"<p>MLX backend configuration for Granite Docling text extraction (Apple Silicon).</p>"},{"location":"reference/tasks/text_extraction/granitedocling/mlx/#omnidocs.tasks.text_extraction.granitedocling.mlx.GraniteDoclingTextMLXConfig","title":"GraniteDoclingTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with MLX backend.</p> <p>This backend is optimized for Apple Silicon Macs (M1/M2/M3/M4). Uses the MLX-optimized model variant.</p>"},{"location":"reference/tasks/text_extraction/granitedocling/overview/","title":"Overview","text":"<p>Granite Docling text extraction with multi-backend support.</p>"},{"location":"reference/tasks/text_extraction/granitedocling/overview/#omnidocs.tasks.text_extraction.granitedocling.GraniteDoclingTextAPIConfig","title":"GraniteDoclingTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction via API.</p> <p>Uses OpenAI-compatible API endpoints (LiteLLM, OpenRouter, etc.).</p>"},{"location":"reference/tasks/text_extraction/granitedocling/overview/#omnidocs.tasks.text_extraction.granitedocling.GraniteDoclingTextExtractor","title":"GraniteDoclingTextExtractor","text":"<pre><code>GraniteDoclingTextExtractor(\n    backend: GraniteDoclingTextBackendConfig,\n)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Granite Docling text extractor supporting PyTorch, VLLM, MLX, and API backends.</p> <p>Granite Docling is IBM's compact vision-language model optimized for document conversion. It outputs DocTags format which is converted to Markdown using the docling_core library.</p> Example <p>from omnidocs.tasks.text_extraction.granitedocling import ( ...     GraniteDoclingTextExtractor, ...     GraniteDoclingTextPyTorchConfig, ... ) config = GraniteDoclingTextPyTorchConfig(device=\"cuda\") extractor = GraniteDoclingTextExtractor(backend=config) result = extractor.extract(image, output_format=\"markdown\") print(result.content)</p> <p>Initialize Granite Docling extractor with backend configuration.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration (PyTorch, VLLM, MLX, or API config)</p> <p> TYPE: <code>GraniteDoclingTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/granitedocling/extractor.py</code> <pre><code>def __init__(self, backend: GraniteDoclingTextBackendConfig):\n    \"\"\"\n    Initialize Granite Docling extractor with backend configuration.\n\n    Args:\n        backend: Backend configuration (PyTorch, VLLM, MLX, or API config)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded: bool = False\n\n    # Backend-specific helpers\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n    self._sampling_params_class: Any = None\n    self._device: str = \"cpu\"\n\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/granitedocling/overview/#omnidocs.tasks.text_extraction.granitedocling.GraniteDoclingTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image using Granite Docling.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\" or \"html\")</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput with extracted content</p> Source code in <code>omnidocs/tasks/text_extraction/granitedocling/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image using Granite Docling.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\" or \"html\")\n\n    Returns:\n        TextOutput with extracted content\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}\")\n\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Dispatch to backend-specific inference\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"GraniteDoclingTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image)\n    elif config_type == \"GraniteDoclingTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image)\n    elif config_type == \"GraniteDoclingTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image)\n    elif config_type == \"GraniteDoclingTextAPIConfig\":\n        raw_output = self._infer_api(pil_image)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Convert DocTags to Markdown\n    markdown_output = self._convert_doctags_to_markdown(raw_output, pil_image)\n\n    # For HTML output, wrap in basic HTML structure\n    if output_format == \"html\":\n        content = f\"&lt;html&gt;&lt;body&gt;\\n{markdown_output}\\n&lt;/body&gt;&lt;/html&gt;\"\n    else:\n        content = markdown_output\n\n    return TextOutput(\n        content=content,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=self._extract_plain_text(markdown_output),\n        image_width=width,\n        image_height=height,\n        model_name=f\"Granite-Docling-258M ({config_type.replace('Config', '')})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/granitedocling/overview/#omnidocs.tasks.text_extraction.granitedocling.GraniteDoclingTextMLXConfig","title":"GraniteDoclingTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with MLX backend.</p> <p>This backend is optimized for Apple Silicon Macs (M1/M2/M3/M4). Uses the MLX-optimized model variant.</p>"},{"location":"reference/tasks/text_extraction/granitedocling/overview/#omnidocs.tasks.text_extraction.granitedocling.GraniteDoclingTextPyTorchConfig","title":"GraniteDoclingTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with PyTorch backend.</p>"},{"location":"reference/tasks/text_extraction/granitedocling/overview/#omnidocs.tasks.text_extraction.granitedocling.GraniteDoclingTextVLLMConfig","title":"GraniteDoclingTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with VLLM backend.</p> <p>IMPORTANT: This config uses revision=\"untied\" by default, which is required for VLLM compatibility with Granite Docling's tied weights.</p>"},{"location":"reference/tasks/text_extraction/granitedocling/overview/#omnidocs.tasks.text_extraction.granitedocling.api","title":"api","text":"<p>API backend configuration for Granite Docling text extraction.</p>"},{"location":"reference/tasks/text_extraction/granitedocling/overview/#omnidocs.tasks.text_extraction.granitedocling.api.GraniteDoclingTextAPIConfig","title":"GraniteDoclingTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction via API.</p> <p>Uses OpenAI-compatible API endpoints (LiteLLM, OpenRouter, etc.).</p>"},{"location":"reference/tasks/text_extraction/granitedocling/overview/#omnidocs.tasks.text_extraction.granitedocling.extractor","title":"extractor","text":"<p>Granite Docling text extractor with multi-backend support.</p>"},{"location":"reference/tasks/text_extraction/granitedocling/overview/#omnidocs.tasks.text_extraction.granitedocling.extractor.GraniteDoclingTextExtractor","title":"GraniteDoclingTextExtractor","text":"<pre><code>GraniteDoclingTextExtractor(\n    backend: GraniteDoclingTextBackendConfig,\n)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Granite Docling text extractor supporting PyTorch, VLLM, MLX, and API backends.</p> <p>Granite Docling is IBM's compact vision-language model optimized for document conversion. It outputs DocTags format which is converted to Markdown using the docling_core library.</p> Example <p>from omnidocs.tasks.text_extraction.granitedocling import ( ...     GraniteDoclingTextExtractor, ...     GraniteDoclingTextPyTorchConfig, ... ) config = GraniteDoclingTextPyTorchConfig(device=\"cuda\") extractor = GraniteDoclingTextExtractor(backend=config) result = extractor.extract(image, output_format=\"markdown\") print(result.content)</p> <p>Initialize Granite Docling extractor with backend configuration.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration (PyTorch, VLLM, MLX, or API config)</p> <p> TYPE: <code>GraniteDoclingTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/granitedocling/extractor.py</code> <pre><code>def __init__(self, backend: GraniteDoclingTextBackendConfig):\n    \"\"\"\n    Initialize Granite Docling extractor with backend configuration.\n\n    Args:\n        backend: Backend configuration (PyTorch, VLLM, MLX, or API config)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded: bool = False\n\n    # Backend-specific helpers\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n    self._sampling_params_class: Any = None\n    self._device: str = \"cpu\"\n\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/granitedocling/overview/#omnidocs.tasks.text_extraction.granitedocling.extractor.GraniteDoclingTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image using Granite Docling.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image (PIL Image, numpy array, or file path)</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Output format (\"markdown\" or \"html\")</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput with extracted content</p> Source code in <code>omnidocs/tasks/text_extraction/granitedocling/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image using Granite Docling.\n\n    Args:\n        image: Input image (PIL Image, numpy array, or file path)\n        output_format: Output format (\"markdown\" or \"html\")\n\n    Returns:\n        TextOutput with extracted content\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}\")\n\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Dispatch to backend-specific inference\n    config_type = type(self.backend_config).__name__\n\n    if config_type == \"GraniteDoclingTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image)\n    elif config_type == \"GraniteDoclingTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image)\n    elif config_type == \"GraniteDoclingTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image)\n    elif config_type == \"GraniteDoclingTextAPIConfig\":\n        raw_output = self._infer_api(pil_image)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Convert DocTags to Markdown\n    markdown_output = self._convert_doctags_to_markdown(raw_output, pil_image)\n\n    # For HTML output, wrap in basic HTML structure\n    if output_format == \"html\":\n        content = f\"&lt;html&gt;&lt;body&gt;\\n{markdown_output}\\n&lt;/body&gt;&lt;/html&gt;\"\n    else:\n        content = markdown_output\n\n    return TextOutput(\n        content=content,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=self._extract_plain_text(markdown_output),\n        image_width=width,\n        image_height=height,\n        model_name=f\"Granite-Docling-258M ({config_type.replace('Config', '')})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/granitedocling/overview/#omnidocs.tasks.text_extraction.granitedocling.mlx","title":"mlx","text":"<p>MLX backend configuration for Granite Docling text extraction (Apple Silicon).</p>"},{"location":"reference/tasks/text_extraction/granitedocling/overview/#omnidocs.tasks.text_extraction.granitedocling.mlx.GraniteDoclingTextMLXConfig","title":"GraniteDoclingTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with MLX backend.</p> <p>This backend is optimized for Apple Silicon Macs (M1/M2/M3/M4). Uses the MLX-optimized model variant.</p>"},{"location":"reference/tasks/text_extraction/granitedocling/overview/#omnidocs.tasks.text_extraction.granitedocling.pytorch","title":"pytorch","text":"<p>PyTorch backend configuration for Granite Docling text extraction.</p>"},{"location":"reference/tasks/text_extraction/granitedocling/overview/#omnidocs.tasks.text_extraction.granitedocling.pytorch.GraniteDoclingTextPyTorchConfig","title":"GraniteDoclingTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with PyTorch backend.</p>"},{"location":"reference/tasks/text_extraction/granitedocling/overview/#omnidocs.tasks.text_extraction.granitedocling.vllm","title":"vllm","text":"<p>VLLM backend configuration for Granite Docling text extraction.</p>"},{"location":"reference/tasks/text_extraction/granitedocling/overview/#omnidocs.tasks.text_extraction.granitedocling.vllm.GraniteDoclingTextVLLMConfig","title":"GraniteDoclingTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with VLLM backend.</p> <p>IMPORTANT: This config uses revision=\"untied\" by default, which is required for VLLM compatibility with Granite Docling's tied weights.</p>"},{"location":"reference/tasks/text_extraction/granitedocling/pytorch/","title":"PyTorch","text":"<p>PyTorch backend configuration for Granite Docling text extraction.</p>"},{"location":"reference/tasks/text_extraction/granitedocling/pytorch/#omnidocs.tasks.text_extraction.granitedocling.pytorch.GraniteDoclingTextPyTorchConfig","title":"GraniteDoclingTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with PyTorch backend.</p>"},{"location":"reference/tasks/text_extraction/granitedocling/vllm/","title":"VLLM","text":"<p>VLLM backend configuration for Granite Docling text extraction.</p>"},{"location":"reference/tasks/text_extraction/granitedocling/vllm/#omnidocs.tasks.text_extraction.granitedocling.vllm.GraniteDoclingTextVLLMConfig","title":"GraniteDoclingTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>Configuration for Granite Docling text extraction with VLLM backend.</p> <p>IMPORTANT: This config uses revision=\"untied\" by default, which is required for VLLM compatibility with Granite Docling's tied weights.</p>"},{"location":"reference/tasks/text_extraction/nanonets/extractor/","title":"Extractor","text":"<p>Nanonets OCR2-3B text extractor.</p> <p>A Vision-Language Model for extracting text from document images with support for tables (HTML), equations (LaTeX), and image captions.</p> <p>Supports PyTorch and VLLM backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import NanonetsTextExtractor\nfrom omnidocs.tasks.text_extraction.nanonets import NanonetsTextPyTorchConfig\n\nextractor = NanonetsTextExtractor(\n        backend=NanonetsTextPyTorchConfig()\n    )\nresult = extractor.extract(image)\nprint(result.content)\n</code></pre>"},{"location":"reference/tasks/text_extraction/nanonets/extractor/#omnidocs.tasks.text_extraction.nanonets.extractor.NanonetsTextExtractor","title":"NanonetsTextExtractor","text":"<pre><code>NanonetsTextExtractor(backend: NanonetsTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Nanonets OCR2-3B Vision-Language Model text extractor.</p> <p>Extracts text from document images with support for: - Tables (output as HTML) - Equations (output as LaTeX) - Image captions (wrapped in  tags) - Watermarks (wrapped in  tags) - Page numbers (wrapped in  tags) - Checkboxes (using \u2610 and \u2611 symbols)</p> <p>Supports PyTorch, VLLM, and MLX backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import NanonetsTextExtractor\nfrom omnidocs.tasks.text_extraction.nanonets import NanonetsTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = NanonetsTextExtractor(\n        backend=NanonetsTextPyTorchConfig()\n    )\n\n# Extract text\nresult = extractor.extract(image)\nprint(result.content)\n</code></pre> <p>Initialize Nanonets text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend - NanonetsTextVLLMConfig: VLLM high-throughput backend - NanonetsTextMLXConfig: MLX backend for Apple Silicon</p> <p> TYPE: <code>NanonetsTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/nanonets/extractor.py</code> <pre><code>def __init__(self, backend: NanonetsTextBackendConfig):\n    \"\"\"\n    Initialize Nanonets text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend\n            - NanonetsTextVLLMConfig: VLLM high-throughput backend\n            - NanonetsTextMLXConfig: MLX backend for Apple Silicon\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._device: str = \"cpu\"\n\n    # MLX-specific helpers\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/nanonets/extractor/#omnidocs.tasks.text_extraction.nanonets.extractor.NanonetsTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> <p>Note: Nanonets OCR2 produces a unified output format that includes tables as HTML and equations as LaTeX inline. The output_format parameter is accepted for API compatibility but does not change the output structure.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Accepted for API compatibility (default: \"markdown\")</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/nanonets/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Note: Nanonets OCR2 produces a unified output format that includes\n    tables as HTML and equations as LaTeX inline. The output_format\n    parameter is accepted for API compatibility but does not change\n    the output structure.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Accepted for API compatibility (default: \"markdown\")\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"NanonetsTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image)\n    elif config_type == \"NanonetsTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image)\n    elif config_type == \"NanonetsTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    cleaned_output = raw_output.replace(\"&lt;|im_end|&gt;\", \"\").strip()\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=cleaned_output,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Nanonets-OCR2-3B ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/nanonets/mlx/","title":"MLX","text":"<p>MLX backend configuration for Nanonets OCR2-3B text extraction.</p>"},{"location":"reference/tasks/text_extraction/nanonets/mlx/#omnidocs.tasks.text_extraction.nanonets.mlx.NanonetsTextMLXConfig","title":"NanonetsTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3/M4+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = NanonetsTextMLXConfig(\n        model=\"mlx-community/Nanonets-OCR2-3B-bf16\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/nanonets/overview/","title":"Overview","text":"<p>Nanonets OCR2-3B backend configurations and extractor for text extraction.</p> Available backends <ul> <li>NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend</li> <li>NanonetsTextVLLMConfig: VLLM high-throughput backend</li> <li>NanonetsTextMLXConfig: MLX backend for Apple Silicon</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction.nanonets import NanonetsTextPyTorchConfig\nconfig = NanonetsTextPyTorchConfig()\n</code></pre>"},{"location":"reference/tasks/text_extraction/nanonets/overview/#omnidocs.tasks.text_extraction.nanonets.NanonetsTextExtractor","title":"NanonetsTextExtractor","text":"<pre><code>NanonetsTextExtractor(backend: NanonetsTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Nanonets OCR2-3B Vision-Language Model text extractor.</p> <p>Extracts text from document images with support for: - Tables (output as HTML) - Equations (output as LaTeX) - Image captions (wrapped in  tags) - Watermarks (wrapped in  tags) - Page numbers (wrapped in  tags) - Checkboxes (using \u2610 and \u2611 symbols)</p> <p>Supports PyTorch, VLLM, and MLX backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import NanonetsTextExtractor\nfrom omnidocs.tasks.text_extraction.nanonets import NanonetsTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = NanonetsTextExtractor(\n        backend=NanonetsTextPyTorchConfig()\n    )\n\n# Extract text\nresult = extractor.extract(image)\nprint(result.content)\n</code></pre> <p>Initialize Nanonets text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend - NanonetsTextVLLMConfig: VLLM high-throughput backend - NanonetsTextMLXConfig: MLX backend for Apple Silicon</p> <p> TYPE: <code>NanonetsTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/nanonets/extractor.py</code> <pre><code>def __init__(self, backend: NanonetsTextBackendConfig):\n    \"\"\"\n    Initialize Nanonets text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend\n            - NanonetsTextVLLMConfig: VLLM high-throughput backend\n            - NanonetsTextMLXConfig: MLX backend for Apple Silicon\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._device: str = \"cpu\"\n\n    # MLX-specific helpers\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/nanonets/overview/#omnidocs.tasks.text_extraction.nanonets.NanonetsTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> <p>Note: Nanonets OCR2 produces a unified output format that includes tables as HTML and equations as LaTeX inline. The output_format parameter is accepted for API compatibility but does not change the output structure.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Accepted for API compatibility (default: \"markdown\")</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/nanonets/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Note: Nanonets OCR2 produces a unified output format that includes\n    tables as HTML and equations as LaTeX inline. The output_format\n    parameter is accepted for API compatibility but does not change\n    the output structure.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Accepted for API compatibility (default: \"markdown\")\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"NanonetsTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image)\n    elif config_type == \"NanonetsTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image)\n    elif config_type == \"NanonetsTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    cleaned_output = raw_output.replace(\"&lt;|im_end|&gt;\", \"\").strip()\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=cleaned_output,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Nanonets-OCR2-3B ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/nanonets/overview/#omnidocs.tasks.text_extraction.nanonets.NanonetsTextMLXConfig","title":"NanonetsTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3/M4+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = NanonetsTextMLXConfig(\n        model=\"mlx-community/Nanonets-OCR2-3B-bf16\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/nanonets/overview/#omnidocs.tasks.text_extraction.nanonets.NanonetsTextPyTorchConfig","title":"NanonetsTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate</p> Example <pre><code>config = NanonetsTextPyTorchConfig(\n        device=\"cuda\",\n        torch_dtype=\"float16\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/nanonets/overview/#omnidocs.tasks.text_extraction.nanonets.NanonetsTextVLLMConfig","title":"NanonetsTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = NanonetsTextVLLMConfig(\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.85,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/nanonets/overview/#omnidocs.tasks.text_extraction.nanonets.extractor","title":"extractor","text":"<p>Nanonets OCR2-3B text extractor.</p> <p>A Vision-Language Model for extracting text from document images with support for tables (HTML), equations (LaTeX), and image captions.</p> <p>Supports PyTorch and VLLM backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import NanonetsTextExtractor\nfrom omnidocs.tasks.text_extraction.nanonets import NanonetsTextPyTorchConfig\n\nextractor = NanonetsTextExtractor(\n        backend=NanonetsTextPyTorchConfig()\n    )\nresult = extractor.extract(image)\nprint(result.content)\n</code></pre>"},{"location":"reference/tasks/text_extraction/nanonets/overview/#omnidocs.tasks.text_extraction.nanonets.extractor.NanonetsTextExtractor","title":"NanonetsTextExtractor","text":"<pre><code>NanonetsTextExtractor(backend: NanonetsTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Nanonets OCR2-3B Vision-Language Model text extractor.</p> <p>Extracts text from document images with support for: - Tables (output as HTML) - Equations (output as LaTeX) - Image captions (wrapped in  tags) - Watermarks (wrapped in  tags) - Page numbers (wrapped in  tags) - Checkboxes (using \u2610 and \u2611 symbols)</p> <p>Supports PyTorch, VLLM, and MLX backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import NanonetsTextExtractor\nfrom omnidocs.tasks.text_extraction.nanonets import NanonetsTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = NanonetsTextExtractor(\n        backend=NanonetsTextPyTorchConfig()\n    )\n\n# Extract text\nresult = extractor.extract(image)\nprint(result.content)\n</code></pre> <p>Initialize Nanonets text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend - NanonetsTextVLLMConfig: VLLM high-throughput backend - NanonetsTextMLXConfig: MLX backend for Apple Silicon</p> <p> TYPE: <code>NanonetsTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/nanonets/extractor.py</code> <pre><code>def __init__(self, backend: NanonetsTextBackendConfig):\n    \"\"\"\n    Initialize Nanonets text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - NanonetsTextPyTorchConfig: PyTorch/HuggingFace backend\n            - NanonetsTextVLLMConfig: VLLM high-throughput backend\n            - NanonetsTextMLXConfig: MLX backend for Apple Silicon\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._device: str = \"cpu\"\n\n    # MLX-specific helpers\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/nanonets/overview/#omnidocs.tasks.text_extraction.nanonets.extractor.NanonetsTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> <p>Note: Nanonets OCR2 produces a unified output format that includes tables as HTML and equations as LaTeX inline. The output_format parameter is accepted for API compatibility but does not change the output structure.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Accepted for API compatibility (default: \"markdown\")</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/nanonets/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Note: Nanonets OCR2 produces a unified output format that includes\n    tables as HTML and equations as LaTeX inline. The output_format\n    parameter is accepted for API compatibility but does not change\n    the output structure.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Accepted for API compatibility (default: \"markdown\")\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"NanonetsTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image)\n    elif config_type == \"NanonetsTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image)\n    elif config_type == \"NanonetsTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    cleaned_output = raw_output.replace(\"&lt;|im_end|&gt;\", \"\").strip()\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=cleaned_output,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Nanonets-OCR2-3B ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/nanonets/overview/#omnidocs.tasks.text_extraction.nanonets.mlx","title":"mlx","text":"<p>MLX backend configuration for Nanonets OCR2-3B text extraction.</p>"},{"location":"reference/tasks/text_extraction/nanonets/overview/#omnidocs.tasks.text_extraction.nanonets.mlx.NanonetsTextMLXConfig","title":"NanonetsTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3/M4+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = NanonetsTextMLXConfig(\n        model=\"mlx-community/Nanonets-OCR2-3B-bf16\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/nanonets/overview/#omnidocs.tasks.text_extraction.nanonets.pytorch","title":"pytorch","text":"<p>PyTorch/HuggingFace backend configuration for Nanonets OCR2-3B text extraction.</p>"},{"location":"reference/tasks/text_extraction/nanonets/overview/#omnidocs.tasks.text_extraction.nanonets.pytorch.NanonetsTextPyTorchConfig","title":"NanonetsTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate</p> Example <pre><code>config = NanonetsTextPyTorchConfig(\n        device=\"cuda\",\n        torch_dtype=\"float16\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/nanonets/overview/#omnidocs.tasks.text_extraction.nanonets.vllm","title":"vllm","text":"<p>VLLM backend configuration for Nanonets OCR2-3B text extraction.</p>"},{"location":"reference/tasks/text_extraction/nanonets/overview/#omnidocs.tasks.text_extraction.nanonets.vllm.NanonetsTextVLLMConfig","title":"NanonetsTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = NanonetsTextVLLMConfig(\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.85,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/nanonets/pytorch/","title":"PyTorch","text":"<p>PyTorch/HuggingFace backend configuration for Nanonets OCR2-3B text extraction.</p>"},{"location":"reference/tasks/text_extraction/nanonets/pytorch/#omnidocs.tasks.text_extraction.nanonets.pytorch.NanonetsTextPyTorchConfig","title":"NanonetsTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate</p> Example <pre><code>config = NanonetsTextPyTorchConfig(\n        device=\"cuda\",\n        torch_dtype=\"float16\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/nanonets/vllm/","title":"VLLM","text":"<p>VLLM backend configuration for Nanonets OCR2-3B text extraction.</p>"},{"location":"reference/tasks/text_extraction/nanonets/vllm/#omnidocs.tasks.text_extraction.nanonets.vllm.NanonetsTextVLLMConfig","title":"NanonetsTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Nanonets OCR2-3B text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = NanonetsTextVLLMConfig(\n        tensor_parallel_size=1,\n        gpu_memory_utilization=0.85,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/api/","title":"API","text":"<p>API backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/qwen/api/#omnidocs.tasks.text_extraction.qwen.api.QwenTextAPIConfig","title":"QwenTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen text extraction.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenTextAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/extractor/","title":"Extractor","text":"<p>Qwen3-VL text extractor.</p> <p>A Vision-Language Model for extracting text from document images as structured HTML or Markdown.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/extractor/#omnidocs.tasks.text_extraction.qwen.extractor.QwenTextExtractor","title":"QwenTextExtractor","text":"<pre><code>QwenTextExtractor(backend: QwenTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Qwen3-VL Vision-Language Model text extractor.</p> <p>Extracts text from document images and outputs as structured HTML or Markdown. Uses Qwen3-VL's built-in document parsing prompts.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Extract as Markdown\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n\n# Extract as HTML\nresult = extractor.extract(image, output_format=\"html\")\nprint(result.content)\n</code></pre> <p>Initialize Qwen text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenTextPyTorchConfig: PyTorch/HuggingFace backend - QwenTextVLLMConfig: VLLM high-throughput backend - QwenTextMLXConfig: MLX backend for Apple Silicon - QwenTextAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def __init__(self, backend: QwenTextBackendConfig):\n    \"\"\"\n    Initialize Qwen text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenTextPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenTextVLLMConfig: VLLM high-throughput backend\n            - QwenTextMLXConfig: MLX backend for Apple Silicon\n            - QwenTextAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/extractor/#omnidocs.tasks.text_extraction.qwen.extractor.QwenTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML with div elements - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format or output_format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML with div elements\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format or output_format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}. Expected 'html' or 'markdown'.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Get prompt for output format\n    prompt = QWEN_PROMPTS[output_format]\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenTextAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    if output_format == \"html\":\n        cleaned_output = _clean_html_output(raw_output)\n    else:\n        cleaned_output = _clean_markdown_output(raw_output)\n\n    # Extract plain text\n    plain_text = _extract_plain_text(raw_output, output_format)\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=plain_text,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/mlx/","title":"MLX","text":"<p>MLX backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/qwen/mlx/#omnidocs.tasks.text_extraction.qwen.mlx.QwenTextMLXConfig","title":"QwenTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenTextMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/","title":"Overview","text":"<p>Qwen3-VL backend configurations and extractor for text extraction.</p> Available backends <ul> <li>QwenTextPyTorchConfig: PyTorch/HuggingFace backend</li> <li>QwenTextVLLMConfig: VLLM high-throughput backend</li> <li>QwenTextMLXConfig: MLX backend for Apple Silicon</li> <li>QwenTextAPIConfig: API backend (OpenRouter, etc.)</li> </ul> Example <pre><code>from omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\nconfig = QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextAPIConfig","title":"QwenTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen text extraction.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenTextAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextExtractor","title":"QwenTextExtractor","text":"<pre><code>QwenTextExtractor(backend: QwenTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Qwen3-VL Vision-Language Model text extractor.</p> <p>Extracts text from document images and outputs as structured HTML or Markdown. Uses Qwen3-VL's built-in document parsing prompts.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Extract as Markdown\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n\n# Extract as HTML\nresult = extractor.extract(image, output_format=\"html\")\nprint(result.content)\n</code></pre> <p>Initialize Qwen text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenTextPyTorchConfig: PyTorch/HuggingFace backend - QwenTextVLLMConfig: VLLM high-throughput backend - QwenTextMLXConfig: MLX backend for Apple Silicon - QwenTextAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def __init__(self, backend: QwenTextBackendConfig):\n    \"\"\"\n    Initialize Qwen text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenTextPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenTextVLLMConfig: VLLM high-throughput backend\n            - QwenTextMLXConfig: MLX backend for Apple Silicon\n            - QwenTextAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML with div elements - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format or output_format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML with div elements\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format or output_format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}. Expected 'html' or 'markdown'.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Get prompt for output format\n    prompt = QWEN_PROMPTS[output_format]\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenTextAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    if output_format == \"html\":\n        cleaned_output = _clean_html_output(raw_output)\n    else:\n        cleaned_output = _clean_markdown_output(raw_output)\n\n    # Extract plain text\n    plain_text = _extract_plain_text(raw_output, output_format)\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=plain_text,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextMLXConfig","title":"QwenTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenTextMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextPyTorchConfig","title":"QwenTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.QwenTextVLLMConfig","title":"QwenTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenTextVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.api","title":"api","text":"<p>API backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.api.QwenTextAPIConfig","title":"QwenTextAPIConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>API backend configuration for Qwen text extraction.</p> <p>This backend uses OpenAI-compatible APIs (OpenRouter, Novita AI, etc.) for serverless inference without local GPU. Requires: openai</p> Example <pre><code>import os\nconfig = QwenTextAPIConfig(\n        model=\"qwen/qwen3-vl-8b-instruct\",\n        api_key=os.environ[\"OPENROUTER_API_KEY\"],\n        base_url=\"https://openrouter.ai/api/v1\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.extractor","title":"extractor","text":"<p>Qwen3-VL text extractor.</p> <p>A Vision-Language Model for extracting text from document images as structured HTML or Markdown.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.extractor.QwenTextExtractor","title":"QwenTextExtractor","text":"<pre><code>QwenTextExtractor(backend: QwenTextBackendConfig)\n</code></pre> <p>               Bases: <code>BaseTextExtractor</code></p> <p>Qwen3-VL Vision-Language Model text extractor.</p> <p>Extracts text from document images and outputs as structured HTML or Markdown. Uses Qwen3-VL's built-in document parsing prompts.</p> <p>Supports PyTorch, VLLM, MLX, and API backends.</p> Example <pre><code>from omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenTextPyTorchConfig\n\n# Initialize with PyTorch backend\nextractor = QwenTextExtractor(\n        backend=QwenTextPyTorchConfig(model=\"Qwen/Qwen3-VL-8B-Instruct\")\n    )\n\n# Extract as Markdown\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n\n# Extract as HTML\nresult = extractor.extract(image, output_format=\"html\")\nprint(result.content)\n</code></pre> <p>Initialize Qwen text extractor.</p> PARAMETER DESCRIPTION <code>backend</code> <p>Backend configuration. One of: - QwenTextPyTorchConfig: PyTorch/HuggingFace backend - QwenTextVLLMConfig: VLLM high-throughput backend - QwenTextMLXConfig: MLX backend for Apple Silicon - QwenTextAPIConfig: API backend (OpenRouter, etc.)</p> <p> TYPE: <code>QwenTextBackendConfig</code> </p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def __init__(self, backend: QwenTextBackendConfig):\n    \"\"\"\n    Initialize Qwen text extractor.\n\n    Args:\n        backend: Backend configuration. One of:\n            - QwenTextPyTorchConfig: PyTorch/HuggingFace backend\n            - QwenTextVLLMConfig: VLLM high-throughput backend\n            - QwenTextMLXConfig: MLX backend for Apple Silicon\n            - QwenTextAPIConfig: API backend (OpenRouter, etc.)\n    \"\"\"\n    self.backend_config = backend\n    self._backend: Any = None\n    self._processor: Any = None\n    self._loaded = False\n\n    # Backend-specific helpers\n    self._process_vision_info: Any = None\n    self._sampling_params_class: Any = None\n    self._mlx_config: Any = None\n    self._apply_chat_template: Any = None\n    self._generate: Any = None\n\n    # Load model\n    self._load_model()\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.extractor.QwenTextExtractor.extract","title":"extract","text":"<pre><code>extract(\n    image: Union[Image, ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput\n</code></pre> <p>Extract text from an image.</p> PARAMETER DESCRIPTION <code>image</code> <p>Input image as: - PIL.Image.Image: PIL image object - np.ndarray: Numpy array (HWC format, RGB) - str or Path: Path to image file</p> <p> TYPE: <code>Union[Image, ndarray, str, Path]</code> </p> <code>output_format</code> <p>Desired output format: - \"html\": Structured HTML with div elements - \"markdown\": Markdown format</p> <p> TYPE: <code>Literal['html', 'markdown']</code> DEFAULT: <code>'markdown'</code> </p> RETURNS DESCRIPTION <code>TextOutput</code> <p>TextOutput containing extracted text content</p> RAISES DESCRIPTION <code>RuntimeError</code> <p>If model is not loaded</p> <code>ValueError</code> <p>If image format or output_format is not supported</p> Source code in <code>omnidocs/tasks/text_extraction/qwen/extractor.py</code> <pre><code>def extract(\n    self,\n    image: Union[Image.Image, np.ndarray, str, Path],\n    output_format: Literal[\"html\", \"markdown\"] = \"markdown\",\n) -&gt; TextOutput:\n    \"\"\"\n    Extract text from an image.\n\n    Args:\n        image: Input image as:\n            - PIL.Image.Image: PIL image object\n            - np.ndarray: Numpy array (HWC format, RGB)\n            - str or Path: Path to image file\n        output_format: Desired output format:\n            - \"html\": Structured HTML with div elements\n            - \"markdown\": Markdown format\n\n    Returns:\n        TextOutput containing extracted text content\n\n    Raises:\n        RuntimeError: If model is not loaded\n        ValueError: If image format or output_format is not supported\n    \"\"\"\n    if not self._loaded:\n        raise RuntimeError(\"Model not loaded. Call _load_model() first.\")\n\n    if output_format not in (\"html\", \"markdown\"):\n        raise ValueError(f\"Invalid output_format: {output_format}. Expected 'html' or 'markdown'.\")\n\n    # Prepare image\n    pil_image = self._prepare_image(image)\n    width, height = pil_image.size\n\n    # Get prompt for output format\n    prompt = QWEN_PROMPTS[output_format]\n\n    # Run inference based on backend\n    config_type = type(self.backend_config).__name__\n    if config_type == \"QwenTextPyTorchConfig\":\n        raw_output = self._infer_pytorch(pil_image, prompt)\n    elif config_type == \"QwenTextVLLMConfig\":\n        raw_output = self._infer_vllm(pil_image, prompt)\n    elif config_type == \"QwenTextMLXConfig\":\n        raw_output = self._infer_mlx(pil_image, prompt)\n    elif config_type == \"QwenTextAPIConfig\":\n        raw_output = self._infer_api(pil_image, prompt)\n    else:\n        raise RuntimeError(f\"Unknown backend: {config_type}\")\n\n    # Clean output\n    if output_format == \"html\":\n        cleaned_output = _clean_html_output(raw_output)\n    else:\n        cleaned_output = _clean_markdown_output(raw_output)\n\n    # Extract plain text\n    plain_text = _extract_plain_text(raw_output, output_format)\n\n    return TextOutput(\n        content=cleaned_output,\n        format=OutputFormat(output_format),\n        raw_output=raw_output,\n        plain_text=plain_text,\n        image_width=width,\n        image_height=height,\n        model_name=f\"Qwen3-VL ({type(self.backend_config).__name__})\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.mlx","title":"mlx","text":"<p>MLX backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.mlx.QwenTextMLXConfig","title":"QwenTextMLXConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>MLX backend configuration for Qwen text extraction.</p> <p>This backend uses MLX for Apple Silicon native inference. Best for local development and testing on macOS M1/M2/M3+. Requires: mlx, mlx-vlm</p> <p>Note: This backend only works on Apple Silicon Macs. Do NOT use for Modal/cloud deployments.</p> Example <pre><code>config = QwenTextMLXConfig(\n        model=\"mlx-community/Qwen3-VL-8B-Instruct-4bit\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.pytorch","title":"pytorch","text":"<p>PyTorch/HuggingFace backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.pytorch.QwenTextPyTorchConfig","title":"QwenTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.vllm","title":"vllm","text":"<p>VLLM backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/qwen/overview/#omnidocs.tasks.text_extraction.qwen.vllm.QwenTextVLLMConfig","title":"QwenTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenTextVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/pytorch/","title":"PyTorch","text":"<p>PyTorch/HuggingFace backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/qwen/pytorch/#omnidocs.tasks.text_extraction.qwen.pytorch.QwenTextPyTorchConfig","title":"QwenTextPyTorchConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>PyTorch/HuggingFace backend configuration for Qwen text extraction.</p> <p>This backend uses the transformers library with PyTorch for local GPU inference. Requires: torch, transformers, accelerate, qwen-vl-utils</p> Example <pre><code>config = QwenTextPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n        torch_dtype=\"bfloat16\",\n    )\n</code></pre>"},{"location":"reference/tasks/text_extraction/qwen/vllm/","title":"VLLM","text":"<p>VLLM backend configuration for Qwen3-VL text extraction.</p>"},{"location":"reference/tasks/text_extraction/qwen/vllm/#omnidocs.tasks.text_extraction.qwen.vllm.QwenTextVLLMConfig","title":"QwenTextVLLMConfig","text":"<p>               Bases: <code>BaseModel</code></p> <p>VLLM backend configuration for Qwen text extraction.</p> <p>This backend uses VLLM for high-throughput inference. Best for batch processing and production deployments. Requires: vllm, torch, transformers, qwen-vl-utils</p> Example <pre><code>config = QwenTextVLLMConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        tensor_parallel_size=2,\n        gpu_memory_utilization=0.9,\n    )\n</code></pre>"},{"location":"reference/utils/aggregation/","title":"Aggregation","text":"<p>Result aggregation utilities for batch processing.</p> <p>Provides containers and utilities for storing, aggregating, and exporting results from batch document processing.</p>"},{"location":"reference/utils/aggregation/#omnidocs.utils.aggregation.DocumentResult","title":"DocumentResult","text":"<pre><code>DocumentResult(\n    source_path: Optional[str] = None, page_count: int = 0\n)\n</code></pre> <p>Container for results from processing a single document.</p> <p>Stores results by page for easy access and serialization.</p> <p>Examples:</p> <pre><code>doc_result = DocumentResult(source_path=\"paper.pdf\", page_count=10)\ndoc_result.add_page_result(0, text_output)\ndoc_result.add_page_result(1, text_output)\n\n# Access results\nall_results = doc_result.all_results\npage_0_result = doc_result.get_page_result(0)\n\n# Save to file\ndoc_result.save_json(\"paper_result.json\")\n</code></pre> <p>Initialize DocumentResult.</p> PARAMETER DESCRIPTION <code>source_path</code> <p>Path to source document</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>page_count</code> <p>Total number of pages</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def __init__(\n    self,\n    source_path: Optional[str] = None,\n    page_count: int = 0,\n):\n    \"\"\"\n    Initialize DocumentResult.\n\n    Args:\n        source_path: Path to source document\n        page_count: Total number of pages\n    \"\"\"\n    self.source_path = source_path\n    self.page_count = page_count\n    self._page_results: Dict[int, Any] = {}\n</code></pre>"},{"location":"reference/utils/aggregation/#omnidocs.utils.aggregation.DocumentResult.all_results","title":"all_results  <code>property</code>","text":"<pre><code>all_results: List[Any]\n</code></pre> <p>Get all results in page order.</p> RETURNS DESCRIPTION <code>List[Any]</code> <p>List of results sorted by page number</p>"},{"location":"reference/utils/aggregation/#omnidocs.utils.aggregation.DocumentResult.processed_pages","title":"processed_pages  <code>property</code>","text":"<pre><code>processed_pages: int\n</code></pre> <p>Number of pages with results.</p>"},{"location":"reference/utils/aggregation/#omnidocs.utils.aggregation.DocumentResult.add_page_result","title":"add_page_result","text":"<pre><code>add_page_result(page_num: int, result: Any) -&gt; None\n</code></pre> <p>Add result for a specific page.</p> PARAMETER DESCRIPTION <code>page_num</code> <p>Page number (0-indexed)</p> <p> TYPE: <code>int</code> </p> <code>result</code> <p>Extraction result (TextOutput, LayoutOutput, etc.)</p> <p> TYPE: <code>Any</code> </p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def add_page_result(self, page_num: int, result: Any) -&gt; None:\n    \"\"\"\n    Add result for a specific page.\n\n    Args:\n        page_num: Page number (0-indexed)\n        result: Extraction result (TextOutput, LayoutOutput, etc.)\n    \"\"\"\n    self._page_results[page_num] = result\n</code></pre>"},{"location":"reference/utils/aggregation/#omnidocs.utils.aggregation.DocumentResult.get_page_result","title":"get_page_result","text":"<pre><code>get_page_result(page_num: int) -&gt; Optional[Any]\n</code></pre> <p>Get result for a specific page.</p> PARAMETER DESCRIPTION <code>page_num</code> <p>Page number (0-indexed)</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Optional[Any]</code> <p>Result for the page, or None if not found</p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def get_page_result(self, page_num: int) -&gt; Optional[Any]:\n    \"\"\"\n    Get result for a specific page.\n\n    Args:\n        page_num: Page number (0-indexed)\n\n    Returns:\n        Result for the page, or None if not found\n    \"\"\"\n    return self._page_results.get(page_num)\n</code></pre>"},{"location":"reference/utils/aggregation/#omnidocs.utils.aggregation.DocumentResult.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert to dictionary for serialization.</p> RETURNS DESCRIPTION <code>dict</code> <p>Dictionary representation</p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"\n    Convert to dictionary for serialization.\n\n    Returns:\n        Dictionary representation\n    \"\"\"\n    results_dict = {}\n    for k, v in self._page_results.items():\n        if hasattr(v, \"model_dump\"):\n            results_dict[str(k)] = v.model_dump()\n        elif hasattr(v, \"to_dict\"):\n            results_dict[str(k)] = v.to_dict()\n        elif hasattr(v, \"__dict__\"):\n            results_dict[str(k)] = v.__dict__\n        else:\n            results_dict[str(k)] = str(v)\n\n    return {\n        \"source_path\": self.source_path,\n        \"page_count\": self.page_count,\n        \"processed_pages\": self.processed_pages,\n        \"results\": results_dict,\n    }\n</code></pre>"},{"location":"reference/utils/aggregation/#omnidocs.utils.aggregation.DocumentResult.save_json","title":"save_json","text":"<pre><code>save_json(path: str) -&gt; None\n</code></pre> <p>Save results to JSON file.</p> PARAMETER DESCRIPTION <code>path</code> <p>Output file path</p> <p> TYPE: <code>str</code> </p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def save_json(self, path: str) -&gt; None:\n    \"\"\"\n    Save results to JSON file.\n\n    Args:\n        path: Output file path\n    \"\"\"\n    out_path = Path(path)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(out_path, \"w\") as f:\n        json.dump(self.to_dict(), f, indent=2, default=str)\n</code></pre>"},{"location":"reference/utils/aggregation/#omnidocs.utils.aggregation.BatchResult","title":"BatchResult","text":"<pre><code>BatchResult()\n</code></pre> <p>Container for results from processing multiple documents.</p> <p>Examples:</p> <pre><code>batch_result = BatchResult()\nbatch_result.add_document_result(\"doc1\", doc_result1)\nbatch_result.add_document_result(\"doc2\", doc_result2)\n\n# Access results\ndoc1_result = batch_result.get_document_result(\"doc1\")\nall_ids = batch_result.document_ids\n\n# Save all results\nbatch_result.save_json(\"all_results.json\")\n</code></pre> <p>Initialize empty BatchResult.</p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize empty BatchResult.\"\"\"\n    self._document_results: Dict[str, DocumentResult] = {}\n</code></pre>"},{"location":"reference/utils/aggregation/#omnidocs.utils.aggregation.BatchResult.document_ids","title":"document_ids  <code>property</code>","text":"<pre><code>document_ids: List[str]\n</code></pre> <p>List of document IDs.</p>"},{"location":"reference/utils/aggregation/#omnidocs.utils.aggregation.BatchResult.document_count","title":"document_count  <code>property</code>","text":"<pre><code>document_count: int\n</code></pre> <p>Number of documents processed.</p>"},{"location":"reference/utils/aggregation/#omnidocs.utils.aggregation.BatchResult.total_pages","title":"total_pages  <code>property</code>","text":"<pre><code>total_pages: int\n</code></pre> <p>Total pages across all documents.</p>"},{"location":"reference/utils/aggregation/#omnidocs.utils.aggregation.BatchResult.add_document_result","title":"add_document_result","text":"<pre><code>add_document_result(\n    doc_id: str, result: DocumentResult\n) -&gt; None\n</code></pre> <p>Add result for a document.</p> PARAMETER DESCRIPTION <code>doc_id</code> <p>Document identifier (usually filename without extension)</p> <p> TYPE: <code>str</code> </p> <code>result</code> <p>DocumentResult instance</p> <p> TYPE: <code>DocumentResult</code> </p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def add_document_result(self, doc_id: str, result: DocumentResult) -&gt; None:\n    \"\"\"\n    Add result for a document.\n\n    Args:\n        doc_id: Document identifier (usually filename without extension)\n        result: DocumentResult instance\n    \"\"\"\n    self._document_results[doc_id] = result\n</code></pre>"},{"location":"reference/utils/aggregation/#omnidocs.utils.aggregation.BatchResult.get_document_result","title":"get_document_result","text":"<pre><code>get_document_result(\n    doc_id: str,\n) -&gt; Optional[DocumentResult]\n</code></pre> <p>Get result for a specific document.</p> PARAMETER DESCRIPTION <code>doc_id</code> <p>Document identifier</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[DocumentResult]</code> <p>DocumentResult or None if not found</p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def get_document_result(self, doc_id: str) -&gt; Optional[DocumentResult]:\n    \"\"\"\n    Get result for a specific document.\n\n    Args:\n        doc_id: Document identifier\n\n    Returns:\n        DocumentResult or None if not found\n    \"\"\"\n    return self._document_results.get(doc_id)\n</code></pre>"},{"location":"reference/utils/aggregation/#omnidocs.utils.aggregation.BatchResult.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert to dictionary.</p> RETURNS DESCRIPTION <code>dict</code> <p>Dictionary representation</p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"\n    Convert to dictionary.\n\n    Returns:\n        Dictionary representation\n    \"\"\"\n    return {\n        \"document_count\": self.document_count,\n        \"total_pages\": self.total_pages,\n        \"documents\": {doc_id: result.to_dict() for doc_id, result in self._document_results.items()},\n    }\n</code></pre>"},{"location":"reference/utils/aggregation/#omnidocs.utils.aggregation.BatchResult.save_json","title":"save_json","text":"<pre><code>save_json(path: str) -&gt; None\n</code></pre> <p>Save all results to JSON file.</p> PARAMETER DESCRIPTION <code>path</code> <p>Output file path</p> <p> TYPE: <code>str</code> </p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def save_json(self, path: str) -&gt; None:\n    \"\"\"\n    Save all results to JSON file.\n\n    Args:\n        path: Output file path\n    \"\"\"\n    out_path = Path(path)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(out_path, \"w\") as f:\n        json.dump(self.to_dict(), f, indent=2, default=str)\n</code></pre>"},{"location":"reference/utils/aggregation/#omnidocs.utils.aggregation.merge_text_results","title":"merge_text_results","text":"<pre><code>merge_text_results(\n    results: List[Any], separator: str = \"\\n\\n\"\n) -&gt; str\n</code></pre> <p>Merge multiple TextOutput results into single string.</p> PARAMETER DESCRIPTION <code>results</code> <p>List of TextOutput (or objects with .content attribute)</p> <p> TYPE: <code>List[Any]</code> </p> <code>separator</code> <p>String to join pages (default: double newline)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'\\n\\n'</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Combined content string</p> <p>Examples:</p> <pre><code>all_results = doc_result.all_results\nfull_text = merge_text_results(all_results)\nfull_text_with_dividers = merge_text_results(all_results, separator=\"\\n\\n---\\n\\n\")\n</code></pre> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def merge_text_results(results: List[Any], separator: str = \"\\n\\n\") -&gt; str:\n    \"\"\"\n    Merge multiple TextOutput results into single string.\n\n    Args:\n        results: List of TextOutput (or objects with .content attribute)\n        separator: String to join pages (default: double newline)\n\n    Returns:\n        Combined content string\n\n    Examples:\n        ```python\n        all_results = doc_result.all_results\n        full_text = merge_text_results(all_results)\n        full_text_with_dividers = merge_text_results(all_results, separator=\"\\\\n\\\\n---\\\\n\\\\n\")\n        ```\n    \"\"\"\n    contents = []\n    for r in results:\n        if hasattr(r, \"content\") and r.content:\n            contents.append(r.content)\n        elif isinstance(r, str) and r:\n            contents.append(r)\n    return separator.join(contents)\n</code></pre>"},{"location":"reference/utils/overview/","title":"Overview","text":"<p>OmniDocs Utilities.</p> <p>Provides utility functions for result aggregation, visualization, and export.</p>"},{"location":"reference/utils/overview/#omnidocs.utils.BatchResult","title":"BatchResult","text":"<pre><code>BatchResult()\n</code></pre> <p>Container for results from processing multiple documents.</p> <p>Examples:</p> <pre><code>batch_result = BatchResult()\nbatch_result.add_document_result(\"doc1\", doc_result1)\nbatch_result.add_document_result(\"doc2\", doc_result2)\n\n# Access results\ndoc1_result = batch_result.get_document_result(\"doc1\")\nall_ids = batch_result.document_ids\n\n# Save all results\nbatch_result.save_json(\"all_results.json\")\n</code></pre> <p>Initialize empty BatchResult.</p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize empty BatchResult.\"\"\"\n    self._document_results: Dict[str, DocumentResult] = {}\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.BatchResult.document_ids","title":"document_ids  <code>property</code>","text":"<pre><code>document_ids: List[str]\n</code></pre> <p>List of document IDs.</p>"},{"location":"reference/utils/overview/#omnidocs.utils.BatchResult.document_count","title":"document_count  <code>property</code>","text":"<pre><code>document_count: int\n</code></pre> <p>Number of documents processed.</p>"},{"location":"reference/utils/overview/#omnidocs.utils.BatchResult.total_pages","title":"total_pages  <code>property</code>","text":"<pre><code>total_pages: int\n</code></pre> <p>Total pages across all documents.</p>"},{"location":"reference/utils/overview/#omnidocs.utils.BatchResult.add_document_result","title":"add_document_result","text":"<pre><code>add_document_result(\n    doc_id: str, result: DocumentResult\n) -&gt; None\n</code></pre> <p>Add result for a document.</p> PARAMETER DESCRIPTION <code>doc_id</code> <p>Document identifier (usually filename without extension)</p> <p> TYPE: <code>str</code> </p> <code>result</code> <p>DocumentResult instance</p> <p> TYPE: <code>DocumentResult</code> </p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def add_document_result(self, doc_id: str, result: DocumentResult) -&gt; None:\n    \"\"\"\n    Add result for a document.\n\n    Args:\n        doc_id: Document identifier (usually filename without extension)\n        result: DocumentResult instance\n    \"\"\"\n    self._document_results[doc_id] = result\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.BatchResult.get_document_result","title":"get_document_result","text":"<pre><code>get_document_result(\n    doc_id: str,\n) -&gt; Optional[DocumentResult]\n</code></pre> <p>Get result for a specific document.</p> PARAMETER DESCRIPTION <code>doc_id</code> <p>Document identifier</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[DocumentResult]</code> <p>DocumentResult or None if not found</p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def get_document_result(self, doc_id: str) -&gt; Optional[DocumentResult]:\n    \"\"\"\n    Get result for a specific document.\n\n    Args:\n        doc_id: Document identifier\n\n    Returns:\n        DocumentResult or None if not found\n    \"\"\"\n    return self._document_results.get(doc_id)\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.BatchResult.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert to dictionary.</p> RETURNS DESCRIPTION <code>dict</code> <p>Dictionary representation</p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"\n    Convert to dictionary.\n\n    Returns:\n        Dictionary representation\n    \"\"\"\n    return {\n        \"document_count\": self.document_count,\n        \"total_pages\": self.total_pages,\n        \"documents\": {doc_id: result.to_dict() for doc_id, result in self._document_results.items()},\n    }\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.BatchResult.save_json","title":"save_json","text":"<pre><code>save_json(path: str) -&gt; None\n</code></pre> <p>Save all results to JSON file.</p> PARAMETER DESCRIPTION <code>path</code> <p>Output file path</p> <p> TYPE: <code>str</code> </p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def save_json(self, path: str) -&gt; None:\n    \"\"\"\n    Save all results to JSON file.\n\n    Args:\n        path: Output file path\n    \"\"\"\n    out_path = Path(path)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(out_path, \"w\") as f:\n        json.dump(self.to_dict(), f, indent=2, default=str)\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.DocumentResult","title":"DocumentResult","text":"<pre><code>DocumentResult(\n    source_path: Optional[str] = None, page_count: int = 0\n)\n</code></pre> <p>Container for results from processing a single document.</p> <p>Stores results by page for easy access and serialization.</p> <p>Examples:</p> <pre><code>doc_result = DocumentResult(source_path=\"paper.pdf\", page_count=10)\ndoc_result.add_page_result(0, text_output)\ndoc_result.add_page_result(1, text_output)\n\n# Access results\nall_results = doc_result.all_results\npage_0_result = doc_result.get_page_result(0)\n\n# Save to file\ndoc_result.save_json(\"paper_result.json\")\n</code></pre> <p>Initialize DocumentResult.</p> PARAMETER DESCRIPTION <code>source_path</code> <p>Path to source document</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>page_count</code> <p>Total number of pages</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def __init__(\n    self,\n    source_path: Optional[str] = None,\n    page_count: int = 0,\n):\n    \"\"\"\n    Initialize DocumentResult.\n\n    Args:\n        source_path: Path to source document\n        page_count: Total number of pages\n    \"\"\"\n    self.source_path = source_path\n    self.page_count = page_count\n    self._page_results: Dict[int, Any] = {}\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.DocumentResult.all_results","title":"all_results  <code>property</code>","text":"<pre><code>all_results: List[Any]\n</code></pre> <p>Get all results in page order.</p> RETURNS DESCRIPTION <code>List[Any]</code> <p>List of results sorted by page number</p>"},{"location":"reference/utils/overview/#omnidocs.utils.DocumentResult.processed_pages","title":"processed_pages  <code>property</code>","text":"<pre><code>processed_pages: int\n</code></pre> <p>Number of pages with results.</p>"},{"location":"reference/utils/overview/#omnidocs.utils.DocumentResult.add_page_result","title":"add_page_result","text":"<pre><code>add_page_result(page_num: int, result: Any) -&gt; None\n</code></pre> <p>Add result for a specific page.</p> PARAMETER DESCRIPTION <code>page_num</code> <p>Page number (0-indexed)</p> <p> TYPE: <code>int</code> </p> <code>result</code> <p>Extraction result (TextOutput, LayoutOutput, etc.)</p> <p> TYPE: <code>Any</code> </p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def add_page_result(self, page_num: int, result: Any) -&gt; None:\n    \"\"\"\n    Add result for a specific page.\n\n    Args:\n        page_num: Page number (0-indexed)\n        result: Extraction result (TextOutput, LayoutOutput, etc.)\n    \"\"\"\n    self._page_results[page_num] = result\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.DocumentResult.get_page_result","title":"get_page_result","text":"<pre><code>get_page_result(page_num: int) -&gt; Optional[Any]\n</code></pre> <p>Get result for a specific page.</p> PARAMETER DESCRIPTION <code>page_num</code> <p>Page number (0-indexed)</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Optional[Any]</code> <p>Result for the page, or None if not found</p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def get_page_result(self, page_num: int) -&gt; Optional[Any]:\n    \"\"\"\n    Get result for a specific page.\n\n    Args:\n        page_num: Page number (0-indexed)\n\n    Returns:\n        Result for the page, or None if not found\n    \"\"\"\n    return self._page_results.get(page_num)\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.DocumentResult.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert to dictionary for serialization.</p> RETURNS DESCRIPTION <code>dict</code> <p>Dictionary representation</p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"\n    Convert to dictionary for serialization.\n\n    Returns:\n        Dictionary representation\n    \"\"\"\n    results_dict = {}\n    for k, v in self._page_results.items():\n        if hasattr(v, \"model_dump\"):\n            results_dict[str(k)] = v.model_dump()\n        elif hasattr(v, \"to_dict\"):\n            results_dict[str(k)] = v.to_dict()\n        elif hasattr(v, \"__dict__\"):\n            results_dict[str(k)] = v.__dict__\n        else:\n            results_dict[str(k)] = str(v)\n\n    return {\n        \"source_path\": self.source_path,\n        \"page_count\": self.page_count,\n        \"processed_pages\": self.processed_pages,\n        \"results\": results_dict,\n    }\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.DocumentResult.save_json","title":"save_json","text":"<pre><code>save_json(path: str) -&gt; None\n</code></pre> <p>Save results to JSON file.</p> PARAMETER DESCRIPTION <code>path</code> <p>Output file path</p> <p> TYPE: <code>str</code> </p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def save_json(self, path: str) -&gt; None:\n    \"\"\"\n    Save results to JSON file.\n\n    Args:\n        path: Output file path\n    \"\"\"\n    out_path = Path(path)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(out_path, \"w\") as f:\n        json.dump(self.to_dict(), f, indent=2, default=str)\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.merge_text_results","title":"merge_text_results","text":"<pre><code>merge_text_results(\n    results: List[Any], separator: str = \"\\n\\n\"\n) -&gt; str\n</code></pre> <p>Merge multiple TextOutput results into single string.</p> PARAMETER DESCRIPTION <code>results</code> <p>List of TextOutput (or objects with .content attribute)</p> <p> TYPE: <code>List[Any]</code> </p> <code>separator</code> <p>String to join pages (default: double newline)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'\\n\\n'</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Combined content string</p> <p>Examples:</p> <pre><code>all_results = doc_result.all_results\nfull_text = merge_text_results(all_results)\nfull_text_with_dividers = merge_text_results(all_results, separator=\"\\n\\n---\\n\\n\")\n</code></pre> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def merge_text_results(results: List[Any], separator: str = \"\\n\\n\") -&gt; str:\n    \"\"\"\n    Merge multiple TextOutput results into single string.\n\n    Args:\n        results: List of TextOutput (or objects with .content attribute)\n        separator: String to join pages (default: double newline)\n\n    Returns:\n        Combined content string\n\n    Examples:\n        ```python\n        all_results = doc_result.all_results\n        full_text = merge_text_results(all_results)\n        full_text_with_dividers = merge_text_results(all_results, separator=\"\\\\n\\\\n---\\\\n\\\\n\")\n        ```\n    \"\"\"\n    contents = []\n    for r in results:\n        if hasattr(r, \"content\") and r.content:\n            contents.append(r.content)\n        elif isinstance(r, str) and r:\n            contents.append(r)\n    return separator.join(contents)\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.aggregation","title":"aggregation","text":"<p>Result aggregation utilities for batch processing.</p> <p>Provides containers and utilities for storing, aggregating, and exporting results from batch document processing.</p>"},{"location":"reference/utils/overview/#omnidocs.utils.aggregation.DocumentResult","title":"DocumentResult","text":"<pre><code>DocumentResult(\n    source_path: Optional[str] = None, page_count: int = 0\n)\n</code></pre> <p>Container for results from processing a single document.</p> <p>Stores results by page for easy access and serialization.</p> <p>Examples:</p> <pre><code>doc_result = DocumentResult(source_path=\"paper.pdf\", page_count=10)\ndoc_result.add_page_result(0, text_output)\ndoc_result.add_page_result(1, text_output)\n\n# Access results\nall_results = doc_result.all_results\npage_0_result = doc_result.get_page_result(0)\n\n# Save to file\ndoc_result.save_json(\"paper_result.json\")\n</code></pre> <p>Initialize DocumentResult.</p> PARAMETER DESCRIPTION <code>source_path</code> <p>Path to source document</p> <p> TYPE: <code>Optional[str]</code> DEFAULT: <code>None</code> </p> <code>page_count</code> <p>Total number of pages</p> <p> TYPE: <code>int</code> DEFAULT: <code>0</code> </p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def __init__(\n    self,\n    source_path: Optional[str] = None,\n    page_count: int = 0,\n):\n    \"\"\"\n    Initialize DocumentResult.\n\n    Args:\n        source_path: Path to source document\n        page_count: Total number of pages\n    \"\"\"\n    self.source_path = source_path\n    self.page_count = page_count\n    self._page_results: Dict[int, Any] = {}\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.aggregation.DocumentResult.all_results","title":"all_results  <code>property</code>","text":"<pre><code>all_results: List[Any]\n</code></pre> <p>Get all results in page order.</p> RETURNS DESCRIPTION <code>List[Any]</code> <p>List of results sorted by page number</p>"},{"location":"reference/utils/overview/#omnidocs.utils.aggregation.DocumentResult.processed_pages","title":"processed_pages  <code>property</code>","text":"<pre><code>processed_pages: int\n</code></pre> <p>Number of pages with results.</p>"},{"location":"reference/utils/overview/#omnidocs.utils.aggregation.DocumentResult.add_page_result","title":"add_page_result","text":"<pre><code>add_page_result(page_num: int, result: Any) -&gt; None\n</code></pre> <p>Add result for a specific page.</p> PARAMETER DESCRIPTION <code>page_num</code> <p>Page number (0-indexed)</p> <p> TYPE: <code>int</code> </p> <code>result</code> <p>Extraction result (TextOutput, LayoutOutput, etc.)</p> <p> TYPE: <code>Any</code> </p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def add_page_result(self, page_num: int, result: Any) -&gt; None:\n    \"\"\"\n    Add result for a specific page.\n\n    Args:\n        page_num: Page number (0-indexed)\n        result: Extraction result (TextOutput, LayoutOutput, etc.)\n    \"\"\"\n    self._page_results[page_num] = result\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.aggregation.DocumentResult.get_page_result","title":"get_page_result","text":"<pre><code>get_page_result(page_num: int) -&gt; Optional[Any]\n</code></pre> <p>Get result for a specific page.</p> PARAMETER DESCRIPTION <code>page_num</code> <p>Page number (0-indexed)</p> <p> TYPE: <code>int</code> </p> RETURNS DESCRIPTION <code>Optional[Any]</code> <p>Result for the page, or None if not found</p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def get_page_result(self, page_num: int) -&gt; Optional[Any]:\n    \"\"\"\n    Get result for a specific page.\n\n    Args:\n        page_num: Page number (0-indexed)\n\n    Returns:\n        Result for the page, or None if not found\n    \"\"\"\n    return self._page_results.get(page_num)\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.aggregation.DocumentResult.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert to dictionary for serialization.</p> RETURNS DESCRIPTION <code>dict</code> <p>Dictionary representation</p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"\n    Convert to dictionary for serialization.\n\n    Returns:\n        Dictionary representation\n    \"\"\"\n    results_dict = {}\n    for k, v in self._page_results.items():\n        if hasattr(v, \"model_dump\"):\n            results_dict[str(k)] = v.model_dump()\n        elif hasattr(v, \"to_dict\"):\n            results_dict[str(k)] = v.to_dict()\n        elif hasattr(v, \"__dict__\"):\n            results_dict[str(k)] = v.__dict__\n        else:\n            results_dict[str(k)] = str(v)\n\n    return {\n        \"source_path\": self.source_path,\n        \"page_count\": self.page_count,\n        \"processed_pages\": self.processed_pages,\n        \"results\": results_dict,\n    }\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.aggregation.DocumentResult.save_json","title":"save_json","text":"<pre><code>save_json(path: str) -&gt; None\n</code></pre> <p>Save results to JSON file.</p> PARAMETER DESCRIPTION <code>path</code> <p>Output file path</p> <p> TYPE: <code>str</code> </p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def save_json(self, path: str) -&gt; None:\n    \"\"\"\n    Save results to JSON file.\n\n    Args:\n        path: Output file path\n    \"\"\"\n    out_path = Path(path)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(out_path, \"w\") as f:\n        json.dump(self.to_dict(), f, indent=2, default=str)\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.aggregation.BatchResult","title":"BatchResult","text":"<pre><code>BatchResult()\n</code></pre> <p>Container for results from processing multiple documents.</p> <p>Examples:</p> <pre><code>batch_result = BatchResult()\nbatch_result.add_document_result(\"doc1\", doc_result1)\nbatch_result.add_document_result(\"doc2\", doc_result2)\n\n# Access results\ndoc1_result = batch_result.get_document_result(\"doc1\")\nall_ids = batch_result.document_ids\n\n# Save all results\nbatch_result.save_json(\"all_results.json\")\n</code></pre> <p>Initialize empty BatchResult.</p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize empty BatchResult.\"\"\"\n    self._document_results: Dict[str, DocumentResult] = {}\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.aggregation.BatchResult.document_ids","title":"document_ids  <code>property</code>","text":"<pre><code>document_ids: List[str]\n</code></pre> <p>List of document IDs.</p>"},{"location":"reference/utils/overview/#omnidocs.utils.aggregation.BatchResult.document_count","title":"document_count  <code>property</code>","text":"<pre><code>document_count: int\n</code></pre> <p>Number of documents processed.</p>"},{"location":"reference/utils/overview/#omnidocs.utils.aggregation.BatchResult.total_pages","title":"total_pages  <code>property</code>","text":"<pre><code>total_pages: int\n</code></pre> <p>Total pages across all documents.</p>"},{"location":"reference/utils/overview/#omnidocs.utils.aggregation.BatchResult.add_document_result","title":"add_document_result","text":"<pre><code>add_document_result(\n    doc_id: str, result: DocumentResult\n) -&gt; None\n</code></pre> <p>Add result for a document.</p> PARAMETER DESCRIPTION <code>doc_id</code> <p>Document identifier (usually filename without extension)</p> <p> TYPE: <code>str</code> </p> <code>result</code> <p>DocumentResult instance</p> <p> TYPE: <code>DocumentResult</code> </p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def add_document_result(self, doc_id: str, result: DocumentResult) -&gt; None:\n    \"\"\"\n    Add result for a document.\n\n    Args:\n        doc_id: Document identifier (usually filename without extension)\n        result: DocumentResult instance\n    \"\"\"\n    self._document_results[doc_id] = result\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.aggregation.BatchResult.get_document_result","title":"get_document_result","text":"<pre><code>get_document_result(\n    doc_id: str,\n) -&gt; Optional[DocumentResult]\n</code></pre> <p>Get result for a specific document.</p> PARAMETER DESCRIPTION <code>doc_id</code> <p>Document identifier</p> <p> TYPE: <code>str</code> </p> RETURNS DESCRIPTION <code>Optional[DocumentResult]</code> <p>DocumentResult or None if not found</p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def get_document_result(self, doc_id: str) -&gt; Optional[DocumentResult]:\n    \"\"\"\n    Get result for a specific document.\n\n    Args:\n        doc_id: Document identifier\n\n    Returns:\n        DocumentResult or None if not found\n    \"\"\"\n    return self._document_results.get(doc_id)\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.aggregation.BatchResult.to_dict","title":"to_dict","text":"<pre><code>to_dict() -&gt; dict\n</code></pre> <p>Convert to dictionary.</p> RETURNS DESCRIPTION <code>dict</code> <p>Dictionary representation</p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"\n    Convert to dictionary.\n\n    Returns:\n        Dictionary representation\n    \"\"\"\n    return {\n        \"document_count\": self.document_count,\n        \"total_pages\": self.total_pages,\n        \"documents\": {doc_id: result.to_dict() for doc_id, result in self._document_results.items()},\n    }\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.aggregation.BatchResult.save_json","title":"save_json","text":"<pre><code>save_json(path: str) -&gt; None\n</code></pre> <p>Save all results to JSON file.</p> PARAMETER DESCRIPTION <code>path</code> <p>Output file path</p> <p> TYPE: <code>str</code> </p> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def save_json(self, path: str) -&gt; None:\n    \"\"\"\n    Save all results to JSON file.\n\n    Args:\n        path: Output file path\n    \"\"\"\n    out_path = Path(path)\n    out_path.parent.mkdir(parents=True, exist_ok=True)\n    with open(out_path, \"w\") as f:\n        json.dump(self.to_dict(), f, indent=2, default=str)\n</code></pre>"},{"location":"reference/utils/overview/#omnidocs.utils.aggregation.merge_text_results","title":"merge_text_results","text":"<pre><code>merge_text_results(\n    results: List[Any], separator: str = \"\\n\\n\"\n) -&gt; str\n</code></pre> <p>Merge multiple TextOutput results into single string.</p> PARAMETER DESCRIPTION <code>results</code> <p>List of TextOutput (or objects with .content attribute)</p> <p> TYPE: <code>List[Any]</code> </p> <code>separator</code> <p>String to join pages (default: double newline)</p> <p> TYPE: <code>str</code> DEFAULT: <code>'\\n\\n'</code> </p> RETURNS DESCRIPTION <code>str</code> <p>Combined content string</p> <p>Examples:</p> <pre><code>all_results = doc_result.all_results\nfull_text = merge_text_results(all_results)\nfull_text_with_dividers = merge_text_results(all_results, separator=\"\\n\\n---\\n\\n\")\n</code></pre> Source code in <code>omnidocs/utils/aggregation.py</code> <pre><code>def merge_text_results(results: List[Any], separator: str = \"\\n\\n\") -&gt; str:\n    \"\"\"\n    Merge multiple TextOutput results into single string.\n\n    Args:\n        results: List of TextOutput (or objects with .content attribute)\n        separator: String to join pages (default: double newline)\n\n    Returns:\n        Combined content string\n\n    Examples:\n        ```python\n        all_results = doc_result.all_results\n        full_text = merge_text_results(all_results)\n        full_text_with_dividers = merge_text_results(all_results, separator=\"\\\\n\\\\n---\\\\n\\\\n\")\n        ```\n    \"\"\"\n    contents = []\n    for r in results:\n        if hasattr(r, \"content\") and r.content:\n            contents.append(r.content)\n        elif isinstance(r, str) and r:\n            contents.append(r)\n    return separator.join(contents)\n</code></pre>"},{"location":"tasks/","title":"Tasks","text":"<p>OmniDocs supports five document processing tasks. Each task defines what you want to extract - the models define how.</p>"},{"location":"tasks/#text-extraction","title":"Text Extraction","text":"<p>Input: Document image (PNG, JPG) or PDF page Output: Formatted text (Markdown or HTML)</p> <p>Extract readable, structured text from documents. Preserves headings, lists, tables, and formatting.</p> <pre><code>result = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)  # \"# Title\\n\\nParagraph text...\"\n</code></pre> <p>Available Models: Qwen3-VL, DotsOCR, Nanonets OCR2</p>"},{"location":"tasks/#layout-analysis","title":"Layout Analysis","text":"<p>Input: Document image Output: List of bounding boxes with labels</p> <p>Detect document structure: titles, paragraphs, tables, figures, formulas, headers, footers.</p> <pre><code>result = detector.extract(image)\nfor box in result.bboxes:\n    print(f\"{box.label}: {box.bbox}\")\n# title: [50, 20, 500, 60]\n# table: [50, 100, 900, 400]\n</code></pre> <p>Available Models: DocLayoutYOLO, RT-DETR, Qwen Layout</p>"},{"location":"tasks/#ocr","title":"OCR","text":"<p>Input: Document image Output: Text blocks with coordinates</p> <p>Extract text with precise word/character positions. Use when you need location information.</p> <pre><code>result = ocr.extract(image)\nfor block in result.text_blocks:\n    print(f\"'{block.text}' @ {block.bbox}\")\n# 'Invoice' @ BoundingBox(x1=100, y1=50, x2=200, y2=80)\n</code></pre> <p>Available Models: Tesseract, EasyOCR, PaddleOCR</p>"},{"location":"tasks/#table-extraction","title":"Table Extraction","text":"<p>Input: Table image (cropped from document) Output: Structured table data (cells, rows, columns)</p> <p>Extract table structure with cell coordinates, row/column spans, and content. Export to HTML, Markdown, or Pandas DataFrame.</p> <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\nextractor = TableFormerExtractor(config=TableFormerConfig(device=\"cuda\"))\nresult = extractor.extract(table_image)\n\n# Get HTML\nhtml = result.to_html()\n\n# Get DataFrame\ndf = result.to_dataframe()\n\n# Access cells\nfor cell in result.cells:\n    print(f\"[{cell.row},{cell.col}] {cell.text}\")\n</code></pre> <p>Available Models: TableFormer</p>"},{"location":"tasks/#reading-order","title":"Reading Order","text":"<p>Input: Layout detection + OCR results Output: Elements in logical reading sequence</p> <p>Determine the correct reading order of document elements. Handles multi-column layouts, headers/footers, and caption associations.</p> <pre><code>from omnidocs.tasks.reading_order import RuleBasedReadingOrderPredictor\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\n# Initialize components\nlayout_extractor = DocLayoutYOLO(config=DocLayoutYOLOConfig())\nocr = EasyOCR(config=EasyOCRConfig())\npredictor = RuleBasedReadingOrderPredictor()\n\n# Process document\nlayout = layout_extractor.extract(image)\nocr_result = ocr.extract(image)\nreading_order = predictor.predict(layout, ocr_result)\n\n# Get text in reading order\ntext = reading_order.get_full_text()\n</code></pre> <p>Available Models: Rule-based (R-tree indexing)</p>"},{"location":"tasks/#choosing-a-task","title":"Choosing a Task","text":"I want to... Use Convert PDF to Markdown Text Extraction Find where tables/figures are Layout Analysis Get word coordinates OCR Extract structured table data Table Extraction Order elements for reading Reading Order Build a full document pipeline Layout \u2192 OCR \u2192 Reading Order"},{"location":"tasks/#coming-soon","title":"Coming Soon","text":"<ul> <li>Math Recognition - LaTeX from equations</li> <li>Chart Understanding - Data from charts</li> <li>Image Captioning - Captions for figures</li> </ul>"},{"location":"tasks/batch-processing/","title":"Batch Processing","text":"<p>Process multiple documents efficiently.</p>"},{"location":"tasks/batch-processing/#quick-start","title":"Quick Start","text":"<pre><code>from pathlib import Path\nfrom omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n\n# Initialize once\nextractor = QwenTextExtractor(backend=QwenPyTorchConfig(device=\"cuda\"))\n\n# Process all PDFs\nfor pdf_path in Path(\"documents/\").glob(\"*.pdf\"):\n    doc = Document.from_pdf(pdf_path)\n\n    for i, page in enumerate(doc.iter_pages()):\n        result = extractor.extract(page, output_format=\"markdown\")\n\n        output_path = Path(\"output\") / f\"{pdf_path.stem}_page_{i+1}.md\"\n        output_path.write_text(result.content)\n</code></pre>"},{"location":"tasks/batch-processing/#with-progress-tracking","title":"With Progress Tracking","text":"<pre><code>from pathlib import Path\nfrom omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\nimport time\n\npdf_files = list(Path(\"documents/\").glob(\"*.pdf\"))\nextractor = QwenTextExtractor(backend=QwenPyTorchConfig(device=\"cuda\"))\n\nstart = time.time()\n\nfor idx, pdf_path in enumerate(pdf_files, 1):\n    doc = Document.from_pdf(pdf_path)\n\n    for page in doc.iter_pages():\n        result = extractor.extract(page, output_format=\"markdown\")\n\n    elapsed = time.time() - start\n    remaining = (len(pdf_files) - idx) * (elapsed / idx)\n    print(f\"[{idx}/{len(pdf_files)}] {pdf_path.name} - ETA: {remaining/60:.1f}min\")\n</code></pre>"},{"location":"tasks/batch-processing/#memory-management","title":"Memory Management","text":"<p>For large batches, clear cache periodically:</p> <pre><code>for i, page in enumerate(doc.iter_pages()):\n    result = extractor.extract(page)\n    save_result(result)\n\n    # Free memory every 10 pages\n    if i % 10 == 0:\n        doc.clear_cache()\n</code></pre>"},{"location":"tasks/batch-processing/#error-handling","title":"Error Handling","text":"<pre><code>results = []\nerrors = []\n\nfor pdf_path in pdf_files:\n    try:\n        doc = Document.from_pdf(pdf_path)\n        result = extractor.extract(doc.get_page(0))\n        results.append({\"path\": str(pdf_path), \"success\": True})\n    except Exception as e:\n        errors.append({\"path\": str(pdf_path), \"error\": str(e)})\n\nprint(f\"Succeeded: {len(results)}, Failed: {len(errors)}\")\n</code></pre>"},{"location":"tasks/batch-processing/#stream-results-to-disk","title":"Stream Results to Disk","text":"<p>Don't accumulate results in memory:</p> <pre><code>import json\n\nwith open(\"results.jsonl\", \"w\") as f:\n    for pdf_path in pdf_files:\n        doc = Document.from_pdf(pdf_path)\n        result = extractor.extract(doc.get_page(0))\n\n        record = {\n            \"path\": str(pdf_path),\n            \"word_count\": result.word_count,\n        }\n        f.write(json.dumps(record) + \"\\n\")\n</code></pre>"},{"location":"tasks/batch-processing/#performance-tips","title":"Performance Tips","text":"Tip Impact Initialize extractor once Saves 2-3s per batch Use VLLM for large batches 2-4x throughput Clear cache periodically Prevents OOM Stream results to disk Constant memory"},{"location":"tasks/batch-processing/#for-cloud-scale","title":"For Cloud Scale","text":"<p>See Deployment for processing on Modal GPUs.</p>"},{"location":"tasks/deployment/","title":"Deployment","text":"<p>Deploy OmniDocs on Modal serverless GPUs.</p>"},{"location":"tasks/deployment/#why-modal","title":"Why Modal","text":"<ul> <li>No infrastructure - GPU provisioning handled for you</li> <li>Pay per use - Only pay for actual GPU time</li> <li>Auto scaling - Handles traffic spikes automatically</li> </ul> <p>Cost: ~$0.35/hour for A10G GPU. Processing 100 pages costs ~$1.</p>"},{"location":"tasks/deployment/#setup","title":"Setup","text":"<pre><code># Install Modal\npip install modal\n\n# Authenticate\nmodal token new\n\n# Create volume for model caching\nmodal volume create omnidocs\n\n# Create HuggingFace secret\nmodal secret create adithya-hf-wandb --key HF_TOKEN --value \"hf_xxx...\"\n</code></pre>"},{"location":"tasks/deployment/#basic-deployment","title":"Basic Deployment","text":"<pre><code>import modal\nfrom typing import Dict, Any\n\n# Image configuration\ncuda_version = \"12.4.0\"\ntag = f\"{cuda_version}-devel-ubuntu22.04\"\n\nIMAGE = (\n    modal.Image.from_registry(f\"nvidia/cuda:{tag}\", add_python=\"3.12\")\n    .apt_install(\"libgl1-mesa-glx\", \"libglib2.0-0\")\n    .uv_pip_install(\n        \"torch\", \"torchvision\", \"transformers\", \"pillow\",\n        \"pydantic\", \"huggingface_hub\", \"accelerate\",\n    )\n    .uv_pip_install(\"qwen-vl-utils\")\n    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\", \"HF_HOME\": \"/data/.cache\"})\n)\n\nvolume = modal.Volume.from_name(\"omnidocs\", create_if_missing=True)\nsecret = modal.Secret.from_name(\"adithya-hf-wandb\")\n\napp = modal.App(\"omnidocs-extraction\")\n\n\n@app.function(\n    image=IMAGE,\n    gpu=\"A10G:1\",\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=600,\n)\ndef extract_text(image_bytes: bytes) -&gt; Dict[str, Any]:\n    from omnidocs.tasks.text_extraction import QwenTextExtractor\n    from omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n    from PIL import Image\n    import io\n\n    image = Image.open(io.BytesIO(image_bytes))\n\n    config = QwenPyTorchConfig(device=\"cuda\")\n    extractor = QwenTextExtractor(backend=config)\n\n    result = extractor.extract(image, output_format=\"markdown\")\n\n    return {\n        \"success\": True,\n        \"word_count\": result.word_count,\n        \"content\": result.content,\n    }\n\n\n@app.local_entrypoint()\ndef main():\n    with open(\"test.png\", \"rb\") as f:\n        image_bytes = f.read()\n\n    result = extract_text.remote(image_bytes)\n    print(f\"Words: {result['word_count']}\")\n    print(result['content'][:500])\n</code></pre> <p>Run: <pre><code>modal run script.py\n</code></pre></p>"},{"location":"tasks/deployment/#batch-processing","title":"Batch Processing","text":"<pre><code>@app.function(\n    image=IMAGE,\n    gpu=\"A10G:1\",\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=1800,  # 30 min for large batches\n)\ndef process_batch(image_bytes_list: list) -&gt; Dict[str, Any]:\n    from omnidocs.tasks.text_extraction import QwenTextExtractor\n    from omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n    from PIL import Image\n    import io\n\n    config = QwenPyTorchConfig(device=\"cuda\")\n    extractor = QwenTextExtractor(backend=config)\n\n    results = []\n    for image_bytes in image_bytes_list:\n        image = Image.open(io.BytesIO(image_bytes))\n        result = extractor.extract(image, output_format=\"markdown\")\n        results.append({\"word_count\": result.word_count})\n\n    return {\"results\": results}\n</code></pre>"},{"location":"tasks/deployment/#multi-gpu-with-vllm","title":"Multi-GPU with VLLM","text":"<pre><code>@app.function(\n    image=IMAGE,\n    gpu=\"A10G:2\",  # 2 GPUs\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=600,\n)\ndef extract_vllm(image_bytes: bytes) -&gt; Dict[str, Any]:\n    from omnidocs.tasks.text_extraction import QwenTextExtractor\n    from omnidocs.tasks.text_extraction.qwen import QwenVLLMConfig\n    from PIL import Image\n    import io\n\n    image = Image.open(io.BytesIO(image_bytes))\n\n    config = QwenVLLMConfig(\n        tensor_parallel_size=2,  # Use both GPUs\n        gpu_memory_utilization=0.9,\n    )\n    extractor = QwenTextExtractor(backend=config)\n\n    result = extractor.extract(image, output_format=\"markdown\")\n\n    return {\"word_count\": result.word_count, \"content\": result.content}\n</code></pre>"},{"location":"tasks/deployment/#gpu-options","title":"GPU Options","text":"GPU $/hour VRAM Best For T4 $0.15 16GB Budget A10G $0.35 24GB General (recommended) A40 $1.10 48GB Large models"},{"location":"tasks/deployment/#troubleshooting","title":"Troubleshooting","text":"<p>Model download stuck - Ensure HF token is set in secret - Increase timeout for first run</p> <p>CUDA out of memory - Use larger GPU (<code>A40</code> instead of <code>A10G</code>) - Use smaller model (<code>2B</code> instead of <code>8B</code>)</p> <p>Timeout errors - Increase <code>timeout</code> parameter - Reduce batch size</p>"},{"location":"tasks/layout-analysis/","title":"Layout Analysis","text":"<p>Detect document structure: titles, tables, figures, and more.</p>"},{"location":"tasks/layout-analysis/#models","title":"Models","text":"Model Speed Labels Backends DocLayoutYOLO 0.1-0.2s/page Fixed (11 types) PyTorch RT-DETR 0.3-0.5s/page Fixed (10 types) PyTorch Qwen Layout 2-3s/page Custom PyTorch, VLLM, MLX, API <p>Recommendation: Use DocLayoutYOLO for speed, RT-DETR for more categories, Qwen Layout for custom labels.</p>"},{"location":"tasks/layout-analysis/#basic-usage","title":"Basic Usage","text":"<pre><code>from omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\n\n# Initialize detector\nlayout = DocLayoutYOLO(\n    config=DocLayoutYOLOConfig(\n        device=\"cuda\",\n        confidence=0.25,\n    )\n)\n\n# Detect layout\nresult = layout.extract(image)\n\n# Print detected elements\nfor elem in result.elements:\n    print(f\"{elem.label}: {elem.bbox} (conf: {elem.confidence:.2f})\")\n</code></pre>"},{"location":"tasks/layout-analysis/#fixed-labels-doclayoutyolo","title":"Fixed Labels (DocLayoutYOLO)","text":"Label Description <code>title</code> Document/section headings <code>text</code> Body paragraphs <code>list</code> Bullet or numbered lists <code>table</code> Data tables <code>figure</code> Images, diagrams, charts <code>caption</code> Figure/table captions <code>formula</code> Math equations <code>footnote</code> Footnotes <code>page_header</code> Page headers <code>page_footer</code> Page footers"},{"location":"tasks/layout-analysis/#filter-results","title":"Filter Results","text":"<pre><code># Get only tables and figures\nvisual_elements = [e for e in result.elements if e.label in [\"table\", \"figure\"]]\n\n# Get high-confidence detections\nconfident = [e for e in result.elements if e.confidence &gt;= 0.8]\n\n# Exclude headers/footers\ncontent = [e for e in result.elements if e.label not in [\"page_header\", \"page_footer\"]]\n</code></pre>"},{"location":"tasks/layout-analysis/#custom-labels-qwen-layout","title":"Custom Labels (Qwen Layout)","text":"<p>Use Qwen for domain-specific labels.</p> <pre><code>from omnidocs.tasks.layout_analysis import QwenLayoutDetector\nfrom omnidocs.tasks.layout_analysis.qwen import QwenLayoutPyTorchConfig\nfrom omnidocs.tasks.layout_analysis import CustomLabel\n\n# Define custom labels\ncustom_labels = [\n    CustomLabel(name=\"code_block\", description=\"Code snippets\"),\n    CustomLabel(name=\"sidebar\", description=\"Sidebar content\"),\n]\n\nconfig = QwenLayoutPyTorchConfig(device=\"cuda\")\ndetector = QwenLayoutDetector(backend=config)\n\nresult = detector.extract(image, custom_labels=custom_labels)\n\nfor elem in result.elements:\n    print(f\"{elem.label}: {elem.bbox}\")\n</code></pre>"},{"location":"tasks/layout-analysis/#process-pdf","title":"Process PDF","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\n\ndoc = Document.from_pdf(\"document.pdf\")\nlayout = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\n\nfor page_idx in range(doc.page_count):\n    page_image = doc.get_page(page_idx)\n    result = layout.extract(page_image)\n    print(f\"Page {page_idx + 1}: {len(result.elements)} elements\")\n</code></pre>"},{"location":"tasks/layout-analysis/#troubleshooting","title":"Troubleshooting","text":"<p>Missing elements <pre><code># Lower confidence threshold\nconfig = DocLayoutYOLOConfig(device=\"cuda\", confidence=0.15)\n</code></pre></p> <p>Too many false detections <pre><code># Increase confidence threshold\nconfig = DocLayoutYOLOConfig(device=\"cuda\", confidence=0.5)\n</code></pre></p> <p>Need custom labels <pre><code># Switch to Qwen Layout\nfrom omnidocs.tasks.layout_analysis import QwenLayoutDetector\n</code></pre></p>"},{"location":"tasks/ocr/","title":"OCR","text":"<p>Extract text with precise bounding boxes.</p>"},{"location":"tasks/ocr/#when-to-use-ocr-vs-text-extraction","title":"When to Use OCR vs Text Extraction","text":"Need Use Readable text (Markdown/HTML) Text Extraction Word/character coordinates OCR Document structure only Layout Analysis"},{"location":"tasks/ocr/#models","title":"Models","text":"Model Speed Languages GPU Required Tesseract Fast 100+ No (CPU only)"},{"location":"tasks/ocr/#basic-usage","title":"Basic Usage","text":"<pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractConfig\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\n\n# Initialize OCR\nocr = TesseractOCR(\n    config=TesseractConfig(languages=[\"eng\"])\n)\n\n# Extract text with coordinates\nresult = ocr.extract(image)\n\n# Print results\nfor block in result.text_blocks:\n    print(f\"'{block.text}' @ {block.bbox} (conf: {block.confidence:.2f})\")\n</code></pre>"},{"location":"tasks/ocr/#multi-language","title":"Multi-Language","text":"<pre><code>config = TesseractConfig(\n    languages=[\"eng\", \"fra\", \"deu\"]  # English, French, German\n)\nocr = TesseractOCR(config=config)\n</code></pre> <p>Tesseract supports 100+ languages. Common codes: - <code>eng</code> - English - <code>chi_sim</code> - Chinese (Simplified) - <code>jpn</code> - Japanese - <code>ara</code> - Arabic - <code>hin</code> - Hindi</p>"},{"location":"tasks/ocr/#filter-results","title":"Filter Results","text":"<pre><code># By confidence\nconfident = [b for b in result.text_blocks if b.confidence &gt;= 0.9]\n\n# By text length\nwords = [b for b in result.text_blocks if len(b.text) &gt;= 2]\n\n# By region\ntop_half = [b for b in result.text_blocks if b.bbox.y1 &lt; image.height / 2]\n</code></pre>"},{"location":"tasks/ocr/#process-pdf","title":"Process PDF","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractConfig\n\ndoc = Document.from_pdf(\"document.pdf\")\nocr = TesseractOCR(config=TesseractConfig(languages=[\"eng\"]))\n\nfor page_idx in range(doc.page_count):\n    page_image = doc.get_page(page_idx)\n    result = ocr.extract(page_image)\n    print(f\"Page {page_idx + 1}: {len(result.text_blocks)} text blocks\")\n</code></pre>"},{"location":"tasks/ocr/#troubleshooting","title":"Troubleshooting","text":"<p>Low accuracy - Increase image resolution - Improve image contrast - Try single language mode</p> <p>Missing text - Check image quality - Ensure correct language is set</p> <p>Slow processing - Use single language - Reduce image size</p>"},{"location":"tasks/reading-order/","title":"Reading Order","text":"<p>Determine the logical reading sequence of document elements.</p>"},{"location":"tasks/reading-order/#models","title":"Models","text":"Model Speed Features Backends Rule-based (R-tree) &lt;0.1s/page Multi-column, captions, footnotes CPU"},{"location":"tasks/reading-order/#basic-usage","title":"Basic Usage","text":"<pre><code>from omnidocs.tasks.reading_order import RuleBasedReadingOrderPredictor\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\n# Initialize components\nlayout_extractor = DocLayoutYOLO(config=DocLayoutYOLOConfig())\nocr = EasyOCR(config=EasyOCRConfig())\npredictor = RuleBasedReadingOrderPredictor()\n\n# Process document\nlayout = layout_extractor.extract(image)\nocr_result = ocr.extract(image)\n\n# Predict reading order\nreading_order = predictor.predict(layout, ocr_result)\n\n# Get text in order\ntext = reading_order.get_full_text()\nprint(text)\n</code></pre>"},{"location":"tasks/reading-order/#output","title":"Output","text":"<pre><code>result = predictor.predict(layout, ocr_result)\n\n# Ordered elements\nfor elem in result.ordered_elements:\n    print(f\"{elem.index}: [{elem.element_type.value}] {elem.text[:50]}\")\n\n# Caption associations\nfor fig_id, captions in result.caption_map.items():\n    print(f\"Figure {fig_id} \u2192 Captions: {captions}\")\n\n# Footnote associations\nfor elem_id, footnotes in result.footnote_map.items():\n    print(f\"Element {elem_id} \u2192 Footnotes: {footnotes}\")\n</code></pre>"},{"location":"tasks/reading-order/#element-types","title":"Element Types","text":"Type Description <code>TITLE</code> Document/section headings <code>TEXT</code> Body paragraphs <code>LIST</code> Bullet/numbered lists <code>FIGURE</code> Images, diagrams <code>TABLE</code> Data tables <code>CAPTION</code> Figure/table captions <code>FORMULA</code> Math equations <code>FOOTNOTE</code> Footnotes <code>PAGE_HEADER</code> Page headers <code>PAGE_FOOTER</code> Page footers <code>CODE</code> Code blocks"},{"location":"tasks/reading-order/#use-cases","title":"Use Cases","text":"<p>Multi-column documents - Newspapers, academic papers - Elements flow column by column</p> <p>Caption linking - Associate captions with figures/tables - Critical for document understanding</p> <p>Full document pipelines <pre><code># Layout \u2192 OCR \u2192 Reading Order \u2192 Structured output\nlayout = layout_extractor.extract(image)\nocr = ocr_extractor.extract(image)\norder = predictor.predict(layout, ocr)\n\n# Process in logical order\nfor elem in order.ordered_elements:\n    if elem.element_type == ElementType.TITLE:\n        output.add_heading(elem.text)\n    elif elem.element_type == ElementType.TEXT:\n        output.add_paragraph(elem.text)\n</code></pre></p>"},{"location":"tasks/reading-order/#how-it-works","title":"How It Works","text":"<ol> <li>R-tree spatial indexing - Efficient spatial queries</li> <li>Column detection - Horizontal dilation for multi-column layouts</li> <li>Vertical flow - Top-to-bottom within columns</li> <li>Header/footer separation - Processed separately</li> <li>Caption proximity - Links captions to nearby figures/tables</li> </ol>"},{"location":"tasks/reading-order/#troubleshooting","title":"Troubleshooting","text":"<p>Wrong reading order - Check layout detection quality first - Use high-confidence layout detections</p> <p>Missing captions - Ensure captions are detected as <code>CAPTION</code> type in layout</p> <p>Complex layouts - Rule-based works best with standard layouts - Very complex layouts may need tuning</p>"},{"location":"tasks/table-extraction/","title":"Table Extraction","text":"<p>Extract structured data from tables in document images.</p>"},{"location":"tasks/table-extraction/#models","title":"Models","text":"Model Speed Output Backends TableFormer 0.5-1s/table Cells, rows, columns, spans PyTorch"},{"location":"tasks/table-extraction/#basic-usage","title":"Basic Usage","text":"<pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Initialize\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(device=\"cuda\")\n)\n\n# Extract table structure\nresult = extractor.extract(table_image)\n\n# Export to different formats\nhtml = result.to_html()      # HTML table\ndf = result.to_dataframe()   # Pandas DataFrame\nmd = result.to_markdown()    # Markdown table\n</code></pre>"},{"location":"tasks/table-extraction/#configuration","title":"Configuration","text":"<pre><code>from omnidocs.tasks.table_extraction import TableFormerConfig, TableFormerMode\n\nconfig = TableFormerConfig(\n    mode=TableFormerMode.ACCURATE,  # \"fast\" or \"accurate\"\n    device=\"cuda\",                   # \"cpu\", \"cuda\", \"mps\", \"auto\"\n    do_cell_matching=True,           # Match cells with text\n)\n</code></pre>"},{"location":"tasks/table-extraction/#modes","title":"Modes","text":"Mode Speed Accuracy Use For <code>fast</code> ~0.3s Good Simple tables <code>accurate</code> ~1.0s Better Complex tables, merged cells"},{"location":"tasks/table-extraction/#access-cells","title":"Access Cells","text":"<pre><code>result = extractor.extract(table_image)\n\nfor cell in result.cells:\n    print(f\"[{cell.row},{cell.col}] {cell.text}\")\n    print(f\"  Span: {cell.row_span}x{cell.col_span}\")\n    print(f\"  Type: {cell.cell_type}\")  # HEADER or DATA\n</code></pre>"},{"location":"tasks/table-extraction/#pipeline-document-tables","title":"Pipeline: Document \u2192 Tables","text":"<pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom PIL import Image\n\n# Load document\ndocument = Image.open(\"document.png\")\n\n# 1. Detect layout\nlayout_extractor = DocLayoutYOLO(config=DocLayoutYOLOConfig())\nlayout = layout_extractor.extract(document)\n\n# 2. Extract each table\ntable_extractor = TableFormerExtractor(\n    config=TableFormerConfig(mode=\"accurate\")\n)\n\nfor box in layout.bboxes:\n    if box.label.value == \"table\":\n        # Crop table region\n        table_crop = document.crop((\n            int(box.bbox.x1), int(box.bbox.y1),\n            int(box.bbox.x2), int(box.bbox.y2)\n        ))\n\n        # Extract structure\n        result = table_extractor.extract(table_crop)\n        df = result.to_dataframe()\n        print(df)\n</code></pre>"},{"location":"tasks/table-extraction/#troubleshooting","title":"Troubleshooting","text":"<p>Table not detected - Make sure to crop the table region first using layout detection - TableFormer expects pre-cropped table images</p> <p>Merged cells incorrect <pre><code># Use accurate mode for complex tables\nconfig = TableFormerConfig(mode=\"accurate\")\n</code></pre></p> <p>Missing cell text <pre><code># Enable cell matching\nconfig = TableFormerConfig(do_cell_matching=True)\n</code></pre></p>"},{"location":"tasks/text-extraction/","title":"Text Extraction","text":"<p>Convert document images to Markdown or HTML.</p>"},{"location":"tasks/text-extraction/#models","title":"Models","text":"Model Speed Quality Backends Qwen3-VL 2-3s/page Excellent PyTorch, VLLM, MLX, API DotsOCR 3-5s/page Very Good PyTorch, VLLM, API Nanonets OCR2 2-4s/page Excellent PyTorch, VLLM, MLX <p>Recommendation: Start with Qwen3-VL-8B for most use cases. Use Nanonets for document digitization.</p>"},{"location":"tasks/text-extraction/#basic-usage","title":"Basic Usage","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n\n# Load document\ndoc = Document.from_pdf(\"document.pdf\")\n\n# Initialize extractor\nextractor = QwenTextExtractor(\n    backend=QwenPyTorchConfig(\n        model=\"Qwen/Qwen3-VL-8B-Instruct\",\n        device=\"cuda\",\n    )\n)\n\n# Extract text\nresult = extractor.extract(doc.get_page(0), output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"tasks/text-extraction/#output-formats","title":"Output Formats","text":"<pre><code># Markdown (default)\nresult = extractor.extract(page, output_format=\"markdown\")\n# Output: # Heading\\n\\nParagraph text...\n\n# HTML\nresult = extractor.extract(page, output_format=\"html\")\n# Output: &lt;h1&gt;Heading&lt;/h1&gt;&lt;p&gt;Paragraph text...&lt;/p&gt;\n</code></pre>"},{"location":"tasks/text-extraction/#backend-configs","title":"Backend Configs","text":""},{"location":"tasks/text-extraction/#pytorch-local-gpu","title":"PyTorch (Local GPU)","text":"<pre><code>from omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n\nconfig = QwenPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cuda\",              # \"cuda\", \"cpu\", or \"mps\"\n    torch_dtype=\"bfloat16\",\n)\n</code></pre>"},{"location":"tasks/text-extraction/#vllm-high-throughput","title":"VLLM (High Throughput)","text":"<pre><code>from omnidocs.tasks.text_extraction.qwen import QwenVLLMConfig\n\nconfig = QwenVLLMConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    tensor_parallel_size=1,     # GPUs to use\n    gpu_memory_utilization=0.9,\n)\n</code></pre>"},{"location":"tasks/text-extraction/#mlx-apple-silicon","title":"MLX (Apple Silicon)","text":"<pre><code>from omnidocs.tasks.text_extraction.qwen import QwenMLXConfig\n\nconfig = QwenMLXConfig(\n    model=\"Qwen/Qwen3-VL-2B-Instruct\",\n    quantization=\"4bit\",\n)\n</code></pre>"},{"location":"tasks/text-extraction/#api-cloud","title":"API (Cloud)","text":"<pre><code>from omnidocs.tasks.text_extraction.qwen import QwenAPIConfig\n\nconfig = QwenAPIConfig(\n    model=\"qwen3-vl-8b\",\n    api_key=\"YOUR_API_KEY\",\n    base_url=\"https://api.provider.com/v1\",\n)\n</code></pre>"},{"location":"tasks/text-extraction/#process-multiple-pages","title":"Process Multiple Pages","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n\ndoc = Document.from_pdf(\"document.pdf\")\nextractor = QwenTextExtractor(backend=QwenPyTorchConfig(device=\"cuda\"))\n\n# Process all pages\nfor i in range(doc.page_count):\n    result = extractor.extract(doc.get_page(i), output_format=\"markdown\")\n    with open(f\"page_{i+1}.md\", \"w\") as f:\n        f.write(result.content)\n</code></pre>"},{"location":"tasks/text-extraction/#dotsocr-layout-aware","title":"DotsOCR (Layout-Aware)","text":"<p>DotsOCR includes layout information with extracted text.</p> <pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nconfig = DotsOCRPyTorchConfig(device=\"cuda\")\nextractor = DotsOCRTextExtractor(backend=config)\n\nresult = extractor.extract(image, include_layout=True)\n\n# Access layout elements\nfor element in result.layout:\n    print(f\"[{element.category}] {element.text[:50]}...\")\n</code></pre>"},{"location":"tasks/text-extraction/#nanonets-ocr2","title":"Nanonets OCR2","text":"<p>Nanonets OCR2-3B is optimized for document text extraction.</p> <pre><code>from omnidocs.tasks.text_extraction import NanonetsTextExtractor\nfrom omnidocs.tasks.text_extraction.nanonets import NanonetsTextPyTorchConfig\n\nconfig = NanonetsTextPyTorchConfig(device=\"cuda\")\nextractor = NanonetsTextExtractor(backend=config)\n\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"tasks/text-extraction/#nanonets-backends","title":"Nanonets Backends","text":"<pre><code># PyTorch\nfrom omnidocs.tasks.text_extraction.nanonets import NanonetsTextPyTorchConfig\nconfig = NanonetsTextPyTorchConfig(device=\"cuda\")\n\n# VLLM (high throughput)\nfrom omnidocs.tasks.text_extraction.nanonets import NanonetsTextVLLMConfig\nconfig = NanonetsTextVLLMConfig(gpu_memory_utilization=0.85)\n\n# MLX (Apple Silicon)\nfrom omnidocs.tasks.text_extraction.nanonets import NanonetsTextMLXConfig\nconfig = NanonetsTextMLXConfig()\n</code></pre>"},{"location":"tasks/text-extraction/#troubleshooting","title":"Troubleshooting","text":"<p>CUDA out of memory <pre><code># Use smaller model\nconfig = QwenPyTorchConfig(model=\"Qwen/Qwen3-VL-2B-Instruct\")\n</code></pre></p> <p>Slow inference <pre><code># Switch to VLLM\nconfig = QwenVLLMConfig(tensor_parallel_size=1)\n</code></pre></p> <p>No GPU available <pre><code># Use API backend\nconfig = QwenAPIConfig(api_key=\"...\", base_url=\"...\")\n</code></pre></p>"},{"location":"usage/","title":"Usage","text":"<p>Everything you need to use OmniDocs in your projects.</p>"},{"location":"usage/#tasks-models","title":"Tasks &amp; Models","text":""},{"location":"usage/#text-extraction","title":"Text Extraction","text":"<p>Convert documents to Markdown/HTML.</p> Model Speed Backends Qwen 2-3s/page PyTorch, VLLM, MLX, API DotsOCR 3-5s/page PyTorch, VLLM, API"},{"location":"usage/#layout-analysis","title":"Layout Analysis","text":"<p>Detect structure (titles, tables, figures).</p> Model Speed Labels DocLayoutYOLO 0.1-0.2s/page Fixed (11) RT-DETR 0.3-0.5s/page Fixed (11) Qwen Layout 2-3s/page Custom"},{"location":"usage/#ocr","title":"OCR","text":"<p>Extract text with coordinates.</p> Model Speed Languages Tesseract 0.5-1s/page 100+ EasyOCR 1-2s/page 80+ PaddleOCR 0.5-1s/page 80+"},{"location":"usage/#workflows","title":"Workflows","text":"<ul> <li>Batch Processing - Process multiple documents</li> <li>Deployment - Deploy on Modal GPUs</li> </ul>"},{"location":"usage/#upcoming","title":"Upcoming","text":"<p>Tasks: Table Extraction, Math Recognition, Chart Understanding</p> <p>Models: Chandra, LightOnOCR-2, MinerU, SuryaOCR, SuryaLayout</p> <p>See Roadmap for full tracking.</p>"},{"location":"usage/batch-processing/","title":"Batch Processing","text":"<p>Process multiple documents efficiently.</p>"},{"location":"usage/batch-processing/#quick-start","title":"Quick Start","text":"<pre><code>from pathlib import Path\nfrom omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n\n# Initialize once (expensive)\nextractor = QwenTextExtractor(backend=QwenPyTorchConfig(device=\"cuda\"))\n\n# Process all PDFs\nfor pdf_path in Path(\"documents/\").glob(\"*.pdf\"):\n    doc = Document.from_pdf(pdf_path)\n\n    for i, page in enumerate(doc.iter_pages()):\n        result = extractor.extract(page, output_format=\"markdown\")\n\n        output = Path(\"output\") / f\"{pdf_path.stem}_page_{i+1}.md\"\n        output.write_text(result.content)\n</code></pre>"},{"location":"usage/batch-processing/#with-progress-tracking","title":"With Progress Tracking","text":"<pre><code>import time\nfrom pathlib import Path\n\npdf_files = list(Path(\"documents/\").glob(\"*.pdf\"))\nstart = time.time()\n\nfor idx, pdf_path in enumerate(pdf_files, 1):\n    doc = Document.from_pdf(pdf_path)\n\n    for page in doc.iter_pages():\n        result = extractor.extract(page)\n\n    # Progress\n    elapsed = time.time() - start\n    remaining = (len(pdf_files) - idx) * (elapsed / idx)\n    print(f\"[{idx}/{len(pdf_files)}] {pdf_path.name} - ETA: {remaining/60:.1f}min\")\n</code></pre>"},{"location":"usage/batch-processing/#memory-management","title":"Memory Management","text":"<p>Clear cache periodically for large batches:</p> <pre><code>for i, page in enumerate(doc.iter_pages()):\n    result = extractor.extract(page)\n    save_result(result)\n\n    # Free memory every 10 pages\n    if i % 10 == 0:\n        doc.clear_cache()\n</code></pre>"},{"location":"usage/batch-processing/#stream-to-disk","title":"Stream to Disk","text":"<p>Don't accumulate results in memory:</p> <pre><code>import json\n\nwith open(\"results.jsonl\", \"w\") as f:\n    for pdf_path in pdf_files:\n        doc = Document.from_pdf(pdf_path)\n        result = extractor.extract(doc.get_page(0))\n\n        record = {\"path\": str(pdf_path), \"word_count\": result.word_count}\n        f.write(json.dumps(record) + \"\\n\")\n</code></pre>"},{"location":"usage/batch-processing/#error-handling","title":"Error Handling","text":"<pre><code>results = []\nerrors = []\n\nfor pdf_path in pdf_files:\n    try:\n        doc = Document.from_pdf(pdf_path)\n        result = extractor.extract(doc.get_page(0))\n        results.append({\"path\": str(pdf_path), \"success\": True})\n    except Exception as e:\n        errors.append({\"path\": str(pdf_path), \"error\": str(e)})\n\nprint(f\"Succeeded: {len(results)}, Failed: {len(errors)}\")\n</code></pre>"},{"location":"usage/batch-processing/#performance-tips","title":"Performance Tips","text":"Tip Why Initialize extractor once Model loading takes 2-3s Use VLLM for large batches 2-4x better throughput Stream results to disk Constant memory usage Clear cache periodically Prevents OOM"},{"location":"usage/batch-processing/#for-cloud-scale","title":"For Cloud Scale","text":"<p>See Deployment for processing on Modal GPUs.</p>"},{"location":"usage/deployment/","title":"Deployment","text":"<p>Deploy OmniDocs on Modal serverless GPUs.</p>"},{"location":"usage/deployment/#why-modal","title":"Why Modal","text":"<ul> <li>No infrastructure - Modal handles GPU provisioning</li> <li>Pay per use - Only pay for GPU time used</li> <li>Auto scaling - Handles traffic spikes</li> </ul> <p>Cost: ~$0.35/hour for A10G. 100 pages \u2248 $1.</p>"},{"location":"usage/deployment/#setup","title":"Setup","text":"<pre><code># Install\npip install modal\n\n# Authenticate\nmodal token new\n\n# Create volume for model caching\nmodal volume create omnidocs\n\n# Create HuggingFace secret\nmodal secret create adithya-hf-wandb --key HF_TOKEN --value \"hf_xxx...\"\n</code></pre>"},{"location":"usage/deployment/#basic-deployment","title":"Basic Deployment","text":"<pre><code>import modal\nfrom typing import Dict, Any\n\n# Image configuration\nIMAGE = (\n    modal.Image.from_registry(\"nvidia/cuda:12.4.0-devel-ubuntu22.04\", add_python=\"3.12\")\n    .apt_install(\"libgl1-mesa-glx\", \"libglib2.0-0\")\n    .uv_pip_install(\n        \"torch\", \"torchvision\", \"transformers\", \"pillow\",\n        \"pydantic\", \"huggingface_hub\", \"accelerate\",\n    )\n    .uv_pip_install(\"qwen-vl-utils\")\n    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\", \"HF_HOME\": \"/data/.cache\"})\n)\n\nvolume = modal.Volume.from_name(\"omnidocs\", create_if_missing=True)\nsecret = modal.Secret.from_name(\"adithya-hf-wandb\")\n\napp = modal.App(\"omnidocs-extraction\")\n\n\n@app.function(\n    image=IMAGE,\n    gpu=\"A10G:1\",\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=600,\n)\ndef extract_text(image_bytes: bytes) -&gt; Dict[str, Any]:\n    from omnidocs.tasks.text_extraction import QwenTextExtractor\n    from omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n    from PIL import Image\n    import io\n\n    image = Image.open(io.BytesIO(image_bytes))\n\n    config = QwenPyTorchConfig(device=\"cuda\")\n    extractor = QwenTextExtractor(backend=config)\n\n    result = extractor.extract(image, output_format=\"markdown\")\n\n    return {\n        \"success\": True,\n        \"word_count\": result.word_count,\n        \"content\": result.content,\n    }\n\n\n@app.local_entrypoint()\ndef main():\n    with open(\"test.png\", \"rb\") as f:\n        image_bytes = f.read()\n\n    result = extract_text.remote(image_bytes)\n    print(f\"Words: {result['word_count']}\")\n</code></pre> <p>Run: <pre><code>modal run script.py\n</code></pre></p>"},{"location":"usage/deployment/#batch-processing","title":"Batch Processing","text":"<pre><code>@app.function(\n    image=IMAGE,\n    gpu=\"A10G:1\",\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=1800,  # 30 min\n)\ndef process_batch(image_bytes_list: list) -&gt; Dict[str, Any]:\n    from omnidocs.tasks.text_extraction import QwenTextExtractor\n    from omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n    from PIL import Image\n    import io\n\n    extractor = QwenTextExtractor(backend=QwenPyTorchConfig(device=\"cuda\"))\n\n    results = []\n    for image_bytes in image_bytes_list:\n        image = Image.open(io.BytesIO(image_bytes))\n        result = extractor.extract(image, output_format=\"markdown\")\n        results.append({\"word_count\": result.word_count})\n\n    return {\"results\": results}\n</code></pre>"},{"location":"usage/deployment/#multi-gpu-with-vllm","title":"Multi-GPU with VLLM","text":"<pre><code>@app.function(\n    image=IMAGE,\n    gpu=\"A10G:2\",  # 2 GPUs\n    volumes={\"/data\": volume},\n    secrets=[secret],\n    timeout=600,\n)\ndef extract_vllm(image_bytes: bytes):\n    from omnidocs.tasks.text_extraction import QwenTextExtractor\n    from omnidocs.tasks.text_extraction.qwen import QwenVLLMConfig\n    from PIL import Image\n    import io\n\n    image = Image.open(io.BytesIO(image_bytes))\n\n    config = QwenVLLMConfig(\n        tensor_parallel_size=2,  # Both GPUs\n        gpu_memory_utilization=0.9,\n    )\n    extractor = QwenTextExtractor(backend=config)\n\n    result = extractor.extract(image, output_format=\"markdown\")\n    return {\"content\": result.content}\n</code></pre>"},{"location":"usage/deployment/#gpu-options","title":"GPU Options","text":"GPU $/hour VRAM Best For T4 $0.15 16GB Budget A10G $0.35 24GB General (recommended) A40 $1.10 48GB Large models"},{"location":"usage/deployment/#troubleshooting","title":"Troubleshooting","text":"<p>Model download stuck - Check HF token in secret - Increase timeout for first run</p> <p>CUDA out of memory - Use larger GPU (A40) - Use smaller model (2B) - Use VLLM with tensor parallelism</p> <p>Timeout errors - Increase <code>timeout</code> parameter - Reduce batch size</p>"},{"location":"usage/models/","title":"Models","text":"<p>All supported models and their configurations.</p>"},{"location":"usage/models/#available-models","title":"Available Models","text":""},{"location":"usage/models/#text-extraction","title":"Text Extraction","text":"Model Speed Backends Status Qwen 2-3s/page PyTorch, VLLM, MLX, API \u2705 Ready DotsOCR 3-5s/page PyTorch, VLLM, API \u2705 Ready Nanonets OCR2 2-4s/page PyTorch, VLLM, MLX \u2705 Ready"},{"location":"usage/models/#layout-analysis","title":"Layout Analysis","text":"Model Speed Backends Status DocLayoutYOLO 0.1-0.2s/page PyTorch \u2705 Ready RT-DETR 0.3-0.5s/page PyTorch \u2705 Ready Qwen Layout 2-3s/page PyTorch, VLLM, MLX, API \u2705 Ready"},{"location":"usage/models/#ocr","title":"OCR","text":"Model Speed Backends Status Tesseract 0.5-1s/page CPU \u2705 Ready EasyOCR 1-2s/page PyTorch \u2705 Ready PaddleOCR 0.5-1s/page PaddlePaddle \u2705 Ready"},{"location":"usage/models/#table-extraction","title":"Table Extraction","text":"Model Speed Backends Status TableFormer 0.5-1s/table PyTorch \u2705 Ready"},{"location":"usage/models/#reading-order","title":"Reading Order","text":"Model Speed Backends Status Rule-based &lt;0.1s/page CPU \u2705 Ready"},{"location":"usage/models/#by-backend","title":"By Backend","text":"Backend Models PyTorch Qwen, DotsOCR, Nanonets, DocLayoutYOLO, RT-DETR, EasyOCR, TableFormer VLLM Qwen, DotsOCR, Nanonets MLX Qwen, Nanonets API Qwen, DotsOCR CPU Tesseract, PaddleOCR, Rule-based Reading Order"},{"location":"usage/models/#upcoming-models","title":"Upcoming Models","text":""},{"location":"usage/models/#text-extraction_1","title":"Text Extraction","text":"Model Parameters Description Status Granite Docling 258M Edge deployment, fast inference \ud83d\udd1c Scripts ready MinerU VL 1.2B Layout-aware extraction \ud83d\udd1c Scripts ready Chandra 9B High accuracy text extraction \ud83d\udd1c Planned"},{"location":"usage/models/#layout-analysis_1","title":"Layout Analysis","text":"Model Description Status SuryaLayout Modern layout detection \ud83d\udd1c Planned"},{"location":"usage/models/#ocr_1","title":"OCR","text":"Model Description Status SuryaOCR Modern multilingual OCR \ud83d\udd1c Planned"},{"location":"usage/models/#new-tasks","title":"New Tasks","text":"Task Models Status Math Recognition UniMERNet, Qwen \ud83d\udd1c Planned Structured Output VLM (GPT-4V, Gemini) \ud83d\udd1c Planned <p>See Roadmap for full tracking.</p>"},{"location":"usage/models/doclayout-yolo/","title":"DocLayoutYOLO","text":"<p>Fast document layout detection.</p>"},{"location":"usage/models/doclayout-yolo/#overview","title":"Overview","text":"Tasks Layout Analysis Backends PyTorch Speed 0.1-0.2s/page Quality Good VRAM 2-4GB"},{"location":"usage/models/doclayout-yolo/#why-doclayoutyolo","title":"Why DocLayoutYOLO","text":"<ul> <li>Extremely fast - 5-10x faster than VLM-based detection</li> <li>Low memory - Runs on modest GPUs or CPU</li> <li>Reliable - YOLO architecture, battle-tested</li> <li>Fixed labels - 11 pre-trained categories</li> </ul>"},{"location":"usage/models/doclayout-yolo/#basic-usage","title":"Basic Usage","text":"<pre><code>from omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\n\ndetector = DocLayoutYOLO(\n    config=DocLayoutYOLOConfig(device=\"cuda\")\n)\n\nresult = detector.extract(image)\n\nfor elem in result.elements:\n    print(f\"{elem.label}: {elem.bbox} ({elem.confidence:.2f})\")\n</code></pre>"},{"location":"usage/models/doclayout-yolo/#configuration","title":"Configuration","text":"<pre><code>config = DocLayoutYOLOConfig(\n    device=\"cuda\",        # \"cuda\", \"cpu\"\n    confidence=0.25,      # Detection threshold (0.0-1.0)\n    img_size=1024,        # Input image size\n)\n</code></pre>"},{"location":"usage/models/doclayout-yolo/#detected-labels","title":"Detected Labels","text":"Label Description <code>title</code> Document/section headings <code>text</code> Body paragraphs <code>list</code> Bullet or numbered lists <code>table</code> Data tables <code>figure</code> Images, diagrams, charts <code>caption</code> Figure/table captions <code>formula</code> Math equations <code>footnote</code> Footnotes <code>page_header</code> Running headers <code>page_footer</code> Running footers <code>unknown</code> Unclassified elements"},{"location":"usage/models/doclayout-yolo/#filtering-results","title":"Filtering Results","text":"<pre><code># By label\ntables = [e for e in result.elements if e.label == \"table\"]\nfigures = [e for e in result.elements if e.label == \"figure\"]\n\n# By confidence\nconfident = [e for e in result.elements if e.confidence &gt;= 0.8]\n\n# Exclude headers/footers\ncontent = [e for e in result.elements\n           if e.label not in [\"page_header\", \"page_footer\"]]\n</code></pre>"},{"location":"usage/models/doclayout-yolo/#when-to-use-doclayoutyolo-vs-qwen-layout","title":"When to Use DocLayoutYOLO vs Qwen Layout","text":"Use Case Model Speed-critical DocLayoutYOLO Custom labels needed Qwen Layout Limited GPU memory DocLayoutYOLO Higher accuracy Qwen Layout Batch processing DocLayoutYOLO"},{"location":"usage/models/doclayout-yolo/#troubleshooting","title":"Troubleshooting","text":"<p>Missing elements <pre><code># Lower confidence threshold\nconfig = DocLayoutYOLOConfig(confidence=0.15)\n</code></pre></p> <p>Too many false detections <pre><code># Increase confidence threshold\nconfig = DocLayoutYOLOConfig(confidence=0.5)\n</code></pre></p> <p>Slow on CPU <pre><code># Expected: ~1-2s/page on CPU vs 0.1-0.2s on GPU\n# Consider using GPU if available\n</code></pre></p>"},{"location":"usage/models/dotsocr/","title":"DotsOCR","text":"<p>Layout-aware text extraction with bounding boxes.</p>"},{"location":"usage/models/dotsocr/#overview","title":"Overview","text":"Tasks Text Extraction Backends PyTorch, VLLM Speed 3-5s/page Quality Very Good VRAM 8-12GB"},{"location":"usage/models/dotsocr/#what-makes-it-different","title":"What Makes It Different","text":"<p>DotsOCR extracts text with layout information. Each text block includes: - Text content - Bounding box coordinates - Element category (title, text, table, etc.)</p> <p>Best for technical documents where structure matters.</p>"},{"location":"usage/models/dotsocr/#basic-usage","title":"Basic Usage","text":"<pre><code>from omnidocs.tasks.text_extraction import DotsOCRTextExtractor\nfrom omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\n\nextractor = DotsOCRTextExtractor(\n    backend=DotsOCRPyTorchConfig(device=\"cuda\")\n)\n\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"usage/models/dotsocr/#with-layout-information","title":"With Layout Information","text":"<pre><code>result = extractor.extract(image, include_layout=True)\n\n# Access layout elements\nfor element in result.layout:\n    print(f\"[{element.category}] {element.bbox}: {element.text[:50]}...\")\n</code></pre> <p>Output: <pre><code>[title] [50, 20, 500, 60]: Introduction to Machine Learning\n[text] [50, 80, 900, 200]: Machine learning is a subset of...\n[table] [50, 220, 900, 450]: | Model | Accuracy | Speed |...\n[figure] [50, 470, 400, 700]: [Figure caption text]\n</code></pre></p>"},{"location":"usage/models/dotsocr/#backend-configs","title":"Backend Configs","text":""},{"location":"usage/models/dotsocr/#pytorch","title":"PyTorch","text":"<pre><code>from omnidocs.tasks.text_extraction.dotsocr import DotsOCRPyTorchConfig\n\nconfig = DotsOCRPyTorchConfig(\n    device=\"cuda\",\n    max_new_tokens=8192,  # Increase for long documents\n)\n</code></pre>"},{"location":"usage/models/dotsocr/#vllm","title":"VLLM","text":"<pre><code>from omnidocs.tasks.text_extraction.dotsocr import DotsOCRVLLMConfig\n\nconfig = DotsOCRVLLMConfig(\n    tensor_parallel_size=1,\n    gpu_memory_utilization=0.9,\n)\n</code></pre>"},{"location":"usage/models/dotsocr/#layout-categories","title":"Layout Categories","text":"<p>DotsOCR detects 11 element types:</p> Category Description <code>title</code> Document/section headings <code>text</code> Body paragraphs <code>list</code> Bullet/numbered lists <code>table</code> Data tables <code>figure</code> Images, diagrams <code>caption</code> Figure/table captions <code>formula</code> Math equations <code>footnote</code> Footnotes <code>header</code> Page headers <code>footer</code> Page footers <code>abstract</code> Abstract sections"},{"location":"usage/models/dotsocr/#when-to-use-dotsocr-vs-qwen","title":"When to Use DotsOCR vs Qwen","text":"Use Case Model General text extraction Qwen Need bounding boxes DotsOCR Technical documents DotsOCR Tables with coordinates DotsOCR Fastest extraction Qwen MLX / API support Qwen"},{"location":"usage/models/dotsocr/#troubleshooting","title":"Troubleshooting","text":"<p>Truncated output <pre><code># Increase token limit\nconfig = DotsOCRPyTorchConfig(max_new_tokens=16384)\n</code></pre></p> <p>Missing layout elements <pre><code># Ensure include_layout=True\nresult = extractor.extract(image, include_layout=True)\n</code></pre></p>"},{"location":"usage/models/easyocr/","title":"EasyOCR","text":"<p>Deep learning OCR with high accuracy.</p>"},{"location":"usage/models/easyocr/#overview","title":"Overview","text":"Tasks OCR Backends PyTorch Speed 1-2s/page Quality Very Good Languages 80+"},{"location":"usage/models/easyocr/#why-easyocr","title":"Why EasyOCR","text":"<ul> <li>Higher accuracy than Tesseract on diverse text</li> <li>Deep learning - handles varied fonts and styles</li> <li>GPU acceleration - optional but faster</li> <li>Easy setup - pip install, no system dependencies</li> </ul>"},{"location":"usage/models/easyocr/#basic-usage","title":"Basic Usage","text":"<pre><code>from omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\n\nocr = EasyOCR(\n    config=EasyOCRConfig(\n        languages=[\"en\"],\n        gpu=True,\n    )\n)\n\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n    print(f\"'{block.text}' @ {block.bbox}\")\n</code></pre>"},{"location":"usage/models/easyocr/#configuration","title":"Configuration","text":"<pre><code>config = EasyOCRConfig(\n    languages=[\"en\"],     # Language codes\n    gpu=True,             # Use GPU if available\n)\n</code></pre>"},{"location":"usage/models/easyocr/#multi-language","title":"Multi-Language","text":"<pre><code># Multiple languages\nconfig = EasyOCRConfig(\n    languages=[\"en\", \"fr\", \"de\"],\n    gpu=True,\n)\n</code></pre> <p>Common language codes: <code>en</code>, <code>ch_sim</code>, <code>ch_tra</code>, <code>ja</code>, <code>ko</code>, <code>ar</code>, <code>hi</code>, <code>fr</code>, <code>de</code>, <code>es</code>, <code>pt</code>, <code>ru</code></p>"},{"location":"usage/models/easyocr/#easyocr-vs-tesseract","title":"EasyOCR vs Tesseract","text":"EasyOCR Tesseract Accuracy Higher Good Speed 1-2s/page 0.5-1s/page GPU Optional No Setup pip install System package Languages 80+ 100+"},{"location":"usage/models/easyocr/#when-to-use","title":"When to Use","text":"<p>\u2705 Need higher accuracy than Tesseract \u2705 Have GPU available \u2705 Diverse fonts and styles</p> <p>\u274c CPU-only, need speed \u2192 Use Tesseract \u274c Asian languages priority \u2192 Use PaddleOCR</p>"},{"location":"usage/models/nanonets/","title":"Nanonets OCR2","text":"<p>Nanonets OCR2-3B is a Vision-Language Model optimized for document text extraction with excellent accuracy on diverse document types.</p>"},{"location":"usage/models/nanonets/#overview","title":"Overview","text":"Property Value Model <code>nanonets/Nanonets-OCR-s</code> Parameters 3B Task Text Extraction Backends PyTorch, VLLM, MLX License Apache 2.0"},{"location":"usage/models/nanonets/#installation","title":"Installation","text":"<pre><code># PyTorch backend\npip install omnidocs[pytorch]\n\n# VLLM backend (high throughput)\npip install omnidocs[vllm]\n\n# MLX backend (Apple Silicon)\npip install omnidocs[mlx]\n</code></pre>"},{"location":"usage/models/nanonets/#quick-start","title":"Quick Start","text":""},{"location":"usage/models/nanonets/#pytorch-backend","title":"PyTorch Backend","text":"<pre><code>from omnidocs.tasks.text_extraction import NanonetsTextExtractor\nfrom omnidocs.tasks.text_extraction.nanonets import NanonetsTextPyTorchConfig\n\nextractor = NanonetsTextExtractor(\n    backend=NanonetsTextPyTorchConfig(device=\"cuda\")\n)\nresult = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"usage/models/nanonets/#vllm-backend-high-throughput","title":"VLLM Backend (High Throughput)","text":"<pre><code>from omnidocs.tasks.text_extraction import NanonetsTextExtractor\nfrom omnidocs.tasks.text_extraction.nanonets import NanonetsTextVLLMConfig\n\nextractor = NanonetsTextExtractor(\n    backend=NanonetsTextVLLMConfig(\n        gpu_memory_utilization=0.85,\n        max_model_len=8192,\n    )\n)\nresult = extractor.extract(image)\n</code></pre>"},{"location":"usage/models/nanonets/#mlx-backend-apple-silicon","title":"MLX Backend (Apple Silicon)","text":"<pre><code>from omnidocs.tasks.text_extraction import NanonetsTextExtractor\nfrom omnidocs.tasks.text_extraction.nanonets import NanonetsTextMLXConfig\n\nextractor = NanonetsTextExtractor(\n    backend=NanonetsTextMLXConfig()\n)\nresult = extractor.extract(image)\n</code></pre>"},{"location":"usage/models/nanonets/#configuration","title":"Configuration","text":""},{"location":"usage/models/nanonets/#pytorch-config","title":"PyTorch Config","text":"<pre><code>from omnidocs.tasks.text_extraction.nanonets import NanonetsTextPyTorchConfig\n\nconfig = NanonetsTextPyTorchConfig(\n    model=\"nanonets/Nanonets-OCR-s\",  # Model ID\n    device=\"cuda\",                     # \"cuda\", \"cpu\", or \"mps\"\n    torch_dtype=\"bfloat16\",           # \"float16\", \"bfloat16\", \"float32\"\n    attn_implementation=\"flash_attention_2\",  # or \"sdpa\", \"eager\"\n)\n</code></pre>"},{"location":"usage/models/nanonets/#vllm-config","title":"VLLM Config","text":"<pre><code>from omnidocs.tasks.text_extraction.nanonets import NanonetsTextVLLMConfig\n\nconfig = NanonetsTextVLLMConfig(\n    model=\"nanonets/Nanonets-OCR-s\",\n    gpu_memory_utilization=0.85,  # GPU memory fraction\n    max_model_len=8192,           # Max sequence length\n    tensor_parallel_size=1,       # Multi-GPU parallelism\n)\n</code></pre>"},{"location":"usage/models/nanonets/#mlx-config","title":"MLX Config","text":"<pre><code>from omnidocs.tasks.text_extraction.nanonets import NanonetsTextMLXConfig\n\nconfig = NanonetsTextMLXConfig(\n    model=\"nanonets/Nanonets-OCR-s\",\n    max_tokens=4096,\n)\n</code></pre>"},{"location":"usage/models/nanonets/#output","title":"Output","text":"<pre><code>result = extractor.extract(image, output_format=\"markdown\")\n\n# Access content\nprint(result.content)       # Extracted Markdown text\nprint(result.model_name)    # \"nanonets/Nanonets-OCR-s\"\nprint(result.output_format) # \"markdown\"\n</code></pre>"},{"location":"usage/models/nanonets/#performance","title":"Performance","text":"Backend Device Load Time Inference Time PyTorch L4 GPU ~44s ~6.3s VLLM L4 GPU ~194s ~8.4s MLX M1/M2/M3 ~8s ~12s <p>Times measured on single-page document with default settings.</p>"},{"location":"usage/models/nanonets/#comparison-with-other-models","title":"Comparison with Other Models","text":"Model Speed Accuracy Memory Nanonets OCR2 Fast High 6-8 GB Qwen3-VL Medium High 8-16 GB DotsOCR Medium High 6-8 GB"},{"location":"usage/models/nanonets/#use-cases","title":"Use Cases","text":"<ul> <li>Document digitization - Convert scanned documents to editable text</li> <li>Invoice processing - Extract text from invoices and receipts</li> <li>Form processing - Extract text from forms and applications</li> <li>OCR pipelines - High-throughput batch processing</li> </ul>"},{"location":"usage/models/nanonets/#tips","title":"Tips","text":"<ol> <li>Use VLLM for batch processing - Higher throughput for multiple documents</li> <li>Use MLX on Mac - Native performance on Apple Silicon</li> <li>Set output_format - Use <code>\"markdown\"</code> for formatted output, <code>\"text\"</code> for plain text</li> </ol>"},{"location":"usage/models/paddleocr/","title":"PaddleOCR","text":"<p>Fast, lightweight OCR optimized for Asian languages.</p>"},{"location":"usage/models/paddleocr/#overview","title":"Overview","text":"Tasks OCR Backends PaddlePaddle Speed 0.5-1s/page Quality Very Good Languages 80+"},{"location":"usage/models/paddleocr/#why-paddleocr","title":"Why PaddleOCR","text":"<ul> <li>Fastest deep learning OCR</li> <li>Excellent Asian language support - Chinese, Japanese, Korean</li> <li>Lightweight - small models, efficient</li> <li>Production-ready - used at scale</li> </ul>"},{"location":"usage/models/paddleocr/#basic-usage","title":"Basic Usage","text":"<pre><code>from omnidocs.tasks.ocr_extraction import PaddleOCR, PaddleOCRConfig\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\n\nocr = PaddleOCR(\n    config=PaddleOCRConfig(\n        languages=[\"en\"],\n        use_gpu=True,\n    )\n)\n\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n    print(f\"'{block.text}' @ {block.bbox}\")\n</code></pre>"},{"location":"usage/models/paddleocr/#configuration","title":"Configuration","text":"<pre><code>config = PaddleOCRConfig(\n    languages=[\"en\"],     # Language codes\n    use_gpu=True,         # Use GPU if available\n)\n</code></pre>"},{"location":"usage/models/paddleocr/#multi-language","title":"Multi-Language","text":"<pre><code># Chinese + English\nconfig = PaddleOCRConfig(\n    languages=[\"ch\", \"en\"],\n    use_gpu=True,\n)\n\n# Japanese\nconfig = PaddleOCRConfig(\n    languages=[\"japan\"],\n    use_gpu=True,\n)\n</code></pre> <p>Common language codes: <code>ch</code> (Chinese), <code>en</code> (English), <code>japan</code>, <code>korean</code>, <code>arabic</code>, <code>hindi</code>, <code>french</code>, <code>german</code></p>"},{"location":"usage/models/paddleocr/#paddleocr-vs-others","title":"PaddleOCR vs Others","text":"PaddleOCR EasyOCR Tesseract Speed Fastest Medium Fast Asian langs Excellent Good Good Accuracy Very Good Very Good Good GPU Optional Optional No"},{"location":"usage/models/paddleocr/#installation","title":"Installation","text":"<pre><code>pip install paddlepaddle paddleocr\n</code></pre>"},{"location":"usage/models/paddleocr/#when-to-use","title":"When to Use","text":"<p>\u2705 Chinese, Japanese, Korean documents \u2705 Need fast deep learning OCR \u2705 Production scale</p> <p>\u274c 100+ languages needed \u2192 Use Tesseract \u274c Highest accuracy \u2192 Use EasyOCR</p>"},{"location":"usage/models/qwen/","title":"Qwen","text":"<p>Vision-language model for text extraction and layout analysis.</p>"},{"location":"usage/models/qwen/#overview","title":"Overview","text":"Tasks Text Extraction, Layout Analysis Backends PyTorch, VLLM, MLX, API Speed 2-3s/page Quality Excellent VRAM 8-16GB (8B model)"},{"location":"usage/models/qwen/#text-extraction","title":"Text Extraction","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n\ndoc = Document.from_pdf(\"document.pdf\")\n\nextractor = QwenTextExtractor(\n    backend=QwenPyTorchConfig(device=\"cuda\")\n)\n\nresult = extractor.extract(doc.get_page(0), output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"usage/models/qwen/#layout-analysis","title":"Layout Analysis","text":"<pre><code>from omnidocs.tasks.layout_analysis import QwenLayoutDetector\nfrom omnidocs.tasks.layout_analysis.qwen import QwenLayoutPyTorchConfig\n\ndetector = QwenLayoutDetector(\n    backend=QwenLayoutPyTorchConfig(device=\"cuda\")\n)\n\nresult = detector.extract(image)\nfor elem in result.elements:\n    print(f\"{elem.label}: {elem.bbox}\")\n</code></pre>"},{"location":"usage/models/qwen/#custom-labels","title":"Custom Labels","text":"<pre><code>from omnidocs.tasks.layout_analysis import CustomLabel\n\ncustom_labels = [\n    CustomLabel(name=\"code_block\", description=\"Code snippets\"),\n    CustomLabel(name=\"sidebar\", description=\"Sidebar content\"),\n]\n\nresult = detector.extract(image, custom_labels=custom_labels)\n</code></pre>"},{"location":"usage/models/qwen/#backend-configs","title":"Backend Configs","text":""},{"location":"usage/models/qwen/#pytorch-local-gpu","title":"PyTorch (Local GPU)","text":"<pre><code>from omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n\nconfig = QwenPyTorchConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    device=\"cuda\",              # \"cuda\", \"cpu\", \"mps\"\n    torch_dtype=\"bfloat16\",\n)\n</code></pre>"},{"location":"usage/models/qwen/#vllm-high-throughput","title":"VLLM (High Throughput)","text":"<pre><code>from omnidocs.tasks.text_extraction.qwen import QwenVLLMConfig\n\nconfig = QwenVLLMConfig(\n    model=\"Qwen/Qwen3-VL-8B-Instruct\",\n    tensor_parallel_size=1,     # GPUs to use\n    gpu_memory_utilization=0.9,\n)\n</code></pre>"},{"location":"usage/models/qwen/#mlx-apple-silicon","title":"MLX (Apple Silicon)","text":"<pre><code>from omnidocs.tasks.text_extraction.qwen import QwenMLXConfig\n\nconfig = QwenMLXConfig(\n    model=\"Qwen/Qwen3-VL-2B-Instruct\",\n    quantization=\"4bit\",\n)\n</code></pre>"},{"location":"usage/models/qwen/#api-cloud","title":"API (Cloud)","text":"<pre><code>from omnidocs.tasks.text_extraction.qwen import QwenAPIConfig\n\nconfig = QwenAPIConfig(\n    model=\"qwen3-vl-8b\",\n    api_key=\"YOUR_API_KEY\",\n    base_url=\"https://api.provider.com/v1\",\n)\n</code></pre>"},{"location":"usage/models/qwen/#model-variants","title":"Model Variants","text":"Model Parameters VRAM Quality Speed <code>Qwen/Qwen3-VL-2B-Instruct</code> 2B 4GB Good Fast <code>Qwen/Qwen3-VL-8B-Instruct</code> 8B 16GB Excellent Medium <code>Qwen/Qwen3-VL-32B-Instruct</code> 32B 64GB Outstanding Slow <p>Recommendation: Start with 8B for best quality/speed balance.</p>"},{"location":"usage/models/qwen/#troubleshooting","title":"Troubleshooting","text":"<p>CUDA out of memory <pre><code># Use smaller model\nconfig = QwenPyTorchConfig(model=\"Qwen/Qwen3-VL-2B-Instruct\")\n</code></pre></p> <p>Slow inference <pre><code># Use VLLM backend\nconfig = QwenVLLMConfig(tensor_parallel_size=1)\n</code></pre></p> <p>No GPU <pre><code># Use API backend\nconfig = QwenAPIConfig(api_key=\"...\", base_url=\"...\")\n</code></pre></p>"},{"location":"usage/models/rtdetr/","title":"RT-DETR","text":"<p>High-accuracy document layout detection.</p>"},{"location":"usage/models/rtdetr/#overview","title":"Overview","text":"Tasks Layout Analysis Backends PyTorch Speed 0.3-0.5s/page Quality Excellent VRAM 4-6GB"},{"location":"usage/models/rtdetr/#why-rt-detr","title":"Why RT-DETR","text":"<ul> <li>Higher accuracy than YOLO-based detectors</li> <li>Better on small elements - catches details YOLO misses</li> <li>Transformer architecture - modern, effective</li> <li>Same labels as DocLayoutYOLO - drop-in replacement</li> </ul>"},{"location":"usage/models/rtdetr/#basic-usage","title":"Basic Usage","text":"<pre><code>from omnidocs.tasks.layout_analysis import RTDETRLayoutDetector, RTDETRConfig\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\n\ndetector = RTDETRLayoutDetector(\n    config=RTDETRConfig(device=\"cuda\")\n)\n\nresult = detector.extract(image)\n\nfor elem in result.elements:\n    print(f\"{elem.label}: {elem.bbox} ({elem.confidence:.2f})\")\n</code></pre>"},{"location":"usage/models/rtdetr/#configuration","title":"Configuration","text":"<pre><code>config = RTDETRConfig(\n    device=\"cuda\",        # \"cuda\" or \"cpu\"\n    confidence=0.3,       # Detection threshold\n)\n</code></pre>"},{"location":"usage/models/rtdetr/#rt-detr-vs-doclayoutyolo","title":"RT-DETR vs DocLayoutYOLO","text":"RT-DETR DocLayoutYOLO Speed 0.3-0.5s/page 0.1-0.2s/page Accuracy Higher Good Small elements Better May miss Memory 4-6GB 2-4GB Use case Accuracy-critical Speed-critical"},{"location":"usage/models/rtdetr/#when-to-use","title":"When to Use","text":"<p>\u2705 Need highest accuracy \u2705 Documents with small elements \u2705 Quality over speed</p> <p>\u274c Speed-critical \u2192 Use DocLayoutYOLO \u274c Custom labels \u2192 Use Qwen Layout</p>"},{"location":"usage/models/tableformer/","title":"TableFormer","text":"<p>TableFormer is a transformer-based model for extracting table structure from document images. It detects cells, rows, columns, and spans to produce structured table data.</p>"},{"location":"usage/models/tableformer/#overview","title":"Overview","text":"Property Value Model <code>ds4sd/docling-models</code> Task Table Extraction Backends PyTorch (CPU/GPU) License MIT"},{"location":"usage/models/tableformer/#installation","title":"Installation","text":"<pre><code>pip install omnidocs[pytorch]\n\n# TableFormer uses docling-models, installed automatically\n</code></pre>"},{"location":"usage/models/tableformer/#quick-start","title":"Quick Start","text":"<pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Initialize extractor\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(device=\"cuda\")\n)\n\n# Extract table structure\nresult = extractor.extract(table_image)\n\n# Get HTML output\nhtml = result.to_html()\n\n# Get Pandas DataFrame\ndf = result.to_dataframe()\n\n# Get Markdown\nmd = result.to_markdown()\n</code></pre>"},{"location":"usage/models/tableformer/#configuration","title":"Configuration","text":"<pre><code>from omnidocs.tasks.table_extraction import TableFormerConfig, TableFormerMode\n\nconfig = TableFormerConfig(\n    mode=TableFormerMode.FAST,        # \"fast\" or \"accurate\"\n    device=\"cuda\",                     # \"cpu\", \"cuda\", \"mps\", or \"auto\"\n    num_threads=4,                     # CPU threads\n    do_cell_matching=True,             # Match cells with OCR text\n    correct_overlapping_cells=False,   # Fix overlapping predictions\n    sort_row_col_indexes=True,         # Sort by row/column\n)\n</code></pre>"},{"location":"usage/models/tableformer/#mode-options","title":"Mode Options","text":"Mode Description Speed Accuracy <code>fast</code> Faster inference ~0.3s Good <code>accurate</code> Higher accuracy ~1.0s Better"},{"location":"usage/models/tableformer/#output","title":"Output","text":""},{"location":"usage/models/tableformer/#tableoutput","title":"TableOutput","text":"<pre><code>result = extractor.extract(table_image)\n\n# Properties\nresult.cells          # List of TableCell objects\nresult.num_rows       # Number of rows\nresult.num_cols       # Number of columns\nresult.image_width    # Source image width\nresult.image_height   # Source image height\n\n# Export methods\nhtml = result.to_html()        # HTML table\nmd = result.to_markdown()      # Markdown table\ndf = result.to_dataframe()     # Pandas DataFrame\n</code></pre>"},{"location":"usage/models/tableformer/#tablecell","title":"TableCell","text":"<p>Each cell contains:</p> <pre><code>for cell in result.cells:\n    print(cell.row)         # Row index (0-based)\n    print(cell.col)         # Column index (0-based)\n    print(cell.row_span)    # Number of rows spanned\n    print(cell.col_span)    # Number of columns spanned\n    print(cell.text)        # Cell text content\n    print(cell.cell_type)   # CellType.HEADER or CellType.DATA\n    print(cell.bbox)        # BoundingBox(x1, y1, x2, y2)\n    print(cell.confidence)  # Detection confidence\n</code></pre>"},{"location":"usage/models/tableformer/#example-complete-table-processing","title":"Example: Complete Table Processing","text":"<pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom PIL import Image\n\n# 1. Detect tables in document\nlayout_extractor = DocLayoutYOLO(config=DocLayoutYOLOConfig())\nlayout = layout_extractor.extract(document_image)\n\n# 2. Extract structure from each table\ntable_extractor = TableFormerExtractor(\n    config=TableFormerConfig(mode=\"accurate\", device=\"cuda\")\n)\n\nfor box in layout.bboxes:\n    if box.label.value == \"table\":\n        # Crop table region\n        table_crop = document_image.crop((\n            box.bbox.x1, box.bbox.y1,\n            box.bbox.x2, box.bbox.y2\n        ))\n\n        # Extract structure\n        result = table_extractor.extract(table_crop)\n\n        # Export to DataFrame\n        df = result.to_dataframe()\n        print(df)\n</code></pre>"},{"location":"usage/models/tableformer/#performance","title":"Performance","text":"Mode Device Load Time Inference Time fast CPU ~0.5s ~0.3s fast GPU ~8s ~0.2s accurate CPU ~0.5s ~0.9s accurate GPU ~8s ~0.5s <p>Times measured on typical table with ~50 cells.</p>"},{"location":"usage/models/tableformer/#tips","title":"Tips","text":"<ol> <li>Crop tables first - TableFormer works best on cropped table images</li> <li>Use layout detection - Combine with DocLayoutYOLO to find tables</li> <li>Enable cell matching - Set <code>do_cell_matching=True</code> for better text extraction</li> <li>Choose mode wisely - Use <code>fast</code> for simple tables, <code>accurate</code> for complex ones</li> </ol>"},{"location":"usage/models/tableformer/#limitations","title":"Limitations","text":"<ul> <li>Requires pre-cropped table images (use layout detection first)</li> <li>Complex nested tables may have reduced accuracy</li> <li>Very small or low-resolution tables may be harder to process</li> </ul>"},{"location":"usage/models/tesseract/","title":"Tesseract","text":"<p>Traditional OCR engine with 100+ language support.</p>"},{"location":"usage/models/tesseract/#overview","title":"Overview","text":"Tasks OCR Backends CPU only Speed 0.5-1s/page Quality Good Memory Minimal"},{"location":"usage/models/tesseract/#why-tesseract","title":"Why Tesseract","text":"<ul> <li>No GPU required - Runs on any machine</li> <li>100+ languages - Best multilingual support</li> <li>Free &amp; open source - Apache 2.0 license</li> <li>Battle-tested - Decades of production use</li> <li>Lightweight - Minimal dependencies</li> </ul>"},{"location":"usage/models/tesseract/#basic-usage","title":"Basic Usage","text":"<pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractConfig\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\n\nocr = TesseractOCR(\n    config=TesseractConfig(languages=[\"eng\"])\n)\n\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n    print(f\"'{block.text}' @ {block.bbox}\")\n</code></pre>"},{"location":"usage/models/tesseract/#configuration","title":"Configuration","text":"<pre><code>config = TesseractConfig(\n    languages=[\"eng\"],           # Language codes\n    config=\"--psm 3\",            # Page segmentation mode\n)\n</code></pre>"},{"location":"usage/models/tesseract/#page-segmentation-modes-psm","title":"Page Segmentation Modes (PSM)","text":"Mode Description <code>--psm 0</code> Orientation and script detection only <code>--psm 1</code> Automatic with OSD <code>--psm 3</code> Fully automatic (default) <code>--psm 6</code> Assume uniform block of text <code>--psm 11</code> Sparse text, no order <code>--psm 13</code> Raw line, single text line"},{"location":"usage/models/tesseract/#multi-language-support","title":"Multi-Language Support","text":"<pre><code># Single language\nconfig = TesseractConfig(languages=[\"eng\"])\n\n# Multiple languages\nconfig = TesseractConfig(languages=[\"eng\", \"fra\", \"deu\"])\n\n# All available languages\nconfig = TesseractConfig(languages=[\"eng\", \"chi_sim\", \"jpn\", \"ara\"])\n</code></pre>"},{"location":"usage/models/tesseract/#common-language-codes","title":"Common Language Codes","text":"Code Language <code>eng</code> English <code>chi_sim</code> Chinese (Simplified) <code>chi_tra</code> Chinese (Traditional) <code>jpn</code> Japanese <code>kor</code> Korean <code>ara</code> Arabic <code>hin</code> Hindi <code>fra</code> French <code>deu</code> German <code>spa</code> Spanish <code>por</code> Portuguese <code>rus</code> Russian <p>Full list: Tesseract Languages</p>"},{"location":"usage/models/tesseract/#filtering-results","title":"Filtering Results","text":"<pre><code># By confidence\nconfident = [b for b in result.text_blocks if b.confidence &gt;= 0.9]\n\n# By text length\nwords = [b for b in result.text_blocks if len(b.text) &gt;= 2]\n\n# By region\ntop_half = [b for b in result.text_blocks if b.bbox.y1 &lt; image.height / 2]\n</code></pre>"},{"location":"usage/models/tesseract/#installation","title":"Installation","text":"<p>Tesseract must be installed on your system:</p> <pre><code># macOS\nbrew install tesseract\n\n# Ubuntu/Debian\nsudo apt install tesseract-ocr\n\n# Install additional languages\nsudo apt install tesseract-ocr-chi-sim  # Chinese\nsudo apt install tesseract-ocr-jpn      # Japanese\n</code></pre>"},{"location":"usage/models/tesseract/#troubleshooting","title":"Troubleshooting","text":"<p>\"tesseract not found\" <pre><code># Install Tesseract system package\nbrew install tesseract  # macOS\nsudo apt install tesseract-ocr  # Linux\n</code></pre></p> <p>Low accuracy - Increase image resolution (300 DPI recommended) - Improve image contrast - Use single language mode - Try different PSM mode</p> <p>Missing language <pre><code># Install language data\nsudo apt install tesseract-ocr-[lang]\n</code></pre></p>"},{"location":"usage/tasks/","title":"Tasks","text":"<p>Tasks define what you want to extract. Models define how.</p>"},{"location":"usage/tasks/#available-tasks","title":"Available Tasks","text":"Task Input Output Status Text Extraction Image / PDF Markdown, HTML \u2705 Ready Layout Analysis Image Bounding boxes + labels \u2705 Ready OCR Image Text + coordinates \u2705 Ready Table Extraction Table image Structured table data \u2705 Ready Reading Order Layout + OCR Ordered elements \u2705 Ready"},{"location":"usage/tasks/#choosing-a-task","title":"Choosing a Task","text":"<p>\"I want readable text from a PDF\" \u2192 Text Extraction</p> <p>\"I need to know where tables and figures are\" \u2192 Layout Analysis</p> <p>\"I need word positions for downstream processing\" \u2192 OCR</p> <p>\"I want structured data from a table\" \u2192 Table Extraction</p> <p>\"I need elements in reading order\" \u2192 Reading Order</p>"},{"location":"usage/tasks/#upcoming-tasks","title":"Upcoming Tasks","text":"Task Description Status Math Recognition LaTeX from equations \ud83d\udd1c Soon Chart Understanding Data extraction from charts \ud83d\udd1c Planned Image Captioning Caption figures and images \ud83d\udd1c Planned Structured Output Extract data with custom schemas \ud83d\udd1c Planned <p>See Roadmap for full tracking.</p>"},{"location":"usage/tasks/layout-analysis/","title":"Layout Analysis","text":"<p>Detect document structure and element boundaries.</p>"},{"location":"usage/tasks/layout-analysis/#input-output","title":"Input / Output","text":"<p>Input: Document image</p> <p>Output: List of bounding boxes with labels and confidence scores</p> <pre><code>result = detector.extract(image)\nfor elem in result.elements:\n    print(f\"{elem.label}: {elem.bbox} ({elem.confidence:.2f})\")\n</code></pre> <pre><code>title: [50, 20, 500, 60] (0.98)\ntext: [50, 80, 900, 300] (0.95)\ntable: [50, 320, 900, 600] (0.92)\nfigure: [50, 620, 400, 900] (0.89)\n</code></pre>"},{"location":"usage/tasks/layout-analysis/#quick-start","title":"Quick Start","text":"<pre><code>from omnidocs.tasks.layout_analysis import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\n\ndetector = DocLayoutYOLO(\n    config=DocLayoutYOLOConfig(device=\"cuda\")\n)\n\nresult = detector.extract(image)\n\nfor elem in result.elements:\n    print(f\"{elem.label}: {elem.bbox}\")\n</code></pre>"},{"location":"usage/tasks/layout-analysis/#available-models","title":"Available Models","text":"Model Speed Labels Best For DocLayoutYOLO 0.1-0.2s/page Fixed (11) Speed RT-DETR 0.3-0.5s/page Fixed (11) Accuracy Qwen Layout 2-3s/page Custom Flexibility"},{"location":"usage/tasks/layout-analysis/#fixed-labels","title":"Fixed Labels","text":"<p>Models like DocLayoutYOLO and RT-DETR detect these predefined labels:</p> Label Description <code>title</code> Document/section headings <code>text</code> Body paragraphs <code>list</code> Bullet or numbered lists <code>table</code> Data tables <code>figure</code> Images, diagrams, charts <code>caption</code> Figure/table captions <code>formula</code> Math equations <code>footnote</code> Footnotes <code>page_header</code> Running headers <code>page_footer</code> Running footers"},{"location":"usage/tasks/layout-analysis/#custom-labels-qwen-layout","title":"Custom Labels (Qwen Layout)","text":"<p>Qwen Layout can detect any custom elements you define.</p>"},{"location":"usage/tasks/layout-analysis/#simple-string-labels","title":"Simple String Labels","text":"<pre><code>from omnidocs.tasks.layout_analysis import QwenLayoutDetector\nfrom omnidocs.tasks.layout_analysis.qwen import QwenLayoutPyTorchConfig\n\ndetector = QwenLayoutDetector(\n    backend=QwenLayoutPyTorchConfig(device=\"cuda\")\n)\n\n# Detect custom elements\nresult = detector.extract(\n    image,\n    custom_labels=[\"code_block\", \"sidebar\", \"pull_quote\", \"diagram\"]\n)\n\nfor elem in result.elements:\n    print(f\"{elem.label}: {elem.bbox}\")\n</code></pre>"},{"location":"usage/tasks/layout-analysis/#structured-labels-with-metadata","title":"Structured Labels with Metadata","text":"<p>For advanced use cases, use <code>CustomLabel</code> with descriptions:</p> <pre><code>from omnidocs.tasks.layout_analysis import QwenLayoutDetector, CustomLabel\nfrom omnidocs.tasks.layout_analysis.qwen import QwenLayoutPyTorchConfig\n\ndetector = QwenLayoutDetector(\n    backend=QwenLayoutPyTorchConfig(device=\"cuda\")\n)\n\n# Structured labels with metadata\nresult = detector.extract(\n    image,\n    custom_labels=[\n        CustomLabel(\n            name=\"code_block\",\n            description=\"Programming source code areas\",\n            detection_prompt=\"Regions with monospace text and syntax highlighting\",\n            color=\"#2ecc71\",\n        ),\n        CustomLabel(\n            name=\"sidebar\",\n            description=\"Sidebar or callout content\",\n            detection_prompt=\"Boxed regions with supplementary information\",\n            color=\"#3498db\",\n        ),\n        CustomLabel(\n            name=\"warning_box\",\n            description=\"Warning or alert boxes\",\n            detection_prompt=\"Highlighted boxes with warning icons or red/yellow colors\",\n            color=\"#e74c3c\",\n        ),\n    ]\n)\n\nfor elem in result.elements:\n    print(f\"{elem.label}: {elem.bbox}\")\n</code></pre>"},{"location":"usage/tasks/layout-analysis/#reusable-label-sets","title":"Reusable Label Sets","text":"<p>Create reusable label collections for your domain:</p> <pre><code>from omnidocs.tasks.layout_analysis import CustomLabel\n\nclass TechnicalDocLabels:\n    \"\"\"Labels for technical documentation.\"\"\"\n\n    CODE_BLOCK = CustomLabel(\n        name=\"code_block\",\n        description=\"Source code listings\",\n        color=\"#2ecc71\"\n    )\n\n    API_REFERENCE = CustomLabel(\n        name=\"api_reference\",\n        description=\"API documentation tables\",\n        color=\"#3498db\"\n    )\n\n    DIAGRAM = CustomLabel(\n        name=\"diagram\",\n        description=\"Architecture diagrams\",\n        color=\"#9b59b6\"\n    )\n\n    @classmethod\n    def all(cls):\n        return [cls.CODE_BLOCK, cls.API_REFERENCE, cls.DIAGRAM]\n\n# Use across projects\nresult = detector.extract(image, custom_labels=TechnicalDocLabels.all())\n</code></pre>"},{"location":"usage/tasks/layout-analysis/#fixed-vs-custom-labels","title":"Fixed vs Custom Labels","text":"Feature Fixed (YOLO, RT-DETR) Custom (Qwen) Speed 0.1-0.5s/page 2-3s/page Labels 11 predefined Unlimited custom Accuracy High on standard docs Good on any doc Use case Standard documents Domain-specific <p>Choose Fixed Labels when: - Processing standard documents - Speed is critical - Standard elements are sufficient</p> <p>Choose Custom Labels when: - Need domain-specific elements (code, sidebars, etc.) - Processing non-standard documents - Flexibility is more important than speed</p>"},{"location":"usage/tasks/layout-analysis/#filtering-results","title":"Filtering Results","text":"<pre><code># By label\ntables = [e for e in result.elements if e.label == \"table\"]\nfigures = [e for e in result.elements if e.label == \"figure\"]\n\n# By confidence\nconfident = [e for e in result.elements if e.confidence &gt;= 0.8]\n\n# Exclude headers/footers\ncontent = [e for e in result.elements\n           if e.label not in [\"page_header\", \"page_footer\"]]\n</code></pre>"},{"location":"usage/tasks/layout-analysis/#when-to-use","title":"When to Use","text":"<p>\u2705 Document structure analysis \u2705 Finding tables and figures \u2705 Building multi-stage pipelines \u2705 Filtering unwanted elements \u2705 Domain-specific element detection (custom labels)</p> <p>\u274c Need readable text \u2192 Use Text Extraction \u274c Need word positions \u2192 Use OCR</p>"},{"location":"usage/tasks/layout-analysis/#upcoming-models","title":"Upcoming Models","text":"Model Description Status SuryaLayout Modern layout detection \ud83d\udd1c Soon VLMLayout API-based layout (GPT-4V, Gemini) \ud83d\udd1c Planned"},{"location":"usage/tasks/ocr/","title":"OCR","text":"<p>Extract text with precise bounding boxes.</p>"},{"location":"usage/tasks/ocr/#input-output","title":"Input / Output","text":"<p>Input: Document image</p> <p>Output: Text blocks with coordinates and confidence scores</p> <pre><code>result = ocr.extract(image)\nfor block in result.text_blocks:\n    print(f\"'{block.text}' @ {block.bbox} ({block.confidence:.2f})\")\n</code></pre> <pre><code>'Invoice' @ BoundingBox(x1=100, y1=50, x2=200, y2=80) (0.98)\n'Date: 2024-01-15' @ BoundingBox(x1=100, y1=100, x2=280, y2=125) (0.96)\n'Total: $1,234.56' @ BoundingBox(x1=100, y1=400, x2=300, y2=430) (0.97)\n</code></pre>"},{"location":"usage/tasks/ocr/#quick-start","title":"Quick Start","text":"<pre><code>from omnidocs.tasks.ocr_extraction import TesseractOCR, TesseractConfig\nfrom PIL import Image\n\nimage = Image.open(\"document.png\")\n\nocr = TesseractOCR(\n    config=TesseractConfig(languages=[\"eng\"])\n)\n\nresult = ocr.extract(image)\n\nfor block in result.text_blocks:\n    print(f\"'{block.text}' @ {block.bbox}\")\n</code></pre>"},{"location":"usage/tasks/ocr/#available-models","title":"Available Models","text":"Model Speed GPU Languages Best For Tesseract Fast No 100+ General, multilingual EasyOCR Medium Optional 80+ Higher accuracy PaddleOCR Fast Optional 80+ Asian languages"},{"location":"usage/tasks/ocr/#when-to-use","title":"When to Use","text":"<p>\u2705 Need word/character coordinates \u2705 Building search indexes with positions \u2705 Form field extraction \u2705 Text location for downstream processing</p> <p>\u274c Just need readable text \u2192 Use Text Extraction \u274c Just need structure \u2192 Use Layout Analysis</p>"},{"location":"usage/tasks/ocr/#upcoming-models","title":"Upcoming Models","text":"Model Description Status SuryaOCR Modern multilingual OCR \ud83d\udd1c Soon QwenOCR VLM-based OCR \ud83d\udd1c Soon"},{"location":"usage/tasks/reading-order/","title":"Reading Order","text":"<p>Determine the logical reading sequence of document elements. Essential for correct text flow in multi-column layouts, documents with figures, and complex page structures.</p>"},{"location":"usage/tasks/reading-order/#overview","title":"Overview","text":"<p>Reading order prediction takes layout detection and OCR results and produces:</p> <ul> <li>Ordered elements - Elements sorted in reading sequence</li> <li>Caption associations - Links between figures/tables and their captions</li> <li>Footnote mapping - Links between content and footnotes</li> <li>Merge suggestions - Elements that should be combined (split paragraphs)</li> </ul>"},{"location":"usage/tasks/reading-order/#quick-start","title":"Quick Start","text":"<pre><code>from omnidocs.tasks.reading_order import RuleBasedReadingOrderPredictor\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\n\n# Initialize components\nlayout_extractor = DocLayoutYOLO(config=DocLayoutYOLOConfig())\nocr = EasyOCR(config=EasyOCRConfig())\npredictor = RuleBasedReadingOrderPredictor()\n\n# Process document\nlayout = layout_extractor.extract(image)\nocr_result = ocr.extract(image)\nreading_order = predictor.predict(layout, ocr_result)\n\n# Get text in reading order\ntext = reading_order.get_full_text()\nprint(text)\n</code></pre>"},{"location":"usage/tasks/reading-order/#available-models","title":"Available Models","text":"Model Speed Use Case Rule-based (R-tree) &lt;0.1s Multi-column, general documents"},{"location":"usage/tasks/reading-order/#output-format","title":"Output Format","text":""},{"location":"usage/tasks/reading-order/#readingorderoutput","title":"ReadingOrderOutput","text":"<pre><code>result = predictor.predict(layout, ocr_result)\n\n# Ordered elements\nfor elem in result.ordered_elements:\n    print(f\"{elem.index}: {elem.element_type.value} - {elem.text[:50]}\")\n\n# Caption associations (figure_id -&gt; [caption_ids])\nfor fig_id, caption_ids in result.caption_map.items():\n    print(f\"Figure {fig_id} has captions: {caption_ids}\")\n\n# Footnote associations\nfor elem_id, footnote_ids in result.footnote_map.items():\n    print(f\"Element {elem_id} has footnotes: {footnote_ids}\")\n\n# Merge suggestions (for split paragraphs)\nfor elem_id, merge_ids in result.merge_map.items():\n    print(f\"Element {elem_id} should merge with: {merge_ids}\")\n</code></pre>"},{"location":"usage/tasks/reading-order/#orderedelement","title":"OrderedElement","text":"<pre><code>elem.index         # Position in reading order\nelem.element_type  # ElementType (TITLE, TEXT, FIGURE, TABLE, etc.)\nelem.bbox          # BoundingBox(x1, y1, x2, y2)\nelem.text          # Text content (from OCR)\nelem.confidence    # Detection confidence\nelem.page_no       # Page number\nelem.original_id   # ID from original layout detection\n</code></pre>"},{"location":"usage/tasks/reading-order/#element-types","title":"Element Types","text":"<pre><code>from omnidocs.tasks.reading_order import ElementType\n\nElementType.TITLE\nElementType.TEXT\nElementType.LIST\nElementType.FIGURE\nElementType.TABLE\nElementType.CAPTION\nElementType.FORMULA\nElementType.FOOTNOTE\nElementType.PAGE_HEADER\nElementType.PAGE_FOOTER\nElementType.CODE\nElementType.OTHER\n</code></pre>"},{"location":"usage/tasks/reading-order/#helper-methods","title":"Helper Methods","text":""},{"location":"usage/tasks/reading-order/#get-full-text","title":"Get Full Text","text":"<pre><code># Get all text in reading order\ntext = result.get_full_text()\n</code></pre>"},{"location":"usage/tasks/reading-order/#get-elements-by-type","title":"Get Elements by Type","text":"<pre><code># Get all tables\ntables = result.get_elements_by_type(ElementType.TABLE)\n\n# Get all figures\nfigures = result.get_elements_by_type(ElementType.FIGURE)\n</code></pre>"},{"location":"usage/tasks/reading-order/#get-captions","title":"Get Captions","text":"<pre><code># Get captions for a specific figure\nfor elem in result.ordered_elements:\n    if elem.element_type == ElementType.FIGURE:\n        captions = result.get_captions_for(elem.original_id)\n        print(f\"Figure captions: {[c.text for c in captions]}\")\n</code></pre>"},{"location":"usage/tasks/reading-order/#pipeline-complete-document-processing","title":"Pipeline: Complete Document Processing","text":"<pre><code>from omnidocs.tasks.reading_order import RuleBasedReadingOrderPredictor\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom omnidocs.tasks.ocr_extraction import EasyOCR, EasyOCRConfig\nfrom PIL import Image\n\n# Load document\nimage = Image.open(\"document.png\")\n\n# 1. Layout detection\nlayout_extractor = DocLayoutYOLO(config=DocLayoutYOLOConfig(device=\"cuda\"))\nlayout = layout_extractor.extract(image)\nprint(f\"Found {len(layout.bboxes)} elements\")\n\n# 2. OCR extraction\nocr = EasyOCR(config=EasyOCRConfig(gpu=True))\nocr_result = ocr.extract(image)\nprint(f\"Found {len(ocr_result.text_blocks)} text blocks\")\n\n# 3. Reading order prediction\npredictor = RuleBasedReadingOrderPredictor()\nreading_order = predictor.predict(layout, ocr_result)\n\n# 4. Process in reading order\nfor elem in reading_order.ordered_elements:\n    if elem.element_type == ElementType.TITLE:\n        print(f\"# {elem.text}\")\n    elif elem.element_type == ElementType.TEXT:\n        print(f\"{elem.text}\\n\")\n    elif elem.element_type == ElementType.TABLE:\n        print(f\"[Table at position {elem.index}]\")\n    elif elem.element_type == ElementType.FIGURE:\n        captions = reading_order.get_captions_for(elem.original_id)\n        print(f\"[Figure: {captions[0].text if captions else 'No caption'}]\")\n</code></pre>"},{"location":"usage/tasks/reading-order/#how-it-works","title":"How It Works","text":"<p>The rule-based predictor uses:</p> <ol> <li>R-tree spatial indexing - Efficient spatial queries</li> <li>Column detection - Identifies multi-column layouts</li> <li>Vertical flow - Elements flow top-to-bottom within columns</li> <li>Header/footer separation - Processes these separately</li> <li>Caption proximity - Associates captions with nearby figures/tables</li> </ol>"},{"location":"usage/tasks/reading-order/#tips","title":"Tips","text":"<ol> <li>Use quality layout detection - Reading order depends on accurate layout</li> <li>Include OCR - Text content enables better merge detection</li> <li>Check caption associations - Verify figures have correct captions</li> <li>Handle page headers/footers - These are processed separately</li> </ol>"},{"location":"usage/tasks/reading-order/#limitations","title":"Limitations","text":"<ul> <li>Works best with standard document layouts</li> <li>Very complex layouts (nested columns) may need tuning</li> <li>Depends on quality of layout detection input</li> <li>Single-page processing (process pages independently for multi-page docs)</li> </ul>"},{"location":"usage/tasks/table-extraction/","title":"Table Extraction","text":"<p>Extract structured data from tables in document images. Returns cells with row/column positions, spans, and text content.</p>"},{"location":"usage/tasks/table-extraction/#overview","title":"Overview","text":"<p>Table extraction converts table images into structured data with:</p> <ul> <li>Cell detection - Location of each cell</li> <li>Structure recognition - Row/column positions and spans</li> <li>Text extraction - Content of each cell</li> <li>Export formats - HTML, Markdown, Pandas DataFrame</li> </ul>"},{"location":"usage/tasks/table-extraction/#quick-start","title":"Quick Start","text":"<pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\n\n# Initialize\nextractor = TableFormerExtractor(\n    config=TableFormerConfig(device=\"cuda\")\n)\n\n# Extract\nresult = extractor.extract(table_image)\n\n# Export\nhtml = result.to_html()\ndf = result.to_dataframe()\nmd = result.to_markdown()\n</code></pre>"},{"location":"usage/tasks/table-extraction/#available-models","title":"Available Models","text":"Model Speed Use Case TableFormer 0.5-1s General tables, complex structures"},{"location":"usage/tasks/table-extraction/#output-format","title":"Output Format","text":""},{"location":"usage/tasks/table-extraction/#tableoutput","title":"TableOutput","text":"<pre><code>result = extractor.extract(table_image)\n\n# Metadata\nresult.num_rows       # Number of rows\nresult.num_cols       # Number of columns\nresult.image_width    # Source image width\nresult.image_height   # Source image height\nresult.model_name     # Model used\n\n# Cells\nfor cell in result.cells:\n    print(f\"[{cell.row},{cell.col}] {cell.text}\")\n</code></pre>"},{"location":"usage/tasks/table-extraction/#tablecell","title":"TableCell","text":"<pre><code>cell.row          # Row index (0-based)\ncell.col          # Column index (0-based)\ncell.row_span     # Rows spanned (default: 1)\ncell.col_span     # Columns spanned (default: 1)\ncell.text         # Cell content\ncell.cell_type    # CellType.HEADER or CellType.DATA\ncell.bbox         # BoundingBox(x1, y1, x2, y2)\ncell.confidence   # Detection confidence\n</code></pre>"},{"location":"usage/tasks/table-extraction/#export-formats","title":"Export Formats","text":""},{"location":"usage/tasks/table-extraction/#html","title":"HTML","text":"<pre><code>html = result.to_html()\n# &lt;table&gt;&lt;tr&gt;&lt;th&gt;Name&lt;/th&gt;&lt;th&gt;Value&lt;/th&gt;&lt;/tr&gt;...&lt;/table&gt;\n</code></pre>"},{"location":"usage/tasks/table-extraction/#markdown","title":"Markdown","text":"<pre><code>md = result.to_markdown()\n# | Name | Value |\n# |------|-------|\n# | A    | 100   |\n</code></pre>"},{"location":"usage/tasks/table-extraction/#pandas-dataframe","title":"Pandas DataFrame","text":"<pre><code>df = result.to_dataframe()\n#    Name  Value\n# 0  A     100\n# 1  B     200\n</code></pre>"},{"location":"usage/tasks/table-extraction/#pipeline-document-tables","title":"Pipeline: Document \u2192 Tables","text":"<p>Typically you'll combine table extraction with layout detection:</p> <pre><code>from omnidocs.tasks.table_extraction import TableFormerExtractor, TableFormerConfig\nfrom omnidocs.tasks.layout_extraction import DocLayoutYOLO, DocLayoutYOLOConfig\nfrom PIL import Image\n\n# Load document\ndocument = Image.open(\"document.png\")\n\n# 1. Detect layout elements\nlayout_extractor = DocLayoutYOLO(config=DocLayoutYOLOConfig())\nlayout = layout_extractor.extract(document)\n\n# 2. Extract each table\ntable_extractor = TableFormerExtractor(\n    config=TableFormerConfig(mode=\"accurate\")\n)\n\ntables = []\nfor box in layout.bboxes:\n    if box.label.value == \"table\":\n        # Crop table region\n        table_crop = document.crop((\n            int(box.bbox.x1), int(box.bbox.y1),\n            int(box.bbox.x2), int(box.bbox.y2)\n        ))\n\n        # Extract structure\n        result = table_extractor.extract(table_crop)\n        tables.append(result.to_dataframe())\n\nprint(f\"Found {len(tables)} tables\")\n</code></pre>"},{"location":"usage/tasks/table-extraction/#configuration-options","title":"Configuration Options","text":"<pre><code>from omnidocs.tasks.table_extraction import TableFormerConfig, TableFormerMode\n\nconfig = TableFormerConfig(\n    # Mode: \"fast\" for speed, \"accurate\" for quality\n    mode=TableFormerMode.ACCURATE,\n\n    # Device: \"cpu\", \"cuda\", \"mps\", or \"auto\"\n    device=\"cuda\",\n\n    # Cell matching with OCR\n    do_cell_matching=True,\n\n    # Fix overlapping cell predictions\n    correct_overlapping_cells=False,\n\n    # Sort cells by position\n    sort_row_col_indexes=True,\n)\n</code></pre>"},{"location":"usage/tasks/table-extraction/#tips","title":"Tips","text":"<ol> <li>Crop tables first - Extract table regions using layout detection</li> <li>Use accurate mode - For complex tables with merged cells</li> <li>Check spans - Handle row_span and col_span for merged cells</li> <li>Validate output - Check num_rows and num_cols match expectations</li> </ol>"},{"location":"usage/tasks/table-extraction/#limitations","title":"Limitations","text":"<ul> <li>Requires pre-cropped table images</li> <li>Complex nested tables may have reduced accuracy</li> <li>Handwritten tables are not supported</li> <li>Very large tables may need to be split</li> </ul>"},{"location":"usage/tasks/text-extraction/","title":"Text Extraction","text":"<p>Convert document images to readable, formatted text.</p>"},{"location":"usage/tasks/text-extraction/#input-output","title":"Input / Output","text":"<p>Input: Document image (PNG, JPG) or PDF page</p> <p>Output: Formatted text (Markdown or HTML)</p> <pre><code>result = extractor.extract(image, output_format=\"markdown\")\nprint(result.content)\n</code></pre> <pre><code># Document Title\n\nThis is the first paragraph with **bold** and *italic* text.\n\n## Section 1\n\n- Bullet point 1\n- Bullet point 2\n\n| Column A | Column B |\n|----------|----------|\n| Data 1   | Data 2   |\n</code></pre>"},{"location":"usage/tasks/text-extraction/#quick-start","title":"Quick Start","text":"<pre><code>from omnidocs import Document\nfrom omnidocs.tasks.text_extraction import QwenTextExtractor\nfrom omnidocs.tasks.text_extraction.qwen import QwenPyTorchConfig\n\n# Load document\ndoc = Document.from_pdf(\"document.pdf\")\n\n# Initialize extractor\nextractor = QwenTextExtractor(\n    backend=QwenPyTorchConfig(device=\"cuda\")\n)\n\n# Extract\nresult = extractor.extract(doc.get_page(0), output_format=\"markdown\")\nprint(result.content)\n</code></pre>"},{"location":"usage/tasks/text-extraction/#output-formats","title":"Output Formats","text":"<pre><code># Markdown (default)\nresult = extractor.extract(image, output_format=\"markdown\")\n\n# HTML\nresult = extractor.extract(image, output_format=\"html\")\n</code></pre>"},{"location":"usage/tasks/text-extraction/#available-models","title":"Available Models","text":"Model Speed Quality Best For Qwen 2-3s/page Excellent General purpose DotsOCR 3-5s/page Very Good Technical docs, layout-aware Nanonets OCR2 2-4s/page Excellent Document digitization, invoices"},{"location":"usage/tasks/text-extraction/#when-to-use","title":"When to Use","text":"<p>\u2705 Converting PDFs to Markdown \u2705 Extracting article content \u2705 Document parsing for RAG pipelines \u2705 Content migration</p> <p>\u274c Need word coordinates \u2192 Use OCR \u274c Need structure only \u2192 Use Layout Analysis</p>"}]}